## [Vi Mode setup]
set -o vi

## GIT Path
alias cdcm='cd /Users/azhekhan/CloudManager/cm/bse/cloud'
alias cdlift='cd /Users/azhekhan/CloudManager/cm/bse/cloud/instance/administer'
alias lifttest='cd /Users/azhekhan/Python_Projects/LIFT_TEST; cl; echo "cd /Users/azhekhan/Python_Projects/LIFT_TEST"'
alias cdazkh='cd /Users/azhekhan/Documents/AK_Oracle_BKUP_07Mar19/azher_khan'
alias cdbugs='cd /Users/azhekhan/Documents/AK_Oracle_BKUP_07Mar19/azher_khan/Oracle/PeopleSoft/OMCS_Lift_Shift/OMCS_BUGLOGS'
alias lsvm='cat /Users/azhekhan/CloudManager/cm_roughbook.txt'
#alias cl=
#alias cl=

## [ Azher Khan bashrc stuff github ]
# Git branch in prompt.

parse_git_branch() {
  git branch 2> /dev/null | sed -e '/^[^*]/d' -e 's/* \(.*\)/ (\1)/'
}

##export PS1="\u@\h \W\[\033[32m\]\$(parse_git_branch)\[\033[00m\] $ "
export PS1="\h:\W\[\033[32m\]\$(parse_git_branch)\[\033[00m\] $ "

# some more ls aliases
alias ll='ls -ltr'
alias la='ls -A'
alias l='ls -CF'
alias cc='clear'
alias cl='cc ; ls -ltr'
alias vibp='vi /Users/azhekhan/.bash_profile'


## [Azher Khan CloudManager]
alias catcm='echo "cat ~/CloudManager/cm_roughbook.txt"; cat ~/CloudManager/cm_roughbook.txt'
alias catosvc='echo "cat /Users/azhekhan/OSVC_Code/osvc_roughbook.txt"; cat /Users/azhekhan/OSVC_Code/osvc_roughbook.txt'
alias cdosvc='cd /Users/azhekhan/OSVC_Code/osvc-cloud-ms'
alias cdgi='cd /Users/azhekhan/OSVC_Code/osvc-cloud-ms/galorndon-infra'
alias cdgp='cd /Users/azhekhan/OSVC_Code/osvc-cloud-ms/galorndon-platform'
alias cdop='cd /Users/azhekhan/OSVC_Code/osvc-cloud-ms/osvc-platform'
alias cdoi='cd /Users/azhekhan/OSVC_Code/osvc-cloud-ms/osvc-infra'
alias cdcpe='cd /Users/azhekhan/Documents/AK_Oracle_BKUP_07Mar19/azher_khan/Oracle/OSvC'
alias macssh="ssh -q -o proxyCommand='nc -x localhost:1080 %h %p' "
alias pushcmlu='cat "/Users/azhekhan/CloudManager/LIFTUTILITY/gitlucmscp.sh"; /Users/azhekhan/CloudManager/LIFTUTILITY/gitlucmscp.sh'

# [AzherKhan] Below are GIT ALIAS with COMMANDS
alias ghelp='cat cat ~/.bash_profile | grep "git"'
alias gckbr='git checkout -b <branch_name> remotes/origin/<branch_name>'
alias gad='git add'
alias gbr='echo "git branch"; git branch'
alias gup='gck main ; echo "git pull --ff-only"; git pull; pwd'
/cpev2
Last login: Thu Jan 13 04:57:20 on ttys001

The default interactive shell is now zsh.
To update your account to use zsh, please run `chsh -s /bin/zsh`.
For more details, please visit https://support.apple.com/kb/HT208050.

 2022-01-13 04:58:00 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → vi ~/.bash_profile

 2022-01-13 10:23:26 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → dr
127.0.0.1       localhost
::1     localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
172.17.0.4      0937f82e82fb
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
"/etc/hosts" 7L, 174C                                                                                                                       1,1           All
127.0.0.1       localhost
HOME_DIR : /root
Aliasing vim --> vi editor
error: could not lock config file .git/config: No such file or directory
[13:January:2022:04:53:45]:(kind-testk8s):/
○ $ cat /etc/hosts
127.0.0.1 localhost
::1 localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
172.17.0.4  0937f82e82fb
[13:January:2022:04:54:05]:(kind-testk8s):/
○ $ vi /etc/hosts
[13:January:2022:04:54:15]:(kind-testk8s):/
○ $ cp ~/galorndon/ctemp/
Display all 109 possibilities? (y or n)
[13:January:2022:04:54:15]:(kind-testk8s):/
○ $ #cp ~/galorndon/ctemp/
[13:January:2022:04:54:29]:(kind-testk8s):/
○ $ ll ~/galorndon/ctemp/traefik-test
total 60
-r-------- 1 root root  2358 Jan 10 05:02 kubeconfig_context-c5ppxaonnda
-rw-r--r-- 1 root root  1154 Jan 13 00:00 traefik-test_values.yaml
-rw-r--r-- 1 root root  4455 Jan 13 00:06 traefik-6ff7ff8ffd-cxgdn2022_01_12_11_33_PM.log
-rw-r--r-- 1 root root  1006 Jan 13 00:11 traefik.service
-rw-r--r-- 1 root root  7670 Jan 13 00:13 kf-kafka.kafka.yaml
-rw-r--r-- 1 root root 12389 Jan 13 00:15 corp.shared-kafka.kafka.yaml
-rw-r--r-- 1 root root   420 Jan 13 04:44 kf-kafka-kafka-0-ingressroutetcp.yaml
-rw-r--r-- 1 root root   452 Jan 13 04:45 kf-kafka-kafka-bootstrap-ingressroutetcp.yaml
-rw-r--r-- 1 root root   420 Jan 13 04:45 kf-kafka-kafka-1-ingressroutetcp.yaml
-rw-r--r-- 1 root root   420 Jan 13 04:45 kf-kafka-kafka-2-ingressroutetcp.yaml
[13:January:2022:04:55:26]:(kind-testk8s):/
○ $ env | grep -i kube
KUBE_EDITOR=vim
KUBECTL_VERSION=v1.13.5
KUBEVAL_EXEC=/usr/local/bin/kubeval
KUBEVAL_VERSION=0.14.0
[13:January:2022:04:55:32]:(kind-testk8s):/
○ $ export KUBECONFIG=~/galorndon/ctemp/traefik-test/kubeconfig_context-c5ppxaonnda
[13:January:2022:04:55:48]:(kind-testk8s):/
○ $ kgns
error: the server doesn't have a resource type "ns"
[13:January:2022:04:56:08]:(kind-testk8s):/
○ $ export KUBECONFIG=/home/opc/galorndon/ctemp/kubeconfig_context-c5ppxaonnda
[13:January:2022:04:57:13]:(kind-testk8s):/
○ $ kgns
The connection to the server localhost:8080 was refused - did you specify the right host or port?
[13:January:2022:04:57:15]:(kind-testk8s):/
○ $ v2dev
[13:January:2022:04:57:27]:(dev_eu-frankfurt-1_controlplane):/
○ $ oong
FileNotFoundError: [Errno 2] No such file or directory: '/home/opc/.oci/oci_api_key.pem'
[13:January:2022:04:57:32]:(dev_eu-frankfurt-1_controlplane):/
○ $ ls -ltr ~/.oci
total 116
-rw-r--r-- 1 root root  535 Apr 25  2019 config_peoplesoft_cm
[DEFAULT]
#tenancy=ocid1.compartment.oc1..aaaaaaaamfzmd7dxhst3a5hbhtu6mwxnkxzacrxqwrgasecvwq2iqcg7mg3q
tenancy=ocid1.tenancy.oc1..aaaaaaaacbb4jhwb2q6tfx223i5siaiuyw6gpl3zywyosiudeimsodjkolga
region=us-ashburn-1
user=ocid1.user.oc1..aaaaaaaaft5c2fkohgco27vmkudak2i3mk5mubwmo5szyejluyso7flvkdqa
fingerprint=1e:57:b5:1f:39:03:b5:e4:16:38:87:5b:88:e5:da:e0
key_file=/home/opc/.oci/oci_api_key.pem
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
"~/.oci/config" 7L, 393C                                                                                                                    1,1           All
[DEFAULT]
-rw-r--r-- 1 root root 1766 Apr 25  2019 ociapi_az_key.pem
-rw-r--r-- 1 root root  517 Apr 25  2019 az_oci_config
drwxr-xr-x 3 root root 4096 Oct  9  2019 osvc_oci
-rw------- 1 root root 1679 Nov  6  2019 oci_api_key.pem
-rw-r--r-- 1 root root  451 Nov  6  2019 oci_api_key_public.pem
-rw-r--r-- 1 root root 2758 Nov  6  2019 oci_cli_rc
-rw-r--r-- 1 root root  414 Jul  6  2021 osvcstage-phx.config
-rw------- 1 root root  393 Jul  6  2021 osvcstage-iad_v2.config
-rw------- 1 root root  507 Jul  6  2021 osvcstage-iad.config
-rw-r--r-- 1 root root  169 Jul  6  2021 osvcstage-.config
drwxr-xr-x 2 root root 4096 Jul  6  2021 osvcstage
-rw-r--r-- 1 root root  300 Jul  6  2021 osvcprod-phx_v2.config
-rw-r--r-- 1 root root  414 Jul  6  2021 osvcprod-phx.config
-rw-r--r-- 1 root root  294 Jul  6  2021 osvcprod-fra.config
drwxr-xr-x 2 root root 4096 Jul  6  2021 osvcprod
-rw-r--r-- 1 root root  300 Jul  6  2021 osvccorp-iad_v2.config
-rw-r--r-- 1 root root  414 Jul  6  2021 osvccorp-iad.config
-rw-r--r-- 1 root root  414 Jul  6  2021 osvccorp-ashburn.config
drwxr-xr-x 2 root root 4096 Jul  6  2021 osvccorp
-rw------- 1 root root  414 Jul  6  2021 config_PROD_us-phoenix-1
-rw------- 1 root root  416 Jul  6  2021 config_PROD_eu-frankfurt-1
-rw------- 1 root root  507 Jul  6  2021 config_phoenix-1
-rw------- 1 root root  593 Jul  6  2021 config_OSVCSTAGE_phoenix-1
-rw------- 1 root root  507 Jul  6  2021 config_OSVCSTAGE_ashburn-1
-rw-r--r-- 1 root root  414 Jul  6  2021 config_OSVCCORP_ashburn-1
-rw-r--r-- 1 root root  500 Jul  6  2021 config_oracleova_phoenix-1
-rw------- 1 root root  593 Jul  6  2021 config_ashburn-1
-rw------- 1 root root  393 Jan 13 04:57 config
[13:January:2022:04:57:44]:(dev_eu-frankfurt-1_controlplane):/
○ $ cd ~
[13:January:2022:04:57:51]:(dev_eu-frankfurt-1_controlplane):~
○ $ pwd
/root
[13:January:2022:04:57:52]:(dev_eu-frankfurt-1_controlplane):~
○ $ cat ~/.oci
.oci/     .oci_sre/
[13:January:2022:04:57:52]:(dev_eu-frankfurt-1_controlplane):~
○ $ cat ~/.oci
.oci/     .oci_sre/
[13:January:2022:04:57:52]:(dev_eu-frankfurt-1_controlplane):~
○ $ cat ~/.oci/config
[DEFAULT]
#tenancy=ocid1.compartment.oc1..aaaaaaaamfzmd7dxhst3a5hbhtu6mwxnkxzacrxqwrgasecvwq2iqcg7mg3q

~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
"/home/opc/galorndon/ctemp/kubeconfig_context-c5ppxaonnda" [New DIRECTORY]                                                                  0,0-1         All
tenancy=ocid1.tenancy.oc1..aaaaaaaacbb4jhwb2q6tfx223i5siaiuyw6gpl3zywyosiudeimsodjkolga
region=us-ashburn-1
user=ocid1.user.oc1..aaaaaaaaft5c2fkohgco27vmkudak2i3mk5mubwmo5szyejluyso7flvkdqa
fingerprint=1e:57:b5:1f:39:03:b5:e4:16:38:87:5b:88:e5:da:e0
key_file=/home/opc/.oci/oci_api_key.pem
[13:January:2022:04:58:01]:(dev_eu-frankfurt-1_controlplane):~
○ $ vi ~/.oci/config
[13:January:2022:04:58:21]:(dev_eu-frankfurt-1_controlplane):~
○ $ oong
{
  "data": "osvcstage"
}
[13:January:2022:04:58:26]:(dev_eu-frankfurt-1_controlplane):~
○ $ kgns
NAME                                  STATUS   AGE
consul                                Active   476d
cpe-cronjob                           Active   484d
default                               Active   531d
elastic-system                        Active   40d
frontend                              Active   484d
infra-monitoring                      Active   124d
ingress-watcher                       Active   484d
127.0.0.1       localhost
::1     localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
172.17.0.4      0937f82e82fb

test
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
"/etc/hosts" 9L, 180C                                                                                                                       9,1           All
127.0.0.1       localhost
istio-system                          Active   16d
istio-test                            Active   16d
kafka                                 Active   266d
karapace-schema-registry-7ywmwp26cs   Active   142d
kube-node-lease                       Active   531d
kube-public                           Active   531d
kube-system                           Active   531d
lumberjack-logging                    Active   484d
m3db                                  Active   40d
ma-global                             Active   40d
master-monitoring-services            Active   310d
monitoring-agent                      Active   475d
pdb-provisioning                      Active   457d
psp-example                           Active   204d
pt2                                   Active   276d
pvc-watcher                           Active   457d
tms                                   Active   189d
tms-dev                               Active   76d
traefik-apps                          Active   98d
traefik-frontend                      Active   98d
vault                                 Active   477d
[13:January:2022:04:58:37]:(dev_eu-frankfurt-1_controlplane):~
○ $ export KUBECONFIG=/home/opc/galorndon/ctemp/kubeconfig_context-c5ppxaonnda
[13:January:2022:04:58:44]:(dev_eu-frankfurt-1_controlplane):~
○ $ kgns
127.0.0.1       localhost
::1     localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
172.17.0.4      0937f82e82fb

10.96.42.117 kf-kafka-kafka-0.service.test
10.96.93.111 kf-kafka-kafka-1.service.test
10.96.12.12 kf-kafka-kafka-2.service.test
10.96.115.85 kf-kafka-kafka-bootstrap.service.test
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
"/etc/hosts" 12L, 354C                                                   12,1          All
127.0.0.1       localhost
The connection to the server localhost:8080 was refused - did you specify the right host or port?
[13:January:2022:04:58:46]:(dev_eu-frankfurt-1_controlplane):~
○ $ vi /home/opc/galorndon/ctemp/kubeconfig_context-c5ppxaonnda
[13:January:2022:04:58:57]:(dev_eu-frankfurt-1_controlplane):~
○ $ ll /home/opc/galorndon/ctemp/kubeconfig_context-c5ppxaonnda
ls: cannot access /home/opc/galorndon/ctemp/kubeconfig_context-c5ppxaonnda: No such file or directory
[13:January:2022:04:59:20]:(dev_eu-frankfurt-1_controlplane):~
○ $ ls -ltr ~/galorndon/ctemp/kubeconfig_context-c5ppxaonnda
-r-------- 1 root root 2358 Jan 10 05:02 /root/galorndon/ctemp/kubeconfig_context-c5ppxaonnda
[13:January:2022:04:59:33]:(dev_eu-frankfurt-1_controlplane):~
○ $ #cp ~/galorndon/ctemp/kubeconfig_context-c5ppxaonnda
[13:January:2022:04:59:44]:(dev_eu-frankfurt-1_controlplane):~
○ $ export KUBECONFIG=/root/galorndon/ctemp/kubeconfig_context-c5ppxaonnda
[13:January:2022:04:59:57]:(dev_eu-frankfurt-1_controlplane):~
○ $ kgns
NAME              STATUS   AGE
default           Active   85d
kafka             Active   64d
kafka-strimzi     Active   7d16h
kube-node-lease   Active   85d
kube-public       Active   85d
kube-system       Active   85d
traefik-test      Active   7d13h
[13:January:2022:05:00:05]:(dev_eu-frankfurt-1_controlplane):~
○ $ vi /etc/hosts
[13:January:2022:05:00:30]:(dev_eu-frankfurt-1_controlplane):~
○ $ cat /etc/hosts
127.0.0.1 localhost
::1 localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
172.17.0.4  0937f82e82fb

10.96.42.117 kf-kafka-kafka-0.service.test
10.96.93.111 kf-kafka-kafka-1.service.test
10.96.12.12 kf-kafka-kafka-2.service.test
10.96.115.85 kf-kafka-kafka-bootstrap.service.test
[13:January:2022:05:00:33]:(dev_eu-frankfurt-1_controlplane):~
○ $ ping kf-kafka-kafka-0.service.test
bash: ping: command not found
[13:January:2022:05:00:39]:(dev_eu-frankfurt-1_controlplane):~
○ $ host kf-kafka-kafka-0.service.test
Host kf-kafka-kafka-0.service.test not found: 3(NXDOMAIN)
[13:January:2022:05:00:42]:(dev_eu-frankfurt-1_controlplane):~
○ $ cat /etc/hosts
127.0.0.1 localhost
::1 localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
172.17.0.4  0937f82e82fb

10.96.42.117 kf-kafka-kafka-0.service.test
10.96.93.111 kf-kafka-kafka-1.service.test
10.96.12.12 kf-kafka-kafka-2.service.test
10.96.115.85 kf-kafka-kafka-bootstrap.service.test
[13:January:2022:07:53:03]:(dev_eu-frankfurt-1_controlplane):~
○ $ vi /etc/hosts
[13:January:2022:07:53:58]:(dev_eu-frankfurt-1_controlplane):~
○ $ cat /etc/hosts
127.0.0.1 localhost
::1 localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
172.17.0.4  0937f82e82fb

147.154.15.167 kf-kafka-kafka-0.service.test
147.154.15.167 kf-kafka-kafka-1.service.test
147.154.15.167 kf-kafka-kafka-2.service.test
147.154.15.167 kf-kafka-kafka-bootstrap.service.test
[13:January:2022:07:54:01]:(dev_eu-frankfurt-1_controlplane):~
○ $ yum install ping
Loaded plugins: ovl
ol7_UEKR5                                                                                                                                                                                    | 3.0 kB  00:00:00
ol7_developer_EPEL                                                                                                                                                                           | 3.6 kB  00:00:00
ol7_latest                                                                                                                                                                                   | 3.6 kB  00:00:00
ol7_optional_latest                                                                                                                                                                          | 3.0 kB  00:00:00
ol7_oracle_instantclient                                                                                                                                                                     | 2.9 kB  00:00:00
ol7_software_collections                                                                                                                                                                     | 3.0 kB  00:00:00
(1/14): ol7_developer_EPEL/x86_64/group_gz                                                                                                                                                   |  88 kB  00:00:03
(2/14): ol7_latest/x86_64/group_gz                                                                                                                                                           | 136 kB  00:00:03
(3/14): ol7_UEKR5/x86_64/updateinfo                                                                                                                                                          | 196 kB  00:00:07
(4/14): ol7_developer_EPEL/x86_64/updateinfo                                                                                                                                                 | 577 kB  00:00:10
(5/14): ol7_optional_latest/x86_64/updateinfo                                                                                                                                                | 1.4 MB  00:00:02
(6/14): ol7_latest/x86_64/updateinfo                                                                                                                                                         | 3.4 MB  00:00:08
(7/14): ol7_oracle_instantclient/x86_64/updateinfo                                                                                                                                           |   71 B  00:00:00
(8/14): ol7_oracle_instantclient/x86_64/primary_db                                                                                                                                           |  25 kB  00:00:00
(9/14): ol7_software_collections/x86_64/updateinfo                                                                                                                                           | 8.9 kB  00:00:00
(10/14): ol7_developer_EPEL/x86_64/primary_db                                                                                                                                                |  15 MB  00:00:13
(11/14): ol7_optional_latest/x86_64/primary_db                                                                                                                                               | 5.6 MB  00:00:04
(12/14): ol7_software_collections/x86_64/primary_db                                                                                                                                          | 5.8 MB  00:00:03
(13/14): ol7_UEKR5/x86_64/primary_db                                                                                                                                                         |  37 MB  00:00:23
(14/14): ol7_latest/x86_64/primary_db                                                                                                                                                        |  38 MB  00:00:15
No package ping available.
Error: Nothing to do
[13:January:2022:07:55:40]:(dev_eu-frankfurt-1_controlplane):~
○ $
[13:January:2022:10:33:10]:(dev_eu-frankfurt-1_controlplane):~
○ $
[13:January:2022:10:33:13]:(dev_eu-frankfurt-1_controlplane):~
○ $ kgn traefik-test
You must specify the type of resource to get. Use "kubectl api-resources" for a complete list of supported resources.

error: Required resource not specified.
Use "kubectl explain <resource>" for a detailed description of that resource (e.g. kubectl explain pods).
See 'kubectl get -h' for help and examples.
[13:January:2022:10:33:18]:(dev_eu-frankfurt-1_controlplane):~
○ $ kgpn traefik-test
NAME                       READY   STATUS    RESTARTS   AGE   IP             NODE         NOMINATED NODE   READINESS GATES
traefik-6ff7ff8ffd-cxgdn   1/1     Running   0          10h   10.244.1.197   10.0.10.12   <none>           <none>
[13:January:2022:10:33:34]:(dev_eu-frankfurt-1_controlplane):~
○ $ kln traefik-test traefik-6ff7ff8ffd-cxgdn
time="2022-01-13T00:01:50Z" level=info msg="Configuration loaded from flags."
time="2022-01-13T00:01:50Z" level=info msg="Traefik version 2.5.3 built on 2021-09-20T15:43:56Z"
time="2022-01-13T00:01:50Z" level=debug msg="Static configuration loaded {\"global\":{\"checkNewVersion\":true,\"sendAnonymousUsage\":true},\"serversTransport\":{\"maxIdleConnsPerHost\":200},\"entryPoints\":{\"metrics\":{\"address\":\":9100/tcp\",\"transport\":{\"lifeCycle\":{\"graceTimeOut\":\"10s\"},\"respondingTimeouts\":{\"idleTimeout\":\"3m0s\"}},\"forwardedHeaders\":{},\"http\":{},\"udp\":{\"timeout\":\"3s\"}},\"traefik\":{\"address\":\":9000/tcp\",\"transport\":{\"lifeCycle\":{\"graceTimeOut\":\"10s\"},\"respondingTimeouts\":{\"idleTimeout\":\"3m0s\"}},\"forwardedHeaders\":{},\"http\":{},\"udp\":{\"timeout\":\"3s\"}},\"web\":{\"address\":\":8000/tcp\",\"transport\":{\"lifeCycle\":{\"graceTimeOut\":\"10s\"},\"respondingTimeouts\":{\"idleTimeout\":\"3m0s\"}},\"forwardedHeaders\":{},\"http\":{},\"udp\":{\"timeout\":\"3s\"}},\"websecure\":{\"address\":\":8443/tcp\",\"transport\":{\"lifeCycle\":{\"graceTimeOut\":\"10s\"},\"respondingTimeouts\":{\"idleTimeout\":\"3m0s\"}},\"forwardedHeaders\":{},\"http\":{\"tls\":{}},\"udp\":{\"timeout\":\"3s\"}}},\"providers\":{\"providersThrottleDuration\":\"2s\",\"file\":{\"watch\":true,\"filename\":\"/config/traefik.yaml\"},\"kubernetesIngress\":{\"ingressClass\":\"traefik-kafka\"},\"kubernetesCRD\":{}},\"api\":{\"dashboard\":true},\"metrics\":{\"prometheus\":{\"buckets\":[0.1,0.3,1.2,5],\"addEntryPointsLabels\":true,\"addServicesLabels\":true,\"entryPoint\":\"metrics\"}},\"ping\":{\"entryPoint\":\"traefik\",\"terminatingStatusCode\":503},\"log\":{\"level\":\"DEBUG\",\"format\":\"common\"},\"pilot\":{\"dashboard\":true}}"
time="2022-01-13T00:01:50Z" level=info msg="Stats collection is enabled."
time="2022-01-13T00:01:50Z" level=info msg="Many thanks for contributing to Traefik's improvement by allowing us to receive anonymous information from your configuration."
time="2022-01-13T00:01:50Z" level=info msg="Help us improve Traefik by leaving this feature on :)"
time="2022-01-13T00:01:50Z" level=info msg="More details on: https://doc.traefik.io/traefik/contributing/data-collection/"
time="2022-01-13T00:01:50Z" level=debug msg="Configured Prometheus metrics" metricsProviderName=prometheus
time="2022-01-13T00:01:50Z" level=debug msg="Start TCP Server" entryPointName=metrics
time="2022-01-13T00:01:50Z" level=debug msg="Start TCP Server" entryPointName=traefik
time="2022-01-13T00:01:50Z" level=info msg="Starting provider aggregator.ProviderAggregator {}"
time="2022-01-13T00:01:50Z" level=debug msg="Start TCP Server" entryPointName=websecure
time="2022-01-13T00:01:50Z" level=debug msg="Start TCP Server" entryPointName=web
time="2022-01-13T00:01:50Z" level=info msg="Starting provider *file.Provider {\"watch\":true,\"filename\":\"/config/traefik.yaml\"}"
time="2022-01-13T00:01:50Z" level=error msg="Cannot start the provider *file.Provider: error reading configuration file: /config/traefik.yaml - open /config/traefik.yaml: no such file or directory"
time="2022-01-13T00:01:50Z" level=info msg="Starting provider *traefik.Provider {}"
time="2022-01-13T00:01:50Z" level=info msg="Starting provider *crd.Provider {}"
time="2022-01-13T00:01:50Z" level=info msg="label selector is: \"\"" providerName=kubernetescrd
time="2022-01-13T00:01:50Z" level=info msg="Creating in-cluster Provider client" providerName=kubernetescrd
time="2022-01-13T00:01:50Z" level=info msg="Starting provider *acme.ChallengeTLSALPN {\"Timeout\":4000000000}"
time="2022-01-13T00:01:50Z" level=info msg="Starting provider *ingress.Provider {\"ingressClass\":\"traefik-kafka\"}"
time="2022-01-13T00:01:50Z" level=info msg="ingress label selector is: \"\"" providerName=kubernetes
time="2022-01-13T00:01:50Z" level=info msg="Creating in-cluster Provider client" providerName=kubernetes
time="2022-01-13T00:01:50Z" level=debug msg="Configuration received from provider internal: {\"http\":{\"routers\":{\"ping\":{\"entryPoints\":[\"traefik\"],\"service\":\"ping@internal\",\"rule\":\"PathPrefix(`/ping`)\",\"priority\":2147483647},\"prometheus\":{\"entryPoints\":[\"metrics\"],\"service\":\"prometheus@internal\",\"rule\":\"PathPrefix(`/metrics`)\",\"priority\":2147483647}},\"services\":{\"api\":{},\"dashboard\":{},\"noop\":{},\"ping\":{},\"prometheus\":{}},\"models\":{\"websecure\":{\"tls\":{}}},\"serversTransports\":{\"default\":{\"maxIdleConnsPerHost\":200}}},\"tcp\":{},\"tls\":{}}" providerName=internal
time="2022-01-13T00:01:50Z" level=debug msg="No default certificate, generating one" tlsStoreName=default
time="2022-01-13T00:01:50Z" level=debug msg="Configuration received from provider kubernetescrd: {\"http\":{\"routers\":{\"traefik-test-traefik-dashboard-d012b7f875133eeab4e5\":{\"entryPoints\":[\"traefik\"],\"service\":\"api@internal\",\"rule\":\"PathPrefix(`/dashboard`) || PathPrefix(`/api`)\"}}},\"tcp\":{},\"udp\":{},\"tls\":{}}" providerName=kubernetescrd
time="2022-01-13T00:01:50Z" level=debug msg="Added outgoing tracing middleware prometheus@internal" middlewareType=TracingForwarder entryPointName=metrics routerName=prometheus@internal middlewareName=tracing
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T00:01:50Z" level=debug msg="Added outgoing tracing middleware ping@internal" entryPointName=traefik routerName=ping@internal middlewareName=tracing middlewareType=TracingForwarder
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" middlewareName=traefik-internal-recovery middlewareType=Recovery entryPointName=traefik
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=web middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=websecure
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=traefik
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=web middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:50Z" level=debug msg="No default certificate, generating one" tlsStoreName=default
time="2022-01-13T00:01:50Z" level=debug msg="Added outgoing tracing middleware prometheus@internal" middlewareName=tracing middlewareType=TracingForwarder entryPointName=metrics routerName=prometheus@internal
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T00:01:50Z" level=debug msg="Added outgoing tracing middleware api@internal" routerName=traefik-test-traefik-dashboard-d012b7f875133eeab4e5@kubernetescrd entryPointName=traefik middlewareType=TracingForwarder middlewareName=tracing
time="2022-01-13T00:01:50Z" level=debug msg="Added outgoing tracing middleware ping@internal" entryPointName=traefik middlewareName=tracing middlewareType=TracingForwarder routerName=ping@internal
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=web
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" middlewareType=Metrics entryPointName=websecure middlewareName=metrics-entrypoint
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" middlewareType=Metrics middlewareName=metrics-entrypoint entryPointName=metrics
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=web middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareType=Metrics middlewareName=metrics-entrypoint
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:51Z" level=debug msg="Configuration received from provider kubernetes: {\"http\":{},\"tcp\":{}}" providerName=kubernetes
time="2022-01-13T00:01:51Z" level=debug msg="No default certificate, generating one" tlsStoreName=default
time="2022-01-13T00:01:51Z" level=debug msg="Configuration received from provider kubernetescrd: {\"http\":{},\"tcp\":{},\"udp\":{},\"tls\":{}}" providerName=kubernetescrd
time="2022-01-13T00:01:51Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T00:01:51Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T00:01:51Z" level=debug msg="Added outgoing tracing middleware prometheus@internal" routerName=prometheus@internal middlewareType=TracingForwarder middlewareName=tracing entryPointName=metrics
time="2022-01-13T00:01:51Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T00:01:51Z" level=debug msg="Added outgoing tracing middleware ping@internal" entryPointName=traefik routerName=ping@internal middlewareName=tracing middlewareType=TracingForwarder
time="2022-01-13T00:01:51Z" level=debug msg="Added outgoing tracing middleware api@internal" middlewareName=tracing middlewareType=TracingForwarder entryPointName=traefik routerName=traefik-test-traefik-dashboard-d012b7f875133eeab4e5@kubernetescrd
time="2022-01-13T00:01:51Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T00:01:51Z" level=debug msg="Creating middleware" entryPointName=web middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:51Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=websecure
time="2022-01-13T00:01:51Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:51Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:51Z" level=debug msg="Creating middleware" middlewareType=Metrics middlewareName=metrics-entrypoint entryPointName=web
time="2022-01-13T00:01:51Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=websecure
time="2022-01-13T00:01:51Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:51Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:52Z" level=debug msg="No default certificate, generating one" tlsStoreName=default
time="2022-01-13T00:01:53Z" level=debug msg="Added outgoing tracing middleware prometheus@internal" middlewareType=TracingForwarder routerName=prometheus@internal entryPointName=metrics middlewareName=tracing
time="2022-01-13T00:01:53Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T00:01:53Z" level=debug msg="Added outgoing tracing middleware ping@internal" middlewareName=tracing middlewareType=TracingForwarder entryPointName=traefik routerName=ping@internal
time="2022-01-13T00:01:53Z" level=debug msg="Creating middleware" middlewareName=traefik-internal-recovery middlewareType=Recovery entryPointName=traefik
time="2022-01-13T00:01:53Z" level=debug msg="Creating middleware" entryPointName=web middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:53Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:53Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:53Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=traefik
time="2022-01-13T00:01:53Z" level=debug msg="Creating middleware" entryPointName=web middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:53Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:53Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareType=Metrics middlewareName=metrics-entrypoint
time="2022-01-13T00:01:53Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:54Z" level=debug msg="Configuration received from provider kubernetescrd: {\"http\":{\"routers\":{\"traefik-test-traefik-dashboard-d012b7f875133eeab4e5\":{\"entryPoints\":[\"traefik\"],\"service\":\"api@internal\",\"rule\":\"PathPrefix(`/dashboard`) || PathPrefix(`/api`)\"}}},\"tcp\":{},\"udp\":{},\"tls\":{}}" providerName=kubernetescrd
time="2022-01-13T00:01:54Z" level=debug msg="No default certificate, generating one" tlsStoreName=default
time="2022-01-13T00:01:54Z" level=debug msg="Added outgoing tracing middleware prometheus@internal" middlewareType=TracingForwarder entryPointName=metrics routerName=prometheus@internal middlewareName=tracing
time="2022-01-13T00:01:54Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T00:01:54Z" level=debug msg="Added outgoing tracing middleware api@internal" entryPointName=traefik routerName=traefik-test-traefik-dashboard-d012b7f875133eeab4e5@kubernetescrd middlewareType=TracingForwarder middlewareName=tracing
time="2022-01-13T00:01:54Z" level=debug msg="Added outgoing tracing middleware ping@internal" middlewareType=TracingForwarder entryPointName=traefik routerName=ping@internal middlewareName=tracing
time="2022-01-13T00:01:54Z" level=debug msg="Creating middleware" middlewareName=traefik-internal-recovery middlewareType=Recovery entryPointName=traefik
time="2022-01-13T00:01:54Z" level=debug msg="Creating middleware" entryPointName=web middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:54Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:54Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:54Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=traefik
time="2022-01-13T00:01:54Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=web
time="2022-01-13T00:01:54Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:54Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=metrics
time="2022-01-13T00:01:54Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:02:04Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T00:02:04Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T00:03:54Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T00:03:54Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T00:11:50Z" level=info msg="Anonymous stats sent to https://collect.traefik.io/9vxmmkcdmalbdi635d4jgc5p5rx0h7h8: {\"global\":{\"checkNewVersion\":true,\"sendAnonymousUsage\":true},\"serversTransport\":{\"maxIdleConnsPerHost\":200},\"entryPoints\":{\"metrics\":{\"address\":\"xxxx\",\"transport\":{\"lifeCycle\":{\"graceTimeOut\":\"10s\"},\"respondingTimeouts\":{\"idleTimeout\":\"3m0s\"}},\"forwardedHeaders\":{},\"http\":{}},\"traefik\":{\"address\":\"xxxx\",\"transport\":{\"lifeCycle\":{\"graceTimeOut\":\"10s\"},\"respondingTimeouts\":{\"idleTimeout\":\"3m0s\"}},\"forwardedHeaders\":{},\"http\":{}},\"web\":{\"address\":\"xxxx\",\"transport\":{\"lifeCycle\":{\"graceTimeOut\":\"10s\"},\"respondingTimeouts\":{\"idleTimeout\":\"3m0s\"}},\"forwardedHeaders\":{},\"http\":{}},\"websecure\":{\"address\":\"xxxx\",\"transport\":{\"lifeCycle\":{\"graceTimeOut\":\"10s\"},\"respondingTimeouts\":{\"idleTimeout\":\"3m0s\"}},\"forwardedHeaders\":{},\"http\":{\"tls\":{}}}},\"providers\":{\"providersThrottleDuration\":\"2s\",\"file\":{\"watch\":true,\"filename\":\"/config/traefik.yaml\"},\"kubernetesIngress\":{\"ingressClass\":\"traefik-kafka\"},\"kubernetesCRD\":{}},\"api\":{\"dashboard\":true},\"metrics\":{\"prometheus\":{\"buckets\":[0.1,0.3,1.2,5],\"addEntryPointsLabels\":true,\"addServicesLabels\":true,\"entryPoint\":\"metrics\"}},\"ping\":{\"entryPoint\":\"traefik\",\"terminatingStatusCode\":503},\"log\":{\"level\":\"DEBUG\",\"format\":\"common\"},\"pilot\":{}}"
time="2022-01-13T00:11:50Z" level=debug msg="unknown kind to hash: func"
time="2022-01-13T00:11:50Z" level=warning msg="A new release has been found: 2.5.6. Please consider updating."
time="2022-01-13T00:28:20Z" level=debug msg="Serving default certificate for request: \"147.154.15.167:443\""
time="2022-01-13T00:57:18Z" level=debug msg="Serving default certificate for request: \"147.154.15.167:80\""
time="2022-01-13T00:59:49Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T00:59:49Z" level=debug msg="http: TLS handshake error from 10.244.0.0:1960: remote error: tls: bad certificate"
time="2022-01-13T00:59:49Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T01:19:47Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T01:19:47Z" level=debug msg="http: TLS handshake error from 10.244.1.129:26484: remote error: tls: bad certificate"
time="2022-01-13T01:19:47Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T01:20:12Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T01:22:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T01:29:36Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T01:29:44Z" level=debug msg="http: TLS handshake error from 10.244.0.0:48048: EOF"
time="2022-01-13T01:33:14Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T01:35:30Z" level=debug msg="http: TLS handshake error from 10.244.1.129:55296: tls: client offered only unsupported versions: []"
time="2022-01-13T01:36:12Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T01:36:12Z" level=debug msg="http: TLS handshake error from 10.244.0.0:12506: tls: no cipher suite supported by both client and server"
time="2022-01-13T01:38:09Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T01:38:10Z" level=debug msg="http: TLS handshake error from 10.244.0.128:5722: EOF"
time="2022-01-13T01:38:10Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T01:38:11Z" level=debug msg="http: TLS handshake error from 10.244.1.129:43126: EOF"
time="2022-01-13T01:38:11Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T01:38:11Z" level=debug msg="http: TLS handshake error from 10.244.0.0:33680: EOF"
time="2022-01-13T01:38:11Z" level=debug msg="http: TLS handshake error from 10.244.0.128:7666: tls: client requested unsupported application protocols ([http/0.9 http/1.0 spdy/1 spdy/2 spdy/3 h2c hq])"
time="2022-01-13T01:38:11Z" level=debug msg="http: TLS handshake error from 10.244.1.129:44466: tls: client requested unsupported application protocols ([hq h2c spdy/3 spdy/2 spdy/1 http/1.0 http/0.9])"
time="2022-01-13T01:38:12Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T01:38:12Z" level=debug msg="http: TLS handshake error from 10.244.0.0:34884: EOF"
time="2022-01-13T01:38:12Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T01:38:13Z" level=debug msg="http: TLS handshake error from 10.244.0.128:8124: EOF"
time="2022-01-13T01:38:13Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T01:38:13Z" level=debug msg="http: TLS handshake error from 10.244.1.129:45138: EOF"
time="2022-01-13T01:38:13Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T01:38:14Z" level=debug msg="http: TLS handshake error from 10.244.0.0:35500: EOF"
time="2022-01-13T01:38:14Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T01:38:14Z" level=debug msg="http: TLS handshake error from 10.244.0.128:8994: EOF"
time="2022-01-13T01:41:13Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T01:49:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T02:14:00Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T02:14:00Z" level=debug msg="http: TLS handshake error from 10.244.0.128:26860: EOF"
time="2022-01-13T03:23:38Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T03:39:12Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T03:39:12Z" level=debug msg="http: TLS handshake error from 10.244.0.0:24762: EOF"
time="2022-01-13T04:14:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T04:14:54Z" level=debug msg="http: TLS handshake error from 10.244.1.129:38592: EOF"
time="2022-01-13T04:30:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:12270: tls: client offered only unsupported versions: []"
time="2022-01-13T04:40:08Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:02:19Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka servicePort="{0 9095 }" serviceName=kf-kafka-kafka-0 providerName=kubernetescrd ingress=kf-kafka-kafka-0
time="2022-01-13T05:02:19Z" level=debug msg="Configuration received from provider kubernetescrd: {\"http\":{\"routers\":{\"traefik-test-traefik-dashboard-d012b7f875133eeab4e5\":{\"entryPoints\":[\"traefik\"],\"service\":\"api@internal\",\"rule\":\"PathPrefix(`/dashboard`) || PathPrefix(`/api`)\"}}},\"tcp\":{\"routers\":{\"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0\":{\"entryPoints\":[\"websecure\"],\"service\":\"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0\",\"rule\":\"HostSNI(`kf-kafka-kafka-0.service.test`)\",\"tls\":{\"passthrough\":true}}}},\"udp\":{},\"tls\":{}}" providerName=kubernetescrd
time="2022-01-13T05:02:19Z" level=debug msg="No default certificate, generating one" tlsStoreName=default
time="2022-01-13T05:02:19Z" level=debug msg="Added outgoing tracing middleware ping@internal" middlewareName=tracing middlewareType=TracingForwarder entryPointName=traefik routerName=ping@internal
time="2022-01-13T05:02:19Z" level=debug msg="Added outgoing tracing middleware api@internal" entryPointName=traefik routerName=traefik-test-traefik-dashboard-d012b7f875133eeab4e5@kubernetescrd middlewareName=tracing middlewareType=TracingForwarder
time="2022-01-13T05:02:19Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareType=Recovery middlewareName=traefik-internal-recovery
time="2022-01-13T05:02:19Z" level=debug msg="Added outgoing tracing middleware prometheus@internal" routerName=prometheus@internal middlewareName=tracing middlewareType=TracingForwarder entryPointName=metrics
time="2022-01-13T05:02:19Z" level=debug msg="Creating middleware" middlewareName=traefik-internal-recovery middlewareType=Recovery entryPointName=metrics
time="2022-01-13T05:02:19Z" level=debug msg="Creating middleware" middlewareType=Metrics entryPointName=web middlewareName=metrics-entrypoint
time="2022-01-13T05:02:19Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:19Z" level=debug msg="Creating middleware" middlewareType=Metrics middlewareName=metrics-entrypoint entryPointName=metrics
time="2022-01-13T05:02:19Z" level=debug msg="Creating middleware" middlewareType=Metrics entryPointName=traefik middlewareName=metrics-entrypoint
time="2022-01-13T05:02:19Z" level=debug msg="Creating middleware" middlewareType=Metrics entryPointName=web middlewareName=metrics-entrypoint
time="2022-01-13T05:02:19Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:19Z" level=debug msg="Creating middleware" middlewareType=Metrics entryPointName=metrics middlewareName=metrics-entrypoint
time="2022-01-13T05:02:19Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:19Z" level=error msg="the service \"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0@kubernetescrd\" does not exist" entryPointName=websecure routerName=kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0@kubernetescrd
time="2022-01-13T05:02:45Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }"
time="2022-01-13T05:02:45Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd namespace=kafka ingress=kf-kafka-kafka-1 serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T05:02:45Z" level=debug msg="Configuration received from provider kubernetescrd: {\"http\":{\"routers\":{\"traefik-test-traefik-dashboard-d012b7f875133eeab4e5\":{\"entryPoints\":[\"traefik\"],\"service\":\"api@internal\",\"rule\":\"PathPrefix(`/dashboard`) || PathPrefix(`/api`)\"}}},\"tcp\":{\"routers\":{\"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0\":{\"entryPoints\":[\"websecure\"],\"service\":\"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0\",\"rule\":\"HostSNI(`kf-kafka-kafka-0.service.test`)\",\"tls\":{\"passthrough\":true}},\"kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d\":{\"entryPoints\":[\"websecure\"],\"service\":\"kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d\",\"rule\":\"HostSNI(`kf-kafka-kafka-1.service.test`)\",\"tls\":{\"passthrough\":true}}}},\"udp\":{},\"tls\":{}}" providerName=kubernetescrd
time="2022-01-13T05:02:45Z" level=debug msg="No default certificate, generating one" tlsStoreName=default
time="2022-01-13T05:02:45Z" level=debug msg="Added outgoing tracing middleware ping@internal" routerName=ping@internal middlewareName=tracing middlewareType=TracingForwarder entryPointName=traefik
time="2022-01-13T05:02:45Z" level=debug msg="Added outgoing tracing middleware api@internal" routerName=traefik-test-traefik-dashboard-d012b7f875133eeab4e5@kubernetescrd entryPointName=traefik middlewareName=tracing middlewareType=TracingForwarder
time="2022-01-13T05:02:45Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T05:02:45Z" level=debug msg="Added outgoing tracing middleware prometheus@internal" middlewareName=tracing middlewareType=TracingForwarder entryPointName=metrics routerName=prometheus@internal
time="2022-01-13T05:02:45Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T05:02:45Z" level=debug msg="Creating middleware" entryPointName=web middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:45Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=websecure
time="2022-01-13T05:02:45Z" level=debug msg="Creating middleware" middlewareType=Metrics entryPointName=metrics middlewareName=metrics-entrypoint
time="2022-01-13T05:02:45Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareType=Metrics middlewareName=metrics-entrypoint
time="2022-01-13T05:02:45Z" level=debug msg="Creating middleware" entryPointName=web middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:45Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:45Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:45Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:45Z" level=error msg="the service \"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0@kubernetescrd\" does not exist" entryPointName=websecure routerName=kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0@kubernetescrd
time="2022-01-13T05:02:45Z" level=error msg="the service \"kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d@kubernetescrd\" does not exist" entryPointName=websecure routerName=kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d@kubernetescrd
time="2022-01-13T05:02:57Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka
time="2022-01-13T05:02:57Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka servicePort="{0 9095 }" serviceName=kf-kafka-kafka-0
time="2022-01-13T05:02:57Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T05:02:57Z" level=debug msg="Configuration received from provider kubernetescrd: {\"http\":{\"routers\":{\"traefik-test-traefik-dashboard-d012b7f875133eeab4e5\":{\"entryPoints\":[\"traefik\"],\"service\":\"api@internal\",\"rule\":\"PathPrefix(`/dashboard`) || PathPrefix(`/api`)\"}}},\"tcp\":{\"routers\":{\"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0\":{\"entryPoints\":[\"websecure\"],\"service\":\"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0\",\"rule\":\"HostSNI(`kf-kafka-kafka-0.service.test`)\",\"tls\":{\"passthrough\":true}},\"kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d\":{\"entryPoints\":[\"websecure\"],\"service\":\"kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d\",\"rule\":\"HostSNI(`kf-kafka-kafka-1.service.test`)\",\"tls\":{\"passthrough\":true}},\"kafka-kf-kafka-kafka-2-8b70efc5c7b1fbc44615\":{\"entryPoints\":[\"websecure\"],\"service\":\"kafka-kf-kafka-kafka-2-8b70efc5c7b1fbc44615\",\"rule\":\"HostSNI(`kf-kafka-kafka-2.service.test`)\",\"tls\":{\"passthrough\":true}}}},\"udp\":{},\"tls\":{}}" providerName=kubernetescrd
time="2022-01-13T05:02:57Z" level=debug msg="No default certificate, generating one" tlsStoreName=default
time="2022-01-13T05:02:57Z" level=debug msg="Added outgoing tracing middleware prometheus@internal" middlewareType=TracingForwarder entryPointName=metrics routerName=prometheus@internal middlewareName=tracing
time="2022-01-13T05:02:57Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T05:02:57Z" level=debug msg="Added outgoing tracing middleware ping@internal" middlewareType=TracingForwarder middlewareName=tracing entryPointName=traefik routerName=ping@internal
time="2022-01-13T05:02:57Z" level=debug msg="Added outgoing tracing middleware api@internal" middlewareType=TracingForwarder entryPointName=traefik routerName=traefik-test-traefik-dashboard-d012b7f875133eeab4e5@kubernetescrd middlewareName=tracing
time="2022-01-13T05:02:57Z" level=debug msg="Creating middleware" middlewareName=traefik-internal-recovery middlewareType=Recovery entryPointName=traefik
time="2022-01-13T05:02:57Z" level=debug msg="Creating middleware" entryPointName=web middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:57Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareType=Metrics middlewareName=metrics-entrypoint
time="2022-01-13T05:02:57Z" level=debug msg="Creating middleware" middlewareType=Metrics entryPointName=metrics middlewareName=metrics-entrypoint
time="2022-01-13T05:02:57Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:57Z" level=debug msg="Creating middleware" middlewareType=Metrics entryPointName=web middlewareName=metrics-entrypoint
time="2022-01-13T05:02:57Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:57Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:57Z" level=debug msg="Creating middleware" middlewareType=Metrics middlewareName=metrics-entrypoint entryPointName=traefik
time="2022-01-13T05:02:57Z" level=error msg="the service \"kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d@kubernetescrd\" does not exist" routerName=kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d@kubernetescrd entryPointName=websecure
time="2022-01-13T05:02:57Z" level=error msg="the service \"kafka-kf-kafka-kafka-2-8b70efc5c7b1fbc44615@kubernetescrd\" does not exist" entryPointName=websecure routerName=kafka-kf-kafka-kafka-2-8b70efc5c7b1fbc44615@kubernetescrd
time="2022-01-13T05:02:57Z" level=error msg="the service \"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0@kubernetescrd\" does not exist" entryPointName=websecure routerName=kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0@kubernetescrd
time="2022-01-13T05:03:05Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka providerName=kubernetescrd serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" ingress=kf-kafka-kafka-0
time="2022-01-13T05:03:05Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd serviceName=kf-kafka-kafka-1 ingress=kf-kafka-kafka-1 servicePort="{0 9095 }" namespace=kafka
time="2022-01-13T05:03:05Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }"
time="2022-01-13T05:03:05Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }"
time="2022-01-13T05:03:05Z" level=debug msg="Configuration received from provider kubernetescrd: {\"http\":{\"routers\":{\"traefik-test-traefik-dashboard-d012b7f875133eeab4e5\":{\"entryPoints\":[\"traefik\"],\"service\":\"api@internal\",\"rule\":\"PathPrefix(`/dashboard`) || PathPrefix(`/api`)\"}}},\"tcp\":{\"routers\":{\"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0\":{\"entryPoints\":[\"websecure\"],\"service\":\"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0\",\"rule\":\"HostSNI(`kf-kafka-kafka-0.service.test`)\",\"tls\":{\"passthrough\":true}},\"kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d\":{\"entryPoints\":[\"websecure\"],\"service\":\"kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d\",\"rule\":\"HostSNI(`kf-kafka-kafka-1.service.test`)\",\"tls\":{\"passthrough\":true}},\"kafka-kf-kafka-kafka-2-8b70efc5c7b1fbc44615\":{\"entryPoints\":[\"websecure\"],\"service\":\"kafka-kf-kafka-kafka-2-8b70efc5c7b1fbc44615\",\"rule\":\"HostSNI(`kf-kafka-kafka-2.service.test`)\",\"tls\":{\"passthrough\":true}},\"kafka-kf-kafka-kafka-bootstrap-7651f1a5984e92618069\":{\"entryPoints\":[\"websecure\"],\"service\":\"kafka-kf-kafka-kafka-bootstrap-7651f1a5984e92618069\",\"rule\":\"HostSNI(`kf-kafka-kafka-bootstrap.service.test`)\",\"tls\":{\"passthrough\":true}}}},\"udp\":{},\"tls\":{}}" providerName=kubernetescrd
time="2022-01-13T05:03:05Z" level=debug msg="No default certificate, generating one" tlsStoreName=default
time="2022-01-13T05:03:05Z" level=debug msg="Added outgoing tracing middleware prometheus@internal" middlewareType=TracingForwarder entryPointName=metrics routerName=prometheus@internal middlewareName=tracing
time="2022-01-13T05:03:05Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T05:03:05Z" level=debug msg="Added outgoing tracing middleware ping@internal" middlewareName=tracing middlewareType=TracingForwarder entryPointName=traefik routerName=ping@internal
time="2022-01-13T05:03:05Z" level=debug msg="Added outgoing tracing middleware api@internal" entryPointName=traefik routerName=traefik-test-traefik-dashboard-d012b7f875133eeab4e5@kubernetescrd middlewareName=tracing middlewareType=TracingForwarder
time="2022-01-13T05:03:05Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T05:03:05Z" level=debug msg="Creating middleware" entryPointName=web middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:03:05Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=websecure
time="2022-01-13T05:03:05Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:03:05Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:03:05Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=web
time="2022-01-13T05:03:05Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareType=Metrics middlewareName=metrics-entrypoint
time="2022-01-13T05:03:05Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:03:05Z" level=debug msg="Creating middleware" middlewareType=Metrics entryPointName=traefik middlewareName=metrics-entrypoint
time="2022-01-13T05:03:05Z" level=error msg="the service \"kafka-kf-kafka-kafka-bootstrap-7651f1a5984e92618069@kubernetescrd\" does not exist" routerName=kafka-kf-kafka-kafka-bootstrap-7651f1a5984e92618069@kubernetescrd entryPointName=websecure
time="2022-01-13T05:03:05Z" level=error msg="the service \"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0@kubernetescrd\" does not exist" entryPointName=websecure routerName=kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0@kubernetescrd
time="2022-01-13T05:03:05Z" level=error msg="the service \"kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d@kubernetescrd\" does not exist" entryPointName=websecure routerName=kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d@kubernetescrd
time="2022-01-13T05:03:05Z" level=error msg="the service \"kafka-kf-kafka-kafka-2-8b70efc5c7b1fbc44615@kubernetescrd\" does not exist" entryPointName=websecure routerName=kafka-kf-kafka-kafka-2-8b70efc5c7b1fbc44615@kubernetescrd
time="2022-01-13T05:10:35Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:10:35Z" level=debug msg="http: TLS handshake error from 10.244.1.129:57276: EOF"
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" ingress=kf-kafka-kafka-1 namespace=kafka
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-bootstrap ingress=kf-kafka-kafka-bootstrap namespace=kafka providerName=kubernetescrd servicePort="{0 9095 }"
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-0
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka servicePort="{0 9095 }" serviceName=kf-kafka-kafka-1
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap providerName=kubernetescrd servicePort="{0 9095 }"
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" namespace=kafka providerName=kubernetescrd ingress=kf-kafka-kafka-2
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 providerName=kubernetescrd servicePort="{0 9095 }" namespace=kafka ingress=kf-kafka-kafka-0
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" ingress=kf-kafka-kafka-2 namespace=kafka providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd namespace=kafka servicePort="{0 9095 }" ingress=kf-kafka-kafka-bootstrap serviceName=kf-kafka-kafka-bootstrap
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" serviceName=kf-kafka-kafka-0 providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd namespace=kafka ingress=kf-kafka-kafka-2 serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }"
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" namespace=kafka providerName=kubernetescrd ingress=kf-kafka-kafka-0
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }"
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 namespace=kafka providerName=kubernetescrd serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-2 namespace=kafka providerName=kubernetescrd serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }"
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }"
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-bootstrap namespace=kafka providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 namespace=kafka providerName=kubernetescrd serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-2 servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-2 providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }"
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka providerName=kubernetescrd servicePort="{0 9095 }" serviceName=kf-kafka-kafka-0 ingress=kf-kafka-kafka-0
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd namespace=kafka ingress=kf-kafka-kafka-1 serviceName=kf-kafka-kafka-1
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-0 providerName=kubernetescrd ingress=kf-kafka-kafka-0 servicePort="{0 9095 }"
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" ingress=kf-kafka-kafka-2 namespace=kafka providerName=kubernetescrd serviceName=kf-kafka-kafka-2
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T05:41:41Z" level=debug msg="http: TLS handshake error from 10.244.0.128:20084: tls: client offered only unsupported versions: []"
time="2022-01-13T05:41:42Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="http: TLS handshake error from 10.244.0.0:56294: tls: unsupported SSLv2 handshake received"
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:49Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:49Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:49Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:49Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:49Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:49Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:49Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:49Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3342: tls: client offered only unsupported versions: []"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58176: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31132: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31138: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31092: EOF"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3404: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31194: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58238: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3380: EOF"
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58128: EOF"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3700: tls: client requested unsupported application protocols ([spdy/1 spdy/2 spdy/3 stun.turn stun.nat-discovery h2c webrtc c-webrtc ftp imap pop3 managesieve grpc-exp])"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3722: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31512: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58556: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3306: EOF"
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58218: EOF"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3746: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31536: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58580: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31560: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3780: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58614: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3812: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31596: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58646: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3836: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31622: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58670: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3878: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31668: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58720: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31782: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58826: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3992: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31810: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.1.129:4020: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58862: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58888: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31834: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.1.129:4054: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31860: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.1.129:4070: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58904: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31878: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.1.129:4088: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58922: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31922: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.1.129:4132: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58966: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.1.129:4158: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31948: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58992: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:32004: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.1.129:4228: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:59098: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:32078: EOF"
time="2022-01-13T06:15:42Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T06:15:42Z" level=debug msg="http: TLS handshake error from 10.244.0.0:30386: EOF"
time="2022-01-13T06:31:45Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T06:32:39Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T06:34:03Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T06:40:11Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd namespace=kafka ingress=kf-kafka-kafka-0 serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-0 servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-0 providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 namespace=kafka providerName=kubernetescrd servicePort="{0 9095 }" serviceName=kf-kafka-kafka-1
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-2 namespace=kafka providerName=kubernetescrd serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" providerName=kubernetescrd namespace=kafka ingress=kf-kafka-kafka-0
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka providerName=kubernetescrd ingress=kf-kafka-kafka-1 servicePort="{0 9095 }" serviceName=kf-kafka-kafka-1
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" namespace=kafka ingress=kf-kafka-kafka-2
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-1
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" serviceName=kf-kafka-kafka-2 providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka ingress=kf-kafka-kafka-bootstrap serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" ingress=kf-kafka-kafka-1 namespace=kafka providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 providerName=kubernetescrd servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 serviceName=kf-kafka-kafka-1 providerName=kubernetescrd namespace=kafka servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-1 servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-1
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka ingress=kf-kafka-kafka-bootstrap serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-0 serviceName=kf-kafka-kafka-0 namespace=kafka
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" providerName=kubernetescrd namespace=kafka ingress=kf-kafka-kafka-1
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2 serviceName=kf-kafka-kafka-2 namespace=kafka
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-0 namespace=kafka
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-1
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-1 providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-1 namespace=kafka
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-bootstrap providerName=kubernetescrd namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-0 serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" namespace=kafka providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" serviceName=kf-kafka-kafka-0 providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-1 namespace=kafka ingress=kf-kafka-kafka-1 servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" ingress=kf-kafka-kafka-2 namespace=kafka providerName=kubernetescrd serviceName=kf-kafka-kafka-2
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-bootstrap providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-1 providerName=kubernetescrd ingress=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" ingress=kf-kafka-kafka-2 namespace=kafka providerName=kubernetescrd serviceName=kf-kafka-kafka-2
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd namespace=kafka ingress=kf-kafka-kafka-bootstrap serviceName=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-0 providerName=kubernetescrd ingress=kf-kafka-kafka-0
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd servicePort="{0 9095 }" serviceName=kf-kafka-kafka-2 namespace=kafka ingress=kf-kafka-kafka-2
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-0 providerName=kubernetescrd ingress=kf-kafka-kafka-0
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 serviceName=kf-kafka-kafka-1 namespace=kafka providerName=kubernetescrd servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2 providerName=kubernetescrd servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-0 servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-0 providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" ingress=kf-kafka-kafka-2 namespace=kafka providerName=kubernetescrd serviceName=kf-kafka-kafka-2
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" namespace=kafka providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap serviceName=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-1 serviceName=kf-kafka-kafka-1 namespace=kafka servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd namespace=kafka ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-0 providerName=kubernetescrd ingress=kf-kafka-kafka-0 servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-2 serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" providerName=kubernetescrd namespace=kafka
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-0 providerName=kubernetescrd ingress=kf-kafka-kafka-0 servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 namespace=kafka providerName=kubernetescrd serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-2 providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-2
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" namespace=kafka providerName=kubernetescrd ingress=kf-kafka-kafka-0 serviceName=kf-kafka-kafka-0
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-1 servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-1
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-2 servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-2 providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" namespace=kafka providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 providerName=kubernetescrd servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:28:22Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:28:22Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:28:22Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:46:06Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T08:13:51Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T08:14:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T08:19:00Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T08:20:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T08:20:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd namespace=kafka ingress=kf-kafka-kafka-2 serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }"
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" ingress=kf-kafka-kafka-bootstrap namespace=kafka providerName=kubernetescrd
time="2022-01-13T08:20:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-1 providerName=kubernetescrd ingress=kf-kafka-kafka-1
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" namespace=kafka providerName=kubernetescrd ingress=kf-kafka-kafka-2
time="2022-01-13T08:20:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-1
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }"
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-bootstrap providerName=kubernetescrd
time="2022-01-13T08:20:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T08:20:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T08:20:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-0 namespace=kafka
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-2 servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-2
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }"
time="2022-01-13T08:20:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T08:20:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T08:20:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T08:20:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T08:24:46Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T08:26:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-bootstrap
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" ingress=kf-kafka-kafka-1 serviceName=kf-kafka-kafka-1 providerName=kubernetescrd namespace=kafka
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2
time="2022-01-13T08:36:24Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T08:36:24Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T08:36:24Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 providerName=kubernetescrd
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }"
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T08:36:24Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2
time="2022-01-13T08:36:24Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-bootstrap providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap namespace=kafka servicePort="{0 9095 }"
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-0 serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" namespace=kafka providerName=kubernetescrd
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T08:36:24Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T08:36:24Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 providerName=kubernetescrd servicePort="{0 9095 }"
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" ingress=kf-kafka-kafka-1 namespace=kafka providerName=kubernetescrd serviceName=kf-kafka-kafka-1
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-2 providerName=kubernetescrd ingress=kf-kafka-kafka-2
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap
time="2022-01-13T08:36:24Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T08:59:34Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T08:59:34Z" level=debug msg="http: TLS handshake error from 10.244.0.0:32756: EOF"
time="2022-01-13T09:09:44Z" level=debug msg="http: TLS handshake error from 10.244.0.128:27370: tls: client offered only unsupported versions: []"
time="2022-01-13T09:09:44Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:62580: tls: unsupported SSLv2 handshake received"
time="2022-01-13T09:09:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:62884: tls: client offered only unsupported versions: []"
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.1.129:8128: EOF"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.0.128:35972: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.1.129:8188: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.1.129:8562: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.1.129:8152: EOF"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.0.128:36398: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.0.0:63372: EOF"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.0.128:36422: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.0.128:36470: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.1.129:8702: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.0.128:36530: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.1.129:8768: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.0.128:36598: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.1.129:8786: EOF"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.0.0:63652: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.1.129:8860: EOF"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.0.0:63708: EOF"
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.1.129:8906: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.0.0:63752: EOF"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.0.128:36734: EOF"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.0.0:63820: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.1.129:9126: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.0.0:63966: EOF"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.1.129:9166: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.0.0:64056: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.0.128:37094: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.0.0:64178: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.0.0:64248: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.1.129:9396: EOF"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.1.129:9446: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.0.0:64302: EOF"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.0.128:37298: EOF"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.1.129:9584: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.1.129:9876: EOF"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.1.129:9956: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.0.0:64746: EOF"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.0.0:64814: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.0.128:37794: EOF"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.0.0:64896: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.1.129:10050: EOF"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.1.129:10078: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.0.128:38066: EOF"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.0.0:65234: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.1.129:10306: EOF"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.0.128:38294: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.1.129:10510: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.0.128:38338: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.1.129:10576: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.0.0:65422: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.0.128:38398: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.1.129:10674: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.0.0:65514: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.1.129:10714: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.0.0:1058: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.0.128:38602: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.1.129:10834: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.0.0:1180: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.0.128:38658: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.1.129:10882: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.0.128:38696: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.1.129:10920: EOF"
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.0.0:1300: EOF"
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.0.128:38862: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.1.129:11100: EOF"
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.1.129:11160: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.0.0:1428: EOF"
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.0.128:39082: EOF"
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.1.129:11332: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.0.0:1550: EOF"
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.0.128:39234: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.0.0:1740: EOF"
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.0.0:1932: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:57Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:57Z" level=debug msg="http: TLS handshake error from 10.244.0.128:39460: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:57Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:57Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:57Z" level=debug msg="http: TLS handshake error from 10.244.0.0:2208: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:57Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:57Z" level=debug msg="http: TLS handshake error from 10.244.0.128:39890: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:57Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:57Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:57Z" level=debug msg="http: TLS handshake error from 10.244.0.0:2570: EOF"
time="2022-01-13T09:09:57Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:57Z" level=debug msg="http: TLS handshake error from 10.244.0.128:40104: EOF"
time="2022-01-13T09:09:58Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:58Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:58Z" level=debug msg="http: TLS handshake error from 10.244.0.0:2710: EOF"
time="2022-01-13T09:09:58Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:58Z" level=debug msg="http: TLS handshake error from 10.244.0.128:40246: EOF"
time="2022-01-13T09:09:58Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:58Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:58Z" level=debug msg="http: TLS handshake error from 10.244.0.0:2976: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:58Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:58Z" level=debug msg="http: TLS handshake error from 10.244.0.128:40666: EOF"
time="2022-01-13T09:09:58Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:58Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:58Z" level=debug msg="http: TLS handshake error from 10.244.0.0:3254: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:59Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:59Z" level=debug msg="http: TLS handshake error from 10.244.0.128:40814: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:59Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:59Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:59Z" level=debug msg="http: TLS handshake error from 10.244.0.0:3394: EOF"
time="2022-01-13T09:09:59Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:59Z" level=debug msg="http: TLS handshake error from 10.244.0.128:40976: EOF"
time="2022-01-13T09:09:59Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:59Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:59Z" level=debug msg="http: TLS handshake error from 10.244.0.0:3562: EOF"
time="2022-01-13T09:09:59Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:00Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:00Z" level=debug msg="http: TLS handshake error from 10.244.0.128:41090: EOF"
time="2022-01-13T09:10:00Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:00Z" level=debug msg="http: TLS handshake error from 10.244.0.0:3734: EOF"
time="2022-01-13T09:10:00Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:00Z" level=debug msg="http: TLS handshake error from 10.244.0.128:41316: EOF"
time="2022-01-13T09:10:00Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:00Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:00Z" level=debug msg="http: TLS handshake error from 10.244.0.0:4368: EOF"
time="2022-01-13T09:10:00Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:00Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:00Z" level=debug msg="http: TLS handshake error from 10.244.0.128:41964: EOF"
time="2022-01-13T09:10:00Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:01Z" level=debug msg="http: TLS handshake error from 10.244.0.0:4540: EOF"
time="2022-01-13T09:10:01Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:01Z" level=debug msg="http: TLS handshake error from 10.244.0.128:42108: EOF"
time="2022-01-13T09:10:01Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:01Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:01Z" level=debug msg="http: TLS handshake error from 10.244.0.0:4820: EOF"
time="2022-01-13T09:10:01Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:01Z" level=debug msg="http: TLS handshake error from 10.244.0.128:42492: EOF"
time="2022-01-13T09:10:01Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:01Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:01Z" level=debug msg="http: TLS handshake error from 10.244.0.0:5062: EOF"
time="2022-01-13T09:10:01Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:02Z" level=debug msg="http: TLS handshake error from 10.244.0.128:42628: EOF"
time="2022-01-13T09:10:02Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:02Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:02Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:03Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:03Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:03Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:04Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:04Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:05Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:05Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:06Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:07Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T10:20:48Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T10:31:49Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T10:31:49Z" level=debug msg="http: TLS handshake error from 10.244.0.0:3614: EOF"
time="2022-01-13T10:31:51Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
[13:January:2022:10:33:49]:(dev_eu-frankfurt-1_controlplane):~
○ $
[13:January:2022:11:28:32]:(dev_eu-frankfurt-1_controlplane):~
○ $
[13:January:2022:11:40:13]:(dev_eu-frankfurt-1_controlplane):~
○ $ kln traefik-test traefik-6ff7ff8ffd-cxgdn
time="2022-01-13T00:01:50Z" level=info msg="Configuration loaded from flags."
time="2022-01-13T00:01:50Z" level=info msg="Traefik version 2.5.3 built on 2021-09-20T15:43:56Z"
time="2022-01-13T00:01:50Z" level=debug msg="Static configuration loaded {\"global\":{\"checkNewVersion\":true,\"sendAnonymousUsage\":true},\"serversTransport\":{\"maxIdleConnsPerHost\":200},\"entryPoints\":{\"metrics\":{\"address\":\":9100/tcp\",\"transport\":{\"lifeCycle\":{\"graceTimeOut\":\"10s\"},\"respondingTimeouts\":{\"idleTimeout\":\"3m0s\"}},\"forwardedHeaders\":{},\"http\":{},\"udp\":{\"timeout\":\"3s\"}},\"traefik\":{\"address\":\":9000/tcp\",\"transport\":{\"lifeCycle\":{\"graceTimeOut\":\"10s\"},\"respondingTimeouts\":{\"idleTimeout\":\"3m0s\"}},\"forwardedHeaders\":{},\"http\":{},\"udp\":{\"timeout\":\"3s\"}},\"web\":{\"address\":\":8000/tcp\",\"transport\":{\"lifeCycle\":{\"graceTimeOut\":\"10s\"},\"respondingTimeouts\":{\"idleTimeout\":\"3m0s\"}},\"forwardedHeaders\":{},\"http\":{},\"udp\":{\"timeout\":\"3s\"}},\"websecure\":{\"address\":\":8443/tcp\",\"transport\":{\"lifeCycle\":{\"graceTimeOut\":\"10s\"},\"respondingTimeouts\":{\"idleTimeout\":\"3m0s\"}},\"forwardedHeaders\":{},\"http\":{\"tls\":{}},\"udp\":{\"timeout\":\"3s\"}}},\"providers\":{\"providersThrottleDuration\":\"2s\",\"file\":{\"watch\":true,\"filename\":\"/config/traefik.yaml\"},\"kubernetesIngress\":{\"ingressClass\":\"traefik-kafka\"},\"kubernetesCRD\":{}},\"api\":{\"dashboard\":true},\"metrics\":{\"prometheus\":{\"buckets\":[0.1,0.3,1.2,5],\"addEntryPointsLabels\":true,\"addServicesLabels\":true,\"entryPoint\":\"metrics\"}},\"ping\":{\"entryPoint\":\"traefik\",\"terminatingStatusCode\":503},\"log\":{\"level\":\"DEBUG\",\"format\":\"common\"},\"pilot\":{\"dashboard\":true}}"
time="2022-01-13T00:01:50Z" level=info msg="Stats collection is enabled."
time="2022-01-13T00:01:50Z" level=info msg="Many thanks for contributing to Traefik's improvement by allowing us to receive anonymous information from your configuration."
time="2022-01-13T00:01:50Z" level=info msg="Help us improve Traefik by leaving this feature on :)"
time="2022-01-13T00:01:50Z" level=info msg="More details on: https://doc.traefik.io/traefik/contributing/data-collection/"
time="2022-01-13T00:01:50Z" level=debug msg="Configured Prometheus metrics" metricsProviderName=prometheus
time="2022-01-13T00:01:50Z" level=debug msg="Start TCP Server" entryPointName=metrics
time="2022-01-13T00:01:50Z" level=debug msg="Start TCP Server" entryPointName=traefik
time="2022-01-13T00:01:50Z" level=info msg="Starting provider aggregator.ProviderAggregator {}"
time="2022-01-13T00:01:50Z" level=debug msg="Start TCP Server" entryPointName=websecure
time="2022-01-13T00:01:50Z" level=debug msg="Start TCP Server" entryPointName=web
time="2022-01-13T00:01:50Z" level=info msg="Starting provider *file.Provider {\"watch\":true,\"filename\":\"/config/traefik.yaml\"}"
time="2022-01-13T00:01:50Z" level=error msg="Cannot start the provider *file.Provider: error reading configuration file: /config/traefik.yaml - open /config/traefik.yaml: no such file or directory"
time="2022-01-13T00:01:50Z" level=info msg="Starting provider *traefik.Provider {}"
time="2022-01-13T00:01:50Z" level=info msg="Starting provider *crd.Provider {}"
time="2022-01-13T00:01:50Z" level=info msg="label selector is: \"\"" providerName=kubernetescrd
time="2022-01-13T00:01:50Z" level=info msg="Creating in-cluster Provider client" providerName=kubernetescrd
time="2022-01-13T00:01:50Z" level=info msg="Starting provider *acme.ChallengeTLSALPN {\"Timeout\":4000000000}"
time="2022-01-13T00:01:50Z" level=info msg="Starting provider *ingress.Provider {\"ingressClass\":\"traefik-kafka\"}"
time="2022-01-13T00:01:50Z" level=info msg="ingress label selector is: \"\"" providerName=kubernetes
time="2022-01-13T00:01:50Z" level=info msg="Creating in-cluster Provider client" providerName=kubernetes
time="2022-01-13T00:01:50Z" level=debug msg="Configuration received from provider internal: {\"http\":{\"routers\":{\"ping\":{\"entryPoints\":[\"traefik\"],\"service\":\"ping@internal\",\"rule\":\"PathPrefix(`/ping`)\",\"priority\":2147483647},\"prometheus\":{\"entryPoints\":[\"metrics\"],\"service\":\"prometheus@internal\",\"rule\":\"PathPrefix(`/metrics`)\",\"priority\":2147483647}},\"services\":{\"api\":{},\"dashboard\":{},\"noop\":{},\"ping\":{},\"prometheus\":{}},\"models\":{\"websecure\":{\"tls\":{}}},\"serversTransports\":{\"default\":{\"maxIdleConnsPerHost\":200}}},\"tcp\":{},\"tls\":{}}" providerName=internal
time="2022-01-13T00:01:50Z" level=debug msg="No default certificate, generating one" tlsStoreName=default
time="2022-01-13T00:01:50Z" level=debug msg="Configuration received from provider kubernetescrd: {\"http\":{\"routers\":{\"traefik-test-traefik-dashboard-d012b7f875133eeab4e5\":{\"entryPoints\":[\"traefik\"],\"service\":\"api@internal\",\"rule\":\"PathPrefix(`/dashboard`) || PathPrefix(`/api`)\"}}},\"tcp\":{},\"udp\":{},\"tls\":{}}" providerName=kubernetescrd
time="2022-01-13T00:01:50Z" level=debug msg="Added outgoing tracing middleware prometheus@internal" middlewareType=TracingForwarder entryPointName=metrics routerName=prometheus@internal middlewareName=tracing
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T00:01:50Z" level=debug msg="Added outgoing tracing middleware ping@internal" entryPointName=traefik routerName=ping@internal middlewareName=tracing middlewareType=TracingForwarder
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" middlewareName=traefik-internal-recovery middlewareType=Recovery entryPointName=traefik
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=web middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=websecure
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=traefik
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=web middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:50Z" level=debug msg="No default certificate, generating one" tlsStoreName=default
time="2022-01-13T00:01:50Z" level=debug msg="Added outgoing tracing middleware prometheus@internal" middlewareName=tracing middlewareType=TracingForwarder entryPointName=metrics routerName=prometheus@internal
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T00:01:50Z" level=debug msg="Added outgoing tracing middleware api@internal" routerName=traefik-test-traefik-dashboard-d012b7f875133eeab4e5@kubernetescrd entryPointName=traefik middlewareType=TracingForwarder middlewareName=tracing
time="2022-01-13T00:01:50Z" level=debug msg="Added outgoing tracing middleware ping@internal" entryPointName=traefik middlewareName=tracing middlewareType=TracingForwarder routerName=ping@internal
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=web
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" middlewareType=Metrics entryPointName=websecure middlewareName=metrics-entrypoint
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" middlewareType=Metrics middlewareName=metrics-entrypoint entryPointName=metrics
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=web middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareType=Metrics middlewareName=metrics-entrypoint
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:50Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:51Z" level=debug msg="Configuration received from provider kubernetes: {\"http\":{},\"tcp\":{}}" providerName=kubernetes
time="2022-01-13T00:01:51Z" level=debug msg="No default certificate, generating one" tlsStoreName=default
time="2022-01-13T00:01:51Z" level=debug msg="Configuration received from provider kubernetescrd: {\"http\":{},\"tcp\":{},\"udp\":{},\"tls\":{}}" providerName=kubernetescrd
time="2022-01-13T00:01:51Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T00:01:51Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T00:01:51Z" level=debug msg="Added outgoing tracing middleware prometheus@internal" routerName=prometheus@internal middlewareType=TracingForwarder middlewareName=tracing entryPointName=metrics
time="2022-01-13T00:01:51Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T00:01:51Z" level=debug msg="Added outgoing tracing middleware ping@internal" entryPointName=traefik routerName=ping@internal middlewareName=tracing middlewareType=TracingForwarder
time="2022-01-13T00:01:51Z" level=debug msg="Added outgoing tracing middleware api@internal" middlewareName=tracing middlewareType=TracingForwarder entryPointName=traefik routerName=traefik-test-traefik-dashboard-d012b7f875133eeab4e5@kubernetescrd
time="2022-01-13T00:01:51Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T00:01:51Z" level=debug msg="Creating middleware" entryPointName=web middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:51Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=websecure
time="2022-01-13T00:01:51Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:51Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:51Z" level=debug msg="Creating middleware" middlewareType=Metrics middlewareName=metrics-entrypoint entryPointName=web
time="2022-01-13T00:01:51Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=websecure
time="2022-01-13T00:01:51Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:51Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:52Z" level=debug msg="No default certificate, generating one" tlsStoreName=default
time="2022-01-13T00:01:53Z" level=debug msg="Added outgoing tracing middleware prometheus@internal" middlewareType=TracingForwarder routerName=prometheus@internal entryPointName=metrics middlewareName=tracing
time="2022-01-13T00:01:53Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T00:01:53Z" level=debug msg="Added outgoing tracing middleware ping@internal" middlewareName=tracing middlewareType=TracingForwarder entryPointName=traefik routerName=ping@internal
time="2022-01-13T00:01:53Z" level=debug msg="Creating middleware" middlewareName=traefik-internal-recovery middlewareType=Recovery entryPointName=traefik
time="2022-01-13T00:01:53Z" level=debug msg="Creating middleware" entryPointName=web middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:53Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:53Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:53Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=traefik
time="2022-01-13T00:01:53Z" level=debug msg="Creating middleware" entryPointName=web middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:53Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:53Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareType=Metrics middlewareName=metrics-entrypoint
time="2022-01-13T00:01:53Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:54Z" level=debug msg="Configuration received from provider kubernetescrd: {\"http\":{\"routers\":{\"traefik-test-traefik-dashboard-d012b7f875133eeab4e5\":{\"entryPoints\":[\"traefik\"],\"service\":\"api@internal\",\"rule\":\"PathPrefix(`/dashboard`) || PathPrefix(`/api`)\"}}},\"tcp\":{},\"udp\":{},\"tls\":{}}" providerName=kubernetescrd
time="2022-01-13T00:01:54Z" level=debug msg="No default certificate, generating one" tlsStoreName=default
time="2022-01-13T00:01:54Z" level=debug msg="Added outgoing tracing middleware prometheus@internal" middlewareType=TracingForwarder entryPointName=metrics routerName=prometheus@internal middlewareName=tracing
time="2022-01-13T00:01:54Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T00:01:54Z" level=debug msg="Added outgoing tracing middleware api@internal" entryPointName=traefik routerName=traefik-test-traefik-dashboard-d012b7f875133eeab4e5@kubernetescrd middlewareType=TracingForwarder middlewareName=tracing
time="2022-01-13T00:01:54Z" level=debug msg="Added outgoing tracing middleware ping@internal" middlewareType=TracingForwarder entryPointName=traefik routerName=ping@internal middlewareName=tracing
time="2022-01-13T00:01:54Z" level=debug msg="Creating middleware" middlewareName=traefik-internal-recovery middlewareType=Recovery entryPointName=traefik
time="2022-01-13T00:01:54Z" level=debug msg="Creating middleware" entryPointName=web middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:54Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:54Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:54Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=traefik
time="2022-01-13T00:01:54Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=web
time="2022-01-13T00:01:54Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:01:54Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=metrics
time="2022-01-13T00:01:54Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T00:02:04Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T00:02:04Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T00:03:54Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T00:03:54Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T00:11:50Z" level=info msg="Anonymous stats sent to https://collect.traefik.io/9vxmmkcdmalbdi635d4jgc5p5rx0h7h8: {\"global\":{\"checkNewVersion\":true,\"sendAnonymousUsage\":true},\"serversTransport\":{\"maxIdleConnsPerHost\":200},\"entryPoints\":{\"metrics\":{\"address\":\"xxxx\",\"transport\":{\"lifeCycle\":{\"graceTimeOut\":\"10s\"},\"respondingTimeouts\":{\"idleTimeout\":\"3m0s\"}},\"forwardedHeaders\":{},\"http\":{}},\"traefik\":{\"address\":\"xxxx\",\"transport\":{\"lifeCycle\":{\"graceTimeOut\":\"10s\"},\"respondingTimeouts\":{\"idleTimeout\":\"3m0s\"}},\"forwardedHeaders\":{},\"http\":{}},\"web\":{\"address\":\"xxxx\",\"transport\":{\"lifeCycle\":{\"graceTimeOut\":\"10s\"},\"respondingTimeouts\":{\"idleTimeout\":\"3m0s\"}},\"forwardedHeaders\":{},\"http\":{}},\"websecure\":{\"address\":\"xxxx\",\"transport\":{\"lifeCycle\":{\"graceTimeOut\":\"10s\"},\"respondingTimeouts\":{\"idleTimeout\":\"3m0s\"}},\"forwardedHeaders\":{},\"http\":{\"tls\":{}}}},\"providers\":{\"providersThrottleDuration\":\"2s\",\"file\":{\"watch\":true,\"filename\":\"/config/traefik.yaml\"},\"kubernetesIngress\":{\"ingressClass\":\"traefik-kafka\"},\"kubernetesCRD\":{}},\"api\":{\"dashboard\":true},\"metrics\":{\"prometheus\":{\"buckets\":[0.1,0.3,1.2,5],\"addEntryPointsLabels\":true,\"addServicesLabels\":true,\"entryPoint\":\"metrics\"}},\"ping\":{\"entryPoint\":\"traefik\",\"terminatingStatusCode\":503},\"log\":{\"level\":\"DEBUG\",\"format\":\"common\"},\"pilot\":{}}"
time="2022-01-13T00:11:50Z" level=debug msg="unknown kind to hash: func"
time="2022-01-13T00:11:50Z" level=warning msg="A new release has been found: 2.5.6. Please consider updating."
time="2022-01-13T00:28:20Z" level=debug msg="Serving default certificate for request: \"147.154.15.167:443\""
time="2022-01-13T00:57:18Z" level=debug msg="Serving default certificate for request: \"147.154.15.167:80\""
time="2022-01-13T00:59:49Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T00:59:49Z" level=debug msg="http: TLS handshake error from 10.244.0.0:1960: remote error: tls: bad certificate"
time="2022-01-13T00:59:49Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T01:19:47Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T01:19:47Z" level=debug msg="http: TLS handshake error from 10.244.1.129:26484: remote error: tls: bad certificate"
time="2022-01-13T01:19:47Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T01:20:12Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T01:22:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T01:29:36Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T01:29:44Z" level=debug msg="http: TLS handshake error from 10.244.0.0:48048: EOF"
time="2022-01-13T01:33:14Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T01:35:30Z" level=debug msg="http: TLS handshake error from 10.244.1.129:55296: tls: client offered only unsupported versions: []"
time="2022-01-13T01:36:12Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T01:36:12Z" level=debug msg="http: TLS handshake error from 10.244.0.0:12506: tls: no cipher suite supported by both client and server"
time="2022-01-13T01:38:09Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T01:38:10Z" level=debug msg="http: TLS handshake error from 10.244.0.128:5722: EOF"
time="2022-01-13T01:38:10Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T01:38:11Z" level=debug msg="http: TLS handshake error from 10.244.1.129:43126: EOF"
time="2022-01-13T01:38:11Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T01:38:11Z" level=debug msg="http: TLS handshake error from 10.244.0.0:33680: EOF"
time="2022-01-13T01:38:11Z" level=debug msg="http: TLS handshake error from 10.244.0.128:7666: tls: client requested unsupported application protocols ([http/0.9 http/1.0 spdy/1 spdy/2 spdy/3 h2c hq])"
time="2022-01-13T01:38:11Z" level=debug msg="http: TLS handshake error from 10.244.1.129:44466: tls: client requested unsupported application protocols ([hq h2c spdy/3 spdy/2 spdy/1 http/1.0 http/0.9])"
time="2022-01-13T01:38:12Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T01:38:12Z" level=debug msg="http: TLS handshake error from 10.244.0.0:34884: EOF"
time="2022-01-13T01:38:12Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T01:38:13Z" level=debug msg="http: TLS handshake error from 10.244.0.128:8124: EOF"
time="2022-01-13T01:38:13Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T01:38:13Z" level=debug msg="http: TLS handshake error from 10.244.1.129:45138: EOF"
time="2022-01-13T01:38:13Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T01:38:14Z" level=debug msg="http: TLS handshake error from 10.244.0.0:35500: EOF"
time="2022-01-13T01:38:14Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T01:38:14Z" level=debug msg="http: TLS handshake error from 10.244.0.128:8994: EOF"
time="2022-01-13T01:41:13Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T01:49:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T02:14:00Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T02:14:00Z" level=debug msg="http: TLS handshake error from 10.244.0.128:26860: EOF"
time="2022-01-13T03:23:38Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T03:39:12Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T03:39:12Z" level=debug msg="http: TLS handshake error from 10.244.0.0:24762: EOF"
time="2022-01-13T04:14:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T04:14:54Z" level=debug msg="http: TLS handshake error from 10.244.1.129:38592: EOF"
time="2022-01-13T04:30:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:12270: tls: client offered only unsupported versions: []"
time="2022-01-13T04:40:08Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:02:19Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka servicePort="{0 9095 }" serviceName=kf-kafka-kafka-0 providerName=kubernetescrd ingress=kf-kafka-kafka-0
time="2022-01-13T05:02:19Z" level=debug msg="Configuration received from provider kubernetescrd: {\"http\":{\"routers\":{\"traefik-test-traefik-dashboard-d012b7f875133eeab4e5\":{\"entryPoints\":[\"traefik\"],\"service\":\"api@internal\",\"rule\":\"PathPrefix(`/dashboard`) || PathPrefix(`/api`)\"}}},\"tcp\":{\"routers\":{\"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0\":{\"entryPoints\":[\"websecure\"],\"service\":\"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0\",\"rule\":\"HostSNI(`kf-kafka-kafka-0.service.test`)\",\"tls\":{\"passthrough\":true}}}},\"udp\":{},\"tls\":{}}" providerName=kubernetescrd
time="2022-01-13T05:02:19Z" level=debug msg="No default certificate, generating one" tlsStoreName=default
time="2022-01-13T05:02:19Z" level=debug msg="Added outgoing tracing middleware ping@internal" middlewareName=tracing middlewareType=TracingForwarder entryPointName=traefik routerName=ping@internal
time="2022-01-13T05:02:19Z" level=debug msg="Added outgoing tracing middleware api@internal" entryPointName=traefik routerName=traefik-test-traefik-dashboard-d012b7f875133eeab4e5@kubernetescrd middlewareName=tracing middlewareType=TracingForwarder
time="2022-01-13T05:02:19Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareType=Recovery middlewareName=traefik-internal-recovery
time="2022-01-13T05:02:19Z" level=debug msg="Added outgoing tracing middleware prometheus@internal" routerName=prometheus@internal middlewareName=tracing middlewareType=TracingForwarder entryPointName=metrics
time="2022-01-13T05:02:19Z" level=debug msg="Creating middleware" middlewareName=traefik-internal-recovery middlewareType=Recovery entryPointName=metrics
time="2022-01-13T05:02:19Z" level=debug msg="Creating middleware" middlewareType=Metrics entryPointName=web middlewareName=metrics-entrypoint
time="2022-01-13T05:02:19Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:19Z" level=debug msg="Creating middleware" middlewareType=Metrics middlewareName=metrics-entrypoint entryPointName=metrics
time="2022-01-13T05:02:19Z" level=debug msg="Creating middleware" middlewareType=Metrics entryPointName=traefik middlewareName=metrics-entrypoint
time="2022-01-13T05:02:19Z" level=debug msg="Creating middleware" middlewareType=Metrics entryPointName=web middlewareName=metrics-entrypoint
time="2022-01-13T05:02:19Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:19Z" level=debug msg="Creating middleware" middlewareType=Metrics entryPointName=metrics middlewareName=metrics-entrypoint
time="2022-01-13T05:02:19Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:19Z" level=error msg="the service \"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0@kubernetescrd\" does not exist" entryPointName=websecure routerName=kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0@kubernetescrd
time="2022-01-13T05:02:45Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }"
time="2022-01-13T05:02:45Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd namespace=kafka ingress=kf-kafka-kafka-1 serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T05:02:45Z" level=debug msg="Configuration received from provider kubernetescrd: {\"http\":{\"routers\":{\"traefik-test-traefik-dashboard-d012b7f875133eeab4e5\":{\"entryPoints\":[\"traefik\"],\"service\":\"api@internal\",\"rule\":\"PathPrefix(`/dashboard`) || PathPrefix(`/api`)\"}}},\"tcp\":{\"routers\":{\"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0\":{\"entryPoints\":[\"websecure\"],\"service\":\"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0\",\"rule\":\"HostSNI(`kf-kafka-kafka-0.service.test`)\",\"tls\":{\"passthrough\":true}},\"kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d\":{\"entryPoints\":[\"websecure\"],\"service\":\"kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d\",\"rule\":\"HostSNI(`kf-kafka-kafka-1.service.test`)\",\"tls\":{\"passthrough\":true}}}},\"udp\":{},\"tls\":{}}" providerName=kubernetescrd
time="2022-01-13T05:02:45Z" level=debug msg="No default certificate, generating one" tlsStoreName=default
time="2022-01-13T05:02:45Z" level=debug msg="Added outgoing tracing middleware ping@internal" routerName=ping@internal middlewareName=tracing middlewareType=TracingForwarder entryPointName=traefik
time="2022-01-13T05:02:45Z" level=debug msg="Added outgoing tracing middleware api@internal" routerName=traefik-test-traefik-dashboard-d012b7f875133eeab4e5@kubernetescrd entryPointName=traefik middlewareName=tracing middlewareType=TracingForwarder
time="2022-01-13T05:02:45Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T05:02:45Z" level=debug msg="Added outgoing tracing middleware prometheus@internal" middlewareName=tracing middlewareType=TracingForwarder entryPointName=metrics routerName=prometheus@internal
time="2022-01-13T05:02:45Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T05:02:45Z" level=debug msg="Creating middleware" entryPointName=web middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:45Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=websecure
time="2022-01-13T05:02:45Z" level=debug msg="Creating middleware" middlewareType=Metrics entryPointName=metrics middlewareName=metrics-entrypoint
time="2022-01-13T05:02:45Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareType=Metrics middlewareName=metrics-entrypoint
time="2022-01-13T05:02:45Z" level=debug msg="Creating middleware" entryPointName=web middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:45Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:45Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:45Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:45Z" level=error msg="the service \"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0@kubernetescrd\" does not exist" entryPointName=websecure routerName=kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0@kubernetescrd
time="2022-01-13T05:02:45Z" level=error msg="the service \"kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d@kubernetescrd\" does not exist" entryPointName=websecure routerName=kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d@kubernetescrd
time="2022-01-13T05:02:57Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka
time="2022-01-13T05:02:57Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka servicePort="{0 9095 }" serviceName=kf-kafka-kafka-0
time="2022-01-13T05:02:57Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T05:02:57Z" level=debug msg="Configuration received from provider kubernetescrd: {\"http\":{\"routers\":{\"traefik-test-traefik-dashboard-d012b7f875133eeab4e5\":{\"entryPoints\":[\"traefik\"],\"service\":\"api@internal\",\"rule\":\"PathPrefix(`/dashboard`) || PathPrefix(`/api`)\"}}},\"tcp\":{\"routers\":{\"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0\":{\"entryPoints\":[\"websecure\"],\"service\":\"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0\",\"rule\":\"HostSNI(`kf-kafka-kafka-0.service.test`)\",\"tls\":{\"passthrough\":true}},\"kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d\":{\"entryPoints\":[\"websecure\"],\"service\":\"kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d\",\"rule\":\"HostSNI(`kf-kafka-kafka-1.service.test`)\",\"tls\":{\"passthrough\":true}},\"kafka-kf-kafka-kafka-2-8b70efc5c7b1fbc44615\":{\"entryPoints\":[\"websecure\"],\"service\":\"kafka-kf-kafka-kafka-2-8b70efc5c7b1fbc44615\",\"rule\":\"HostSNI(`kf-kafka-kafka-2.service.test`)\",\"tls\":{\"passthrough\":true}}}},\"udp\":{},\"tls\":{}}" providerName=kubernetescrd
time="2022-01-13T05:02:57Z" level=debug msg="No default certificate, generating one" tlsStoreName=default
time="2022-01-13T05:02:57Z" level=debug msg="Added outgoing tracing middleware prometheus@internal" middlewareType=TracingForwarder entryPointName=metrics routerName=prometheus@internal middlewareName=tracing
time="2022-01-13T05:02:57Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T05:02:57Z" level=debug msg="Added outgoing tracing middleware ping@internal" middlewareType=TracingForwarder middlewareName=tracing entryPointName=traefik routerName=ping@internal
time="2022-01-13T05:02:57Z" level=debug msg="Added outgoing tracing middleware api@internal" middlewareType=TracingForwarder entryPointName=traefik routerName=traefik-test-traefik-dashboard-d012b7f875133eeab4e5@kubernetescrd middlewareName=tracing
time="2022-01-13T05:02:57Z" level=debug msg="Creating middleware" middlewareName=traefik-internal-recovery middlewareType=Recovery entryPointName=traefik
time="2022-01-13T05:02:57Z" level=debug msg="Creating middleware" entryPointName=web middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:57Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareType=Metrics middlewareName=metrics-entrypoint
time="2022-01-13T05:02:57Z" level=debug msg="Creating middleware" middlewareType=Metrics entryPointName=metrics middlewareName=metrics-entrypoint
time="2022-01-13T05:02:57Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:57Z" level=debug msg="Creating middleware" middlewareType=Metrics entryPointName=web middlewareName=metrics-entrypoint
time="2022-01-13T05:02:57Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:57Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:02:57Z" level=debug msg="Creating middleware" middlewareType=Metrics middlewareName=metrics-entrypoint entryPointName=traefik
time="2022-01-13T05:02:57Z" level=error msg="the service \"kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d@kubernetescrd\" does not exist" routerName=kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d@kubernetescrd entryPointName=websecure
time="2022-01-13T05:02:57Z" level=error msg="the service \"kafka-kf-kafka-kafka-2-8b70efc5c7b1fbc44615@kubernetescrd\" does not exist" entryPointName=websecure routerName=kafka-kf-kafka-kafka-2-8b70efc5c7b1fbc44615@kubernetescrd
time="2022-01-13T05:02:57Z" level=error msg="the service \"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0@kubernetescrd\" does not exist" entryPointName=websecure routerName=kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0@kubernetescrd
time="2022-01-13T05:03:05Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka providerName=kubernetescrd serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" ingress=kf-kafka-kafka-0
time="2022-01-13T05:03:05Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd serviceName=kf-kafka-kafka-1 ingress=kf-kafka-kafka-1 servicePort="{0 9095 }" namespace=kafka
time="2022-01-13T05:03:05Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }"
time="2022-01-13T05:03:05Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }"
time="2022-01-13T05:03:05Z" level=debug msg="Configuration received from provider kubernetescrd: {\"http\":{\"routers\":{\"traefik-test-traefik-dashboard-d012b7f875133eeab4e5\":{\"entryPoints\":[\"traefik\"],\"service\":\"api@internal\",\"rule\":\"PathPrefix(`/dashboard`) || PathPrefix(`/api`)\"}}},\"tcp\":{\"routers\":{\"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0\":{\"entryPoints\":[\"websecure\"],\"service\":\"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0\",\"rule\":\"HostSNI(`kf-kafka-kafka-0.service.test`)\",\"tls\":{\"passthrough\":true}},\"kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d\":{\"entryPoints\":[\"websecure\"],\"service\":\"kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d\",\"rule\":\"HostSNI(`kf-kafka-kafka-1.service.test`)\",\"tls\":{\"passthrough\":true}},\"kafka-kf-kafka-kafka-2-8b70efc5c7b1fbc44615\":{\"entryPoints\":[\"websecure\"],\"service\":\"kafka-kf-kafka-kafka-2-8b70efc5c7b1fbc44615\",\"rule\":\"HostSNI(`kf-kafka-kafka-2.service.test`)\",\"tls\":{\"passthrough\":true}},\"kafka-kf-kafka-kafka-bootstrap-7651f1a5984e92618069\":{\"entryPoints\":[\"websecure\"],\"service\":\"kafka-kf-kafka-kafka-bootstrap-7651f1a5984e92618069\",\"rule\":\"HostSNI(`kf-kafka-kafka-bootstrap.service.test`)\",\"tls\":{\"passthrough\":true}}}},\"udp\":{},\"tls\":{}}" providerName=kubernetescrd
time="2022-01-13T05:03:05Z" level=debug msg="No default certificate, generating one" tlsStoreName=default
time="2022-01-13T05:03:05Z" level=debug msg="Added outgoing tracing middleware prometheus@internal" middlewareType=TracingForwarder entryPointName=metrics routerName=prometheus@internal middlewareName=tracing
time="2022-01-13T05:03:05Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T05:03:05Z" level=debug msg="Added outgoing tracing middleware ping@internal" middlewareName=tracing middlewareType=TracingForwarder entryPointName=traefik routerName=ping@internal
time="2022-01-13T05:03:05Z" level=debug msg="Added outgoing tracing middleware api@internal" entryPointName=traefik routerName=traefik-test-traefik-dashboard-d012b7f875133eeab4e5@kubernetescrd middlewareName=tracing middlewareType=TracingForwarder
time="2022-01-13T05:03:05Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=traefik-internal-recovery middlewareType=Recovery
time="2022-01-13T05:03:05Z" level=debug msg="Creating middleware" entryPointName=web middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:03:05Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=websecure
time="2022-01-13T05:03:05Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:03:05Z" level=debug msg="Creating middleware" entryPointName=traefik middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:03:05Z" level=debug msg="Creating middleware" middlewareName=metrics-entrypoint middlewareType=Metrics entryPointName=web
time="2022-01-13T05:03:05Z" level=debug msg="Creating middleware" entryPointName=websecure middlewareType=Metrics middlewareName=metrics-entrypoint
time="2022-01-13T05:03:05Z" level=debug msg="Creating middleware" entryPointName=metrics middlewareName=metrics-entrypoint middlewareType=Metrics
time="2022-01-13T05:03:05Z" level=debug msg="Creating middleware" middlewareType=Metrics entryPointName=traefik middlewareName=metrics-entrypoint
time="2022-01-13T05:03:05Z" level=error msg="the service \"kafka-kf-kafka-kafka-bootstrap-7651f1a5984e92618069@kubernetescrd\" does not exist" routerName=kafka-kf-kafka-kafka-bootstrap-7651f1a5984e92618069@kubernetescrd entryPointName=websecure
time="2022-01-13T05:03:05Z" level=error msg="the service \"kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0@kubernetescrd\" does not exist" entryPointName=websecure routerName=kafka-kf-kafka-kafka-0-bf19cd7b9cacbb72eaf0@kubernetescrd
time="2022-01-13T05:03:05Z" level=error msg="the service \"kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d@kubernetescrd\" does not exist" entryPointName=websecure routerName=kafka-kf-kafka-kafka-1-a53ce594809f8953ad3d@kubernetescrd
time="2022-01-13T05:03:05Z" level=error msg="the service \"kafka-kf-kafka-kafka-2-8b70efc5c7b1fbc44615@kubernetescrd\" does not exist" entryPointName=websecure routerName=kafka-kf-kafka-kafka-2-8b70efc5c7b1fbc44615@kubernetescrd
time="2022-01-13T05:10:35Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:10:35Z" level=debug msg="http: TLS handshake error from 10.244.1.129:57276: EOF"
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" ingress=kf-kafka-kafka-1 namespace=kafka
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-bootstrap ingress=kf-kafka-kafka-bootstrap namespace=kafka providerName=kubernetescrd servicePort="{0 9095 }"
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-0
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka servicePort="{0 9095 }" serviceName=kf-kafka-kafka-1
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap providerName=kubernetescrd servicePort="{0 9095 }"
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" namespace=kafka providerName=kubernetescrd ingress=kf-kafka-kafka-2
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 providerName=kubernetescrd servicePort="{0 9095 }" namespace=kafka ingress=kf-kafka-kafka-0
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" ingress=kf-kafka-kafka-2 namespace=kafka providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd namespace=kafka servicePort="{0 9095 }" ingress=kf-kafka-kafka-bootstrap serviceName=kf-kafka-kafka-bootstrap
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" serviceName=kf-kafka-kafka-0 providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd namespace=kafka ingress=kf-kafka-kafka-2 serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }"
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" namespace=kafka providerName=kubernetescrd ingress=kf-kafka-kafka-0
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }"
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 namespace=kafka providerName=kubernetescrd serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-2 namespace=kafka providerName=kubernetescrd serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }"
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }"
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-bootstrap namespace=kafka providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 namespace=kafka providerName=kubernetescrd serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-2 servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-2 providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }"
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka providerName=kubernetescrd servicePort="{0 9095 }" serviceName=kf-kafka-kafka-0 ingress=kf-kafka-kafka-0
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd namespace=kafka ingress=kf-kafka-kafka-1 serviceName=kf-kafka-kafka-1
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-0 providerName=kubernetescrd ingress=kf-kafka-kafka-0 servicePort="{0 9095 }"
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" ingress=kf-kafka-kafka-2 namespace=kafka providerName=kubernetescrd serviceName=kf-kafka-kafka-2
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T05:18:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T05:41:41Z" level=debug msg="http: TLS handshake error from 10.244.0.128:20084: tls: client offered only unsupported versions: []"
time="2022-01-13T05:41:42Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="http: TLS handshake error from 10.244.0.0:56294: tls: unsupported SSLv2 handshake received"
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:48Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:49Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:49Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:49Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:49Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:49Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:49Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:49Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:49Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3342: tls: client offered only unsupported versions: []"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58176: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31132: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31138: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31092: EOF"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3404: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31194: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58238: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3380: EOF"
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58128: EOF"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3700: tls: client requested unsupported application protocols ([spdy/1 spdy/2 spdy/3 stun.turn stun.nat-discovery h2c webrtc c-webrtc ftp imap pop3 managesieve grpc-exp])"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3722: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31512: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58556: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3306: EOF"
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58218: EOF"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3746: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31536: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58580: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31560: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3780: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:50Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:50Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58614: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3812: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31596: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58646: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3836: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31622: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58670: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3878: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31668: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58720: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31782: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58826: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.1.129:3992: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31810: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.1.129:4020: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58862: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58888: tls: no cipher suite supported by both client and server"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31834: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.1.129:4054: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31860: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.1.129:4070: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58904: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31878: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.1.129:4088: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58922: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31922: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.1.129:4132: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58966: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.1.129:4158: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:31948: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:58992: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:32004: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.1.129:4228: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:59098: EOF"
time="2022-01-13T05:41:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T05:41:51Z" level=debug msg="http: TLS handshake error from 10.244.0.128:32078: EOF"
time="2022-01-13T06:15:42Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T06:15:42Z" level=debug msg="http: TLS handshake error from 10.244.0.0:30386: EOF"
time="2022-01-13T06:31:45Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T06:32:39Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T06:34:03Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T06:40:11Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd namespace=kafka ingress=kf-kafka-kafka-0 serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-0 servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-0 providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 namespace=kafka providerName=kubernetescrd servicePort="{0 9095 }" serviceName=kf-kafka-kafka-1
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-2 namespace=kafka providerName=kubernetescrd serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" providerName=kubernetescrd namespace=kafka ingress=kf-kafka-kafka-0
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka providerName=kubernetescrd ingress=kf-kafka-kafka-1 servicePort="{0 9095 }" serviceName=kf-kafka-kafka-1
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" namespace=kafka ingress=kf-kafka-kafka-2
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-1
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" serviceName=kf-kafka-kafka-2 providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka ingress=kf-kafka-kafka-bootstrap serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" ingress=kf-kafka-kafka-1 namespace=kafka providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 providerName=kubernetescrd servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 serviceName=kf-kafka-kafka-1 providerName=kubernetescrd namespace=kafka servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-1 servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-1
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka ingress=kf-kafka-kafka-bootstrap serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-0 serviceName=kf-kafka-kafka-0 namespace=kafka
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" providerName=kubernetescrd namespace=kafka ingress=kf-kafka-kafka-1
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }"
time="2022-01-13T07:28:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2 serviceName=kf-kafka-kafka-2 namespace=kafka
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-0 namespace=kafka
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-1
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-1 providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-1 namespace=kafka
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-bootstrap providerName=kubernetescrd namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-0 serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" namespace=kafka providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" serviceName=kf-kafka-kafka-0 providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-1 namespace=kafka ingress=kf-kafka-kafka-1 servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" ingress=kf-kafka-kafka-2 namespace=kafka providerName=kubernetescrd serviceName=kf-kafka-kafka-2
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-bootstrap providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-1 providerName=kubernetescrd ingress=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" ingress=kf-kafka-kafka-2 namespace=kafka providerName=kubernetescrd serviceName=kf-kafka-kafka-2
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd namespace=kafka ingress=kf-kafka-kafka-bootstrap serviceName=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-0 providerName=kubernetescrd ingress=kf-kafka-kafka-0
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd servicePort="{0 9095 }" serviceName=kf-kafka-kafka-2 namespace=kafka ingress=kf-kafka-kafka-2
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-0 providerName=kubernetescrd ingress=kf-kafka-kafka-0
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 serviceName=kf-kafka-kafka-1 namespace=kafka providerName=kubernetescrd servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2 providerName=kubernetescrd servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-0 servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-0 providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" ingress=kf-kafka-kafka-2 namespace=kafka providerName=kubernetescrd serviceName=kf-kafka-kafka-2
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" namespace=kafka providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap serviceName=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-1 serviceName=kf-kafka-kafka-1 namespace=kafka servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd namespace=kafka ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-0 providerName=kubernetescrd ingress=kf-kafka-kafka-0 servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-2 serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" providerName=kubernetescrd namespace=kafka
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-0 providerName=kubernetescrd ingress=kf-kafka-kafka-0 servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 namespace=kafka providerName=kubernetescrd serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-2 providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-2
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" namespace=kafka providerName=kubernetescrd ingress=kf-kafka-kafka-0 serviceName=kf-kafka-kafka-0
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-1 servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-1
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-2 servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-2 providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" namespace=kafka providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 providerName=kubernetescrd servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }"
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Endpoints" providerName=kubernetescrd
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:28:21Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:28:22Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:28:22Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:28:22Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T07:46:06Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T08:13:51Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T08:14:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T08:19:00Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T08:20:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T08:20:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd namespace=kafka ingress=kf-kafka-kafka-2 serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }"
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" ingress=kf-kafka-kafka-bootstrap namespace=kafka providerName=kubernetescrd
time="2022-01-13T08:20:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-1 providerName=kubernetescrd ingress=kf-kafka-kafka-1
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" namespace=kafka providerName=kubernetescrd ingress=kf-kafka-kafka-2
time="2022-01-13T08:20:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-1
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }"
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-bootstrap providerName=kubernetescrd
time="2022-01-13T08:20:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T08:20:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T08:20:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 providerName=kubernetescrd servicePort="{0 9095 }" ingress=kf-kafka-kafka-0 namespace=kafka
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-2 servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-2
time="2022-01-13T08:20:20Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }"
time="2022-01-13T08:20:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T08:20:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T08:20:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T08:20:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T08:24:46Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T08:26:20Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-bootstrap
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-0 namespace=kafka
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" ingress=kf-kafka-kafka-1 serviceName=kf-kafka-kafka-1 providerName=kubernetescrd namespace=kafka
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2
time="2022-01-13T08:36:24Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T08:36:24Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T08:36:24Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 providerName=kubernetescrd
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }"
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" providerName=kubernetescrd ingress=kf-kafka-kafka-2 namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }"
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-bootstrap servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap
time="2022-01-13T08:36:24Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" namespace=kafka serviceName=kf-kafka-kafka-2 servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-2
time="2022-01-13T08:36:24Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" serviceName=kf-kafka-kafka-bootstrap providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap namespace=kafka servicePort="{0 9095 }"
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-0 serviceName=kf-kafka-kafka-0 servicePort="{0 9095 }" namespace=kafka providerName=kubernetescrd
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-1 namespace=kafka serviceName=kf-kafka-kafka-1 servicePort="{0 9095 }" providerName=kubernetescrd
time="2022-01-13T08:36:24Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T08:36:24Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetes
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" ingress=kf-kafka-kafka-0 namespace=kafka serviceName=kf-kafka-kafka-0 providerName=kubernetescrd servicePort="{0 9095 }"
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" ingress=kf-kafka-kafka-1 namespace=kafka providerName=kubernetescrd serviceName=kf-kafka-kafka-1
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" namespace=kafka serviceName=kf-kafka-kafka-2 providerName=kubernetescrd ingress=kf-kafka-kafka-2
time="2022-01-13T08:36:24Z" level=error msg="Cannot create service: service port not found: 9095" servicePort="{0 9095 }" providerName=kubernetescrd ingress=kf-kafka-kafka-bootstrap namespace=kafka serviceName=kf-kafka-kafka-bootstrap
time="2022-01-13T08:36:24Z" level=debug msg="Skipping Kubernetes event kind *v1.Service" providerName=kubernetescrd
time="2022-01-13T08:59:34Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T08:59:34Z" level=debug msg="http: TLS handshake error from 10.244.0.0:32756: EOF"
time="2022-01-13T09:09:44Z" level=debug msg="http: TLS handshake error from 10.244.0.128:27370: tls: client offered only unsupported versions: []"
time="2022-01-13T09:09:44Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:62580: tls: unsupported SSLv2 handshake received"
time="2022-01-13T09:09:51Z" level=debug msg="http: TLS handshake error from 10.244.0.0:62884: tls: client offered only unsupported versions: []"
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:51Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.1.129:8128: EOF"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.0.128:35972: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.1.129:8188: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.1.129:8562: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.1.129:8152: EOF"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.0.128:36398: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.0.0:63372: EOF"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.0.128:36422: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.0.128:36470: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.1.129:8702: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.0.128:36530: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.1.129:8768: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.0.128:36598: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.1.129:8786: EOF"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="http: TLS handshake error from 10.244.0.0:63652: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:52Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.1.129:8860: EOF"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.0.0:63708: EOF"
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.1.129:8906: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.0.0:63752: EOF"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.0.128:36734: EOF"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.0.0:63820: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.1.129:9126: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.0.0:63966: EOF"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.1.129:9166: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.0.0:64056: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.0.128:37094: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.0.0:64178: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.0.0:64248: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.1.129:9396: EOF"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:53Z" level=debug msg="http: TLS handshake error from 10.244.1.129:9446: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:53Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.0.0:64302: EOF"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.0.128:37298: EOF"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.1.129:9584: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.1.129:9876: EOF"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.1.129:9956: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.0.0:64746: EOF"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.0.0:64814: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.0.128:37794: EOF"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.0.0:64896: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.1.129:10050: EOF"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.1.129:10078: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.0.128:38066: EOF"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.0.0:65234: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:54Z" level=debug msg="http: TLS handshake error from 10.244.1.129:10306: EOF"
time="2022-01-13T09:09:54Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.0.128:38294: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.1.129:10510: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.0.128:38338: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.1.129:10576: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.0.0:65422: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.0.128:38398: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.1.129:10674: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.0.0:65514: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.1.129:10714: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.0.0:1058: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.0.128:38602: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.1.129:10834: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.0.0:1180: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.0.128:38658: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.1.129:10882: EOF"
time="2022-01-13T09:09:55Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:55Z" level=debug msg="http: TLS handshake error from 10.244.0.128:38696: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.1.129:10920: EOF"
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.0.0:1300: EOF"
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.0.128:38862: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.1.129:11100: EOF"
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.1.129:11160: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.0.0:1428: EOF"
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.0.128:39082: EOF"
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.1.129:11332: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.0.0:1550: EOF"
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.0.128:39234: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.0.0:1740: EOF"
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:56Z" level=debug msg="http: TLS handshake error from 10.244.0.0:1932: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:57Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:57Z" level=debug msg="http: TLS handshake error from 10.244.0.128:39460: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:57Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:57Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:57Z" level=debug msg="http: TLS handshake error from 10.244.0.0:2208: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:57Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:57Z" level=debug msg="http: TLS handshake error from 10.244.0.128:39890: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:57Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:57Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:57Z" level=debug msg="http: TLS handshake error from 10.244.0.0:2570: EOF"
time="2022-01-13T09:09:57Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:57Z" level=debug msg="http: TLS handshake error from 10.244.0.128:40104: EOF"
time="2022-01-13T09:09:58Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:58Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:58Z" level=debug msg="http: TLS handshake error from 10.244.0.0:2710: EOF"
time="2022-01-13T09:09:58Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:58Z" level=debug msg="http: TLS handshake error from 10.244.0.128:40246: EOF"
time="2022-01-13T09:09:58Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:58Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:58Z" level=debug msg="http: TLS handshake error from 10.244.0.0:2976: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:58Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:58Z" level=debug msg="http: TLS handshake error from 10.244.0.128:40666: EOF"
time="2022-01-13T09:09:58Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:58Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:58Z" level=debug msg="http: TLS handshake error from 10.244.0.0:3254: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:59Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:59Z" level=debug msg="http: TLS handshake error from 10.244.0.128:40814: tls: no cipher suite supported by both client and server"
time="2022-01-13T09:09:59Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:59Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:59Z" level=debug msg="http: TLS handshake error from 10.244.0.0:3394: EOF"
time="2022-01-13T09:09:59Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:59Z" level=debug msg="http: TLS handshake error from 10.244.0.128:40976: EOF"
time="2022-01-13T09:09:59Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:59Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:09:59Z" level=debug msg="http: TLS handshake error from 10.244.0.0:3562: EOF"
time="2022-01-13T09:09:59Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:00Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:00Z" level=debug msg="http: TLS handshake error from 10.244.0.128:41090: EOF"
time="2022-01-13T09:10:00Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:00Z" level=debug msg="http: TLS handshake error from 10.244.0.0:3734: EOF"
time="2022-01-13T09:10:00Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:00Z" level=debug msg="http: TLS handshake error from 10.244.0.128:41316: EOF"
time="2022-01-13T09:10:00Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:00Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:00Z" level=debug msg="http: TLS handshake error from 10.244.0.0:4368: EOF"
time="2022-01-13T09:10:00Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:00Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:00Z" level=debug msg="http: TLS handshake error from 10.244.0.128:41964: EOF"
time="2022-01-13T09:10:00Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:01Z" level=debug msg="http: TLS handshake error from 10.244.0.0:4540: EOF"
time="2022-01-13T09:10:01Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:01Z" level=debug msg="http: TLS handshake error from 10.244.0.128:42108: EOF"
time="2022-01-13T09:10:01Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:01Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:01Z" level=debug msg="http: TLS handshake error from 10.244.0.0:4820: EOF"
time="2022-01-13T09:10:01Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:01Z" level=debug msg="http: TLS handshake error from 10.244.0.128:42492: EOF"
time="2022-01-13T09:10:01Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:01Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:01Z" level=debug msg="http: TLS handshake error from 10.244.0.0:5062: EOF"
time="2022-01-13T09:10:01Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:02Z" level=debug msg="http: TLS handshake error from 10.244.0.128:42628: EOF"
time="2022-01-13T09:10:02Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:02Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:02Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:03Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:03Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:03Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:04Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:04Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:05Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:05Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:06Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T09:10:07Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T10:20:48Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T10:31:49Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T10:31:49Z" level=debug msg="http: TLS handshake error from 10.244.0.0:3614: EOF"
time="2022-01-13T10:31:51Z" level=debug msg="Serving default certificate for request: \"147.154.15.167\""
time="2022-01-13T10:46:18Z" level=debug msg="Serving default certificate for request: \"\""
time="2022-01-13T11:37:24Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T11:39:05Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T11:39:24Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
time="2022-01-13T11:39:53Z" level=debug msg="Skipping Kubernetes event kind *v1.Ingress" providerName=kubernetes
[13:January:2022:11:40:22]:(dev_eu-frankfurt-1_controlplane):~
○ $ keytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt
bash: keytool: command not found
[13:January:2022:11:57:05]:(dev_eu-frankfurt-1_controlplane):~
○ $ exit
exit

 2022-01-13 22:00:29 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → env | grep | kube
usage: grep [-abcDEFGHhIiJLlmnOoqRSsUVvwxZ] [-A num] [-B num] [-C[num]]
  [-e pattern] [-f file] [--binary-files=value] [--color=when]
  [--context[=num]] [--directories=action] [--label] [--line-buffered]
  [--null] [pattern] [file ...]
-bash: kube: command not found

 2022-01-17 15:25:45 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → env | grep -i kube
KUBECONFIG=/Users/azhekhan/.kube/config

 2022-01-17 15:25:50 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → kgns
NAME                 STATUS   AGE
default              Active   3d17h
kube-node-lease      Active   3d17h
kube-public          Active   3d17h
kube-system          Active   3d17h
local-path-storage   Active   3d17h
traefik-apps         Active   3d17h
traefik-test         Active   2d2h

 2022-01-17 15:25:55 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → oong
{
  "data": "osvcprod"
}

 2022-01-17 15:26:07 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → ls -ltr ~/.oci/config
[DEFAULT]
-rw-------  1 azhekhan  staff  414 Aug  2 10:54 /Users/azhekhan/.oci/config

 2022-01-17 15:26:17 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → ls -ltr ~/.oci/
total 200
-rw-r--r--  1 azhekhan  staff   535 Apr 25  2019 config_peoplesoft_cm
-rw-r--r--  1 azhekhan  staff  1766 Apr 25  2019 ociapi_az_key.pem
-rw-r--r--  1 azhekhan  staff   517 Apr 25  2019 az_oci_config
drwxr-xr-x  6 azhekhan  staff   192 Oct  9  2019 osvc_oci
-rw-------  1 azhekhan  staff  1679 Nov  6  2019 oci_api_key.pem
-rw-r--r--  1 azhekhan  staff   451 Nov  6  2019 oci_api_key_public.pem
-rw-r--r--  1 azhekhan  staff  2758 Nov  6  2019 oci_cli_rc
-rw-r--r--  1 azhekhan  staff   414 Jul  6  2021 osvccorp-ashburn.config
-rw-------  1 azhekhan  staff   393 Jul  6  2021 osvcstage-iad_v2.config
-rw-r--r--  1 azhekhan  staff   300 Jul  6  2021 osvccorp-iad_v2.config
-rw-------  1 azhekhan  staff   593 Jul  6  2021 config_OSVCSTAGE_phoenix-1
drwxr-xr-x  4 azhekhan  staff   128 Jul  6  2021 osvcstage
-rw-------  1 azhekhan  staff   507 Jul  6  2021 config_OSVCSTAGE_ashburn-1
-rw-r--r--  1 azhekhan  staff   500 Jul  6  2021 config_oracleova_phoenix-1
drwxr-xr-x  4 azhekhan  staff   128 Jul  6  2021 osvccorp
[DEFAULT]
-rw-r--r--  1 azhekhan  staff   414 Jul  6  2021 osvcprod-phx.config
-rw-------  1 azhekhan  staff   414 Jul  6  2021 config_PROD_us-phoenix-1
drwxr-xr-x  4 azhekhan  staff   128 Jul  6  2021 osvcprod
-rw-r--r--  1 azhekhan  staff   300 Jul  6  2021 osvcprod-phx_v2.config
-rw-------  1 azhekhan  staff   416 Jul  6  2021 config_PROD_eu-frankfurt-1
-rw-r--r--  1 azhekhan  staff   169 Jul  6  2021 osvcstage-.config
-rw-------  1 azhekhan  staff   507 Jul  6  2021 config_phoenix-1
-rw-------  1 azhekhan  staff   507 Jul  6  2021 osvcstage-iad.config
-rw-r--r--  1 azhekhan  staff   414 Jul  6  2021 config_OSVCCORP_ashburn-1
-rw-r--r--  1 azhekhan  staff   294 Jul  6  2021 osvcprod-fra.config
-rw-r--r--  1 azhekhan  staff   414 Jul  6  2021 osvccorp-iad.config
-rw-------  1 azhekhan  staff   593 Jul  6  2021 config_ashburn-1
apiVersion: v1
    server: https://147.154.151.92:6443
    server: https://147.154.151.190:6443
    cluster: cluster-cydknrqgzrg
      - generate-token
      command: oci
-rw-r--r--  1 azhekhan  staff   414 Jul  6  2021 osvcstage-phx.config
-rw-------  1 azhekhan  staff   414 Aug  2 10:54 config

 2022-01-17 15:26:21 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → env | grep -i oci
PATH=/Users/azhekhan/bin:/Library/Frameworks/Python.framework/Versions/3.8/bin:/Users/azhekhan/bin/oci:/usr/local/go/bin:/Users/azhekhan/bin:/Users/azhekhan/bin:/Library/Frameworks/Python.framework/Versions/3.8/bin:/Users/azhekhan/bin/oci:/usr/local/go/bin:/Users/azhekhan/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/go/bin:/usr/local/MacGPG2/bin:/Users/azhekhan/terraform:/Users/azhekhan/vault:/Users/azhekhan/kafka_2.12-2.4.0/bin:/Users/azhekhan/terraform:/Users/azhekhan/vault:/Users/azhekhan/kafka_2.12-2.4.0/bin

 2022-01-17 15:26:26 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → env | grep -i config
KUBECONFIG=/Users/azhekhan/.kube/config

 2022-01-17 15:26:34 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → vi /Users/azhekhan/.oci/config

 2022-01-17 15:27:05 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → #ll /Users/azhekhan/.oci/

 2022-01-17 15:27:13 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → cat /Users/azhekhan/.oci/config
[DEFAULT]
#tenancy=ocid1.compartment.oc1..aaaaaaaamfzmd7dxhst3a5hbhtu6mwxnkxzacrxqwrgasecvwq2iqcg7mg3q
tenancy=ocid1.tenancy.oc1..aaaaaaaacbb4jhwb2q6tfx223i5siaiuyw6gpl3zywyosiudeimsodjkolga
region=us-ashburn-1
user=ocid1.user.oc1..aaaaaaaaft5c2fkohgco27vmkudak2i3mk5mubwmo5szyejluyso7flvkdqa
fingerprint=1e:57:b5:1f:39:03:b5:e4:16:38:87:5b:88:e5:da:e0
key_file=/home/opc/.oci/oci_api_key.pem

 2022-01-17 15:27:16 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → ll /Users/azhekhan/.oci/oci_api_key.pem
-rw-------  1 azhekhan  staff  1679 Nov  6  2019 /Users/azhekhan/.oci/oci_api_key.pem

 2022-01-17 15:27:24 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → vi /Users/azhekhan/.oci/config

 2022-01-17 15:27:43 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → oong
{
  "data": "osvcstage"
}

 2022-01-17 15:27:48 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → cp /Users/azhekhan/.kube/config /Users/azhekhan/.kube/kind-cluster-17jan22.config

 2022-01-17 15:28:07 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → vi /Users/azhekhan/.kube/config

 2022-01-17 15:28:47 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → kgns
NAME                 STATUS   AGE
analytics            Active   159d
apicurio             Active   180d
az-dev-test          Active   298d
bui                  Active   159d
consul               Active   480d
cpeidc               Active   159d
cpeidcblr            Active   121d
default              Active   535d
example              Active   159d
example2             Active   72d
frontend             Active   488d
grafana-kafka-test   Active   474d
infra-monitoring     Active   488d
ingress-watcher      Active   488d
kafka                Active   270d
kafkadr              Active   341d
kafkareco            Active   314d
kube-node-lease      Active   535d
kube-public          Active   535d
kube-system          Active   535d
kubeform             Active   187d
kubernauts           Active   136d
kweetdev             Active   492d
lumberjack-logging   Active   488d
ma-global            Active   44d
monitoring-agent     Active   479d
patdemo              Active   278d
pattest              Active   285d
ph-namespace-test    Active   270d
psr                  Active   453d
psrkafka             Active   395d
pt2                  Active   280d
pvc-watcher          Active   462d
shared-kafka-pint    Active   47d
sitekafka            Active   404d
srtest               Active   180d
test123              Active   184d
tms                  Active   159d
traefik-apps         Active   110d
traefik-frontend     Active   110d
traefik-kafka        Active   59d
traefik-whoami       Active   58d
vault                Active   481d

 2022-01-17 15:28:55 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → kgpnk
NAME                                                              READY   STATUS      RESTARTS   AGE
cpe-cronjob-sr-backup-sr-backup-1642411500-4qrff                  0/1     Completed   0          33m
cpe-cronjob-sr-backup-sr-backup-1642412400-dmxpn                  0/1     Completed   0          18m
cpe-cronjob-sr-backup-sr-backup-1642413300-s92kw                  0/1     Completed   0          3m52s
drpod                                                             1/1     Running     32         13d
karapace-schemaregistry-proxy-584f58c5bd-t5nh8                    2/2     Running     1          24d
prom-karapace-exporter-prometheus-statsd-exporter-5896797d9nd9g   1/1     Running     0          31d
schemaregistry-proxy-5dfdd6458b-r5fmx                             2/2     Running     1          13d
shared-kafka-dr-kafka-0                                           1/1     Running     0          4d22h
shared-kafka-dr-kafka-1                                           1/1     Running     0          4d22h
shared-kafka-dr-kafka-2                                           1/1     Running     0          4d22h
shared-kafka-dr-kafka-exporter-7cc5dfbfd-dggfv                    1/1     Running     0          4d22h
shared-kafka-dr-zookeeper-0                                       1/1     Running     0          4d22h
shared-kafka-dr-zookeeper-1                                       1/1     Running     0          4d22h
shared-kafka-dr-zookeeper-2                                       1/1     Running     0          4d22h
shared-kafka-entity-operator-57c5d68b47-9jz6d                     3/3     Running     0          4d22h
shared-kafka-kafka-0                                              1/1     Running     0          4d22h
shared-kafka-kafka-1                                              1/1     Running     0          4d22h
shared-kafka-kafka-2                                              1/1     Running     0          4d22h
shared-kafka-kafka-exporter-5659f8bdfc-6n9l4                      1/1     Running     0          4d22h
shared-kafka-mm2-dr-uk-london-1-mirrormaker2-66c4f655ff-72t64     1/1     Running     0          4d22h
shared-kafka-mm2-dr-uk-london-1-mirrormaker2-66c4f655ff-lnmkx     1/1     Running     0          4d22h
shared-kafka-mm2-uk-london-1-mirrormaker2-6969bf676d-k8xlj        1/1     Running     0          4d22h
shared-kafka-mm2-uk-london-1-mirrormaker2-6969bf676d-rr267        1/1     Running     0          4d22h
shared-kafka-zookeeper-0                                          1/1     Running     0          4d22h
shared-kafka-zookeeper-1                                          1/1     Running     0          4d22h
shared-kafka-zookeeper-2                                          1/1     Running     0          4d22h
shared-schemaregistry-6b8d97df79-76nr7                            2/2     Running     0          11d
shared-schemaregistry-6b8d97df79-h5hq8                            2/2     Running     0          11d
strimzi-cluster-operator-7fbc69c4f4-spnmx                         1/1     Running     0          4d22h

 2022-01-17 15:29:01 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → #k port-forward traefik-kafka-controller-5549bc588f-2zwf6 32215:9000

 2022-01-17 15:29:33 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → kgpn traefik-frontend
NAME                                               READY   STATUS    RESTARTS   AGE
ingress-traefik-grafana-lhr-da-c54765df8-9mrws     1/1     Running   0          31d
ingress-traefik-grafana-lhr-da-c54765df8-gbf4n     1/1     Running   0          31d
ingress-traefik-grafana-lhr-da-c54765df8-jzt6n     1/1     Running   0          31d
ingress-traefik-grafana-lhr-da-c54765df8-k7xqb     1/1     Running   0          31d
ingress-traefik-grafana-lhr-da-c54765df8-mfrbt     1/1     Running   0          31d
ingress-traefik-grafana-lhr-da-c54765df8-q9zct     1/1     Running   0          31d
ingress-traefik-grafana-lhr-da-c54765df8-sx8dx     1/1     Running   0          31d
ingress-traefik-grafana-lhr-da-c54765df8-tvspd     1/1     Running   0          31d
ingress-traefik-grafana-lhr-da-c54765df8-w88rw     1/1     Running   0          31d
ingress-traefik-grafana-lhr-da-c54765df8-wwgrh     1/1     Running   0          31d
ingress-traefik-internet-lhr-da-5d5db4c8d9-htbxw   1/1     Running   0          31d
ingress-traefik-internet-lhr-da-5d5db4c8d9-j8c2g   1/1     Running   0          31d
ingress-traefik-internet-lhr-da-5d5db4c8d9-rsw5r   1/1     Running   0          31d
ingress-traefik-kafka-lhr-da-565f6b54b8-4cxvc      1/1     Running   0          31d
ingress-traefik-kafka-lhr-da-565f6b54b8-5fzj9      1/1     Running   0          31d
ingress-traefik-kafka-lhr-da-565f6b54b8-7t6v2      1/1     Running   0          31d
ingress-traefik-kafka-lhr-da-565f6b54b8-9wwn8      1/1     Running   0          31d
ingress-traefik-kafka-lhr-da-565f6b54b8-hvmj6      1/1     Running   0          31d
ingress-traefik-kafka-lhr-da-565f6b54b8-l454c      1/1     Running   0          31d
ingress-traefik-kafka-lhr-da-565f6b54b8-prh2n      1/1     Running   0          31d
ingress-traefik-kafka-lhr-da-565f6b54b8-rt6mk      1/1     Running   0          31d
ingress-traefik-kafka-lhr-da-565f6b54b8-sd4wg      1/1     Running   0          31d
ingress-traefik-kafka-lhr-da-565f6b54b8-zkqmq      1/1     Running   0          31d
ingress-traefik-ocna-lhr-da-55fdfc8f4f-77z2x       1/1     Running   0          31d
ingress-traefik-ocna-lhr-da-55fdfc8f4f-r4qkk       1/1     Running   0          31d
ingress-traefik-ocna-lhr-da-55fdfc8f4f-vtcxk       1/1     Running   0          28d
ingress-traefik-private-lhr-da-76ff4c8d9c-9vm7p    1/1     Running   0          31d
ingress-traefik-private-lhr-da-76ff4c8d9c-cd7zw    1/1     Running   0          31d
ingress-traefik-private-lhr-da-76ff4c8d9c-dg695    1/1     Running   0          31d

 2022-01-17 15:29:50 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → kgpn traefik-kafka
NAME                                        READY   STATUS    RESTARTS   AGE
traefik-kafka-controller-5549bc588f-2zwf6   1/1     Running   0          31d

 2022-01-17 15:30:12 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → k port-forward traefik-kafka-controller-5549bc588f-2zwf6 32215:9000
Error from server (NotFound): pods "traefik-kafka-controller-5549bc588f-2zwf6" not found

 2022-01-17 15:30:20 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → k port-forward traefik-kafka-controller-5549bc588f-2zwf6 32215:9000 -n traefik-kafka
Forwarding from 127.0.0.1:32215 -> 9000
Forwarding from [::1]:32215 -> 9000
Handling connection for 32215
Handling connection for 32215
Handling connection for 32215
Handling connection for 32215
Handling connection for 32215
Handling connection for 32215
E0117 15:38:04.670184   34437 portforward.go:233] lost connection to pod

 2022-01-17 15:38:04 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → k port-forward traefik-kafka-controller-5549bc588f-2zwf6 32215:9000 -n traefik-kafka
Forwarding from 127.0.0.1:32215 -> 9000
Forwarding from [::1]:32215 -> 9000
Handling connection for 32215
Handling connection for 32215
Handling connection for 32215
E0117 19:18:42.237410   49249 portforward.go:340] error creating error stream for port 32215 -> 9000: write tcp 10.74.105.84:60974->138.1.19.169:6443: write: can't assign requested address
Handling connection for 32215
E0117 19:19:44.157096   49249 portforward.go:340] error creating error stream for port 32215 -> 9000: write tcp 10.74.105.84:60974->138.1.19.169:6443: write: can't assign requested address
E0117 19:19:48.072585   49249 portforward.go:233] lost connection to pod

 2022-01-17 19:19:48 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ →

 2022-01-17 22:08:33 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → env | grep -i kube
KUBECONFIG=/Users/azhekhan/.kube/config

 2022-01-17 22:08:40 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → kgns
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURqRENDQW5TZ0F3SUJBZ0lVUUlVRTMxc0ZZcDJ0UTQyOVVEWFNvWUd6dVUwd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1hqRUxNQWtHQTFVRUJoTUNWVk14RGpBTUJnTlZCQWdUQlZSbGVHRnpNUTh3RFFZRFZRUUhFd1pCZFhOMAphVzR4RHpBTkJnTlZCQW9UQms5eVlXTnNaVEVNTUFvR0ExVUVDeE1EVDBSWU1ROHdEUVlEVlFRREV3WkxPRk1nClEwRXdIaGNOTWpBd056TXdNVGN4TXpBd1doY05NalV3TnpJNU1UY3hNekF3V2pCZU1Rc3dDUVlEVlFRR0V3SlYKVXpFT01Bd0dBMVVFQ0JNRlZHVjRZWE14RHpBTkJnTlZCQWNUQmtGMWMzUnBiakVQTUEwR0ExVUVDaE1HVDNKaApZMnhsTVF3d0NnWURWUVFMRXdOUFJGZ3hEekFOQmdOVkJBTVRCa3M0VXlCRFFUQ0NBU0l3RFFZSktvWklodmNOCkFRRUJCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFOZzRWdnRKenArdllCV2pidWx3M1ZTaGF0REpqYytWbkt4WGRmaG0KUCtCcWdxREoxKzVDTDA5WDBJOUpLWlh0aG1GRm9teFdlMUFLMkduNE5SOWVqRDdxVkhmZllJSjRLTGJVQmhMSgpMT1BtSTA3QUN3UWxuQlhSUUxVcTZxc0dsb1puUXFNeWs4bkRjeHo5SzVqTzdIQ1FlUnFlNVNSUWhrMnJ3aU9uCnQ1U0dTL1hCS2ZibmZoVGZYZVhDa3hKeUhuckJ4N1lTSHB1eGJBRkRoK0FkOVFqNi91dnNUTC9taWJPWEpJM0sKWWJRMm5qSU4zTzlQY2NBWDBBeldZNE1jSGpUaWJ3amVLVHQ4N1RqZVJub0ZORXlFcDJnaFZKcENzaVBod0NGbApXUVpiZU5TRmxUbDY5RlVPUkVpS1Q5dlg4bEJsL2ljRnJyMGNFTC9yYU1FT1dqY0NBd0VBQWFOQ01FQXdEZ1lEClZSMFBBUUgvQkFRREFnRUdNQThHQTFVZEV3RUIvd1FGTUFNQkFmOHdIUVlEVlIwT0JCWUVGTEgrZ3ppVHBJMGYKenZmNVFXNDJ5aGJyN09TK01BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQWdZelQzRm0wS0poRnBhUGJUUEc3UQpVMEZGOFpnaEtFV1BNWmRSRC85bjY0OFQ4L0FITyt3NHpzZkY1Z3ROSkh0dEJHNmNhUkl2TjQ3RldSM21aV2NsClQzaFNRK0RvUXVvV25xZHRLUE1MSEVrV2dDdFM5Vi9MVXRlRy8xT3dCQWN5eUhUUjhGYjY1T3dFRE5UUEJRQzUKdkNUdzc5N1lhVHZDaHlzRVgrME1CeGY4OHNpNjBrekt2UVFVb3k2cVZrUHVrZVRTakNzajl1blJESTc2UWcrdgpKRkl1UUJmNFRBVk1YZUpNc0FrbGFhdGNkQ2ZVTEp1YnVva3FMaGNycHp5K0Z5QjAwYlQ5M0NlWU1va0E5d28xCk5EdWtqWHMwY2p5bVJhMlRFcGFaL2gvWGllK1VKOC9oeEpKV0tXaWZCMTBrWmVMVkJxdmwrZVNNNlpZMHo0Z1UKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    server: https://147.154.151.92:6443
  name: cluster-c4dsmruga4w
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURqRENDQW5TZ0F3SUJBZ0lVSWJYRHJqdGd3VjY2OW8xem8ybzZnK2NpWlYwd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1hqRUxNQWtHQTFVRUJoTUNWVk14RGpBTUJnTlZCQWdUQlZSbGVHRnpNUTh3RFFZRFZRUUhFd1pCZFhOMAphVzR4RHpBTkJnTlZCQW9UQms5eVlXTnNaVEVNTUFvR0ExVUVDeE1EVDBSWU1ROHdEUVlEVlFRREV3WkxPRk1nClEwRXdIaGNOTWpBd056TXdNVGN4TXpBd1doY05NalV3TnpJNU1UY3hNekF3V2pCZU1Rc3dDUVlEVlFRR0V3SlYKVXpFT01Bd0dBMVVFQ0JNRlZHVjRZWE14RHpBTkJnTlZCQWNUQmtGMWMzUnBiakVQTUEwR0ExVUVDaE1HVDNKaApZMnhsTVF3d0NnWURWUVFMRXdOUFJGZ3hEekFOQmdOVkJBTVRCa3M0VXlCRFFUQ0NBU0l3RFFZSktvWklodmNOCkFRRUJCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFMTXIzZGNCWExOZW5STmZuVHhCaE5ONGtVVS80andQZm5adnVVK2oKSkY0d3E4K2JTcVl1ZEZ6T081bVdDK1Bxd0VNMlhHMGVlZUFhY0g1Y3V4RFhSTWRpQjBIbExxU1VCR1hVSkpyZQpLSFVYb0hVMWhXWTBIaVp1b1gvankzM281cnRxWHFnWE9LRURXeVJmcXk1Y1BXcjFtalI3bTJVNlB1cktmUUE4CmF5TEl3RjBXb1REbXN0K09hYnhwRmg3eUlEK0V3bFFnd2p0RDduWE9oRkRPenkxbVZlcVJOMG1OWHBOTlJtVEEKaFVMM2hobDAxM3BtM1dZLzR1RkJKaDZaTXFMZjNjQ1NvcmNzQTljWDRET2hRcnhrQTR5Yjc1ZmkwL0pkWldPVgpLNElLbTl1OFQ4MnU5dXdFTlFINzByaHFSU0FjeDJQMGRhYUZhTmRBa3YyZUxyOENBd0VBQWFOQ01FQXdEZ1lEClZSMFBBUUgvQkFRREFnRUdNQThHQTFVZEV3RUIvd1FGTUFNQkFmOHdIUVlEVlIwT0JCWUVGTVdKVHhZakwyZ2gKbkNYR00xTG5WY2hDeVFYdk1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQVdpaG9tcjBGdkRaYmtseXY1Yk9CZQprbWRhUDZVem15VGhndGlNZEgwcFpsSGhSSXdtazJMbm1melZvT2YxRWNadWtVZ3hJNklPb0hKeGNnRGU0QWRTCk1GTVlzTnp1djZYUEdlMm5qUWVyZWpMWU5TZjB3YkhDUVcrbHFoK0d5VXNsRnlvOUpkUml2R01KUEs1MURld3AKTVRQY1BweXlpOGUrbUlxejdUKzE4dzgwTmxocDhmM0dQUlhYam11eW9HUDZaQjJJYWVaQlVDUjdaOUhTNWwwaQpkcVN2UVBtV3FnK2JmQ3JhWUVMb0llSW1YbmtlWDVOMTlXaUJPeVMrVDdMR2ZUdUN5RWxKWmowaDRqZjFCSnM2CkFJa3E2ajhaVXM3TS9aSjY2TitKV1ZDSTdEZXYvOUVmZFJ2VHRUazNKR0U3RytmM1hTSFJZMjFoN09Zd1FrVlMKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    server: https://147.154.151.190:6443
  name: cluster-cqtkntfgrtg
- cluster:
@                                                                                                                                              @                                                                                                                                              @                                                                                                                                              @                                                                                                                                              @                                                                                                                                              @                                                                                                                                              @                                                                                                                                              @                                                                                                                                              @                                                                                                                                              @                                                                                                                                              @                                                                                                                                              @
"~/.kube/config" 122L, 11742C
      env: []
      provideClusterInfo: false
- name: user-cqtozrugy3t
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - ce
      - cluster
      - generate-token
      - --cluster-id
      - ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
      - --region
      - uk-london-1
      command: oci
      env: []
      provideClusterInfo: false
- name: user-cqtqnbymy2t
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - ce
      - cluster
      - generate-token
      - --cluster-id
      - ocid1.cluster.oc1.eu-frankfurt-1.aaaaaaaaafrdeytbgi3gimrtmyzwmmtcgu2ggn3eg42tsyztmcqtqnbymy2t
      - --region
      - eu-frankfurt-1
      command: oci
      env: []
      provideClusterInfo: false
- name: user-cydknrqgzrg
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - ce
      - cluster
      - generate-token
      - --cluster-id
      - ocid1.cluster.oc1.uk-london-1.aaaaaaaaae2giobtmmytamzvmqzgmmrygu4wcolfmezdin3dhcydknrqgzrg
      - --region
      - uk-london-1
      command: oci
      env: []
      provideClusterInfo: false
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURqRENDQW5TZ0F3SUJBZ0lVUUlVRTMxc0ZZcDJ0UTQyOVVEWFNvWUd6dVUwd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1hqRUxNQWtHQTFVRUJoTUNWVk14RGpBTUJnTlZCQWdUQlZSbGVHRnpNUTh3RFFZRFZRUUhFd1pCZFhOMAphVzR4RHpBTkJnTlZCQW9UQms5eVlXTnNaVEVNTUFvR0ExVUVDeE1EVDBSWU1ROHdEUVlEVlFRREV3WkxPRk1nClEwRXdIaGNOTWpBd056TXdNVGN4TXpBd1doY05NalV3TnpJNU1UY3hNekF3V2pCZU1Rc3dDUVlEVlFRR0V3SlYKVXpFT01Bd0dBMVVFQ0JNRlZHVjRZWE14RHpBTkJnTlZCQWNUQmtGMWMzUnBiakVQTUEwR0ExVUVDaE1HVDNKaApZMnhsTVF3d0NnWURWUVFMRXdOUFJGZ3hEekFOQmdOVkJBTVRCa3M0VXlCRFFUQ0NBU0l3RFFZSktvWklodmNOCkFRRUJCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFOZzRWdnRKenArdllCV2pidWx3M1ZTaGF0REpqYytWbkt4WGRmaG0KUCtCcWdxREoxKzVDTDA5WDBJOUpLWlh0aG1GRm9teFdlMUFLMkduNE5SOWVqRDdxVkhmZllJSjRLTGJVQmhMSgpMT1BtSTA3QUN3UWxuQlhSUUxVcTZxc0dsb1puUXFNeWs4bkRjeHo5SzVqTzdIQ1FlUnFlNVNSUWhrMnJ3aU9uCnQ1U0dTL1hCS2ZibmZoVGZYZVhDa3hKeUhuckJ4N1lTSHB1eGJBRkRoK0FkOVFqNi91dnNUTC9taWJPWEpJM0sKWWJRMm5qSU4zTzlQY2NBWDBBeldZNE1jSGpUaWJ3amVLVHQ4N1RqZVJub0ZORXlFcDJnaFZKcENzaVBod0NGbApXUVpiZU5TRmxUbDY5RlVPUkVpS1Q5dlg4bEJsL2ljRnJyMGNFTC9yYU1FT1dqY0NBd0VBQWFOQ01FQXdEZ1lEClZSMFBBUUgvQkFRREFnRUdNQThHQTFVZEV3RUIvd1FGTUFNQkFmOHdIUVlEVlIwT0JCWUVGTEgrZ3ppVHBJMGYKenZmNVFXNDJ5aGJyN09TK01BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQWdZelQzRm0wS0poRnBhUGJUUEc3UQpVMEZGOFpnaEtFV1BNWmRSRC85bjY0OFQ4L0FITyt3NHpzZkY1Z3ROSkh0dEJHNmNhUkl2TjQ3RldSM21aV2NsClQzaFNRK0RvUXVvV25xZHRLUE1MSEVrV2dDdFM5Vi9MVXRlRy8xT3dCQWN5eUhUUjhGYjY1T3dFRE5UUEJRQzUKdkNUdzc5N1lhVHZDaHlzRVgrME1CeGY4OHNpNjBrekt2UVFVb3k2cVZrUHVrZVRTakNzajl1blJESTc2UWcrdgpKRkl1UUJmNFRBVk1YZUpNc0FrbGFhdGNkQ2ZVTEp1YnVva3FMaGNycHp5K0Z5QjAwYlQ5M0NlWU1va0E5d28xCk5EdWtqWHMwY2p5bVJhMlRFcGFaL2gvWGllK1VKOC9oeEpKV0tXaWZCMTBrWmVMVkJxdmwrZVNNNlpZMHo0Z1UKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    server: https://147.154.151.92:6443
  name: cluster-c4dsmruga4w
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURqRENDQW5TZ0F3SUJBZ0lVSWJYRHJqdGd3VjY2OW8xem8ybzZnK2NpWlYwd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1hqRUxNQWtHQTFVRUJoTUNWVk14RGpBTUJnTlZCQWdUQlZSbGVHRnpNUTh3RFFZRFZRUUhFd1pCZFhOMAphVzR4RHpBTkJnTlZCQW9UQms5eVlXTnNaVEVNTUFvR0ExVUVDeE1EVDBSWU1ROHdEUVlEVlFRREV3WkxPRk1nClEwRXdIaGNOTWpBd056TXdNVGN4TXpBd1doY05NalV3TnpJNU1UY3hNekF3V2pCZU1Rc3dDUVlEVlFRR0V3SlYKVXpFT01Bd0dBMVVFQ0JNRlZHVjRZWE14RHpBTkJnTlZCQWNUQmtGMWMzUnBiakVQTUEwR0ExVUVDaE1HVDNKaApZMnhsTVF3d0NnWURWUVFMRXdOUFJGZ3hEekFOQmdOVkJBTVRCa3M0VXlCRFFUQ0NBU0l3RFFZSktvWklodmNOCkFRRUJCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFMTXIzZGNCWExOZW5STmZuVHhCaE5ONGtVVS80andQZm5adnVVK2oKSkY0d3E4K2JTcVl1ZEZ6T081bVdDK1Bxd0VNMlhHMGVlZUFhY0g1Y3V4RFhSTWRpQjBIbExxU1VCR1hVSkpyZQpLSFVYb0hVMWhXWTBIaVp1b1gvankzM281cnRxWHFnWE9LRURXeVJmcXk1Y1BXcjFtalI3bTJVNlB1cktmUUE4CmF5TEl3RjBXb1REbXN0K09hYnhwRmg3eUlEK0V3bFFnd2p0RDduWE9oRkRPenkxbVZlcVJOMG1OWHBOTlJtVEEKaFVMM2hobDAxM3BtM1dZLzR1RkJKaDZaTXFMZjNjQ1NvcmNzQTljWDRET2hRcnhrQTR5Yjc1ZmkwL0pkWldPVgpLNElLbTl1OFQ4MnU5dXdFTlFINzByaHFSU0FjeDJQMGRhYUZhTmRBa3YyZUxyOENBd0VBQWFOQ01FQXdEZ1lEClZSMFBBUUgvQkFRREFnRUdNQThHQTFVZEV3RUIvd1FGTUFNQkFmOHdIUVlEVlIwT0JCWUVGTVdKVHhZakwyZ2gKbkNYR00xTG5WY2hDeVFYdk1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQVdpaG9tcjBGdkRaYmtseXY1Yk9CZQprbWRhUDZVem15VGhndGlNZEgwcFpsSGhSSXdtazJMbm1melZvT2YxRWNadWtVZ3hJNklPb0hKeGNnRGU0QWRTCk1GTVlzTnp1djZYUEdlMm5qUWVyZWpMWU5TZjB3YkhDUVcrbHFoK0d5VXNsRnlvOUpkUml2R01KUEs1MURld3AKTVRQY1BweXlpOGUrbUlxejdUKzE4dzgwTmxocDhmM0dQUlhYam11eW9HUDZaQjJJYWVaQlVDUjdaOUhTNWwwaQpkcVN2UVBtV3FnK2JmQ3JhWUVMb0llSW1YbmtlWDVOMTlXaUJPeVMrVDdMR2ZUdUN5RWxKWmowaDRqZjFCSnM2CkFJa3E2ajhaVXM3TS9aSjY2TitKV1ZDSTdEZXYvOUVmZFJ2VHRUazNKR0U3RytmM1hTSFJZMjFoN09Zd1FrVlMKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    server: https://147.154.151.190:6443
  name: cluster-cqtkntfgrtg
- cluster:
@                                                                                                                                              @                                                                                                                                              @                                                                                                                                              @                                                                                                                                              @                                                                                                                                              @                                                                                                                                              @                                                                                                                                              @                                                                                                                                              @                                                                                                                                              @                                                                                                                                              @                                                                                                                                              @
      env: []
      provideClusterInfo: false
- name: user-cqtozrugy3t
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - ce
      - cluster
      - generate-token
      - --cluster-id
      - ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
      - --region
      - uk-london-1
      command: oci
      env: []
      provideClusterInfo: false
- name: user-cqtqnbymy2t
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - ce
      - cluster
      - generate-token
      - --cluster-id
      - ocid1.cluster.oc1.eu-frankfurt-1.aaaaaaaaafrdeytbgi3gimrtmyzwmmtcgu2ggn3eg42tsyztmcqtqnbymy2t
      - --region
      - eu-frankfurt-1
      command: oci
      env: []
      provideClusterInfo: false
- name: user-cydknrqgzrg
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - ce
      - cluster
      - generate-token
      - --cluster-id
      - ocid1.cluster.oc1.uk-london-1.aaaaaaaaae2giobtmmytamzvmqzgmmrygu4wcolfmezdin3dhcydknrqgzrg
      - --region
      - uk-london-1
      command: oci
      env: []
      provideClusterInfo: false
apiVersion: v1
clusters:
- cluster:
apiVersion: v1
NAME                 STATUS   AGE
analytics            Active   159d
apicurio             Active   180d
az-dev-test          Active   298d
bui                  Active   159d
consul               Active   480d
cpeidc               Active   159d
cpeidcblr            Active   122d
default              Active   535d
example              Active   159d
example2             Active   73d
frontend             Active   488d
grafana-kafka-test   Active   475d
infra-monitoring     Active   488d
ingress-watcher      Active   488d
kafka                Active   270d
kafkadr              Active   342d
kafkareco            Active   314d
kube-node-lease      Active   535d
kube-public          Active   535d
kube-system          Active   535d
kubeform             Active   188d
kubernauts           Active   136d
kweetdev             Active   493d
lumberjack-logging   Active   488d
ma-global            Active   45d
monitoring-agent     Active   479d
patdemo              Active   278d
pattest              Active   285d
ph-namespace-test    Active   271d
psr                  Active   453d
psrkafka             Active   395d
pt2                  Active   280d
pvc-watcher          Active   462d
shared-kafka-pint    Active   47d
sitekafka            Active   404d
srtest               Active   180d
test123              Active   185d
tms                  Active   159d
traefik-apps         Active   110d
traefik-frontend     Active   110d
traefik-kafka        Active   60d
traefik-whoami       Active   59d
vault                Active   481d

 2022-01-17 22:08:53 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → vi ~/.kube/config

 2022-01-17 22:10:17 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → env | grep -i kube
KUBECONFIG=/Users/azhekhan/.kube/config

 2022-01-17 22:10:23 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → kgns
NAME                 STATUS   AGE
default              Active   4d
kube-node-lease      Active   4d
kube-public          Active   4d
kube-system          Active   4d
local-path-storage   Active   4d
traefik-apps         Active   4d
traefik-test         Active   2d9h

 2022-01-17 22:10:27 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → kgpn traefik-apps
NAME                    READY   STATUS    RESTARTS   AGE
bear-5cff87db68-st4fz   1/1     Running   0          4d
bear-5cff87db68-vw22p   1/1     Running   0          4d

 2022-01-17 22:10:35 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → kgn traefik-apps services
NAME   TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
bear   ClusterIP   10.96.24.6   <none>        80/TCP    3d9h

 2022-01-17 22:10:49 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → k
kubectl controls the Kubernetes cluster manager.

 Find more information at: https://kubernetes.io/docs/reference/kubectl/overview/

Basic Commands (Beginner):
  create        Create a resource from a file or from stdin.
  expose        Take a replication controller, service, deployment or pod and expose it as a new Kubernetes Service
  run           Run a particular image on the cluster
  set           Set specific features on objects

Basic Commands (Intermediate):
  explain       Documentation of resources
  get           Display one or many resources
  edit          Edit a resource on the server
  delete        Delete resources by filenames, stdin, resources and names, or by resources and label selector

Deploy Commands:
  rollout       Manage the rollout of a resource
  scale         Set a new size for a Deployment, ReplicaSet or Replication Controller
  autoscale     Auto-scale a Deployment, ReplicaSet, or ReplicationController

Cluster Management Commands:
  certificate   Modify certificate resources.
  cluster-info  Display cluster info
  top           Display Resource (CPU/Memory/Storage) usage.
  cordon        Mark node as unschedulable
  uncordon      Mark node as schedulable
  drain         Drain node in preparation for maintenance
  taint         Update the taints on one or more nodes

Troubleshooting and Debugging Commands:
  describe      Show details of a specific resource or group of resources
  logs          Print the logs for a container in a pod
  attach        Attach to a running container
  exec          Execute a command in a container
  port-forward  Forward one or more local ports to a pod
  proxy         Run a proxy to the Kubernetes API server
  cp            Copy files and directories to and from containers.
  auth          Inspect authorization

Advanced Commands:
  diff          Diff live version against would-be applied version
  apply         Apply a configuration to a resource by filename or stdin
  patch         Update field(s) of a resource using strategic merge patch
  replace       Replace a resource by filename or stdin
  wait          Experimental: Wait for a specific condition on one or many resources.
  convert       Convert config files between different API versions
  kustomize     Build a kustomization target from a directory or a remote url.

Settings Commands:
  label         Update the labels on a resource
  annotate      Update the annotations on a resource
  completion    Output shell completion code for the specified shell (bash or zsh)

Other Commands:
  alpha         Commands for features in alpha
  api-resources Print the supported API resources on the server
  api-versions  Print the supported API versions on the server, in the form of "group/version"
  config        Modify kubeconfig files
  plugin        Provides utilities for interacting with plugins.
  version       Print the client and server version information

Usage:
  kubectl [flags] [options]

Use "kubectl <command> --help" for more information about a given command.
Use "kubectl options" for a list of global command-line options (applies to all commands).

 2022-01-17 22:10:54 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → k deployments explain
Error: unknown command "deployments" for "kubectl"
Run 'kubectl --help' for usage.

 2022-01-17 22:11:06 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → kgn traefik-apps ingress
NAME      CLASS    HOSTS                                   ADDRESS   PORTS     AGE
animals   <none>   bear-lhr.dev.channels.ocs.oc-test.com             80, 443   8h

 2022-01-17 22:11:18 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → kgn traefik-apps ingress -oyaml
apiVersion: v1
items:
- apiVersion: networking.k8s.io/v1
  kind: Ingress
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Ingress","metadata":{"annotations":{},"labels":{"app":"animals","app.kubernetes.io/instance":"traefik","app.kubernetes.io/name":"traefik","task":"bear"},"name":"animals","namespace":"traefik-apps"},"spec":{"rules":[{"host":"bear-lhr.dev.channels.ocs.oc-test.com","http":{"paths":[{"backend":{"serviceName":"bear","servicePort":"http"},"path":"/","pathType":"ImplementationSpecific"}]}}],"tls":[{"hosts":["bear-lhr.dev.channels.ocs.oc-test.com"]}]}}
    creationTimestamp: "2022-01-17T08:04:25Z"
    generation: 1
    labels:
      app: animals
      app.kubernetes.io/instance: traefik
      app.kubernetes.io/name: traefik
      task: bear
    managedFields:
    - apiVersion: extensions/v1beta1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:kubectl.kubernetes.io/last-applied-configuration: {}
          f:labels:
            .: {}
            f:app: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/name: {}
            f:task: {}
        f:spec:
          f:rules: {}
          f:tls: {}
      manager: kubectl
      operation: Update
      time: "2022-01-17T08:06:45Z"
    name: animals
    namespace: traefik-apps
    resourceVersion: "218662"
    uid: 47c8f0cf-8cd4-4c34-b7ad-bff009beb090
  spec:
    rules:
    - host: bear-lhr.dev.channels.ocs.oc-test.com
      http:
        paths:
        - backend:
            service:
              name: bear
              port:
                name: http
          path: /
          pathType: ImplementationSpecific
    tls:
    - hosts:
      - bear-lhr.dev.channels.ocs.oc-test.com
  status:
    loadBalancer: {}
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

 2022-01-17 22:11:24 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ →

 2022-01-17 22:11:32 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → kdn traefik-apps ingress
Name:             animals
Namespace:        traefik-apps
Address:
Default backend:  default-http-backend:80 (<error: endpoints "default-http-backend" not found>)
TLS:
  SNI routes bear-lhr.dev.channels.ocs.oc-test.com
Rules:
  Host                                   Path  Backends
  ----                                   ----  --------
  bear-lhr.dev.channels.ocs.oc-test.com
                                         /        bear:http (10.244.1.2:80,10.244.1.3:80)
Annotations:                             Events:  <none>

 2022-01-17 22:11:38 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → kdn traefik-apps services
Name:              bear
Namespace:         traefik-apps
Labels:            <none>
Annotations:       Selector:  app=animals,task=bear
Type:              ClusterIP
IP:                10.96.24.6
Port:              http  80/TCP
TargetPort:        80/TCP
Endpoints:         10.244.1.2:80,10.244.1.3:80
Session Affinity:  None
Events:            <none>

 2022-01-17 22:11:43 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → #○ → k port-forward traefik-kafka-controller-5549bc588f-2zwf6 32215:9000 -n traefik-kafka

 2022-01-17 22:12:44 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → #k port-forward bear-5cff87db68-st4fz

 2022-01-17 22:13:32 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → kgpn traefik-apps
NAME                    READY   STATUS    RESTARTS   AGE
bear-5cff87db68-st4fz   1/1     Running   0          4d
bear-5cff87db68-vw22p   1/1     Running   0          4d

 2022-01-17 22:13:39 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → kgpn traefik-apps bear-5cff87db68-st4fz -oyaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2022-01-13T16:37:00Z"
  generateName: bear-5cff87db68-
  labels:
    app: animals
    pod-template-hash: 5cff87db68
    task: bear
    version: v0.0.1
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:generateName: {}
        f:labels:
          .: {}
          f:app: {}
          f:pod-template-hash: {}
          f:task: {}
          f:version: {}
        f:ownerReferences:
          .: {}
          k:{"uid":"c333d887-3a98-4c05-834e-fdd5946ada19"}:
            .: {}
            f:apiVersion: {}
            f:blockOwnerDeletion: {}
            f:controller: {}
            f:kind: {}
            f:name: {}
            f:uid: {}
      f:spec:
        f:containers:
          k:{"name":"bear"}:
            .: {}
            f:image: {}
            f:imagePullPolicy: {}
            f:name: {}
            f:ports:
              .: {}
              k:{"containerPort":80,"protocol":"TCP"}:
                .: {}
                f:containerPort: {}
                f:protocol: {}
            f:resources: {}
            f:terminationMessagePath: {}
            f:terminationMessagePolicy: {}
        f:dnsPolicy: {}
        f:enableServiceLinks: {}
        f:restartPolicy: {}
        f:schedulerName: {}
        f:securityContext: {}
        f:terminationGracePeriodSeconds: {}
    manager: kube-controller-manager
    operation: Update
    time: "2022-01-13T16:37:00Z"
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:status:
        f:conditions:
          k:{"type":"ContainersReady"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
          k:{"type":"Initialized"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
          k:{"type":"Ready"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
        f:containerStatuses: {}
        f:hostIP: {}
        f:phase: {}
        f:podIP: {}
        f:podIPs:
          .: {}
          k:{"ip":"10.244.1.3"}:
            .: {}
            f:ip: {}
        f:startTime: {}
    manager: kubelet
    operation: Update
    time: "2022-01-13T16:37:32Z"
  name: bear-5cff87db68-st4fz
  namespace: traefik-apps
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: bear-5cff87db68
    uid: c333d887-3a98-4c05-834e-fdd5946ada19
  resourceVersion: "1143"
  uid: ce6e2929-56f7-43a0-9609-4b361c2692a4
spec:
  containers:
  - image: supergiantkir/animals:bear
    imagePullPolicy: IfNotPresent
    name: bear
    ports:
    - containerPort: 80
      protocol: TCP
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-wqsg4
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: testk8s-worker
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: default-token-wqsg4
    secret:
      defaultMode: 420
      secretName: default-token-wqsg4
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2022-01-13T16:37:00Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2022-01-13T16:37:32Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2022-01-13T16:37:32Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2022-01-13T16:37:00Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: containerd://1c497fd4dc6be5dc8a653d70d2488f298b94f0257e10d14604a862b7d490931a
    image: docker.io/supergiantkir/animals:bear
    imageID: docker.io/supergiantkir/animals@sha256:9a3ffc224acc82bfe3264aa4322cf823ea0074017ce271b4d74cb308b918ee16
    lastState: {}
    name: bear
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2022-01-13T16:37:32Z"
  hostIP: 172.18.0.3
  phase: Running
  podIP: 10.244.1.3
  podIPs:
  - ip: 10.244.1.3
  qosClass: BestEffort
  startTime: "2022-01-13T16:37:00Z"

 2022-01-17 22:13:51 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → kgpn traefik-apps services
Error from server (NotFound): pods "services" not found

 2022-01-17 22:17:05 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → kgn traefik-apps services
NAME   TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
bear   ClusterIP   10.96.24.6   <none>        80/TCP    3d9h

 2022-01-17 22:17:10 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → #○ → #○ → k port-forward traefik-kafka-controller-5549bc588f-2zwf6 32215:9000 -n traefik-kafka

 2022-01-17 22:17:45 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → k port-forward bear-5cff87db68-st4fz 80:80 -n traefik-apps
Unable to listen on port 80: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:80: bind: permission denied unable to create listener: Error listen tcp6 [::1]:80: bind: permission denied]
error: unable to listen on any of the requested ports: [{80 80}]

 2022-01-17 22:18:13 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → kgpn traefik-test
NAME                      READY   STATUS    RESTARTS   AGE
traefik-8b7b787f5-n8dj9   1/1     Running   0          2d9h

 2022-01-17 22:19:26 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → kgpn traefik-test -oyaml
apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "9100"
      prometheus.io/scrape: "true"
    creationTimestamp: "2022-01-15T07:06:38Z"
    generateName: traefik-8b7b787f5-
    labels:
      app.kubernetes.io/instance: traefik
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-10.9.1
      pod-template-hash: 8b7b787f5
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:prometheus.io/path: {}
            f:prometheus.io/port: {}
            f:prometheus.io/scrape: {}
          f:generateName: {}
          f:labels:
            .: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:helm.sh/chart: {}
            f:pod-template-hash: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"13471cf2-8a8f-4a76-b063-9c38a174bff9"}:
              .: {}
              f:apiVersion: {}
              f:blockOwnerDeletion: {}
              f:controller: {}
              f:kind: {}
              f:name: {}
              f:uid: {}
        f:spec:
          f:containers:
            k:{"name":"traefik"}:
              .: {}
              f:args: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:livenessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:name: {}
              f:ports:
                .: {}
                k:{"containerPort":8000,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:name: {}
                  f:protocol: {}
                k:{"containerPort":8443,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:name: {}
                  f:protocol: {}
                k:{"containerPort":9000,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:name: {}
                  f:protocol: {}
                k:{"containerPort":9100,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:name: {}
                  f:protocol: {}
              f:readinessProbe:
                .: {}
                f:failureThreshold: {}
                f:httpGet:
                  .: {}
                  f:path: {}
                  f:port: {}
                  f:scheme: {}
                f:initialDelaySeconds: {}
                f:periodSeconds: {}
                f:successThreshold: {}
                f:timeoutSeconds: {}
              f:resources: {}
              f:securityContext:
                .: {}
                f:capabilities:
                  .: {}
                  f:drop: {}
                f:readOnlyRootFilesystem: {}
                f:runAsGroup: {}
                f:runAsNonRoot: {}
                f:runAsUser: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/data"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/tmp"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext:
            .: {}
            f:fsGroup: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:volumes:
            .: {}
            k:{"name":"data"}:
              .: {}
              f:emptyDir: {}
              f:name: {}
            k:{"name":"tmp"}:
              .: {}
              f:emptyDir: {}
              f:name: {}
      manager: kube-controller-manager
      operation: Update
      time: "2022-01-15T07:06:38Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"10.244.1.4"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      time: "2022-01-17T05:35:43Z"
    name: traefik-8b7b787f5-n8dj9
    namespace: traefik-test
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: traefik-8b7b787f5
      uid: 13471cf2-8a8f-4a76-b063-9c38a174bff9
    resourceVersion: "202477"
    uid: ee1cd981-b939-447f-a73d-9df01b150773
  spec:
    containers:
    - args:
      - --global.checknewversion
      - --global.sendanonymoususage
      - --entryPoints.metrics.address=:9100/tcp
      - --entryPoints.traefik.address=:9000/tcp
      - --entryPoints.web.address=:8000/tcp
      - --entryPoints.websecure.address=:8443/tcp
      - --api.dashboard=true
      - --ping=true
      - --metrics.prometheus=true
      - --metrics.prometheus.entrypoint=metrics
      - --providers.kubernetescrd
      - --providers.kubernetesingress
      image: traefik:2.5.6
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /ping
          port: 9000
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      name: traefik
      ports:
      - containerPort: 9100
        name: metrics
        protocol: TCP
      - containerPort: 9000
        name: traefik
        protocol: TCP
      - containerPort: 8000
        name: web
        protocol: TCP
      - containerPort: 8443
        name: websecure
        protocol: TCP
      readinessProbe:
        failureThreshold: 1
        httpGet:
          path: /ping
          port: 9000
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources: {}
      securityContext:
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsGroup: 65532
        runAsNonRoot: true
        runAsUser: 65532
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data
        name: data
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: traefik-token-zr9sz
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: testk8s-worker
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65532
    serviceAccount: traefik
    serviceAccountName: traefik
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: data
    - emptyDir: {}
      name: tmp
    - name: traefik-token-zr9sz
      secret:
        defaultMode: 420
        secretName: traefik-token-zr9sz
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-01-15T07:06:38Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-01-17T05:35:42Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-01-17T05:35:42Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-01-15T07:06:38Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e1ada2be1e46cb456b06348888960b87a93c4a9a26daa7d8025bdd8f104d91a3
      image: docker.io/library/traefik:2.5.6
      imageID: docker.io/library/traefik@sha256:2f603f8d3abe1dd3a4eb28960c55506be48293b41ea2c6ed4a4297c851a57a05
      lastState: {}
      name: traefik
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-01-15T07:08:20Z"
    hostIP: 172.18.0.3
    phase: Running
    podIP: 10.244.1.4
    podIPs:
    - ip: 10.244.1.4
    qosClass: BestEffort
    startTime: "2022-01-15T07:06:38Z"
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

 2022-01-17 22:19:31 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → kgn traefik-test services
NAME      TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
traefik   LoadBalancer   10.96.134.198   <pending>     80:32370/TCP,443:30395/TCP   2d9h

 2022-01-17 22:20:01 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → k port-forward traefik-8b7b787f5-n8dj9  -n traefik-apps 32370:9000 -n traefik-test
Forwarding from 127.0.0.1:32370 -> 9000
Forwarding from [::1]:32370 -> 9000
Handling connection for 32370
Handling connection for 32370
^C
 2022-01-17 22:23:05 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → kgn traefik-test services -oyaml
apiVersion: v1
items:
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: traefik
      meta.helm.sh/release-namespace: traefik-test
    creationTimestamp: "2022-01-15T07:06:38Z"
    labels:
      app.kubernetes.io/instance: traefik
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: traefik
      helm.sh/chart: traefik-10.9.1
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:meta.helm.sh/release-name: {}
            f:meta.helm.sh/release-namespace: {}
          f:labels:
            .: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:helm.sh/chart: {}
        f:spec:
          f:externalTrafficPolicy: {}
          f:ports:
            .: {}
            k:{"port":80,"protocol":"TCP"}:
              .: {}
              f:name: {}
              f:port: {}
              f:protocol: {}
              f:targetPort: {}
            k:{"port":443,"protocol":"TCP"}:
              .: {}
              f:name: {}
              f:port: {}
              f:protocol: {}
              f:targetPort: {}
          f:selector:
            .: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/name: {}
          f:sessionAffinity: {}
          f:type: {}
      manager: Go-http-client
      operation: Update
      time: "2022-01-15T07:06:38Z"
    name: traefik
    namespace: traefik-test
    resourceVersion: "103001"
    uid: e77b14d2-2c64-4252-9191-27ebcbdec704
  spec:
    clusterIP: 10.96.134.198
    clusterIPs:
    - 10.96.134.198
    externalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: web
      nodePort: 32370
      port: 80
      protocol: TCP
      targetPort: web
    - name: websecure
      nodePort: 30395
      port: 443
      protocol: TCP
      targetPort: websecure
    selector:
      app.kubernetes.io/instance: traefik
      app.kubernetes.io/name: traefik
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer: {}
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

 2022-01-17 22:23:20 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → cd ctemp/traefik
traefik-concept/                                                 traefik-telemetry-alert-rules.yaml
traefik-dashboard.yaml                                           traefik-test/
traefik-kafka-controller-58458f4cf-gh2rb2021_11_18_01_01_AM.log  traefik_code/
traefik-telemetry-alert-routes.yaml

 2022-01-17 22:23:20 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms
○ → cd ctemp/traefik-test/

 2022-01-17 22:25:29 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → ll
total 18728
-r--------  1 azhekhan  staff     2358 Jan 10 10:32 kubeconfig_context-c5ppxaonnda
-rw-r--r--  1 azhekhan  staff     4455 Jan 13 05:36 traefik-6ff7ff8ffd-cxgdn2022_01_12_11_33_PM.log
-rw-r--r--  1 azhekhan  staff     1006 Jan 13 05:41 traefik.service
-rw-r--r--  1 azhekhan  staff     7670 Jan 13 05:43 kf-kafka.kafka.yaml
-rw-r--r--  1 azhekhan  staff    12389 Jan 13 05:45 corp.shared-kafka.kafka.yaml
-rw-r--r--  1 azhekhan  staff      420 Jan 13 10:14 kf-kafka-kafka-0-ingressroutetcp.yaml
-rw-r--r--  1 azhekhan  staff      452 Jan 13 10:15 kf-kafka-kafka-bootstrap-ingressroutetcp.yaml
-rw-r--r--  1 azhekhan  staff      420 Jan 13 10:15 kf-kafka-kafka-1-ingressroutetcp.yaml
-rw-r--r--  1 azhekhan  staff      420 Jan 13 10:15 kf-kafka-kafka-2-ingressroutetcp.yaml
-rw-r--r--  1 azhekhan  staff     3186 Jan 13 10:35 kf-kafka.kafka_updated.yaml
-rw-r--r--  1 azhekhan  staff     1358 Jan 13 13:42 kf-kafka-kafka-tlsexternal-1.ingress.yaml
-rw-r--r--  1 azhekhan  staff     1331 Jan 13 13:54 kf-kafka-kafka-tlsexternal-0.ingress.yaml
-rw-r--r--  1 azhekhan  staff     3782 Jan 13 14:04 kf-kafka-kafka_updated2.yaml
-rw-r--r--  1 azhekhan  staff     2197 Jan 13 17:25 ca.crt
-rw-r--r--  1 azhekhan  staff     2897 Jan 13 19:01 traefik-test_values.yaml
-rw-r--r--  1 azhekhan  staff     1334 Jan 13 19:56 kf-kafka-kafka-0.ingress.1.yaml
-rw-r--r--  1 azhekhan  staff     1334 Jan 13 19:56 kf-kafka-kafka-1.ingress.1.yaml
-rw-r--r--  1 azhekhan  staff     1334 Jan 13 19:56 kf-kafka-kafka-2.ingress.1.yaml
-rw-r--r--  1 azhekhan  staff     1383 Jan 13 19:56 kf-kafka-kafka-bootstrap.ingress.1.yaml
-rw-r--r--  1 azhekhan  staff     3800 Jan 14 09:45 kf-kafka.cluster.kind.14jan22.yaml
-rw-r--r--  1 azhekhan  staff     8904 Jan 14 10:12 kf-kafka.kafka.WORKING.14Jan22.yaml
-rw-r--r--  1 azhekhan  staff     1379 Jan 14 10:23 kf-kafka-kafka-0.ingress.LB-14jan22.yaml
-rw-r--r--  1 azhekhan  staff     1379 Jan 14 10:23 kf-kafka-kafka-1.ingress.LB-14jan22.yaml
-rw-r--r--  1 azhekhan  staff     1379 Jan 14 10:23 kf-kafka-kafka-2.ingress.LB-14jan22.yaml
-rw-r--r--  1 azhekhan  staff     1428 Jan 14 10:23 kf-kafka-kafka-bootstrap.ingress.LB-14jan22.yaml
-rw-r--r--  1 azhekhan  staff     8904 Jan 17 09:53 kf-kafka.kafka.WORKING.17Jan22.yaml
-rw-r--r--  1 azhekhan  staff  8656381 Jan 17 16:40 traefik-kafka-controller-5f9f755d5-j6nzw2022_01_12_11_33_PM.log
-rw-r--r--  1 azhekhan  staff      238 Jan 17 22:25 traefik-web-ui.yaml

 2022-01-17 22:25:30 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → cat traefik-web-ui.yaml
apiVersion: v1
kind: Service
metadata:
  name: traefik-web-ui
  namespace: traefik-test
spec:
  selector:
    app.kubernetes.io/instance: traefik
    app.kubernetes.io/name: traefik
  ports:
  - name: web
    port: 80
    targetPort: 8080
 2022-01-17 22:25:41 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → k apply -f traefik-web-ui.yaml
service/traefik-web-ui created

 2022-01-17 22:25:46 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kgn traefik-test services
NAME             TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
traefik          LoadBalancer   10.96.134.198   <pending>     80:32370/TCP,443:30395/TCP   2d9h
traefik-web-ui   ClusterIP      10.96.110.208   <none>        80/TCP                       6s

 2022-01-17 22:25:52 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kdn traefik-test services traefik-web-ui
Name:              traefik-web-ui
Namespace:         traefik-test
Labels:            <none>
Annotations:       Selector:  app.kubernetes.io/instance=traefik,app.kubernetes.io/name=traefik
Type:              ClusterIP
IP:                10.96.110.208
Port:              web  80/TCP
TargetPort:        8080/TCP
Endpoints:         10.244.1.4:8080
Session Affinity:  None
Events:            <none>

 2022-01-17 22:26:26 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → mv traefik-web-ui.yaml  traefik-web-ui_svc.yaml

 2022-01-17 22:27:42 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → ll
total 18736
-r--------  1 azhekhan  staff     2358 Jan 10 10:32 kubeconfig_context-c5ppxaonnda
-rw-r--r--  1 azhekhan  staff     4455 Jan 13 05:36 traefik-6ff7ff8ffd-cxgdn2022_01_12_11_33_PM.log
-rw-r--r--  1 azhekhan  staff     1006 Jan 13 05:41 traefik.service
-rw-r--r--  1 azhekhan  staff     7670 Jan 13 05:43 kf-kafka.kafka.yaml
-rw-r--r--  1 azhekhan  staff    12389 Jan 13 05:45 corp.shared-kafka.kafka.yaml
-rw-r--r--  1 azhekhan  staff      420 Jan 13 10:14 kf-kafka-kafka-0-ingressroutetcp.yaml
-rw-r--r--  1 azhekhan  staff      452 Jan 13 10:15 kf-kafka-kafka-bootstrap-ingressroutetcp.yaml
-rw-r--r--  1 azhekhan  staff      420 Jan 13 10:15 kf-kafka-kafka-1-ingressroutetcp.yaml
-rw-r--r--  1 azhekhan  staff      420 Jan 13 10:15 kf-kafka-kafka-2-ingressroutetcp.yaml
-rw-r--r--  1 azhekhan  staff     3186 Jan 13 10:35 kf-kafka.kafka_updated.yaml
-rw-r--r--  1 azhekhan  staff     1358 Jan 13 13:42 kf-kafka-kafka-tlsexternal-1.ingress.yaml
-rw-r--r--  1 azhekhan  staff     1331 Jan 13 13:54 kf-kafka-kafka-tlsexternal-0.ingress.yaml
-rw-r--r--  1 azhekhan  staff     3782 Jan 13 14:04 kf-kafka-kafka_updated2.yaml
-rw-r--r--  1 azhekhan  staff     2197 Jan 13 17:25 ca.crt
-rw-r--r--  1 azhekhan  staff     2897 Jan 13 19:01 traefik-test_values.yaml
-rw-r--r--  1 azhekhan  staff     1334 Jan 13 19:56 kf-kafka-kafka-0.ingress.1.yaml
-rw-r--r--  1 azhekhan  staff     1334 Jan 13 19:56 kf-kafka-kafka-1.ingress.1.yaml
-rw-r--r--  1 azhekhan  staff     1334 Jan 13 19:56 kf-kafka-kafka-2.ingress.1.yaml
-rw-r--r--  1 azhekhan  staff     1383 Jan 13 19:56 kf-kafka-kafka-bootstrap.ingress.1.yaml
-rw-r--r--  1 azhekhan  staff     3800 Jan 14 09:45 kf-kafka.cluster.kind.14jan22.yaml
-rw-r--r--  1 azhekhan  staff     8904 Jan 14 10:12 kf-kafka.kafka.WORKING.14Jan22.yaml
-rw-r--r--  1 azhekhan  staff     1379 Jan 14 10:23 kf-kafka-kafka-0.ingress.LB-14jan22.yaml
-rw-r--r--  1 azhekhan  staff     1379 Jan 14 10:23 kf-kafka-kafka-1.ingress.LB-14jan22.yaml
-rw-r--r--  1 azhekhan  staff     1379 Jan 14 10:23 kf-kafka-kafka-2.ingress.LB-14jan22.yaml
-rw-r--r--  1 azhekhan  staff     1428 Jan 14 10:23 kf-kafka-kafka-bootstrap.ingress.LB-14jan22.yaml
-rw-r--r--  1 azhekhan  staff     8904 Jan 17 09:53 kf-kafka.kafka.WORKING.17Jan22.yaml
-rw-r--r--  1 azhekhan  staff  8656381 Jan 17 16:40 traefik-kafka-controller-5f9f755d5-j6nzw2022_01_12_11_33_PM.log
-rw-r--r--  1 azhekhan  staff      238 Jan 17 22:25 traefik-web-ui_svc.yaml
-rw-r--r--  1 azhekhan  staff      269 Jan 17 22:27 traefik-web-ui_ingress.yaml

 2022-01-17 22:27:52 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → #k apply -f traefik-web-ui_ingress.yaml

 2022-01-17 22:28:01 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → cat traefik-web-ui_ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: traefik-web-ui
  namespace: traefik-test
spec:
  rules:
  - host: traefik-ui.minikube
    http:
      paths:
      - path: /
        backend:
          serviceName: traefik-web-ui
          servicePort: web
 2022-01-17 22:28:06 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → k apply -f traefik-web-ui_ingress.yaml
ingress.extensions/traefik-web-ui created

 2022-01-17 22:28:09 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kgn traefik-test ingress
NAME             CLASS    HOSTS                 ADDRESS   PORTS   AGE
traefik-web-ui   <none>   traefik-ui.minikube             80      6s

 2022-01-17 22:28:15 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kdn traefik-test ingress
Name:             traefik-web-ui
Namespace:        traefik-test
Address:
Default backend:  default-http-backend:80 (<error: endpoints "default-http-backend" not found>)
Rules:
  Host                 Path  Backends
  ----                 ----  --------
  traefik-ui.minikube
                       /        traefik-web-ui:web (10.244.1.4:8080)
Annotations:           Events:  <none>

 2022-01-17 22:28:27 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → cat /etc/hosts
##
# Host Database
#
# localhost is used to configure the loopback interface
# when the system is booting.  Do not change this entry.
##
127.0.0.1 localhost
255.255.255.255 broadcasthost
::1             localhost
#144.25.105.44   versioning-corp-versioning.channels.ocs.oc-test.com

# Added by Docker Desktop
# To allow the same kube context to work on the host and the container:
127.0.0.1 kubernetes.docker.internal
# End of section

## TESTING for OCI App Gateway
130.35.238.196 admin.internal.local
130.35.238.196 az.oracle.com
130.35.238.196 admin

127.0.0.1 whoami.docker.local
147.154.156.34 hare.fra.dev.channels.ocs.oc-test.com
10.10.9.71 shared-kafka-broker-0-kafka.service.lhr-dataplane.dev.consul

 2022-01-17 22:28:57 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kgn traefik-test services
NAME             TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
traefik          LoadBalancer   10.96.134.198   <pending>     80:32370/TCP,443:30395/TCP   2d9h
traefik-web-ui   ClusterIP      10.96.110.208   <none>        80/TCP                       4m46s

 2022-01-17 22:30:32 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kgno -owide
NAME                    STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION     CONTAINER-RUNTIME
testk8s-control-plane   Ready    control-plane,master   4d    v1.20.7   172.18.0.2    <none>        Ubuntu 21.04   4.9.184-linuxkit   containerd://1.5.2
testk8s-worker          Ready    <none>                 4d    v1.20.7   172.18.0.3    <none>        Ubuntu 21.04   4.9.184-linuxkit   containerd://1.5.2

 2022-01-17 22:31:47 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kgn traefik-test services  traefik -oyaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    meta.helm.sh/release-name: traefik
    meta.helm.sh/release-namespace: traefik-test
  creationTimestamp: "2022-01-15T07:06:38Z"
  labels:
    app.kubernetes.io/instance: traefik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-10.9.1
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:meta.helm.sh/release-name: {}
          f:meta.helm.sh/release-namespace: {}
        f:labels:
          .: {}
          f:app.kubernetes.io/instance: {}
          f:app.kubernetes.io/managed-by: {}
          f:app.kubernetes.io/name: {}
          f:helm.sh/chart: {}
      f:spec:
        f:externalTrafficPolicy: {}
        f:ports:
          .: {}
          k:{"port":80,"protocol":"TCP"}:
            .: {}
            f:name: {}
            f:port: {}
            f:protocol: {}
            f:targetPort: {}
          k:{"port":443,"protocol":"TCP"}:
            .: {}
            f:name: {}
            f:port: {}
            f:protocol: {}
            f:targetPort: {}
        f:selector:
          .: {}
          f:app.kubernetes.io/instance: {}
          f:app.kubernetes.io/name: {}
        f:sessionAffinity: {}
        f:type: {}
    manager: Go-http-client
    operation: Update
    time: "2022-01-15T07:06:38Z"
  name: traefik
  namespace: traefik-test
  resourceVersion: "103001"
  uid: e77b14d2-2c64-4252-9191-27ebcbdec704
spec:
  clusterIP: 10.96.134.198
  clusterIPs:
  - 10.96.134.198
  externalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: web
    nodePort: 32370
    port: 80
    protocol: TCP
    targetPort: web
  - name: websecure
    nodePort: 30395
    port: 443
    protocol: TCP
    targetPort: websecure
  selector:
    app.kubernetes.io/instance: traefik
    app.kubernetes.io/name: traefik
  sessionAffinity: None
  type: LoadBalancer
status:
  loadBalancer: {}

 2022-01-17 22:33:27 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kdn traefik-test services  traefik
Name:                     traefik
Namespace:                traefik-test
Labels:                   app.kubernetes.io/instance=traefik
                          app.kubernetes.io/managed-by=Helm
                          app.kubernetes.io/name=traefik
                          helm.sh/chart=traefik-10.9.1
Annotations:              meta.helm.sh/release-name: traefik
                          meta.helm.sh/release-namespace: traefik-test
Selector:                 app.kubernetes.io/instance=traefik,app.kubernetes.io/name=traefik
Type:                     LoadBalancer
IP:                       10.96.134.198
Port:                     web  80/TCP
TargetPort:               web/TCP
NodePort:                 web  32370/TCP
Endpoints:                10.244.1.4:8000
Port:                     websecure  443/TCP
TargetPort:               websecure/TCP
NodePort:                 websecure  30395/TCP
Endpoints:                10.244.1.4:8443
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>

 2022-01-17 22:33:36 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → #10.96.110.208

 2022-01-17 22:36:22 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → #wrd traefik-8b7b787f5-n8dj9  -n traefik-apps 32370:9000 -n traefik-testdn traefik-test services  traefik

 2022-01-17 22:36:58 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → k port-forward traefik-8b7b787f5-n8dj9  -n traefik-apps 80:8080 -n traefik-test
Unable to listen on port 80: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:80: bind: permission denied unable to create listener: Error listen tcp6 [::1]:80: bind: permission denied]
error: unable to listen on any of the requested ports: [{80 8080}]

 2022-01-17 22:37:13 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → k port-forward traefik-8b7b787f5-n8dj9  -n traefik-apps 9000:8080 -n traefik-test
Forwarding from 127.0.0.1:9000 -> 8080
Forwarding from [::1]:9000 -> 8080
Handling connection for 9000
Handling connection for 9000
E0117 22:37:47.932447   70039 portforward.go:400] an error occurred forwarding 9000 -> 8080: error forwarding port 8080 to pod 586dbb1ee438ff789a2382aa6b13bbb994fda93955ad7f2d949548c2ebdc1b78, uid : failed to execute portforward in network namespace "/var/run/netns/cni-cb9db548-eaf3-c5db-01a5-e9b7a8c68cfa": failed to connect to localhost:8080 inside namespace "586dbb1ee438ff789a2382aa6b13bbb994fda93955ad7f2d949548c2ebdc1b78", IPv4: dial tcp4 127.0.0.1:8080: connect: connection refused IPv6 dial tcp6 [::1]:8080: connect: connection refused
E0117 22:37:47.932447   70039 portforward.go:400] an error occurred forwarding 9000 -> 8080: error forwarding port 8080 to pod 586dbb1ee438ff789a2382aa6b13bbb994fda93955ad7f2d949548c2ebdc1b78, uid : failed to execute portforward in network namespace "/var/run/netns/cni-cb9db548-eaf3-c5db-01a5-e9b7a8c68cfa": failed to connect to localhost:8080 inside namespace "586dbb1ee438ff789a2382aa6b13bbb994fda93955ad7f2d949548c2ebdc1b78", IPv4: dial tcp4 127.0.0.1:8080: connect: connection refused IPv6 dial tcp6 [::1]:8080: connect: connection refused
Handling connection for 9000
E0117 22:37:48.082703   70039 portforward.go:400] an error occurred forwarding 9000 -> 8080: error forwarding port 8080 to pod 586dbb1ee438ff789a2382aa6b13bbb994fda93955ad7f2d949548c2ebdc1b78, uid : failed to execute portforward in network namespace "/var/run/netns/cni-cb9db548-eaf3-c5db-01a5-e9b7a8c68cfa": failed to connect to localhost:8080 inside namespace "586dbb1ee438ff789a2382aa6b13bbb994fda93955ad7f2d949548c2ebdc1b78", IPv4: dial tcp4 127.0.0.1:8080: connect: connection refused IPv6 dial tcp6 [::1]:8080: connect: connection refused
Handling connection for 9000
Handling connection for 9000
E0117 22:37:49.214297   70039 portforward.go:400] an error occurred forwarding 9000 -> 8080: error forwarding port 8080 to pod 586dbb1ee438ff789a2382aa6b13bbb994fda93955ad7f2d949548c2ebdc1b78, uid : failed to execute portforward in network namespace "/var/run/netns/cni-cb9db548-eaf3-c5db-01a5-e9b7a8c68cfa": failed to connect to localhost:8080 inside namespace "586dbb1ee438ff789a2382aa6b13bbb994fda93955ad7f2d949548c2ebdc1b78", IPv4: dial tcp4 127.0.0.1:8080: connect: connection refused IPv6 dial tcp6 [::1]:8080: connect: connection refused
E0117 22:37:49.217423   70039 portforward.go:400] an error occurred forwarding 9000 -> 8080: error forwarding port 8080 to pod 586dbb1ee438ff789a2382aa6b13bbb994fda93955ad7f2d949548c2ebdc1b78, uid : failed to execute portforward in network namespace "/var/run/netns/cni-cb9db548-eaf3-c5db-01a5-e9b7a8c68cfa": failed to connect to localhost:8080 inside namespace "586dbb1ee438ff789a2382aa6b13bbb994fda93955ad7f2d949548c2ebdc1b78", IPv4: dial tcp4 127.0.0.1:8080: connect: connection refused IPv6 dial tcp6 [::1]:8080: connect: connection refused
Handling connection for 9000
E0117 22:38:19.267604   70039 portforward.go:340] error creating error stream for port 9000 -> 8080: Timeout occured
Handling connection for 9000
Handling connection for 9000
E0117 22:38:20.401864   70039 portforward.go:400] an error occurred forwarding 9000 -> 8080: error forwarding port 8080 to pod 586dbb1ee438ff789a2382aa6b13bbb994fda93955ad7f2d949548c2ebdc1b78, uid : failed to execute portforward in network namespace "/var/run/netns/cni-cb9db548-eaf3-c5db-01a5-e9b7a8c68cfa": failed to connect to localhost:8080 inside namespace "586dbb1ee438ff789a2382aa6b13bbb994fda93955ad7f2d949548c2ebdc1b78", IPv4: dial tcp4 127.0.0.1:8080: connect: connection refused IPv6 dial tcp6 [::1]:8080: connect: connection refused
E0117 22:38:20.464782   70039 portforward.go:400] an error occurred forwarding 9000 -> 8080: error forwarding port 8080 to pod 586dbb1ee438ff789a2382aa6b13bbb994fda93955ad7f2d949548c2ebdc1b78, uid : failed to execute portforward in network namespace "/var/run/netns/cni-cb9db548-eaf3-c5db-01a5-e9b7a8c68cfa": failed to connect to localhost:8080 inside namespace "586dbb1ee438ff789a2382aa6b13bbb994fda93955ad7f2d949548c2ebdc1b78", IPv4: dial tcp4 127.0.0.1:8080: connect: connection refused IPv6 dial tcp6 [::1]:8080: connect: connection refused
Handling connection for 9000
^C
 2022-01-17 22:38:23 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kgn traefik-test services
NAME             TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
traefik          LoadBalancer   10.96.134.198   <pending>     80:32370/TCP,443:30395/TCP   2d10h
traefik-web-ui   ClusterIP      10.96.110.208   <none>        80/TCP                       13m

 2022-01-17 22:39:10 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kdn traefik-test services traefik-web-ui
Name:              traefik-web-ui
Namespace:         traefik-test
Labels:            <none>
Annotations:       Selector:  app.kubernetes.io/instance=traefik,app.kubernetes.io/name=traefik
Type:              ClusterIP
IP:                10.96.110.208
Port:              web  80/TCP
TargetPort:        8080/TCP
Endpoints:         10.244.1.4:8080
Session Affinity:  None
Events:            <none>

 2022-01-17 22:39:19 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kind cluster list
kind creates and manages local Kubernetes clusters using Docker container 'nodes'

Usage:
  kind [command]

Available Commands:
  build       Build one of [node-image]
  completion  Output shell completion code for the specified shell (bash, zsh or fish)
  create      Creates one of [cluster]
  delete      Deletes one of [cluster]
  export      Exports one of [kubeconfig, logs]
  get         Gets one of [clusters, nodes, kubeconfig]
  help        Help about any command
  load        Loads images into nodes
  version     Prints the kind CLI version

Flags:
  -h, --help              help for kind
      --loglevel string   DEPRECATED: see -v instead
  -q, --quiet             silence all stderr output
  -v, --verbosity int32   info log verbosity
      --version           version for kind

Use "kind [command] --help" for more information about a command.

 2022-01-17 22:40:24 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kind cluster get
kind creates and manages local Kubernetes clusters using Docker container 'nodes'

Usage:
  kind [command]

Available Commands:
  build       Build one of [node-image]
  completion  Output shell completion code for the specified shell (bash, zsh or fish)
  create      Creates one of [cluster]
  delete      Deletes one of [cluster]
  export      Exports one of [kubeconfig, logs]
  get         Gets one of [clusters, nodes, kubeconfig]
  help        Help about any command
  load        Loads images into nodes
  version     Prints the kind CLI version

Flags:
  -h, --help              help for kind
      --loglevel string   DEPRECATED: see -v instead
  -q, --quiet             silence all stderr output
  -v, --verbosity int32   info log verbosity
      --version           version for kind

Use "kind [command] --help" for more information about a command.

 2022-01-17 22:40:28 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kind cluster get --help
kind creates and manages local Kubernetes clusters using Docker container 'nodes'

Usage:
  kind [command]

Available Commands:
  build       Build one of [node-image]
  completion  Output shell completion code for the specified shell (bash, zsh or fish)
  create      Creates one of [cluster]
  delete      Deletes one of [cluster]
  export      Exports one of [kubeconfig, logs]
  get         Gets one of [clusters, nodes, kubeconfig]
  help        Help about any command
  load        Loads images into nodes
  version     Prints the kind CLI version

Flags:
  -h, --help              help for kind
      --loglevel string   DEPRECATED: see -v instead
  -q, --quiet             silence all stderr output
  -v, --verbosity int32   info log verbosity
      --version           version for kind

Use "kind [command] --help" for more information about a command.

 2022-01-17 22:40:36 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kind get cluster  --help
Gets one of [clusters, nodes, kubeconfig]

Usage:
  kind get [flags]
  kind get [command]

Available Commands:
  clusters    Lists existing kind clusters by their name
  kubeconfig  Prints cluster kubeconfig
  nodes       Lists existing kind nodes by their name

Flags:
  -h, --help   help for get

Global Flags:
      --loglevel string   DEPRECATED: see -v instead
  -q, --quiet             silence all stderr output
  -v, --verbosity int32   info log verbosity

Use "kind get [command] --help" for more information about a command.

 2022-01-17 22:40:45 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kind get clusters
testk8s

 2022-01-17 22:40:54 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kind get nodes
No kind nodes found for cluster "kind".

 2022-01-17 22:41:11 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kind get kubeconfig
ERROR: could not locate any control plane nodes

 2022-01-17 22:41:29 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kind get cluster --help
Gets one of [clusters, nodes, kubeconfig]

Usage:
  kind get [flags]
  kind get [command]

Available Commands:
  clusters    Lists existing kind clusters by their name
  kubeconfig  Prints cluster kubeconfig
  nodes       Lists existing kind nodes by their name

Flags:
  -h, --help   help for get

Global Flags:
      --loglevel string   DEPRECATED: see -v instead
  -q, --quiet             silence all stderr output
  -v, --verbosity int32   info log verbosity

Use "kind get [command] --help" for more information about a command.

 2022-01-17 22:41:41 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kind get cluster nodes
ERROR: unknown command "cluster" for "kind get"

 2022-01-17 22:41:51 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kgno
NAME                    STATUS   ROLES                  AGE   VERSION
testk8s-control-plane   Ready    control-plane,master   4d    v1.20.7
testk8s-worker          Ready    <none>                 4d    v1.20.7

 2022-01-17 22:42:06 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kgno -owide
NAME                    STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION     CONTAINER-RUNTIME
testk8s-control-plane   Ready    control-plane,master   4d    v1.20.7   172.18.0.2    <none>        Ubuntu 21.04   4.9.184-linuxkit   containerd://1.5.2
testk8s-worker          Ready    <none>                 4d    v1.20.7   172.18.0.3    <none>        Ubuntu 21.04   4.9.184-linuxkit   containerd://1.5.2

 2022-01-17 22:42:16 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → ll /usr/local/bin/minikube
-rwxr-xr-x  1 azhekhan  wheel  54279240 Jun  9  2020 /usr/local/bin/minikube

 2022-01-17 22:46:13 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → minikube start
🎉  minikube 1.24.0 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.24.0
💡  To disable this notice, run: 'minikube config set WantUpdateNotification false'

🙄  minikube v1.9.2 on Darwin 10.15.7
    ▪ KUBECONFIG=/Users/azhekhan/.kube/config
✨  Using the virtualbox driver based on existing profile
👍  Starting control plane node m01 in cluster minikube
🔄  Restarting existing virtualbox VM for "minikube" ...
🐳  Preparing Kubernetes v1.18.0 on Docker 19.03.8 ...
🤦  Unable to restart cluster, will reset it: getting k8s client: client config: client config: context "minikube" does not exist
💥  initialization failed, will try again: run: /bin/bash -c "sudo env PATH=/var/lib/minikube/binaries/v1.18.0:$PATH kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,SystemVerification": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.18.0
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority

stderr:
W0117 23:52:06.224786    4364 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
  [WARNING Service-Docker]: docker service is not enabled, please run 'systemctl enable docker.service'
  [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
  [WARNING DirAvailable--var-lib-minikube-etcd]: /var/lib/minikube/etcd is not empty
error execution phase certs/apiserver: failed to write or validate certificate "apiserver": failure loading apiserver certificate: failed to load certificate: the certificate has expired
To see the stack trace of this error execute with --v=5 or higher


💣  Error starting cluster: run: /bin/bash -c "sudo env PATH=/var/lib/minikube/binaries/v1.18.0:$PATH kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,SystemVerification": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.18.0
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority

stderr:
W0117 23:52:08.295342    4542 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
  [WARNING Service-Docker]: docker service is not enabled, please run 'systemctl enable docker.service'
  [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
  [WARNING DirAvailable--var-lib-minikube-etcd]: /var/lib/minikube/etcd is not empty
error execution phase certs/apiserver: failed to write or validate certificate "apiserver": failure loading apiserver certificate: failed to load certificate: the certificate has expired
To see the stack trace of this error execute with --v=5 or higher


😿  minikube is exiting due to an error. If the above message is not useful, open an issue:
👉  https://github.com/kubernetes/minikube/issues/new/choose

 2022-01-18 05:22:12 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ →

 2022-01-18 05:23:05 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → systemctl enable docker.service
-bash: systemctl: command not found

 2022-01-18 05:24:18 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → docker ps
CONTAINER ID        IMAGE                              COMMAND                  CREATED             STATUS              PORTS                     NAMES
b92c49bffede        kindest/node:v1.20.7               "/usr/local/bin/entr…"   4 days ago          Up 4 days                                     testk8s-worker
d807e44762f9        kindest/node:v1.20.7               "/usr/local/bin/entr…"   4 days ago          Up 4 days           0.0.0.0:52594->6443/tcp   testk8s-control-plane
667461b30e1b        cpe-workstation-v2-230921:1.0.84   "bash"                   5 days ago          Up 5 days                                     suspicious_mayer
b32ff8da6bc7        cpe-workstation-v2-230921:1.0.84   "bash"                   5 days ago          Up 5 days                                     bold_borg

 2022-01-18 05:24:22 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → sudo minikube start
Password:
😄  minikube v1.9.2 on Darwin 10.15.7
✨  Using the virtualbox driver based on existing profile
🛑  The "virtualbox" driver should not be used with root privileges.
💡  If you are running minikube within a VM, consider using --driver=none:
📘    https://minikube.sigs.k8s.io/docs/reference/drivers/none/

 2022-01-18 05:25:09 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kubectl get po -A
NAMESPACE            NAME                                            READY   STATUS    RESTARTS   AGE
kube-system          coredns-74ff55c5b-5v2qs                         1/1     Running   0          4d7h
kube-system          coredns-74ff55c5b-hwkjm                         1/1     Running   0          4d7h
kube-system          etcd-testk8s-control-plane                      1/1     Running   0          4d7h
kube-system          kindnet-8wt72                                   1/1     Running   0          4d7h
kube-system          kindnet-xnxs9                                   1/1     Running   0          4d7h
kube-system          kube-apiserver-testk8s-control-plane            1/1     Running   0          4d7h
kube-system          kube-controller-manager-testk8s-control-plane   1/1     Running   5          4d7h
kube-system          kube-proxy-qmvtd                                1/1     Running   0          4d7h
kube-system          kube-proxy-rrf5p                                1/1     Running   0          4d7h
kube-system          kube-scheduler-testk8s-control-plane            1/1     Running   7          4d7h
local-path-storage   local-path-provisioner-547f784dff-pslrd         1/1     Running   5          4d7h
traefik-apps         bear-5cff87db68-st4fz                           1/1     Running   0          4d7h
traefik-apps         bear-5cff87db68-vw22p                           1/1     Running   0          4d7h
traefik-test         traefik-8b7b787f5-n8dj9                         1/1     Running   0          2d16h

 2022-01-18 05:25:40 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → minikube start
😄  minikube v1.9.2 on Darwin 10.15.7
    ▪ KUBECONFIG=/Users/azhekhan/.kube/config
✨  Using the virtualbox driver based on existing profile
👍  Starting control plane node m01 in cluster minikube
🏃  Updating the running virtualbox "minikube" VM ...
🐳  Preparing Kubernetes v1.18.0 on Docker 19.03.8 ...
🤦  Unable to restart cluster, will reset it: getting k8s client: client config: client config: context "minikube" does not exist
💥  initialization failed, will try again: run: /bin/bash -c "sudo env PATH=/var/lib/minikube/binaries/v1.18.0:$PATH kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,SystemVerification": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.18.0
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority

stderr:
W0117 23:56:04.224858    8891 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
  [WARNING Service-Docker]: docker service is not enabled, please run 'systemctl enable docker.service'
  [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
  [WARNING DirAvailable--var-lib-minikube-etcd]: /var/lib/minikube/etcd is not empty
error execution phase certs/apiserver: failed to write or validate certificate "apiserver": failure loading apiserver certificate: failed to load certificate: the certificate has expired
To see the stack trace of this error execute with --v=5 or higher


💣  Error starting cluster: run: /bin/bash -c "sudo env PATH=/var/lib/minikube/binaries/v1.18.0:$PATH kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,SystemVerification": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.18.0
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority

stderr:
W0117 23:56:05.529817    9073 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
  [WARNING Service-Docker]: docker service is not enabled, please run 'systemctl enable docker.service'
  [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
  [WARNING DirAvailable--var-lib-minikube-etcd]: /var/lib/minikube/etcd is not empty
error execution phase certs/apiserver: failed to write or validate certificate "apiserver": failure loading apiserver certificate: failed to load certificate: the certificate has expired
To see the stack trace of this error execute with --v=5 or higher


😿  minikube is exiting due to an error. If the above message is not useful, open an issue:
👉  https://github.com/kubernetes/minikube/issues/new/choose

 2022-01-18 05:26:09 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → minikube status
E0118 05:27:15.570876   72505 status.go:233] kubeconfig endpoint: extract IP: "minikube" does not appear in /Users/azhekhan/.kube/config
m01
host: Running
kubelet: Starting
apiserver: Stopped
kubeconfig: Misconfigured


WARNING: Your kubectl is pointing to stale minikube-vm.
To fix the kubectl context, run `minikube update-context`

 2022-01-18 05:27:15 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → ls -altr ~
total 14488
-rw-------     1 azhekhan  staff        3 Mar  7  2019 .CFUserTextEncoding
drwxr-xr-x     4 azhekhan  staff      128 Mar  7  2019 Public
drwxr-xr-x     3 azhekhan  staff       96 Mar 13  2019 opensource
drwxr-xr-x     4 azhekhan  staff      128 Mar 14  2019 .cisco
drwxr-xr-x    15 azhekhan  staff      480 Mar 20  2019 .atom
drwx------     4 azhekhan  staff      128 Apr  2  2019 .putty
drwx------     3 azhekhan  staff       96 Apr  9  2019 .cups
-rwxrwxrwx     1 azhekhan  staff      303 Apr 10  2019 azsetup.sh
-rw-r--r--@    1 azhekhan  staff    14461 Apr 10  2019 macos_bash_profile
drwxr-xr-x     3 azhekhan  staff       96 Apr 10  2019 .gem
-rw-r--r--     1 azhekhan  staff        0 Apr 11  2019 OMCS
-rwxr-xr-x@    1 azhekhan  staff  2281261 Apr 21  2019 childhood_ver_2.0.pptx
drwxr-xr-x     3 azhekhan  staff       96 Apr 30  2019 PycharmProjects
-rwxr-xr-x     1 azhekhan  staff     4253 Apr 30  2019 .bash_profile_30apr2019
-rwxrwxrwx     1 azhekhan  staff     6159 Apr 30  2019 .bash_profile.bak
-rw-r--r--     1 azhekhan  staff       53 Apr 30  2019 .nwinkler_random_colors
drwxr-xr-x    27 azhekhan  staff      864 Apr 30  2019 .bash_it
drwxr-xr-x    57 azhekhan  staff     1824 May 15  2019 .pylint.d
-rw-r--r--     1 azhekhan  staff      233 Jun  7  2019 .gitconfig
drwxr-xr-x    12 azhekhan  staff      384 Jun 10  2019 terraform
drwxr-xr-x     4 azhekhan  staff      128 Jun 11  2019 .ansible
drwxr-xr-x     3 azhekhan  staff       96 Jun 13  2019 bash_scripting
drwxr-xr-x     8 azhekhan  staff      256 Jun 17  2019 Python_Projects
-rw-------     1 azhekhan  staff     2086 Jun 21  2019 .python_history
drwxr-xr-x@    3 azhekhan  staff       96 Jul  4  2019 aws
drwx------     3 azhekhan  staff       96 Jul  7  2019 .local
drwx------     5 azhekhan  staff      160 Jul  7  2019 .cache
-rwxr-xr-x     1 azhekhan  staff    46167 Jul 11  2019 masterkey_HCMTDE92856.zip
-rw-r--r--     1 azhekhan  staff      350 Jul 29  2019 test_amex.sh
-rw-r--r--     1 azhekhan  staff     5395 Aug 22  2019 a
drwxrwxrwx     6 azhekhan  staff      192 Sep 20  2019 vault
-rw-------     1 azhekhan  staff       26 Sep 22  2019 .vault-token
drwxr-xr-x    14 azhekhan  staff      448 Sep 23  2019 CloudManager
drwx------     5 azhekhan  staff      160 Oct 22  2019 .gnupg_pre_2.1
-rw-r--r--@    1 azhekhan  staff    38698 Oct 24  2019 Namaz_Time_BLR.pdf
drwxr-xr-x     5 azhekhan  staff      160 Nov  6  2019 bin
drwxr-xr-x     6 azhekhan  staff      192 Nov  6  2019 .kube_bkup
drwxr-xr-x     5 azhekhan  staff      160 Nov  6  2019 go
drwxr-xr-x     6 azhekhan  staff      192 Nov  6  2019 .helm
drwxr-xr-x    10 azhekhan  staff      320 Nov 12  2019 .kube_bkup_12Nov19
drwx------    16 azhekhan  staff      512 Nov 15  2019 .gnupg_bkup
drwx------    13 azhekhan  staff      416 Nov 28  2019 .gnupg_workingSOPs
drwxr-xr-x     4 azhekhan  staff      128 Nov 29  2019 .vscode
-rwxrwxrwx     1 azhekhan  staff    12353 Dec 12  2019 az_oci.zip
-rwxrwxrwx     1 azhekhan  staff   339804 Dec 12  2019 az_kubeconfig.zip
-rwxrwxrwx     1 azhekhan  staff   715936 Dec 12  2019 az_gnupgconfig.zip
-rwxrwxrwx     1 azhekhan  staff  1068589 Dec 12  2019 az_sre.zip
drwxr-xr-x@   10 azhekhan  staff      320 Dec 20  2019 kafka_2.12-2.4.0
drwxr-xr-x     6 azhekhan  staff      192 Jan  6  2020 .kafkatool2
drwxr-xr-x     4 azhekhan  staff      128 Feb 11  2020 ansible
drwxr-xr-x     8 azhekhan  staff      256 Feb 11  2020 vagrant_dev
-rw-r--r--     1 azhekhan  staff      347 Mar  2  2020 .t
drwxr-xr-x     6 azhekhan  staff      192 Mar 12  2020 .bkupdocker
drwx------     8 azhekhan  staff      256 Apr  5  2020 VirtualBox VMs
drwx------    13 azhekhan  staff      416 Apr  7  2020 .gnupg_sre
-rw-r--r--     1 root      staff  2407973 Apr 17  2020 az_ocikube_setup.zip
drwxr-xr-x    23 azhekhan  staff      736 Apr 17  2020 .kube_sre
drwxr-xr-x    23 azhekhan  staff      736 Apr 17  2020 kube_oci
-rw-r--r--     1 azhekhan  staff       32 Apr 20  2020 .artifactory_user
drwxr-xr-x    27 azhekhan  staff      864 Apr 21  2020 .oci_sre
drwxr-xr-x     4 azhekhan  staff      128 Apr 21  2020 kubernetes
drwx------     5 azhekhan  staff      160 May  8  2020 .ssh_sre
drwxr-xr-x     9 azhekhan  staff      288 Jun 10  2020 k8s
drwxr-xr-x     5 azhekhan  staff      160 Jun 12  2020 minikubetraining
drwxr-xr-x    27 azhekhan  staff      864 Jun 25  2020 sre_bkup_oci
-rw-r--r--     1 azhekhan  staff    21861 Jun 25  2020 az_sre_oci.zip
drwxr-xr-x     3 azhekhan  staff       96 Jul  2  2020 OSVC
drwxr-xr-x     2 azhekhan  staff       64 Jul 22  2020 .aws
drwxr-xr-x     5 root      admin      160 Sep 22  2020 ..
drwxr-xr-x     3 azhekhan  staff       96 Sep 29  2020 .m2
drwxr-xr-x     2 azhekhan  staff       64 Oct 13  2020 GoogleDriveAzherCan
drwxr-xr-x     3 azhekhan  staff       96 Oct 13  2020 .vim
drwxr-xr-x    12 azhekhan  staff      384 Dec 24  2020 .anydesk
drwx------    15 azhekhan  staff      480 Jan 21  2021 .gnupg
drwx------     8 azhekhan  staff      256 Jan 27  2021 .ssh
drwxr-xr-x     2 azhekhan  staff       64 Feb 17  2021 .matplotlib
drwx------     5 azhekhan  staff      160 Feb 26  2021 Pictures
drwxr-xr-x    12 azhekhan  staff      384 Mar 22  2021 docker
drwxr-xr-x    10 azhekhan  staff      320 Mar 22  2021 .vagrant.d
drwxr-xr-x     3 azhekhan  staff       96 Mar 28  2021 .oracle_jre_usage
drwxr-xr-x     2 azhekhan  staff       64 Mar 28  2021 Oracle Content Extension for Adobe
drwxr-xr-x    29 azhekhan  staff      928 Apr 24  2021 jenkins_home
drwx------     6 azhekhan  staff      192 Jun  2  2021 Music
drwx------     6 azhekhan  staff      192 Jun  2  2021 Movies
drwxr-xr-x     6 azhekhan  staff      192 Jun  4  2021 .rest-client
drwx------@   84 azhekhan  staff     2688 Jun 16  2021 Library
drwx------     6 azhekhan  staff      192 Jun 19  2021 .config
drwx------@    5 azhekhan  staff      160 Jul  2  2021 Applications
-rw-r--r--     1 azhekhan  staff    23464 Jul  6  2021 oci_config_bkup060721.zip
drwxr-xr-x    31 azhekhan  staff      992 Jul  6  2021 oci_config_bkup02Aug2021
drwxr-xr-x    29 azhekhan  staff      928 Jul 15  2021 OSVC_Code
-rwxrwxrwx     1 azhekhan  staff    15797 Jul 15  2021 .bash_profile.pysave
drwxr-xr-x     3 azhekhan  staff       96 Aug  2 10:30 lib
-rw-r--r--     1 azhekhan  staff    15965 Aug  2 10:42 .bash_profile.backup
-rwxrwxrwx     1 azhekhan  staff    16586 Sep 23 13:01 .bash_profile
-rw-r--r--     1 azhekhan  staff      454 Oct 19 06:33 .wget-hsts
drwx------@   23 azhekhan  staff      736 Oct 26 22:19 Dropbox
drwx------    21 azhekhan  staff      672 Oct 28 09:32 .bash_sessions
drwx------    15 azhekhan  staff      480 Nov 10 11:20 .dropbox
drwx------@   10 azhekhan  staff      320 Nov 10 18:04 Google Drive
-rw-r--r--     1 azhekhan  staff      770 Nov 17 00:56 .anyconnect
drwxr-xr-x    25 azhekhan  staff      800 Nov 25 05:30 kube_oci_sre
drwxr-xr-x    24 azhekhan  staff      768 Dec  1 03:41 Documents
-rw-------     1 azhekhan  staff      726 Dec 17 02:15 .lesshst
drwx------@    4 azhekhan  staff      128 Dec 20 01:31 Creative Cloud Files
-rwxrwxrwx     1 root      staff   199070 Jan 11 17:45 .bash_history
lrwx------     1 azhekhan  staff       20 Jan 12 15:56 azhercan@gmail.com - Google Drive -> /Volumes/GoogleDrive
drwx------@   39 azhekhan  staff     1248 Jan 12 20:57 Desktop
lrwx------     1 azhekhan  staff       42 Jan 13 04:55 azherullahkhan@gmail.com - Google Drive -> /Volumes/GoogleDrive-115210426746736877791
drwxr-xr-x     6 azhekhan  staff      192 Jan 13 04:55 .terraform.d
drwx------     5 azhekhan  staff      160 Jan 13 04:56 .docker
drwx------@   25 azhekhan  staff      800 Jan 14 06:59 OneDrive
drwx------     2 azhekhan  staff       64 Jan 17 12:55 .Trash
drwxr-xr-x    31 azhekhan  staff      992 Jan 17 15:27 .oci
drwx------@ 1302 azhekhan  staff    41664 Jan 17 17:04 Downloads
-rw-r--r--@    1 azhekhan  staff    53252 Jan 17 17:09 .DS_Store
-rw-------     1 azhekhan  staff    44793 Jan 17 22:10 .viminfo
drwxr-xr-x   120 azhekhan  staff     3840 Jan 17 22:10 .
drwxr-xr-x    45 azhekhan  staff     1440 Jan 17 22:10 .kube
drwxr-xr-x    20 azhekhan  staff      640 Jan 18 05:26 .minikube

 2022-01-18 05:28:09 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → ls -altr ~/.minikube
total 64
drwxr-xr-x    2 azhekhan  staff    64 Jun  9  2020 config
drwxr-xr-x    2 azhekhan  staff    64 Jun  9  2020 addons
drwxr-xr-x    2 azhekhan  staff    64 Jun  9  2020 files
drwxr-xr-x    2 azhekhan  staff    64 Jun  9  2020 logs
drwxr-xr-x    3 azhekhan  staff    96 Jun  9  2020 bin
drwxr-xr-x    4 azhekhan  staff   128 Jun  9  2020 cache
drwx------    3 azhekhan  staff    96 Jun  9  2020 profiles
drwxr-xr-x    6 azhekhan  staff   192 Jun  9  2020 certs
drwxr-xr-x    5 azhekhan  staff   160 Jun  9  2020 machines
-rw-r--r--    1 azhekhan  staff  1066 Jun  9  2020 ca.crt
-rw-------    1 azhekhan  staff  1679 Jun  9  2020 ca.key
-rw-r--r--    1 azhekhan  staff  1074 Jun  9  2020 proxy-client-ca.crt
-rw-------    1 azhekhan  staff  1675 Jun  9  2020 proxy-client-ca.key
drwxr-xr-x    5 azhekhan  staff   160 Jun 11  2020 .kube
drwxr-xr-x  120 azhekhan  staff  3840 Jan 17 22:10 ..
-rw-r--r--    1 azhekhan  staff    29 Jan 18 05:21 last_update_check
-rwxrwxrwx    1 azhekhan  staff  1042 Jan 18 05:26 ca.pem
-rwxrwxrwx    1 azhekhan  staff  1082 Jan 18 05:26 cert.pem
drwxr-xr-x   20 azhekhan  staff   640 Jan 18 05:26 .
-rwxrwxrwx    1 azhekhan  staff  1675 Jan 18 05:26 key.pem

 2022-01-18 05:28:21 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → mv ~/.minikube ~/minikube_bkup

 2022-01-18 05:29:04 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → minikube start
🎉  minikube 1.24.0 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.24.0
💡  To disable this notice, run: 'minikube config set WantUpdateNotification false'

🙄  minikube v1.9.2 on Darwin 10.15.7
    ▪ KUBECONFIG=/Users/azhekhan/.kube/config
✨  Automatically selected the hyperkit driver. Other choices: docker, virtualbox
💾  Downloading driver docker-machine-driver-hyperkit:
    > docker-machine-driver-hyperkit.sha256: 65 B / 65 B [---] 100.00% ? p/s 0s
    > docker-machine-driver-hyperkit: 10.90 MiB / 10.90 MiB  100.00% 2.94 MiB p
🔑  The 'hyperkit' driver requires elevated permissions. The following commands will be executed:

    $ sudo chown root:wheel /Users/azhekhan/.minikube/bin/docker-machine-driver-hyperkit
    $ sudo chmod u+s /Users/azhekhan/.minikube/bin/docker-machine-driver-hyperkit


💿  Downloading VM boot image ...
    > minikube-v1.9.0.iso.sha256: 65 B / 65 B [--------------] 100.00% ? p/s 0s
    > minikube-v1.9.0.iso: 174.93 MiB / 174.93 MiB [ 100.00% 2.04 MiB p/s 1m26s
👍  Starting control plane node m01 in cluster minikube
💾  Downloading Kubernetes v1.18.0 preload ...
    > preloaded-images-k8s-v2-v1.18.0-docker-overlay2-amd64.tar.lz4: 542.91 MiB
🔥  Creating hyperkit VM (CPUs=2, Memory=4000MB, Disk=20000MB) ...
🐳  Preparing Kubernetes v1.18.0 on Docker 19.03.8 ...
🌟  Enabling addons: default-storageclass, storage-provisioner
🏄  Done! kubectl is now configured to use "minikube"

 2022-01-18 05:37:16 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → minikube --help
Minikube is a CLI tool that provisions and manages single-node Kubernetes clusters optimized for development workflows.

Basic Commands:
  start          Starts a local kubernetes cluster
  status         Gets the status of a local kubernetes cluster
  stop           Stops a running local kubernetes cluster
  delete         Deletes a local kubernetes cluster
  dashboard      Access the kubernetes dashboard running within the minikube cluster
  pause          pause containers
  unpause        unpause Kubernetes

Images Commands:
  docker-env     Sets up docker env variables; similar to '$(docker-machine env)'
  podman-env     Sets up podman env variables; similar to '$(podman-machine env)'
  cache          Add or delete an image from the local cache.

Configuration and Management Commands:
  addons         Modify minikube's kubernetes addons
  config         Modify minikube config
  profile        Profile gets or sets the current minikube profile
  update-context Verify the IP address of the running cluster in kubeconfig.

Networking and Connectivity Commands:
  service        Gets the kubernetes URL(s) for the specified service in your local cluster
  tunnel         tunnel makes services of type LoadBalancer accessible on localhost

Advanced Commands:
  mount          Mounts the specified directory into minikube
  ssh            Log into or run a command on a machine with SSH; similar to 'docker-machine ssh'
  kubectl        Run kubectl
  node           Node operations

Troubleshooting Commands:
  ssh-key        Retrieve the ssh identity key path of the specified cluster
  ip             Retrieves the IP address of the running cluster
  logs           Gets the logs of the running instance, used for debugging minikube, not user code.
  update-check   Print current and latest version number
  version        Print the version of minikube
  options        Show a list of global command-line options (applies to all commands).

Other Commands:
  completion     Outputs minikube shell completion for the given shell (bash or zsh)

Use "minikube <command> --help" for more information about a given command.

 2022-01-18 05:37:26 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → minikube status
m01
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured


 2022-01-18 05:37:31 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kgns
NAME              STATUS   AGE
default           Active   31s
kube-node-lease   Active   32s
kube-public       Active   32s
kube-system       Active   32s

 2022-01-18 05:37:43 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → env | grep -i kube
KUBECONFIG=/Users/azhekhan/.kube/config

 2022-01-18 05:37:53 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → cat /Users/azhekhan/.kube/config
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1ERXhNekUyTXpFek4xb1hEVE15TURFeE1URTJNekV6TjFvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTG1FCnV5RWc1cWdpZDlsVFhzM01tdXE3Ymk5N1U0ZXk4M3dIclgrZUFRYXV1b1A3dXlzS3VPbmRlOXhwNndCWGFqRVcKQ2VINDNwZEJxZzV3UWZ1TEJPVWE2UkV4NHlsYTVCT21GU3cxMW4xQkFFYmhORUdDNmg2ZE8xUXUyOVZZVjhzdgpOUmxrOWEvekdrYklDRW00QmJHblMySjhPVVFteW5OQThuVWZkdWNPNnU3dHZaZFdZTzNPamlmREg4NlFFbHozCmlQdGNlMUhwS3lPcXZiRndBak5GNFRpZ2hFV09CcHp2SDZoWXZDWHU5STQ4M04zUmhtdU5UZGRNaVgvb0tiOEYKdVp1UmpvMmJOd2k5UFVxZnBOVVY3RUV2TitoN0QzUkhWTUU1SDdRdVEveXRoZ2RFK3UzUkdsSGVtQWZJSFFCMQpvL0g0U29oS2FhYTc5T3dOUkxrQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZQNXVJUXNxMHVVTStJZ1JqRjRrYW5jOWs3djZNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFDYlR2OUNYTEdFbU5xL0RBZTV6cXYwamszNDVlQ24rZHdiZnlXT3JEeVhBdlhPcU9qSAo3L2UybHB5RE00a0Q5cFNtVnRlTFE2SFhydUtYaGRTTWZEVXdCNFZRZXM1ZDQzMXlLUmw4aG5EeEpSb1lLaGxwCnlCUTFQVFIrR0pGSFRPRVJqQm1TQTIyU0VqWXVXOWcxZUw4ekJkUGZsUVhCSlAyY2U2MnZvZm5Icy9rWS9sY3MKaU51YUplV1IyRGpXSlM0YjduYUNLcE9CZnVJYnhrcnpaQWVNbVFJcStzSklUYWloZGlxaE9Iam0za2pyMTRZago2VGtXa2gyMW9Xdm9CSjZ5cEZkUnRYbHFpMk9sNnFlSndzb3I4S3c2Q0JVRXcybG1ER2RNM0hhYTBHZDNKditiCkJCUU9Kd2RTQk8zdEdLSFpCUWRmZWVWemJvbWFFNjBRZ0YrZgotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://0.0.0.0:52594
  name: kind-testk8s
- cluster:
    certificate-authority: /Users/azhekhan/.minikube/ca.crt
    server: https://192.168.64.2:8443
  name: minikube
contexts:
- context:
    cluster: kind-testk8s
    user: kind-testk8s
  name: kind-testk8s
- context:
    cluster: minikube
    user: minikube
  name: minikube
current-context: minikube
kind: Config
preferences: {}
users:
- name: kind-testk8s
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURFekNDQWZ1Z0F3SUJBZ0lJTi9lbkwyeFd0VXd3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TWpBeE1UTXhOak14TXpkYUZ3MHlNekF4TVRNeE5qTXhOREJhTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQXpWWDJUK3hqcE5zaGt4SkQKUnJHTngzY3VYdkNHbk5xb1RpRmFqYWlTQlI2MDU4TmNOVE5zcW1kRERZcjhtUGlrUFJISllwakttdCttelJzTgpqaDRtSW5sYkRKOWV6d0lwbDBsWXVDaXhwcnZ1SFBtaTRLa1R3Y1lsVjVTSjFNL2M2UTNzTkFVdmRENEl3MnlrCmRDOW1KbUZyM3BmRVVsVzd4VTlrR0ppa2dVQjI5TER1YUVTNmxGZzF6aGZSdHE0MG9ZTXZsTnpWM3Jac3hNa1UKQXVhSTlQMEhTTjlWcTNZekV6K3VScXBhRkoycW9NTk1JYWRYYUNqd3RzQ3hmMHZmL2FKcG16a29neDRqMGxsTgpsZVBCUFQ5N3BSTzE2WkVDeEYrUXBscWwvVm0zZzBLaStYOFkyekxzdm44UkNsY2RyZjdWL25SUkMyU3FqZmxWCnBFYlFId0lEQVFBQm8wZ3dSakFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0h3WURWUjBqQkJnd0ZvQVUvbTRoQ3lyUzVRejRpQkdNWGlScWR6MlR1L293RFFZSktvWklodmNOQVFFTApCUUFEZ2dFQkFGdXNhWVd5emgrK2NHd3grNkVlWERseXFmYmNpYi83SktxNmcrMFFuU1prMGYwQnR0bHkxV1A4CmkyT3RoSW41STFlRVpJTFhhL2Fnc0pOK1l3emZzSW5PdHZoU3NFZ0MwQWNXNTEvM0xseVJQQk5CUjFkSGc0NjEKWjJrTkdVa21WdEE2d0pKN0RVMjJmM1BCZ01pVENHZW12T0RUa1pWV0p5SEZUd1pHUm5NZW5HVTB5WFdaYjJmQwp0T2RZcm9peGtrZFdZdS9SZER6S1dMR05UR0lnTTVsU00zUElGWTQ3WVJ4a2xnWlZ5aW4xUlJhL1pIMEkrREQ3CjJXSlJpMlJZTEIwc3lmRGJkY3F3bFkvdnMwREp2d0JJM0tpeTFNaktubkRHMzBnMXFwZzE4Qy9iTkltSzRVVTUKbjdQTHBRUXUrWkF5NjVLSjU1aUVwVDRKQnorZmF1cz0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    client-key-data: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFb0FJQkFBS0NBUUVBelZYMlQreGpwTnNoa3hKRFJyR054M2N1WHZDR25OcW9UaUZhamFpU0JSNjA1OE5jCk5UTnNxbWRERFlyOG1QaWtQUkhKWXBqS210K216UnNOamg0bUlubGJESjllendJcGwwbFl1Q2l4cHJ2dUhQbWkKNEtrVHdjWWxWNVNKMU0vYzZRM3NOQVV2ZEQ0SXcyeWtkQzltSm1GcjNwZkVVbFc3eFU5a0dKaWtnVUIyOUxEdQphRVM2bEZnMXpoZlJ0cTQwb1lNdmxOelYzclpzeE1rVUF1YUk5UDBIU045VnEzWXpFeit1UnFwYUZKMnFvTU5NCklhZFhhQ2p3dHNDeGYwdmYvYUpwbXprb2d4NGowbGxObGVQQlBUOTdwUk8xNlpFQ3hGK1FwbHFsL1ZtM2cwS2kKK1g4WTJ6THN2bjhSQ2xjZHJmN1YvblJSQzJTcWpmbFZwRWJRSHdJREFRQUJBb0lCQUZ6NmkrcHM5RUgyS0NIYwpweklmdkQrU0xDRTVjUHNkMkJCbzhnUXJUTk9jci9ZeWRGSnkwc1htdEdveGdXZ1RUdXVJWGxTVE0wc2w4dlgxClJoSVFmTnduYjU4cm0xYkpkK1BWaHFMOWFsUkJ6UFM1NVpNdGZKREdGS3F2ekNuZFpmN3lHdjhEU2hXczhYTHMKZ3h0SlMwd2QyU2FXT29POTVxVFY2bEZvN1RUSkdFQUp6MDVqQ1dsWWQ5ZUhDMjhZcG9heXcrQ0dwSzNzTHZJcAp0eUlBMEN1MEFtMHBqK2dIRldvQW1HNEROemlKcGtBaTY3djFnUnBRUUtnOTBNVExWWTVybnF4ZDd5VzdtVFh6CjFpTldSTUlObHMzNkZ2N1N2MWpFTVVPQ3RkbDI0ZlpJanY2dno1eTE1Vk1RV0d3R21OQXh3aWdEdFpPVlM0VWYKZ3VLaTJTRUNnWUVBNzFUNFdMdVBvUWFMWFVHR2o0c0JSRWxKUzhjYStGdjhNTDQ3L0paNWVyMVQyTi9LbjZ1VApFaU9CTzQxOG91aldleS9HZFBxTGxWTDVtUVVhR1Rrc2Q4NUh6OWdxKzJKWEZ3LzE3NTh1eUE4bDFPYkVWMnBZClFpNFEyekxpSVZCcjY1R1VobEJORHhCOGhQSjR5Z3J4Ykkxd0R3UmYzalNZci9mZ3d4b21JajBDZ1lFQTI2TGcKdUlZazNkSWJHR0tlSTE5dkRqdUJpVjZnVUtiOFgvNi9RSVhKMHNzc0pjN3hWVVA1N0xnMDJndmJwa3k1U2pQZQpEK3kvZzdLT0d0c0pZSFNJU1BFaEk0S085UFNFWkxPbEludUVXb1RMSkk1V3g0OVNLaHhWTGF3cUV3dTE1dUlZCm52Wkt5ekdIQXR2UVdWRXZWblN5RjFONEJPaTNFM0c4S2xBYnJZc0NnWUFCTTVML2x3bDBqSHpjbmZ5WDhlR3kKdnc5d0l2Syt0bStBcXlQUDg4MXVVVTcvY3Jnb3J3clE5MzZwbHQ4K1RoN3RoV0xncVhWVDdaTzFNZkpjbUEzaQpHQllDTkJxYkluWS9kbVFNck9ibU9EVS9hVjBvOXk2MFBsajFVTDhUM2J4ODlVRWluM3FUamNpaVBqbGZBa2tOCiszUEpqNE1Qc1lIbXVEN1htK1QvSFFKL1hZUDhLcFR6SFg5WUNnbnpTeHhYTExncGIxUGE3bndxK0xSK3VmbzgKOUlPTlNJdlVOWG03OEE1K2NsaWFMVFI5c0JwbGNtSmJLTGZkeVpGNWJKZ3hidDA4UHVKWCsvUWdhc0FTajV3SAp2b0xMKzhTZkxsalhveFp0TGdVZjVQd1hkemJOUk9HWTl1c2ZrUmdHbmdEekp0eUk5MldTdElSaGJ1WCtPQVhqCnd3S0JnRmNoYnRxYUxWekY4STQ3VHRyWGZRaER4SEdHb3gvOVVubld5aGpHZmpodERRZlE5eXhDdU1PS0R5eWMKcHFGWUUrVGlOUzdQaTljMjVSOVdIQnR4OW9WbUY1U3BxMUJsWG5vaEdndEdTakcwZ244SFRrNW1PamRiTm9vdgovYzR1RWRjenV3cHJGVWxUams4OTdtb0pTc3dZNEEyTFlSODhLa201UDNCY1NWM0EKLS0tLS1FTkQgUlNBIFBSSVZBVEUgS0VZLS0tLS0K
- name: minikube
  user:
    client-certificate: /Users/azhekhan/.minikube/profiles/minikube/client.crt
    client-key: /Users/azhekhan/.minikube/profiles/minikube/client.key

 2022-01-18 05:38:01 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → ls -ltr
total 18736
-r--------  1 azhekhan  staff     2358 Jan 10 10:32 kubeconfig_context-c5ppxaonnda
-rw-r--r--  1 azhekhan  staff     4455 Jan 13 05:36 traefik-6ff7ff8ffd-cxgdn2022_01_12_11_33_PM.log
-rw-r--r--  1 azhekhan  staff     1006 Jan 13 05:41 traefik.service
-rw-r--r--  1 azhekhan  staff     7670 Jan 13 05:43 kf-kafka.kafka.yaml
-rw-r--r--  1 azhekhan  staff    12389 Jan 13 05:45 corp.shared-kafka.kafka.yaml
-rw-r--r--  1 azhekhan  staff      420 Jan 13 10:14 kf-kafka-kafka-0-ingressroutetcp.yaml
-rw-r--r--  1 azhekhan  staff      452 Jan 13 10:15 kf-kafka-kafka-bootstrap-ingressroutetcp.yaml
-rw-r--r--  1 azhekhan  staff      420 Jan 13 10:15 kf-kafka-kafka-1-ingressroutetcp.yaml
-rw-r--r--  1 azhekhan  staff      420 Jan 13 10:15 kf-kafka-kafka-2-ingressroutetcp.yaml
-rw-r--r--  1 azhekhan  staff     3186 Jan 13 10:35 kf-kafka.kafka_updated.yaml
-rw-r--r--  1 azhekhan  staff     1358 Jan 13 13:42 kf-kafka-kafka-tlsexternal-1.ingress.yaml
-rw-r--r--  1 azhekhan  staff     1331 Jan 13 13:54 kf-kafka-kafka-tlsexternal-0.ingress.yaml
-rw-r--r--  1 azhekhan  staff     3782 Jan 13 14:04 kf-kafka-kafka_updated2.yaml
-rw-r--r--  1 azhekhan  staff     2197 Jan 13 17:25 ca.crt
-rw-r--r--  1 azhekhan  staff     2897 Jan 13 19:01 traefik-test_values.yaml
-rw-r--r--  1 azhekhan  staff     1334 Jan 13 19:56 kf-kafka-kafka-0.ingress.1.yaml
-rw-r--r--  1 azhekhan  staff     1334 Jan 13 19:56 kf-kafka-kafka-1.ingress.1.yaml
-rw-r--r--  1 azhekhan  staff     1334 Jan 13 19:56 kf-kafka-kafka-2.ingress.1.yaml
-rw-r--r--  1 azhekhan  staff     1383 Jan 13 19:56 kf-kafka-kafka-bootstrap.ingress.1.yaml
-rw-r--r--  1 azhekhan  staff     3800 Jan 14 09:45 kf-kafka.cluster.kind.14jan22.yaml
-rw-r--r--  1 azhekhan  staff     8904 Jan 14 10:12 kf-kafka.kafka.WORKING.14Jan22.yaml
-rw-r--r--  1 azhekhan  staff     1379 Jan 14 10:23 kf-kafka-kafka-0.ingress.LB-14jan22.yaml
-rw-r--r--  1 azhekhan  staff     1379 Jan 14 10:23 kf-kafka-kafka-1.ingress.LB-14jan22.yaml
-rw-r--r--  1 azhekhan  staff     1379 Jan 14 10:23 kf-kafka-kafka-2.ingress.LB-14jan22.yaml
-rw-r--r--  1 azhekhan  staff     1428 Jan 14 10:23 kf-kafka-kafka-bootstrap.ingress.LB-14jan22.yaml
-rw-r--r--  1 azhekhan  staff     8904 Jan 17 09:53 kf-kafka.kafka.WORKING.17Jan22.yaml
-rw-r--r--  1 azhekhan  staff  8656381 Jan 17 16:40 traefik-kafka-controller-5f9f755d5-j6nzw2022_01_12_11_33_PM.log
-rw-r--r--  1 azhekhan  staff      238 Jan 17 22:25 traefik-web-ui_svc.yaml
-rw-r--r--  1 azhekhan  staff      269 Jan 17 22:27 traefik-web-ui_ingress.yaml

 2022-01-18 05:38:09 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → cp /Users/azhekhan/.kube/config kubeconfig_minikube_18jan2022

 2022-01-18 05:38:59 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kgns
NAME              STATUS   AGE
default           Active   111s
kube-node-lease   Active   112s
kube-public       Active   112s
kube-system       Active   112s

 2022-01-18 05:39:02 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kgp --all-namespaces
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE
kube-system   coredns-66bff467f8-hjv9n           1/1     Running   0          114s
kube-system   coredns-66bff467f8-r8znt           1/1     Running   0          114s
kube-system   etcd-minikube                      1/1     Running   0          119s
kube-system   kube-apiserver-minikube            1/1     Running   0          119s
kube-system   kube-controller-manager-minikube   1/1     Running   0          119s
kube-system   kube-proxy-4dz9l                   1/1     Running   0          114s
kube-system   kube-scheduler-minikube            1/1     Running   0          119s
kube-system   storage-provisioner                1/1     Running   0          119s

 2022-01-18 05:39:14 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → helm repo list
NAME    URL
bitnami https://charts.bitnami.com/bitnami
metallb https://metallb.github.io/metallb
strimzi https://strimzi.io/charts
traefik https://helm.traefik.io/traefik

 2022-01-18 05:39:23 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "strimzi" chart repository
...Successfully got an update from the "metallb" chart repository
...Successfully got an update from the "traefik" chart repository
...Successfully got an update from the "bitnami" chart repository
Update Complete. ⎈Happy Helming!⎈

 2022-01-18 05:39:37 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kubectl get po -A
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE
kube-system   coredns-66bff467f8-hjv9n           1/1     Running   0          2m18s
kube-system   coredns-66bff467f8-r8znt           1/1     Running   0          2m18s
kube-system   etcd-minikube                      1/1     Running   0          2m23s
kube-system   kube-apiserver-minikube            1/1     Running   0          2m23s
kube-system   kube-controller-manager-minikube   1/1     Running   0          2m23s
kube-system   kube-proxy-4dz9l                   1/1     Running   0          2m18s
kube-system   kube-scheduler-minikube            1/1     Running   0          2m23s
kube-system   storage-provisioner                1/1     Running   0          2m23s

 2022-01-18 05:39:38 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ →

 2022-01-18 05:39:39 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → minikube dashboard
🔌  Enabling dashboard ...
🤔  Verifying dashboard health ...
🚀  Launching proxy ...
🤔  Verifying proxy health ...
🎉  Opening http://127.0.0.1:50586/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ in your default browser...
^C

 2022-01-18 05:41:34 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4
deployment.apps/hello-minikube created

 2022-01-18 05:41:36 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kgp -A
NAMESPACE              NAME                                         READY   STATUS              RESTARTS   AGE
default                hello-minikube-5655c9d946-lrpsc              0/1     ContainerCreating   0          9s
kube-system            coredns-66bff467f8-hjv9n                     1/1     Running             0          4m25s
kube-system            coredns-66bff467f8-r8znt                     1/1     Running             0          4m25s
kube-system            etcd-minikube                                1/1     Running             0          4m30s
kube-system            kube-apiserver-minikube                      1/1     Running             0          4m30s
kube-system            kube-controller-manager-minikube             1/1     Running             0          4m30s
kube-system            kube-proxy-4dz9l                             1/1     Running             0          4m25s
kube-system            kube-scheduler-minikube                      1/1     Running             0          4m30s
kube-system            storage-provisioner                          1/1     Running             0          4m30s
kubernetes-dashboard   dashboard-metrics-scraper-84bfdf55ff-4mw5l   1/1     Running             0          99s
kubernetes-dashboard   kubernetes-dashboard-bc446cc64-6dp6z         1/1     Running             0          98s

 2022-01-18 05:41:45 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → watch kgp -A
-bash: watch: command not found

 2022-01-18 05:41:52 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → watch kgp -A -w
-bash: watch: command not found

 2022-01-18 05:41:55 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kgp -A -w
NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE
default                hello-minikube-5655c9d946-lrpsc              1/1     Running   0          24s
kube-system            coredns-66bff467f8-hjv9n                     1/1     Running   0          4m40s
kube-system            coredns-66bff467f8-r8znt                     1/1     Running   0          4m40s
kube-system            etcd-minikube                                1/1     Running   0          4m45s
kube-system            kube-apiserver-minikube                      1/1     Running   0          4m45s
kube-system            kube-controller-manager-minikube             1/1     Running   0          4m45s
kube-system            kube-proxy-4dz9l                             1/1     Running   0          4m40s
kube-system            kube-scheduler-minikube                      1/1     Running   0          4m45s
kube-system            storage-provisioner                          1/1     Running   0          4m45s
kubernetes-dashboard   dashboard-metrics-scraper-84bfdf55ff-4mw5l   1/1     Running   0          114s
kubernetes-dashboard   kubernetes-dashboard-bc446cc64-6dp6z         1/1     Running   0          113s
^C
 2022-01-18 05:42:12 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kubectl expose deployment hello-minikube --type=NodePort --port=8080
service/hello-minikube exposed

 2022-01-18 05:42:13 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kubectl get services hello-minikube
NAME             TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
hello-minikube   NodePort   10.104.176.152   <none>        8080:30635/TCP   30s

 2022-01-18 05:42:43 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kubectl describe services hello-minikube
Name:                     hello-minikube
Namespace:                default
Labels:                   app=hello-minikube
Annotations:              <none>
Selector:                 app=hello-minikube
Type:                     NodePort
IP:                       10.104.176.152
Port:                     <unset>  8080/TCP
TargetPort:               8080/TCP
NodePort:                 <unset>  30635/TCP
Endpoints:                172.17.0.6:8080
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>

 2022-01-18 05:43:21 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → k port-forward service/hello-minikube 7080:8080
Forwarding from 127.0.0.1:7080 -> 8080
Forwarding from [::1]:7080 -> 8080
Handling connection for 7080
Handling connection for 7080
^C
 2022-01-18 05:44:52 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kdpn default hello-minikube-5655c9d946-lrpsc
Name:         hello-minikube-5655c9d946-lrpsc
Namespace:    default
Priority:     0
Node:         minikube/192.168.64.2
Start Time:   Tue, 18 Jan 2022 05:41:36 +0530
Labels:       app=hello-minikube
              pod-template-hash=5655c9d946
Annotations:  <none>
Status:       Running
IP:           172.17.0.6
IPs:
  IP:           172.17.0.6
Controlled By:  ReplicaSet/hello-minikube-5655c9d946
Containers:
  echoserver:
    Container ID:   docker://31b115a1e674f8314058cdce55274db2d3c5b9d5fe7b92947b479ae6c58d03dc
    Image:          k8s.gcr.io/echoserver:1.4
    Image ID:       docker-pullable://k8s.gcr.io/echoserver@sha256:5d99aa1120524c801bc8c1a7077e8f5ec122ba16b6dda1a5d3826057f67b9bcb
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 18 Jan 2022 05:41:58 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-w5msz (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  default-token-w5msz:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-w5msz
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  3m30s  default-scheduler  Successfully assigned default/hello-minikube-5655c9d946-lrpsc to minikube
  Normal  Pulling    3m29s  kubelet, minikube  Pulling image "k8s.gcr.io/echoserver:1.4"
  Normal  Pulled     3m8s   kubelet, minikube  Successfully pulled image "k8s.gcr.io/echoserver:1.4"
  Normal  Created    3m8s   kubelet, minikube  Created container echoserver
  Normal  Started    3m8s   kubelet, minikube  Started container echoserver

 2022-01-18 05:45:06 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ →

 2022-01-18 05:51:02 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → kubectl get services hello-minikube
NAME             TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
hello-minikube   NodePort   10.104.176.152   <none>        8080:30635/TCP   13m

 2022-01-18 05:55:29 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test
○ → cd traefik-setup/

 2022-01-18 05:55:35 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → ll
total 8
-rw-r--r--  1 azhekhan  staff  94 Jan 18 05:55 traefik_sa.yaml

 2022-01-18 05:55:36 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → k apply -f traefik_sa.yaml
serviceaccount/traefik-ingress created

 2022-01-18 05:55:46 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → kg serviceaccount -A
NAMESPACE              NAME                                 SECRETS   AGE
default                default                              1         18m
kube-node-lease        default                              1         18m
kube-public            default                              1         18m
kube-system            attachdetach-controller              1         18m
kube-system            bootstrap-signer                     1         18m
kube-system            certificate-controller               1         18m
kube-system            clusterrole-aggregation-controller   1         18m
kube-system            coredns                              1         18m
kube-system            cronjob-controller                   1         18m
kube-system            daemon-set-controller                1         18m
kube-system            default                              1         18m
kube-system            deployment-controller                1         18m
kube-system            disruption-controller                1         18m
kube-system            endpoint-controller                  1         18m
kube-system            endpointslice-controller             1         18m
kube-system            expand-controller                    1         18m
kube-system            generic-garbage-collector            1         18m
kube-system            horizontal-pod-autoscaler            1         18m
kube-system            job-controller                       1         18m
kube-system            kube-proxy                           1         18m
kube-system            namespace-controller                 1         18m
kube-system            node-controller                      1         18m
kube-system            persistent-volume-binder             1         18m
kube-system            pod-garbage-collector                1         18m
kube-system            pv-protection-controller             1         18m
kube-system            pvc-protection-controller            1         18m
kube-system            replicaset-controller                1         18m
kube-system            replication-controller               1         18m
kube-system            resourcequota-controller             1         18m
kube-system            service-account-controller           1         18m
kube-system            service-controller                   1         18m
kube-system            statefulset-controller               1         18m
kube-system            storage-provisioner                  1         18m
kube-system            token-cleaner                        1         18m
kube-system            traefik-ingress                      1         10s
kube-system            ttl-controller                       1         18m
kubernetes-dashboard   default                              1         15m
kubernetes-dashboard   kubernetes-dashboard                 1         15m

 2022-01-18 05:55:56 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → kg serviceaccount traefik-ingress -n kube-system
NAME              SECRETS   AGE
traefik-ingress   1         41s

 2022-01-18 05:56:27 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → kd serviceaccount traefik-ingress -n kube-system
Name:                traefik-ingress
Namespace:           kube-system
Labels:              <none>
Annotations:         Image pull secrets:  <none>
Mountable secrets:   traefik-ingress-token-scpwj
Tokens:              traefik-ingress-token-scpwj
Events:              <none>

 2022-01-18 05:56:30 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → k apply -f traefik_clusterrole.yaml
clusterrole.rbac.authorization.k8s.io/traefik-ingress created

 2022-01-18 05:57:39 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → k apply -f traefik_clusterrolebinding.yaml
clusterrolebinding.rbac.authorization.k8s.io/traefik-ingress created

 2022-01-18 05:58:52 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → k apply -f traefik_deployment.yaml
error: unable to recognize "traefik_deployment.yaml": no matches for kind "Deployment" in version "extensions/v1beta1"

 2022-01-18 06:00:02 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → k apiresources | more
Error: unknown command "apiresources" for "kubectl"

Did you mean this?
        api-resources

Run 'kubectl --help' for usage.

 2022-01-18 06:00:34 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → k api-resources | grep -i deployment
deployments                       deploy       apps                           true         Deployment

 2022-01-18 06:00:48 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → for kind in `kubectl api-resources | tail +2 | awk '{ print $1 }'`; do kubectl explain $kind; done | grep -e "KIND:" -e "VERSION:"
KIND:     Binding
VERSION:  v1
KIND:     ComponentStatus
VERSION:  v1
KIND:     ConfigMap
VERSION:  v1
KIND:     Endpoints
VERSION:  v1
KIND:     Event
VERSION:  v1
KIND:     LimitRange
VERSION:  v1
KIND:     Namespace
VERSION:  v1
KIND:     Node
VERSION:  v1
KIND:     PersistentVolumeClaim
VERSION:  v1
KIND:     PersistentVolume
VERSION:  v1
KIND:     Pod
VERSION:  v1
KIND:     PodTemplate
VERSION:  v1
KIND:     ReplicationController
VERSION:  v1
KIND:     ResourceQuota
VERSION:  v1
KIND:     Secret
VERSION:  v1
KIND:     ServiceAccount
VERSION:  v1
KIND:     Service
VERSION:  v1
KIND:     MutatingWebhookConfiguration
VERSION:  admissionregistration.k8s.io/v1
KIND:     ValidatingWebhookConfiguration
VERSION:  admissionregistration.k8s.io/v1
KIND:     CustomResourceDefinition
VERSION:  apiextensions.k8s.io/v1
KIND:     APIService
VERSION:  apiregistration.k8s.io/v1
KIND:     ControllerRevision
VERSION:  apps/v1
KIND:     DaemonSet
VERSION:  apps/v1
KIND:     Deployment
VERSION:  apps/v1
KIND:     ReplicaSet
VERSION:  apps/v1
KIND:     StatefulSet
VERSION:  apps/v1
KIND:     TokenReview
VERSION:  authentication.k8s.io/v1
KIND:     LocalSubjectAccessReview
VERSION:  authorization.k8s.io/v1
KIND:     SelfSubjectAccessReview
VERSION:  authorization.k8s.io/v1
KIND:     SelfSubjectRulesReview
VERSION:  authorization.k8s.io/v1
KIND:     SubjectAccessReview
VERSION:  authorization.k8s.io/v1
KIND:     HorizontalPodAutoscaler
VERSION:  autoscaling/v1
KIND:     CronJob
VERSION:  batch/v1beta1
KIND:     Job
VERSION:  batch/v1
KIND:     CertificateSigningRequest
VERSION:  certificates.k8s.io/v1beta1
KIND:     Lease
VERSION:  coordination.k8s.io/v1
KIND:     EndpointSlice
VERSION:  discovery.k8s.io/v1beta1
KIND:     Event
VERSION:  v1
KIND:     Ingress
VERSION:  extensions/v1beta1
KIND:     IngressClass
VERSION:  networking.k8s.io/v1beta1
KIND:     Ingress
VERSION:  extensions/v1beta1
KIND:     NetworkPolicy
VERSION:  networking.k8s.io/v1
KIND:     RuntimeClass
VERSION:  node.k8s.io/v1beta1
KIND:     PodDisruptionBudget
VERSION:  policy/v1beta1
KIND:     PodSecurityPolicy
VERSION:  policy/v1beta1
KIND:     ClusterRoleBinding
VERSION:  rbac.authorization.k8s.io/v1
KIND:     ClusterRole
VERSION:  rbac.authorization.k8s.io/v1
KIND:     RoleBinding
VERSION:  rbac.authorization.k8s.io/v1
KIND:     Role
VERSION:  rbac.authorization.k8s.io/v1
KIND:     PriorityClass
VERSION:  scheduling.k8s.io/v1
KIND:     CSIDriver
VERSION:  storage.k8s.io/v1
KIND:     CSINode
VERSION:  storage.k8s.io/v1
KIND:     StorageClass
VERSION:  storage.k8s.io/v1
KIND:     VolumeAttachment
VERSION:  storage.k8s.io/v1

 2022-01-18 06:03:00 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → ll
total 32
-rw-r--r--  1 azhekhan  staff   94 Jan 18 05:55 traefik_sa.yaml
-rw-r--r--  1 azhekhan  staff  363 Jan 18 05:57 traefik_clusterrole.yaml
-rw-r--r--  1 azhekhan  staff  277 Jan 18 05:58 traefik_clusterrolebinding.yaml
-rw-r--r--  1 azhekhan  staff  690 Jan 18 06:01 traefik_deployment.yaml

 2022-01-18 06:04:27 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → k apply -f traefik_deployment.yaml
deployment.apps/traefik-ingress created

 2022-01-18 06:04:32 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → kgpn kube-system
NAME                               READY   STATUS              RESTARTS   AGE
coredns-66bff467f8-hjv9n           1/1     Running             0          27m
coredns-66bff467f8-r8znt           1/1     Running             0          27m
etcd-minikube                      1/1     Running             0          27m
kube-apiserver-minikube            1/1     Running             0          27m
kube-controller-manager-minikube   1/1     Running             0          27m
kube-proxy-4dz9l                   1/1     Running             0          27m
kube-scheduler-minikube            1/1     Running             0          27m
storage-provisioner                1/1     Running             0          27m
traefik-ingress-74884f777-tw2rq    0/1     ContainerCreating   0          18s

 2022-01-18 06:04:50 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → kgpn kube-system -w
NAME                               READY   STATUS              RESTARTS   AGE
coredns-66bff467f8-hjv9n           1/1     Running             0          27m
coredns-66bff467f8-r8znt           1/1     Running             0          27m
etcd-minikube                      1/1     Running             0          27m
kube-apiserver-minikube            1/1     Running             0          27m
kube-controller-manager-minikube   1/1     Running             0          27m
kube-proxy-4dz9l                   1/1     Running             0          27m
kube-scheduler-minikube            1/1     Running             0          27m
storage-provisioner                1/1     Running             0          27m
traefik-ingress-74884f777-tw2rq    0/1     ContainerCreating   0          21s
traefik-ingress-74884f777-tw2rq    0/1     Error               0          22s
traefik-ingress-74884f777-tw2rq    0/1     Error               1          28s
traefik-ingress-74884f777-tw2rq    0/1     CrashLoopBackOff    1          29s
traefik-ingress-74884f777-tw2rq    0/1     Error               2          47s
traefik-ingress-74884f777-tw2rq    0/1     CrashLoopBackOff    2          62s
traefik-ingress-74884f777-tw2rq    1/1     Running             3          83s
traefik-ingress-74884f777-tw2rq    0/1     Error               3          84s
traefik-ingress-74884f777-tw2rq    0/1     CrashLoopBackOff    3          98s
traefik-ingress-74884f777-tw2rq    0/1     Error               4          2m12s
traefik-ingress-74884f777-tw2rq    0/1     CrashLoopBackOff    4          2m26s
traefik-ingress-74884f777-tw2rq    0/1     Error               5          3m46s
traefik-ingress-74884f777-tw2rq    0/1     CrashLoopBackOff    5          3m57s
traefik-ingress-74884f777-tw2rq    0/1     Error               6          6m45s
traefik-ingress-74884f777-tw2rq    0/1     CrashLoopBackOff    6          6m57s
traefik-ingress-74884f777-tw2rq    0/1     Error               7          11m
traefik-ingress-74884f777-tw2rq    0/1     CrashLoopBackOff    7          12m
traefik-ingress-74884f777-tw2rq    0/1     Error               8          17m
traefik-ingress-74884f777-tw2rq    0/1     CrashLoopBackOff    8          17m
^C
 2022-01-18 06:23:09 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → kdpn kube-system traefik-ingress-74884f777-tw2rq
Name:         traefik-ingress-74884f777-tw2rq
Namespace:    kube-system
Priority:     0
Node:         minikube/192.168.64.2
Start Time:   Tue, 18 Jan 2022 06:04:32 +0530
Labels:       k8s-app=traefik-ingress-lb
              name=traefik-ingress-lb
              pod-template-hash=74884f777
Annotations:  <none>
Status:       Running
IP:           172.17.0.7
IPs:
  IP:           172.17.0.7
Controlled By:  ReplicaSet/traefik-ingress-74884f777
Containers:
  traefik-ingress-lb:
    Container ID:  docker://bd8c7f79507d730822a725e3a111d8d3a2f77c218adcedd14bea7e6771d320d3
    Image:         traefik
    Image ID:      docker-pullable://traefik@sha256:2f603f8d3abe1dd3a4eb28960c55506be48293b41ea2c6ed4a4297c851a57a05
    Ports:         80/TCP, 8080/TCP
    Host Ports:    0/TCP, 0/TCP
    Args:
      --api
      --kubernetes
      --logLevel=INFO
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Tue, 18 Jan 2022 06:21:44 +0530
      Finished:     Tue, 18 Jan 2022 06:21:44 +0530
    Ready:          False
    Restart Count:  8
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from traefik-ingress-token-scpwj (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  traefik-ingress-token-scpwj:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  traefik-ingress-token-scpwj
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  18m                   default-scheduler  Successfully assigned kube-system/traefik-ingress-74884f777-tw2rq to minikube
  Normal   Created    17m (x4 over 18m)     kubelet, minikube  Created container traefik-ingress-lb
  Normal   Started    17m (x4 over 18m)     kubelet, minikube  Started container traefik-ingress-lb
  Normal   Pulling    16m (x5 over 18m)     kubelet, minikube  Pulling image "traefik"
  Normal   Pulled     16m (x5 over 18m)     kubelet, minikube  Successfully pulled image "traefik"
  Warning  BackOff    3m51s (x67 over 18m)  kubelet, minikube  Back-off restarting failed container

 2022-01-18 06:23:29 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → kln kube-system traefik-ingress-74884f777-tw2rq
2022/01/18 00:51:44 command traefik error: failed to decode configuration from flags: field not found, node: kubernetes

 2022-01-18 06:23:52 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → k apply -f traefik_deployment.yaml
deployment.apps/traefik-ingress configured

 2022-01-18 06:26:41 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → kln kube-system traefik-ingress-74884f777-tw2rq
2022/01/18 00:51:44 command traefik error: failed to decode configuration from flags: field not found, node: kubernetes

 2022-01-18 06:26:47 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → kgpn kube-system -w
NAME                               READY   STATUS             RESTARTS   AGE
coredns-66bff467f8-hjv9n           1/1     Running            0          49m
coredns-66bff467f8-r8znt           1/1     Running            0          49m
etcd-minikube                      1/1     Running            0          49m
kube-apiserver-minikube            1/1     Running            0          49m
kube-controller-manager-minikube   1/1     Running            0          49m
kube-proxy-4dz9l                   1/1     Running            0          49m
kube-scheduler-minikube            1/1     Running            0          49m
storage-provisioner                1/1     Running            0          49m
traefik-ingress-74884f777-tw2rq    0/1     CrashLoopBackOff   8          22m
traefik-ingress-b7fdf97b6-8przr    0/1     CrashLoopBackOff   1          17s
^C
 2022-01-18 06:27:05 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → kln kube-system traefik-ingress-b7fdf97b6-8przr
2022/01/18 00:56:55 command traefik error: failed to decode configuration from flags: field not found, node: kubernetes

 2022-01-18 06:27:10 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → kln kube-system traefik-ingress-b7fdf97b6-8przr
2022/01/18 00:57:13 command traefik error: failed to decode configuration from flags: field not found, node: kubernetes

 2022-01-18 06:27:17 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → kln kube-system traefik-ingress-b7fdf97b6-8przr
2022/01/18 00:57:44 command traefik error: failed to decode configuration from flags: field not found, node: kubernetes

 2022-01-18 06:28:03 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ →

 2022-01-18 06:31:38 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → k apply -f traefik_deployment.yaml
deployment.apps/traefik-ingress configured

 2022-01-18 06:31:46 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → kgpn kube-system -w
NAME                               READY   STATUS              RESTARTS   AGE
coredns-66bff467f8-hjv9n           1/1     Running             0          54m
coredns-66bff467f8-r8znt           1/1     Running             0          54m
etcd-minikube                      1/1     Running             0          54m
kube-apiserver-minikube            1/1     Running             0          54m
kube-controller-manager-minikube   1/1     Running             0          54m
kube-proxy-4dz9l                   1/1     Running             0          54m
kube-scheduler-minikube            1/1     Running             0          54m
storage-provisioner                1/1     Running             0          54m
traefik-ingress-55c99674d9-mbp5h   0/1     ContainerCreating   0          7s
traefik-ingress-b7fdf97b6-8przr    0/1     CrashLoopBackOff    5          5m13s
traefik-ingress-55c99674d9-mbp5h   1/1     Running             0          19s
traefik-ingress-b7fdf97b6-8przr    0/1     Terminating         5          5m25s
traefik-ingress-b7fdf97b6-8przr    0/1     Terminating         5          5m31s
traefik-ingress-b7fdf97b6-8przr    0/1     Terminating         5          5m31s
^C
 2022-01-18 06:32:42 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → kgpn kube-system
NAME                               READY   STATUS    RESTARTS   AGE
coredns-66bff467f8-hjv9n           1/1     Running   0          55m
coredns-66bff467f8-r8znt           1/1     Running   0          55m
etcd-minikube                      1/1     Running   0          55m
kube-apiserver-minikube            1/1     Running   0          55m
kube-controller-manager-minikube   1/1     Running   0          55m
kube-proxy-4dz9l                   1/1     Running   0          55m
kube-scheduler-minikube            1/1     Running   0          55m
storage-provisioner                1/1     Running   0          55m
traefik-ingress-55c99674d9-mbp5h   1/1     Running   0          60s

 2022-01-18 06:32:47 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → kgpn kube-system
NAME                               READY   STATUS    RESTARTS   AGE
coredns-66bff467f8-hjv9n           1/1     Running   0          56m
coredns-66bff467f8-r8znt           1/1     Running   0          56m
etcd-minikube                      1/1     Running   0          56m
kube-apiserver-minikube            1/1     Running   0          56m
kube-controller-manager-minikube   1/1     Running   0          56m
kube-proxy-4dz9l                   1/1     Running   0          56m
kube-scheduler-minikube            1/1     Running   0          56m
storage-provisioner                1/1     Running   0          56m
traefik-ingress-55c99674d9-mbp5h   1/1     Running   0          111s

 2022-01-18 06:33:37 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → kdpn kube-system traefik-ingress-55c99674d9-mbp5h
Name:         traefik-ingress-55c99674d9-mbp5h
Namespace:    kube-system
Priority:     0
Node:         minikube/192.168.64.2
Start Time:   Tue, 18 Jan 2022 06:31:46 +0530
Labels:       k8s-app=traefik-ingress-lb
              name=traefik-ingress-lb
              pod-template-hash=55c99674d9
Annotations:  <none>
Status:       Running
IP:           172.17.0.7
IPs:
  IP:           172.17.0.7
Controlled By:  ReplicaSet/traefik-ingress-55c99674d9
Containers:
  traefik-ingress-lb:
    Container ID:  docker://3490a34ed3a9442a444bccf3959c7ea92f58f9caff84b1237be47d8aae9745b9
    Image:         traefik:v1.7.16
    Image ID:      docker-pullable://traefik@sha256:1961e48f78c27f76ddcff0956155dd62419cc4e74cd03f3a812589c4e9b423a9
    Ports:         80/TCP, 8080/TCP
    Host Ports:    0/TCP, 0/TCP
    Args:
      --api
      --kubernetes
      --logLevel=DEBUG
    State:          Running
      Started:      Tue, 18 Jan 2022 06:32:05 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from traefik-ingress-token-scpwj (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  traefik-ingress-token-scpwj:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  traefik-ingress-token-scpwj
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  3m29s  default-scheduler  Successfully assigned kube-system/traefik-ingress-55c99674d9-mbp5h to minikube
  Normal  Pulling    3m28s  kubelet, minikube  Pulling image "traefik:v1.7.16"
  Normal  Pulled     3m11s  kubelet, minikube  Successfully pulled image "traefik:v1.7.16"
  Normal  Created    3m10s  kubelet, minikube  Created container traefik-ingress-lb
  Normal  Started    3m10s  kubelet, minikube  Started container traefik-ingress-lb

 2022-01-18 06:35:15 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → ll
total 40
-rw-r--r--  1 azhekhan  staff   94 Jan 18 05:55 traefik_sa.yaml
-rw-r--r--  1 azhekhan  staff  363 Jan 18 05:57 traefik_clusterrole.yaml
-rw-r--r--  1 azhekhan  staff  277 Jan 18 05:58 traefik_clusterrolebinding.yaml
-rw-r--r--  1 azhekhan  staff  699 Jan 18 06:29 traefik_deployment.yaml
-rw-r--r--  1 azhekhan  staff  277 Jan 18 06:35 traefik_service.yaml

 2022-01-18 06:35:41 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → k apply -f traefik_service.yaml
service/traefik-ingress-service created

 2022-01-18 06:35:51 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → kgn kube-system services
NAME                      TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                       AGE
kube-dns                  ClusterIP   10.96.0.10    <none>        53/UDP,53/TCP,9153/TCP        58m
traefik-ingress-service   NodePort    10.97.110.0   <none>        80:30499/TCP,8080:31549/TCP   12s

 2022-01-18 06:36:04 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → kdn kube-system services traefik-ingress-service
Name:                     traefik-ingress-service
Namespace:                kube-system
Labels:                   <none>
Annotations:              Selector:  k8s-app=traefik-ingress-lb
Type:                     NodePort
IP:                       10.97.110.0
Port:                     web  80/TCP
TargetPort:               80/TCP
NodePort:                 web  30499/TCP
Endpoints:                172.17.0.7:80
Port:                     admin  8080/TCP
TargetPort:               8080/TCP
NodePort:                 admin  31549/TCP
Endpoints:                172.17.0.7:8080
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>

 2022-01-18 06:36:23 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → curl $(minikube ip):30499
404 page not found

 2022-01-18 06:37:10 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → k apply -f traefik_service.yaml
error: error parsing traefik_service.yaml: error converting YAML to JSON: yaml: line 2: mapping values are not allowed in this context

 2022-01-18 06:40:54 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → cat traefik_service.yaml
----
kind: Service
apiVersion: v1
metadata:
  name: traefik-ingress-service
  namespace: kube-system
spec:
  selector:
    k8s-app: traefik-ingress-lb
  ports:
    - protocol: TCP
      port: 80
      name: web
    - protocol: TCP
      port: 8080
      name: admin
  type: NodePort
---
apiVersion: v1
kind: Service
metadata:
  name: traefik-web-ui
  namespace: kube-system
spec:
  selector:
    k8s-app: traefik-ingress-lb
  ports:
  - name: web
    port: 80
    targetPort: 8080
 2022-01-18 06:41:07 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → k apply -f traefik_service.yaml
service/traefik-ingress-service unchanged
service/traefik-web-ui created

 2022-01-18 06:41:15 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → kubectl describe svc traefik-web-ui --namespace=kube-system
Name:              traefik-web-ui
Namespace:         kube-system
Labels:            <none>
Annotations:       Selector:  k8s-app=traefik-ingress-lb
Type:              ClusterIP
IP:                10.111.164.76
Port:              web  80/TCP
TargetPort:        8080/TCP
Endpoints:         172.17.0.7:8080
Session Affinity:  None
Events:            <none>

 2022-01-18 06:41:31 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → k apply -f traefik_ingress.yaml
ingress.extensions/traefik-web-ui created

 2022-01-18 06:42:32 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → kgn kube-system ingress
NAME             CLASS    HOSTS                 ADDRESS   PORTS   AGE
traefik-web-ui   <none>   traefik-ui.minikube             80      7s

 2022-01-18 06:42:39 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → kdn kube-system ingress
Name:             traefik-web-ui
Namespace:        kube-system
Address:
Default backend:  default-http-backend:80 (<error: endpoints "default-http-backend" not found>)
Rules:
##
# Host Database
  Host                 Path  Backends
  ----                 ----  --------
  traefik-ui.minikube
                       /        traefik-web-ui:web (172.17.0.7:8080)
Annotations:           Events:  <none>

##
 2022-01-18 06:42:44 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → echo "$(minikube ip) traefik-ui.minikube"
192.168.64.2 traefik-ui.minikube

 2022-01-18 06:43:13 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → cat /etc/hosts
##
# Host Database
#
# localhost is used to configure the loopback interface
# when the system is booting.  Do not change this entry.
##
127.0.0.1 localhost
255.255.255.255 broadcasthost
::1             localhost
#144.25.105.44   versioning-corp-versioning.channels.ocs.oc-test.com

# Added by Docker Desktop
# To allow the same kube context to work on the host and the container:
127.0.0.1 kubernetes.docker.internal
# End of section

## TESTING for OCI App Gateway
130.35.238.196 admin.internal.local
130.35.238.196 az.oracle.com
130.35.238.196 admin

127.0.0.1 whoami.docker.local
147.154.156.34 hare.fra.dev.channels.ocs.oc-test.com
10.10.9.71 shared-kafka-broker-0-kafka.service.lhr-dataplane.dev.consul

 2022-01-18 06:43:20 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → echo "192.168.64.2 traefik-ui.minikube" >> /etc/hosts
-bash: /etc/hosts: Permission denied

 2022-01-18 06:43:33 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → sudo echo "192.168.64.2 traefik-ui.minikube" >> /etc/hosts
-bash: /etc/hosts: Permission denied

 2022-01-18 06:43:39 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → vi /etc/hosts

 2022-01-18 06:44:03 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → sudo vi /etc/hosts
Password:
Sorry, try again.
Password:

 2022-01-18 06:44:28 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → cat /etc/hosts
##
# Host Database
#
# localhost is used to configure the loopback interface
# when the system is booting.  Do not change this entry.
##
127.0.0.1 localhost
255.255.255.255 broadcasthost
::1             localhost
#144.25.105.44   versioning-corp-versioning.channels.ocs.oc-test.com

# Added by Docker Desktop
# To allow the same kube context to work on the host and the container:
127.0.0.1 kubernetes.docker.internal
# End of section

## TESTING for OCI App Gateway
130.35.238.196 admin.internal.local
130.35.238.196 az.oracle.com
130.35.238.196 admin

127.0.0.1 whoami.docker.local
192.168.64.2 traefik-ui.minikube

 2022-01-18 06:44:33 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ →

 2022-01-18 06:50:39 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup
○ → cd namebasedrouting/

 2022-01-18 06:50:43 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → ll
total 8
-rw-r--r--  1 azhekhan  staff  1325 Jan 18 06:50 nbr_deployment.yaml

 2022-01-18 06:50:43 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → cat nbr_deployment.yaml
---
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: bear
  labels:
    app: animals
    animal: bear
spec:
  replicas: 2
  selector:
    matchLabels:
      app: animals
      task: bear
  template:
    metadata:
      labels:
        app: animals
        task: bear
        version: v0.0.1
    spec:
      containers:
      - name: bear
        image: supergiantkir/animals:bear
        ports:
        - containerPort: 80
---
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: moose
  labels:
    app: animals
    animal: moose
spec:
  replicas: 2
  selector:
    matchLabels:
      app: animals
      task: moose
  template:
    metadata:
      labels:
        app: animals
        task: moose
        version: v0.0.1
    spec:
      containers:
      - name: moose
        image: supergiantkir/animals:moose
        ports:
        - containerPort: 80
---
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: hare
  labels:
    app: animals
    animal: hare
spec:
  replicas: 2
  selector:
    matchLabels:
      app: animals
      task: hare
  template:
    metadata:
      labels:
        app: animals
        task: hare
        version: v0.0.1
    spec:
      containers:
      - name: hare
        image: supergiantkir/animals:hare
        ports:
        - containerPort: 80
 2022-01-18 06:50:48 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → k apply -f nbr_deployment.yaml
unable to recognize "nbr_deployment.yaml": no matches for kind "Deployment" in version "extensions/v1beta1"
unable to recognize "nbr_deployment.yaml": no matches for kind "Deployment" in version "extensions/v1beta1"
unable to recognize "nbr_deployment.yaml": no matches for kind "Deployment" in version "extensions/v1beta1"

 2022-01-18 06:50:57 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ →

 2022-01-18 06:51:46 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → cat ../traefik_deployment.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: traefik-ingress
  namespace: kube-system
  labels:
    k8s-app: traefik-ingress-lb
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: traefik-ingress-lb
  template:
    metadata:
      labels:
        k8s-app: traefik-ingress-lb
        name: traefik-ingress-lb
    spec:
      serviceAccountName: traefik-ingress
      terminationGracePeriodSeconds: 60
      containers:
      - image: traefik:v1.7.16
        name: traefik-ingress-lb
        ports:
        - name: http
          containerPort: 80
        - name: admin
          containerPort: 8080
        args:
        - --api
        - --kubernetes
        - --logLevel=DEBUG
 2022-01-18 06:51:50 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → k apply -f nbr_deployment.yaml
deployment.apps/bear created
deployment.apps/moose created
deployment.apps/hare created

 2022-01-18 06:52:09 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → kgp -A
NAMESPACE              NAME                                         READY   STATUS              RESTARTS   AGE
default                bear-7c57fc57dc-6pkgl                        0/1     ContainerCreating   0          6s
default                bear-7c57fc57dc-lrnpx                        0/1     ContainerCreating   0          6s
default                hare-58c6bd8f9-gnbmn                         0/1     ContainerCreating   0          6s
default                hare-58c6bd8f9-gnbzx                         0/1     ContainerCreating   0          6s
default                hello-minikube-5655c9d946-lrpsc              1/1     Running             0          70m
default                moose-868fbc6b78-4d6ff                       0/1     ContainerCreating   0          6s
default                moose-868fbc6b78-v2778                       0/1     ContainerCreating   0          6s
kube-system            coredns-66bff467f8-hjv9n                     1/1     Running             0          74m
kube-system            coredns-66bff467f8-r8znt                     1/1     Running             0          74m
kube-system            etcd-minikube                                1/1     Running             0          75m
kube-system            kube-apiserver-minikube                      1/1     Running             0          75m
kube-system            kube-controller-manager-minikube             1/1     Running             0          75m
kube-system            kube-proxy-4dz9l                             1/1     Running             0          74m
kube-system            kube-scheduler-minikube                      1/1     Running             0          75m
kube-system            storage-provisioner                          1/1     Running             0          75m
kube-system            traefik-ingress-55c99674d9-mbp5h             1/1     Running             0          20m
kubernetes-dashboard   dashboard-metrics-scraper-84bfdf55ff-4mw5l   1/1     Running             0          72m
kubernetes-dashboard   kubernetes-dashboard-bc446cc64-6dp6z         1/1     Running             0          72m

 2022-01-18 06:52:15 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → kgp -A -w
NAMESPACE              NAME                                         READY   STATUS              RESTARTS   AGE
default                bear-7c57fc57dc-6pkgl                        0/1     ContainerCreating   0          13s
default                bear-7c57fc57dc-lrnpx                        0/1     ContainerCreating   0          13s
default                hare-58c6bd8f9-gnbmn                         0/1     ContainerCreating   0          13s
default                hare-58c6bd8f9-gnbzx                         0/1     ContainerCreating   0          13s
default                hello-minikube-5655c9d946-lrpsc              1/1     Running             0          70m
default                moose-868fbc6b78-4d6ff                       0/1     ContainerCreating   0          13s
default                moose-868fbc6b78-v2778                       0/1     ContainerCreating   0          13s
kube-system            coredns-66bff467f8-hjv9n                     1/1     Running             0          75m
kube-system            coredns-66bff467f8-r8znt                     1/1     Running             0          75m
kube-system            etcd-minikube                                1/1     Running             0          75m
kube-system            kube-apiserver-minikube                      1/1     Running             0          75m
kube-system            kube-controller-manager-minikube             1/1     Running             0          75m
kube-system            kube-proxy-4dz9l                             1/1     Running             0          75m
kube-system            kube-scheduler-minikube                      1/1     Running             0          75m
kube-system            storage-provisioner                          1/1     Running             0          75m
kube-system            traefik-ingress-55c99674d9-mbp5h             1/1     Running             0          20m
kubernetes-dashboard   dashboard-metrics-scraper-84bfdf55ff-4mw5l   1/1     Running             0          72m
kubernetes-dashboard   kubernetes-dashboard-bc446cc64-6dp6z         1/1     Running             0          72m
default                bear-7c57fc57dc-lrnpx                        1/1     Running             0          21s
default                bear-7c57fc57dc-6pkgl                        1/1     Running             0          25s
default                hare-58c6bd8f9-gnbmn                         1/1     Running             0          33s
default                hare-58c6bd8f9-gnbzx                         1/1     Running             0          38s
default                moose-868fbc6b78-v2778                       1/1     Running             0          44s
default                moose-868fbc6b78-4d6ff                       1/1     Running             0          49s
^C
 2022-01-18 06:54:04 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → ll
total 16
-rw-r--r--  1 azhekhan  staff  1292 Jan 18 06:52 nbr_deployment.yaml
-rw-r--r--  1 azhekhan  staff   566 Jan 18 06:53 nbr_services.yaml

 2022-01-18 06:54:06 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → k apply -f nbr_services.yaml
service/bear created
service/moose created
service/hare created

 2022-01-18 06:54:13 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → ll
total 24
-rw-r--r--  1 azhekhan  staff  1292 Jan 18 06:52 nbr_deployment.yaml
-rw-r--r--  1 azhekhan  staff   566 Jan 18 06:53 nbr_services.yaml
-rw-r--r--  1 azhekhan  staff   551 Jan 18 06:55 nbr_ingress.yaml

 2022-01-18 06:55:08 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → k apply -f nbr_ingress.yaml
ingress.extensions/animals created

 2022-01-18 06:55:15 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → kgn default ingress
NAME      CLASS    HOSTS                                        ADDRESS   PORTS   AGE
animals   <none>   hare.minikube,bear.minikube,moose.minikube             80      16s

 2022-01-18 06:55:31 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → kdn default ingress
Name:             animals
Namespace:        default
Address:
Default backend:  default-http-backend:80 (<error: endpoints "default-http-backend" not found>)
Rules:
  Host            Path  Backends
  ----            ----  --------
  hare.minikube
##
                  /   hare:http (172.17.0.10:80,172.17.0.11:80)
  bear.minikube
                  /   bear:http (172.17.0.8:80,172.17.0.9:80)
  moose.minikube
                  /   moose:http (172.17.0.12:80,172.17.0.13:80)
Annotations:      kubernetes.io/ingress.class: traefik
Events:           <none>

 2022-01-18 06:55:33 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → echo “$(minikube ip) bear.minikube hare.minikube moose.minikube”
“192.168.64.2 bear.minikube hare.minikube moose.minikube”

 2022-01-18 06:56:34 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → echo “$(minikube ip) bear.minikube hare.minikube moose.minikube” | sudo tee -a /etc/hosts
Password:

[1]+  Stopped                 echo “$(minikube ip) bear.minikube hare.minikube moose.minikube” | sudo tee -a /etc/hosts

 2022-01-18 06:56:58 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → cat /etc/hosts
##
# Host Database
#
# localhost is used to configure the loopback interface
# when the system is booting.  Do not change this entry.
##
127.0.0.1 localhost
255.255.255.255 broadcasthost
::1             localhost
#144.25.105.44   versioning-corp-versioning.channels.ocs.oc-test.com

# Added by Docker Desktop
# To allow the same kube context to work on the host and the container:
127.0.0.1 kubernetes.docker.internal
# End of section

## TESTING for OCI App Gateway
130.35.238.196 admin.internal.local
130.35.238.196 az.oracle.com
130.35.238.196 admin

127.0.0.1 whoami.docker.local
192.168.64.2 traefik-ui.minikube

 2022-01-18 06:57:03 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → sudo vi /etc/hosts
Password:

 2022-01-18 06:57:27 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → kd services
Name:              bear
Namespace:         default
Labels:            <none>
Annotations:       Selector:  app=animals,task=bear
Type:              ClusterIP
IP:                10.104.189.252
Port:              http  80/TCP
TargetPort:        80/TCP
Endpoints:         172.17.0.8:80,172.17.0.9:80
Session Affinity:  None
Events:            <none>


Name:              hare
Namespace:         default
Labels:            <none>
Annotations:       traefik.backend.circuitbreaker: NetworkErrorRatio() > 0.5
Selector:          app=animals,task=hare
Type:              ClusterIP
IP:                10.105.102.6
Port:              http  80/TCP
TargetPort:        80/TCP
Endpoints:         172.17.0.10:80,172.17.0.11:80
Session Affinity:  None
Events:            <none>


Name:                     hello-minikube
Namespace:                default
Labels:                   app=hello-minikube
Annotations:              <none>
Selector:                 app=hello-minikube
Type:                     NodePort
IP:                       10.104.176.152
Port:                     <unset>  8080/TCP
TargetPort:               8080/TCP
NodePort:                 <unset>  30635/TCP
Endpoints:                172.17.0.6:8080
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>


Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              ClusterIP
IP:                10.96.0.1
Port:              https  443/TCP
TargetPort:        8443/TCP
Endpoints:         192.168.64.2:8443
Session Affinity:  None
Events:            <none>


Name:              moose
Namespace:         default
Labels:            <none>
Annotations:       Selector:  app=animals,task=moose
Type:              ClusterIP
IP:                10.109.78.236
Port:              http  80/TCP
TargetPort:        80/TCP
Endpoints:         172.17.0.12:80,172.17.0.13:80
Session Affinity:  None
Events:            <none>

 2022-01-18 06:58:22 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → kg services
NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
bear             ClusterIP   10.104.189.252   <none>        80/TCP           6m32s
hare             ClusterIP   10.105.102.6     <none>        80/TCP           6m32s
hello-minikube   NodePort    10.104.176.152   <none>        8080:30635/TCP   78m
kubernetes       ClusterIP   10.96.0.1        <none>        443/TCP          83m
moose            ClusterIP   10.109.78.236    <none>        80/TCP           6m32s

 2022-01-18 07:00:45 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → kg ingress
NAME      CLASS    HOSTS                                        ADDRESS   PORTS   AGE
animals   <none>   hare.minikube,bear.minikube,moose.minikube             80      5m36s

 2022-01-18 07:00:51 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → kd services moose
Name:              moose
Namespace:         default
Labels:            <none>
Annotations:       Selector:  app=animals,task=moose
Type:              ClusterIP
IP:                10.109.78.236
Port:              http  80/TCP
TargetPort:        80/TCP
Endpoints:         172.17.0.12:80,172.17.0.13:80
Session Affinity:  None
Events:            <none>

 2022-01-18 07:01:08 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → k apply -f nbr_ingress.yaml
ingress.extensions/animals configured
ingress.extensions/all-animals created

 2022-01-18 07:06:45 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → kg ingress
NAME          CLASS    HOSTS                                        ADDRESS   PORTS   AGE
all-animals   <none>   animals.minikube                                       80      5s
animals       <none>   hare.minikube,bear.minikube,moose.minikube             80      11m

 2022-01-18 07:06:50 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → kd ingress all-animals
Name:             all-animals
Namespace:        default
##
Address:
Default backend:  default-http-backend:80 (<error: endpoints "default-http-backend" not found>)
Rules:
  Host              Path  Backends
  ----              ----  --------
  animals.minikube
                    /bear    bear:http (172.17.0.8:80,172.17.0.9:80)
                    /moose   moose:http (172.17.0.12:80,172.17.0.13:80)
                    /hare    hare:http (172.17.0.10:80,172.17.0.11:80)
Annotations:        kubernetes.io/ingress.class: traefik
                    traefik.frontend.rule.type: PathPrefixStrip
Events:             <none>

 2022-01-18 07:07:01 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → echo “$(minikube ip) animals.minikube”
“192.168.64.2 animals.minikube”

 2022-01-18 07:07:17 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → cat /etc/hosts
##
# Host Database
#
# localhost is used to configure the loopback interface
# when the system is booting.  Do not change this entry.
##
127.0.0.1 localhost
255.255.255.255 broadcasthost
::1             localhost
#144.25.105.44   versioning-corp-versioning.channels.ocs.oc-test.com

# Added by Docker Desktop
# To allow the same kube context to work on the host and the container:
127.0.0.1 kubernetes.docker.internal
# End of section

## TESTING for OCI App Gateway
130.35.238.196 admin.internal.local
130.35.238.196 az.oracle.com
130.35.238.196 admin

127.0.0.1 whoami.docker.local
192.168.64.2 traefik-ui.minikube
192.168.64.2 bear.minikube hare.minikube moose.minikube

 2022-01-18 07:07:21 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → sudo vi /etc/hosts
Password:

 2022-01-18 07:07:50 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ →

 2022-01-18 10:12:26 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ →
Session Contents Restored on 18-Jan-2022 at 6:09 PM
Last login: Tue Jan 18 18:09:46 on ttys005

The default interactive shell is now zsh.
To update your account to use zsh, please run `chsh -s /bin/zsh`.
For more details, please visit https://support.apple.com/kb/HT208050.

 2022-01-18 18:09:51 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ →
Session Contents Restored on 18-Jan-2022 at 8:31 PM
Last login: Tue Jan 18 20:30:57 on console

The default interactive shell is now zsh.
To update your account to use zsh, please run `chsh -s /bin/zsh`.
For more details, please visit https://support.apple.com/kb/HT208050.

 2022-01-18 20:33:31 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → cpev2
HOME_DIR : /home/opc
Review: The following Wiki for more setup instructions
https://confluence.oraclecorp.com/confluence/display/CPE/Development+Setup+for+Platform+v2

kgcAliasing vim --> vi editor

v2dev
kgc
rp
cp: cannot create regular file ‘/home/opc/.kube/pv2_dev.config’: Permission denied
error: could not lock config file .git/config: No such file or directory
[20:January:2022:03:36:08]:(minikube):~
○ $
[20:January:2022:03:36:08]:(minikube):~
○ $ kgc
CURRENT   NAME           CLUSTER        AUTHINFO       NAMESPACE
          kind-testk8s   kind-testk8s   kind-testk8s
*         minikube       minikube       minikube
[20:January:2022:03:36:08]:(minikube):~
○ $ v2dev
[20:January:2022:03:36:08]:(dev_eu-frankfurt-1_dataplane):~
○ $ kgc
CURRENT   NAME                              CLUSTER               AUTHINFO           NAMESPACE
          dev_eu-frankfurt-1_controlplane   cluster-c4dsmruga4w   user-c4dsmruga4w
*         dev_eu-frankfurt-1_dataplane      cluster-cqtkntfgrtg   user-cqtkntfgrtg
          dev_eu-frankfurt-1_deployment     cluster-cqtqnbymy2t   user-cqtqnbymy2t
          dev_uk-london-1_controlplane      cluster-cydknrqgzrg   user-cydknrqgzrg
          dev_uk-london-1_dataplane         cluster-cqtozrugy3t   user-cqtozrugy3t
[20:January:2022:03:36:08]:(dev_eu-frankfurt-1_dataplane):~
○ $ rp
[20:January:2022:03:36:08]:(dev_eu-frankfurt-1_dataplane):~
○ $ v2corp
[20:January:2022:04:30:06]:(corp_us-ashburn-1_controlplane):~
○ $ kgc
CURRENT   NAME                             CLUSTER               AUTHINFO           NAMESPACE
*         corp_us-ashburn-1_controlplane   cluster-c3wgzdgmfqt   user-c3wgzdgmfqt
          corp_us-ashburn-1_dataplane      cluster-c2tqztfmy3t   user-c2tqztfmy3t
          corp_us-ashburn-1_deployment     cluster-crdcyjvmztg   user-crdcyjvmztg
[20:January:2022:04:30:07]:(corp_us-ashburn-1_controlplane):~
○ $ #acs2-dev1
[20:January:2022:04:30:09]:(corp_us-ashburn-1_controlplane):~
○ $ kuc corp_us-ashburn-1_dataplane
Switched to context "corp_us-ashburn-1_dataplane".
[20:January:2022:04:30:12]:(corp_us-ashburn-1_controlplane):~
○ $ rp
[20:January:2022:04:30:13]:(corp_us-ashburn-1_dataplane):~
○ $ kgpn acs2-dev1
NAME                                              READY   STATUS             RESTARTS   AGE     IP              NODE          NOMINATED NODE   READINESS GATES
acs-calculate-billable-sessions-6c6c76966-t2s42   2/2     Running            11         7h43m   10.245.22.149   10.11.8.110   <none>           <none>
acs-compute-correlations-5cbfd8759-w2ktl          1/2     CrashLoopBackOff   90         7h43m   10.245.23.250   10.11.8.174   <none>           <none>
acs-compute-engagements-6d56555994-bvtzn          2/2     Running            0          7h43m   10.245.17.99    10.11.9.124   <none>           <none>
acs-ingress-7bf9c769f6-pb8mb                      1/2     CrashLoopBackOff   91         7h43m   10.245.26.166   10.11.9.14    <none>           <none>
acs-ip-ban-write-6d9dfb5cf9-d6dwf                 1/2     CrashLoopBackOff   90         7h43m   10.245.16.126   10.11.8.222   <none>           <none>
acs-query-api-74c8f7bcc7-jr5vz                    2/2     Running            0          7h43m   10.245.22.148   10.11.8.110   <none>           <none>
acs-sink-connect-5d97cf77d8-8cwnz                 2/2     Running            0          7h43m   10.245.4.159    10.11.8.80    <none>           <none>
acs-source-connect-56cc98888d-7b9nr               2/2     Running            0          7h43m   10.245.14.62    10.11.9.104   <none>           <none>
[20:January:2022:04:30:26]:(corp_us-ashburn-1_dataplane):~
○ $ kgpn data-pipeline-ci
NAME                                            READY   STATUS             RESTARTS   AGE    IP              NODE          NOMINATED NODE   READINESS GATES
data-pipeline-metadata-publisher-9pr7c          0/1     Completed          0          146m   10.245.13.201   10.11.9.145   <none>           <none>
datapipeline-provisioner-56b5b4ddf8-dz5rs       1/1     Running            0          146m   10.245.0.90     10.11.9.49    <none>           <none>
helm-integration-test-client-6b4c576c6c-cr2nk   1/1     Running            2          146m   10.245.3.94     10.11.8.232   <none>           <none>
kafka-connect-5b8f5768d4-wlqwg                  1/1     Running            0          146m   10.245.8.46     10.11.8.229   <none>           <none>
kafka-osvc-c7fd55d7d-7vr4q                      1/1     Running            1          146m   10.245.19.149   10.11.9.93    <none>           <none>
kafka-setup-lm8v9                               0/1     Completed          0          146m   10.245.1.248    10.11.8.82    <none>           <none>
raw2com-c99ff8df-hpz29                          0/1     CrashLoopBackOff   30         146m   10.245.10.14    10.11.9.95    <none>           <none>
republishing-service-7fbbfd44c5-p2h5t           1/1     Running            0          146m   10.245.2.38     10.11.9.23    <none>           <none>
schema-registry-65d5df65b5-dscp6                1/1     Running            0          146m   10.245.20.148   10.11.8.225   <none>           <none>
schema-service-7694d5b7f-fz4vf                  1/1     Running            0          146m   10.245.11.184   10.11.9.208   <none>           <none>
vault-74fc9b6cd4-mdxsr                          2/2     Running            0          146m   10.245.18.71    10.11.9.160   <none>           <none>
zookeeper-57d49f5645-6xcp6                      1/1     Running            0          146m   10.245.20.149   10.11.8.225   <none>           <none>
[20:January:2022:04:34:01]:(corp_us-ashburn-1_dataplane):~
○ $ kgpn as-ciat
NAME                                           READY   STATUS    RESTARTS   AGE     IP              NODE          NOMINATED NODE   READINESS GATES
lorax-analyticsmanager-75d985dfdf-6jmjm        2/2     Running   0          4h40m   10.245.14.74    10.11.9.104   <none>           <none>
lorax-analyticsmanager-75d985dfdf-mmxrw        2/2     Running   0          4h40m   10.245.13.192   10.11.9.145   <none>           <none>
lorax-analyticsmanager-75d985dfdf-zfr7b        2/2     Running   0          4h40m   10.245.20.140   10.11.8.225   <none>           <none>
lorax-cachehost-647b6cf7b4-4v6n7               2/2     Running   0          3h9m    10.245.13.197   10.11.9.145   <none>           <none>
lorax-cachehost-647b6cf7b4-jsqlv               2/2     Running   0          3h9m    10.245.27.178   10.11.8.60    <none>           <none>
lorax-cachehost-6c8948bf68-8gmdf               2/2     Running   0          4h40m   10.245.19.21    10.11.9.238   <none>           <none>
lorax-cachehost-6c8948bf68-mzvx7               2/2     Running   0          4h40m   10.245.26.8     10.11.8.67    <none>           <none>
lorax-cachehost-6c8948bf68-vf26t               2/2     Running   0          4h40m   10.245.15.225   10.11.9.78    <none>           <none>
lorax-commandmanager-586d58947d-2h89f          2/2     Running   0          4h40m   10.245.28.203   10.11.9.56    <none>           <none>
lorax-commandmanager-586d58947d-2rcmp          2/2     Running   0          4h40m   10.245.11.175   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-c4482          2/2     Running   0          4h40m   10.245.27.176   10.11.8.60    <none>           <none>
lorax-contextmanager-67956c7584-c4xkw          2/2     Running   0          4h40m   10.245.11.177   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-h82q5          2/2     Running   0          4h40m   10.245.3.92     10.11.8.232   <none>           <none>
lorax-customizationmanager-657bbf9586-h242p    2/2     Running   0          4h40m   10.245.16.201   10.11.8.203   <none>           <none>
lorax-customizationmanager-657bbf9586-rsbqg    2/2     Running   0          4h40m   10.245.29.214   10.11.9.62    <none>           <none>
lorax-elementmanager-7f74646f4d-n4cqx          2/2     Running   0          4h40m   10.245.28.39    10.11.8.31    <none>           <none>
lorax-eventmanager-759c5b9484-ch28f            2/2     Running   0          4h40m   10.245.15.226   10.11.9.78    <none>           <none>
lorax-eventmanager-759c5b9484-xfl42            2/2     Running   0          4h40m   10.245.22.186   10.11.8.110   <none>           <none>
lorax-eventworker-7c5b779bc-z4bb5              2/2     Running   0          4h40m   10.245.22.102   10.11.8.250   <none>           <none>
lorax-queuemanager-769996b97b-z4kqc            2/2     Running   0          4h40m   10.245.16.24    10.11.8.222   <none>           <none>
lorax-realtimedatamanager-866cd5886b-qq54f     2/2     Running   0          4h40m   10.245.29.213   10.11.9.62    <none>           <none>
lorax-reportdesignmanager-5c88cb777d-sxlx9     2/2     Running   0          4h40m   10.245.20.94    10.11.8.160   <none>           <none>
lorax-reportpublishmanager-58445646c4-9ztc7    2/2     Running   0          4h40m   10.245.11.176   10.11.9.208   <none>           <none>
lorax-searchmanager-7d97d6dfcc-gn4zg           2/2     Running   0          4h40m   10.245.29.212   10.11.9.62    <none>           <none>
lorax-searchmanager-7d97d6dfcc-q2x5t           2/2     Running   0          4h40m   10.245.14.73    10.11.9.104   <none>           <none>
lorax-securityservice-b9bb8695d-6mgbq          2/2     Running   0          4h40m   10.245.22.187   10.11.8.110   <none>           <none>
lorax-securityservice-b9bb8695d-t7bz5          2/2     Running   0          4h40m   10.245.12.189   10.11.8.194   <none>           <none>
who-internal-who-deployment-65d44d6f4c-fqhbs   2/2     Running   0          4h39m   10.245.25.112   10.11.8.142   <none>           <none>
who-internal-who-deployment-65d44d6f4c-w8lnq   2/2     Running   0          4h39m   10.245.11.179   10.11.9.208   <none>           <none>
who-internal-who-deployment-65d44d6f4c-xljbh   2/2     Running   0          4h40m   10.245.16.199   10.11.8.203   <none>           <none>
who-who-deployment-576cd44f87-4dr8c            2/2     Running   0          4h39m   10.245.9.42     10.11.9.136   <none>           <none>
who-who-deployment-576cd44f87-lhb49            2/2     Running   0          4h40m   10.245.16.200   10.11.8.203   <none>           <none>
who-who-deployment-576cd44f87-lkhrc            2/2     Running   0          4h38m   10.245.23.136   10.11.8.174   <none>           <none>
[20:January:2022:04:38:55]:(corp_us-ashburn-1_dataplane):~
○ $ helm ls -n as-ciat
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
NAME          NAMESPACE REVISION  UPDATED                                 STATUS    CHART               APP VERSION
horton        as-ciat   1         2022-01-20 04:28:35.908707209 +0000 UTC deployed  horton-dock-22.4.10 565ff6986f4ec0511b1de2218ba3c3244ad23e75
lorax         as-ciat   1         2022-01-20 04:28:46.513641785 +0000 UTC deployed  lorax-dock-22.4.7   431ac7a500783c0421ee8721a3f7547f480becc9
who           as-ciat   468       2022-01-20 04:28:27.240742774 +0000 UTC deployed  who-master-22.4.1   49f19d6e243010f2154536e8a95e18c1c5f07b48
who-internal  as-ciat   118       2022-01-20 04:28:26.105703774 +0000 UTC deployed  who-master-22.4.1   49f19d6e243010f2154536e8a95e18c1c5f07b48
[20:January:2022:04:39:33]:(corp_us-ashburn-1_dataplane):~
○ $ kgdpn as-ciat

kgssn as-ciat
NAME                          READY   UP-TO-DATE   AVAILABLE   AGE
horton-horton-deployment      0/3     0            0           11m
lorax-analyticsmanager        0/3     0            0           11m
lorax-cachehost               0/3     0            0           11m
lorax-commandmanager          0/2     0            0           11m
lorax-contextmanager          0/3     0            0           11m
lorax-customizationmanager    0/2     0            0           11m
lorax-elementmanager          0/1     0            0           11m
lorax-eventmanager            0/2     0            0           11m
lorax-eventworker             0/1     0            0           11m
lorax-queuemanager            0/1     0            0           11m
lorax-realtimedatamanager     0/1     0            0           11m
lorax-reportdesignmanager     0/1     0            0           11m
lorax-reportpublishmanager    0/1     0            0           11m
lorax-searchmanager           0/2     0            0           11m
lorax-securityservice         0/2     0            0           11m
who-internal-who-deployment   3/3     0            3           8d
who-who-deployment            3/3     0            3           27d
[20:January:2022:04:40:00]:(corp_us-ashburn-1_dataplane):~
○ $
[20:January:2022:04:40:00]:(corp_us-ashburn-1_dataplane):~
○ $ kgssn as-ciat
No resources found in as-ciat namespace.
[20:January:2022:04:40:05]:(corp_us-ashburn-1_dataplane):~
○ $ kgc
CURRENT   NAME                             CLUSTER               AUTHINFO           NAMESPACE
          corp_us-ashburn-1_controlplane   cluster-c3wgzdgmfqt   user-c3wgzdgmfqt
*         corp_us-ashburn-1_dataplane      cluster-c2tqztfmy3t   user-c2tqztfmy3t
          corp_us-ashburn-1_deployment     cluster-crdcyjvmztg   user-crdcyjvmztg
[20:January:2022:04:40:51]:(corp_us-ashburn-1_dataplane):~
○ $ kuc corp_us-ashburn-1_controlplane
Switched to context "corp_us-ashburn-1_controlplane".
[20:January:2022:04:40:56]:(corp_us-ashburn-1_dataplane):~
○ $ rp
[20:January:2022:04:40:56]:(corp_us-ashburn-1_controlplane):~
○ $ kgpn as-ciat
No resources found in as-ciat namespace.
[20:January:2022:04:41:11]:(corp_us-ashburn-1_controlplane):~
○ $ kuc corp_us-ashburn-1_dataplane
Switched to context "corp_us-ashburn-1_dataplane".
[20:January:2022:04:41:19]:(corp_us-ashburn-1_controlplane):~
○ $ rp
[20:January:2022:04:41:19]:(corp_us-ashburn-1_dataplane):~
○ $ kgpn as-ciat
NAME                                           READY   STATUS    RESTARTS   AGE     IP              NODE          NOMINATED NODE   READINESS GATES
lorax-analyticsmanager-75d985dfdf-6jmjm        2/2     Running   0          4h42m   10.245.14.74    10.11.9.104   <none>           <none>
lorax-analyticsmanager-75d985dfdf-mmxrw        2/2     Running   0          4h42m   10.245.13.192   10.11.9.145   <none>           <none>
lorax-analyticsmanager-75d985dfdf-zfr7b        2/2     Running   0          4h42m   10.245.20.140   10.11.8.225   <none>           <none>
lorax-cachehost-647b6cf7b4-4v6n7               2/2     Running   0          3h12m   10.245.13.197   10.11.9.145   <none>           <none>
lorax-cachehost-647b6cf7b4-jsqlv               2/2     Running   0          3h12m   10.245.27.178   10.11.8.60    <none>           <none>
lorax-cachehost-6c8948bf68-8gmdf               2/2     Running   0          4h42m   10.245.19.21    10.11.9.238   <none>           <none>
lorax-cachehost-6c8948bf68-mzvx7               2/2     Running   0          4h42m   10.245.26.8     10.11.8.67    <none>           <none>
lorax-cachehost-6c8948bf68-vf26t               2/2     Running   0          4h42m   10.245.15.225   10.11.9.78    <none>           <none>
lorax-commandmanager-586d58947d-2h89f          2/2     Running   0          4h42m   10.245.28.203   10.11.9.56    <none>           <none>
lorax-commandmanager-586d58947d-2rcmp          2/2     Running   0          4h42m   10.245.11.175   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-c4482          2/2     Running   0          4h42m   10.245.27.176   10.11.8.60    <none>           <none>
lorax-contextmanager-67956c7584-c4xkw          2/2     Running   0          4h42m   10.245.11.177   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-h82q5          2/2     Running   0          4h42m   10.245.3.92     10.11.8.232   <none>           <none>
lorax-customizationmanager-657bbf9586-h242p    2/2     Running   0          4h42m   10.245.16.201   10.11.8.203   <none>           <none>
lorax-customizationmanager-657bbf9586-rsbqg    2/2     Running   0          4h42m   10.245.29.214   10.11.9.62    <none>           <none>
lorax-elementmanager-7f74646f4d-n4cqx          2/2     Running   0          4h42m   10.245.28.39    10.11.8.31    <none>           <none>
lorax-eventmanager-759c5b9484-ch28f            2/2     Running   0          4h42m   10.245.15.226   10.11.9.78    <none>           <none>
lorax-eventmanager-759c5b9484-xfl42            2/2     Running   0          4h42m   10.245.22.186   10.11.8.110   <none>           <none>
lorax-eventworker-7c5b779bc-z4bb5              2/2     Running   0          4h42m   10.245.22.102   10.11.8.250   <none>           <none>
lorax-queuemanager-769996b97b-z4kqc            2/2     Running   0          4h42m   10.245.16.24    10.11.8.222   <none>           <none>
lorax-realtimedatamanager-866cd5886b-qq54f     2/2     Running   0          4h42m   10.245.29.213   10.11.9.62    <none>           <none>
lorax-reportdesignmanager-5c88cb777d-sxlx9     2/2     Running   0          4h42m   10.245.20.94    10.11.8.160   <none>           <none>
lorax-reportpublishmanager-58445646c4-9ztc7    2/2     Running   0          4h42m   10.245.11.176   10.11.9.208   <none>           <none>
lorax-searchmanager-7d97d6dfcc-gn4zg           2/2     Running   0          4h42m   10.245.29.212   10.11.9.62    <none>           <none>
lorax-searchmanager-7d97d6dfcc-q2x5t           2/2     Running   0          4h42m   10.245.14.73    10.11.9.104   <none>           <none>
lorax-securityservice-b9bb8695d-6mgbq          2/2     Running   0          4h42m   10.245.22.187   10.11.8.110   <none>           <none>
lorax-securityservice-b9bb8695d-t7bz5          2/2     Running   0          4h42m   10.245.12.189   10.11.8.194   <none>           <none>
who-internal-who-deployment-65d44d6f4c-fqhbs   2/2     Running   0          4h41m   10.245.25.112   10.11.8.142   <none>           <none>
who-internal-who-deployment-65d44d6f4c-w8lnq   2/2     Running   0          4h41m   10.245.11.179   10.11.9.208   <none>           <none>
who-internal-who-deployment-65d44d6f4c-xljbh   2/2     Running   0          4h43m   10.245.16.199   10.11.8.203   <none>           <none>
who-who-deployment-576cd44f87-4dr8c            2/2     Running   0          4h41m   10.245.9.42     10.11.9.136   <none>           <none>
who-who-deployment-576cd44f87-lhb49            2/2     Running   0          4h42m   10.245.16.200   10.11.8.203   <none>           <none>
who-who-deployment-576cd44f87-lkhrc            2/2     Running   0          4h40m   10.245.23.136   10.11.8.174   <none>           <none>
[20:January:2022:04:41:33]:(corp_us-ashburn-1_dataplane):~
○ $
[20:January:2022:04:50:34]:(corp_us-ashburn-1_dataplane):~
○ $ kgpn as-jcs
NAME                                           READY   STATUS              RESTARTS   AGE     IP              NODE          NOMINATED NODE   READINESS GATES
echo-769c9d8c54-tjvkp                          0/1     ErrImageNeverPull   0          3m48s   10.245.11.112   10.11.8.46    <none>           <none>
who-internal-who-deployment-869d6dd88d-kt955   2/2     Running             317        5d6h    10.245.18.30    10.11.9.160   <none>           <none>
[20:January:2022:04:50:49]:(corp_us-ashburn-1_dataplane):~
○ $ helm ls -n as-jcs
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
NAME  NAMESPACE REVISION  UPDATED STATUS  CHART APP VERSION
[20:January:2022:04:51:01]:(corp_us-ashburn-1_dataplane):~
○ $ kgn as-jcs all
NAME                                               READY   STATUS              RESTARTS   AGE
pod/echo-769c9d8c54-tjvkp                          0/1     ErrImageNeverPull   0          6m32s
pod/who-internal-who-deployment-869d6dd88d-kt955   2/2     Running             317        5d6h

NAME                                                     DESIRED   CURRENT   READY   AGE
replicaset.apps/echo-6677859554                          1         0         0       9m39s
replicaset.apps/echo-769c9d8c54                          1         1         0       6m37s
replicaset.apps/who-internal-who-deployment-869d6dd88d   1         1         1       5d6h
[20:January:2022:04:53:43]:(corp_us-ashburn-1_dataplane):~
○ $ kgn as-jcs secrets
kgn as-jcs configmaps
NAME                   TYPE                                  DATA   AGE
consul-tls             Opaque                                1      51d
default-token-f8w7v    kubernetes.io/service-account-token   3      51d
ocirsecret             kubernetes.io/dockerconfigjson        1      51d
osvc-token             kubernetes.io/dockerconfigjson        1      48d
osvcstage-ocirsecret   kubernetes.io/dockerconfigjson        1      51d
vault-tls              Opaque                                1      51d
[20:January:2022:04:55:22]:(corp_us-ashburn-1_dataplane):~
○ $ kgn as-jcs configmaps
NAME                  DATA   AGE
as-jcs-va-configmap   1      51d
[20:January:2022:04:55:26]:(corp_us-ashburn-1_dataplane):~
○ $ kgn as-jcs secrets -oyaml
apiVersion: v1
items:
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM3akNDQXBTZ0F3SUJBZ0lSQU9nK0oyVk5SL1U2UHE0dnhBclNBTnd3Q2dZSUtvWkl6ajBFQXdJd2dia3gKQ3pBSkJnTlZCQVlUQWxWVE1Rc3dDUVlEVlFRSUV3SkRRVEVXTUJRR0ExVUVCeE1OVTJGdUlFWnlZVzVqYVhOagpiekVhTUJnR0ExVUVDUk1STVRBeElGTmxZMjl1WkNCVGRISmxaWFF4RGpBTUJnTlZCQkVUQlRrME1UQTFNUmN3CkZRWURWUVFLRXc1SVlYTm9hVU52Y25BZ1NXNWpMakZBTUQ0R0ExVUVBeE0zUTI5dWMzVnNJRUZuWlc1MElFTkIKSURNd09EY3dNell4TmpRMk56STJOVGt6TlRjeU5qRTBPVEF5TnprME5EazNOVEUzTVRnd05EQWVGdzB5TURFdwpNamt5TURRNE5UTmFGdzB5TlRFd01qZ3lNRFE0TlROYU1JRzVNUXN3Q1FZRFZRUUdFd0pWVXpFTE1Ba0dBMVVFCkNCTUNRMEV4RmpBVUJnTlZCQWNURFZOaGJpQkdjbUZ1WTJselkyOHhHakFZQmdOVkJBa1RFVEV3TVNCVFpXTnYKYm1RZ1UzUnlaV1YwTVE0d0RBWURWUVFSRXdVNU5ERXdOVEVYTUJVR0ExVUVDaE1PU0dGemFHbERiM0p3SUVsdQpZeTR4UURBK0JnTlZCQU1UTjBOdmJuTjFiQ0JCWjJWdWRDQkRRU0F6TURnM01ETTJNVFkwTmpjeU5qVTVNelUzCk1qWXhORGt3TWpjNU5EUTVOelV4TnpFNE1EUXdXVEFUQmdjcWhrak9QUUlCQmdncWhrak9QUU1CQndOQ0FBUVkKQ0lCVHliN3RPMFhnQW55VVBkWHdCRWRJOGRvcUZ3bE96bVdVSU1oYUNlMmdxektpQWduODJJTkNWbzRzM1NmSApOVGpQdEs1UlEySEQ3WFlPeDByS28zc3dlVEFPQmdOVkhROEJBZjhFQkFNQ0FZWXdEd1lEVlIwVEFRSC9CQVV3CkF3RUIvekFwQmdOVkhRNEVJZ1FnM1ZRK1dqUHpQZmxxRkI5eW1rdStFSGtWVmpyc0l5OCtyUjd5T3dXd3ZVY3cKS3dZRFZSMGpCQ1F3SW9BZzNWUStXalB6UGZscUZCOXlta3UrRUhrVlZqcnNJeTgrclI3eU93V3d2VWN3Q2dZSQpLb1pJemowRUF3SURTQUF3UlFJZ1pNcHB5UjZLcHhtbkMrSjJQSE5Pd1RWNWR3UnM3WjBKS2R1cGI0TFVTSUVDCklRQ0JpVzBUNEdJZE5nVHdMcUJUelRZN09KUHNFTEt2cjlFeFVoSXAybFJKMkE9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
  kind: Secret
  metadata:
    annotations:
      meta.helm.sh/release-name: as-jcs
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2021-11-29T23:57:16Z"
    labels:
      app.kubernetes.io/managed-by: Helm
    name: consul-tls
    namespace: as-jcs
    resourceVersion: "666981857"
    selfLink: /api/v1/namespaces/as-jcs/secrets/consul-tls
    uid: 838af050-e90a-4ce9-a801-356a79d02e55
  type: Opaque
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURqRENDQW5TZ0F3SUJBZ0lVYjhVUEF2QzlvRnlzazlzMnlqRFNlZlk0ZVFrd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1hqRUxNQWtHQTFVRUJoTUNWVk14RGpBTUJnTlZCQWdUQlZSbGVHRnpNUTh3RFFZRFZRUUhFd1pCZFhOMAphVzR4RHpBTkJnTlZCQW9UQms5eVlXTnNaVEVNTUFvR0ExVUVDeE1EVDBSWU1ROHdEUVlEVlFRREV3WkxPRk1nClEwRXdIaGNOTWpBeE1ESXdNakV6TVRBd1doY05NalV4TURFNU1qRXpNVEF3V2pCZU1Rc3dDUVlEVlFRR0V3SlYKVXpFT01Bd0dBMVVFQ0JNRlZHVjRZWE14RHpBTkJnTlZCQWNUQmtGMWMzUnBiakVQTUEwR0ExVUVDaE1HVDNKaApZMnhsTVF3d0NnWURWUVFMRXdOUFJGZ3hEekFOQmdOVkJBTVRCa3M0VXlCRFFUQ0NBU0l3RFFZSktvWklodmNOCkFRRUJCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFNTU83blF6Y0xBVXg3aGRRTWxjUnZQNHNrejBTdkkrL0xpYXFDdHEKV0V4V1gxSnI5MzFDbmpzY29CYkFLWGtSZ3VXdnYvbWVYaDRiQlBQMDNmYkxVMk96UlJMYWR0OWYzL2NtOENxTwpGVC8rb3ozUzh6NjRkaENGU2k5RU5aWUJnbFMrYzFDVXdueUVyQ3hjSDJ1d0dIRVI4SmNrRUpsMHMrakppd0dmCkx4b0QyZXpud1F2SE5VdTlmL0RTaDVLZFNvSTBaV0RscFRObHdiNWhIYWxQeERmY0dZSi9aNU84SThnV1BMUTcKVG1YR1JPVnJoUTBrRHlRVnN1dVgza0pGOVZGUll0VHViS284SEJSZFVPdllFWGtMcU1NR0N0S0pNcnhyaSt4VQpCai9kMkRjeC84Y0VpTkZJdXUycnMzVEJBdDZsbmlpbmIrVFdjcU9KTDZLMDRjc0NBd0VBQWFOQ01FQXdEZ1lEClZSMFBBUUgvQkFRREFnRUdNQThHQTFVZEV3RUIvd1FGTUFNQkFmOHdIUVlEVlIwT0JCWUVGTnRpL0gzbW1mUlkKaU5PT1pPdDZCQ3RTSzNhU01BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQS9oam93aVlsWWdHU05adFozdFkrLwpCcCsrZmVSTGFBUTF0cTR3OTJPWlZIQ1JUdkxobkJGc21xQzh0MXZqT3JXVzJwK0txRnIrUkhKZzJVRWFKaHJECm5mTWVudWlqQ01WbXFDbnl5aEtOT0RtaHdNb3lEcXMzTFArd2pUNHhYcmhPUWg1V2hPemo4bjNielI0NnJCdEIKSFM5Q1BGa3pPY0swSDhSUEp5a2dzOXFMdkZIQk1SUEFlNjZhd0FFTXozZmppWmxNVWEzTFFjalV4K2diSzAwKwpSTEY3SHl6NlcxM0pmTDdiSTRzS2ZiQXpRL2E4Ri94UlBUWldZN1p6S3RxVTRVaWNGUlR6SHk0cU1wQTMxNC8vCnFsL1I0RTRBQ1M0RU1pUDRtYkFpMHZWYkY1TE5OMWxTSEJwZ3hMSlBCaDZVNGlZenBEL1k3czd1TTRBZWlmbDAKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    namespace: YXMtamNz
    token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJbXRwWkNJNkluSnZTM0kwUlROUVF5MVJObmxzY0hwM2QzUnhaRlZFYWtkUVZHOWphbXhTVFdOdmFHMXRhRXB5ZVZFaWZRLmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpoY3kxcVkzTWlMQ0pyZFdKbGNtNWxkR1Z6TG1sdkwzTmxjblpwWTJWaFkyTnZkVzUwTDNObFkzSmxkQzV1WVcxbElqb2laR1ZtWVhWc2RDMTBiMnRsYmkxbU9IYzNkaUlzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVnlkbWxqWlMxaFkyTnZkVzUwTG01aGJXVWlPaUprWldaaGRXeDBJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5elpYSjJhV05sTFdGalkyOTFiblF1ZFdsa0lqb2lNRGt3WkdZellqVXRNREV6TXkwME9UZGxMV0l4TlRRdFlUazNOVE0xWXpSbE1qTXpJaXdpYzNWaUlqb2ljM2x6ZEdWdE9uTmxjblpwWTJWaFkyTnZkVzUwT21GekxXcGpjenBrWldaaGRXeDBJbjAudURfaGZ0LVZZamdFamVraWJ1TWI5ZTBfaUZsOXVfT1RjZU9GamdKLThGTzBUUzNXQTQwYXBMTmZEMVBBMUVLSGpFdEZHQ1k0eHktLVd5ZmdZX0EtMHIxX1NUR3IwMXdkbGpCZW5QMW9ibkU4cEkwd0NtVjY0ZzREc1prSkJxWEZOQkE5YXpqNV9NdkNNanV5ZGRpUHpDdDZKYzY5eS1FUHkwa1M5d0FyTGU5QXkweFB1b0pKZkNzOGZzdmVuZWVnLU83UHpBU1dkYkhGU3JmN2UyN1RwOWhnRmZ4ZUhhT1c4UjhaaEUwUmEwaEx4UTZCV0xtdHFGRXZaU2d5UXRncXpHVXJtbjNDMjFGM2VaNWtwZEo4OUhjeXExeTNZWndrMXZXeWlXc3AxcWotVThGNmx0Zl9OZHU0YkNJank0YkF5a0QwRW1sTGJjRnpTM0psM1lTVGpn
  kind: Secret
  metadata:
    annotations:
      kubernetes.io/service-account.name: default
      kubernetes.io/service-account.uid: 090df3b5-0133-497e-b154-a97535c4e233
    creationTimestamp: "2021-11-29T23:57:15Z"
    name: default-token-f8w7v
    namespace: as-jcs
    resourceVersion: "666981851"
    selfLink: /api/v1/namespaces/as-jcs/secrets/default-token-f8w7v
    uid: 9f671145-c6c7-452b-869b-15ed85219698
  type: kubernetes.io/service-account-token
- apiVersion: v1
  data:
    .dockerconfigjson: ewoJImF1dGhzIjogewoJCSJpYWQub2Npci5pbyI6IHsKCQkJInVzZXJuYW1lIjogIm9yYWNsZW92YWRldi9vc3ZjaW50ZWdfb2NpciIsCgkJCSJwYXNzd29yZCI6ICJ0cFx1MDAzYysweFg5OFY7Vlx1MDAzZVJqdXNNcXMiLAoJCQkiZW1haWwiOiAib3JhY2xlX2Nsb3VkX2N4X2RldnRvb2xzX2dycEBvcmFjbGUuY29tIiwKCQkJImF1dGgiOiAiYjNKaFkyeGxiM1poWkdWMkwyOXpkbU5wYm5SbFoxOXZZMmx5T25Sd1BDc3dlRmc1T0ZZN1ZqNVNhblZ6VFhGeiIKCQl9Cgl9Cn0=
  kind: Secret
  metadata:
    annotations:
      meta.helm.sh/release-name: as-jcs
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2021-11-29T23:57:16Z"
    labels:
      app.kubernetes.io/managed-by: Helm
    name: ocirsecret
    namespace: as-jcs
    resourceVersion: "666981859"
    selfLink: /api/v1/namespaces/as-jcs/secrets/ocirsecret
    uid: 3a90fbe4-7a37-4f8a-ac33-50f3a1e59c3e
  type: kubernetes.io/dockerconfigjson
- apiVersion: v1
  data:
    .dockerconfigjson: eyJhdXRocyI6eyJpYWQub2Npci5pbyI6eyJ1c2VybmFtZSI6Im9zdmNjb3JwL2pvaG4uc3RpdGVzQG9yYWNsZS5jb20iLCJwYXNzd29yZCI6IngjNUp0O2VGTHIyXUpvWzJkWFhhIiwiZW1haWwiOiJqb2huLnN0aXRlc0BvcmFjbGUuY29tIiwiYXV0aCI6ImIzTjJZMk52Y25BdmFtOW9iaTV6ZEdsMFpYTkFiM0poWTJ4bExtTnZiVHA0SXpWS2REdGxSa3h5TWwxS2Ixc3laRmhZWVE9PSJ9fX0=
  kind: Secret
  metadata:
    creationTimestamp: "2021-12-02T19:17:54Z"
    name: osvc-token
    namespace: as-jcs
    resourceVersion: "672558096"
    selfLink: /api/v1/namespaces/as-jcs/secrets/osvc-token
    uid: 1a093e5b-1fa9-454d-8075-d127f75e258e
  type: kubernetes.io/dockerconfigjson
- apiVersion: v1
  data:
    .dockerconfigjson: ewoJImF1dGhzIjogewoJCSJpYWQub2Npci5pbyI6IHsKCQkJInVzZXJuYW1lIjogIm9zdmNzdGFnZS9vc3ZjY2RfdXNAb3JhY2xlLmNvbSIsCgkJCSJwYXNzd29yZCI6ICJ5XHUwMDNlVUtHU2s0dGNTUjAocStnOUxqIiwKCQkJImVtYWlsIjogIm9zdmNjZF91c0BvcmFjbGUuY29tIgoJCX0KCX0KfQ==
  kind: Secret
  metadata:
    annotations:
      meta.helm.sh/release-name: as-jcs
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2021-11-29T23:57:16Z"
    labels:
      app.kubernetes.io/managed-by: Helm
    name: osvcstage-ocirsecret
    namespace: as-jcs
    resourceVersion: "666981858"
    selfLink: /api/v1/namespaces/as-jcs/secrets/osvcstage-ocirsecret
    uid: cd44b1db-af3d-4b44-a3d5-8ded4f52fb0f
  type: kubernetes.io/dockerconfigjson
- apiVersion: v1
  data:
    ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM3akNDQXBTZ0F3SUJBZ0lSQU9nK0oyVk5SL1U2UHE0dnhBclNBTnd3Q2dZSUtvWkl6ajBFQXdJd2dia3gKQ3pBSkJnTlZCQVlUQWxWVE1Rc3dDUVlEVlFRSUV3SkRRVEVXTUJRR0ExVUVCeE1OVTJGdUlFWnlZVzVqYVhOagpiekVhTUJnR0ExVUVDUk1STVRBeElGTmxZMjl1WkNCVGRISmxaWFF4RGpBTUJnTlZCQkVUQlRrME1UQTFNUmN3CkZRWURWUVFLRXc1SVlYTm9hVU52Y25BZ1NXNWpMakZBTUQ0R0ExVUVBeE0zUTI5dWMzVnNJRUZuWlc1MElFTkIKSURNd09EY3dNell4TmpRMk56STJOVGt6TlRjeU5qRTBPVEF5TnprME5EazNOVEUzTVRnd05EQWVGdzB5TURFdwpNamt5TURRNE5UTmFGdzB5TlRFd01qZ3lNRFE0TlROYU1JRzVNUXN3Q1FZRFZRUUdFd0pWVXpFTE1Ba0dBMVVFCkNCTUNRMEV4RmpBVUJnTlZCQWNURFZOaGJpQkdjbUZ1WTJselkyOHhHakFZQmdOVkJBa1RFVEV3TVNCVFpXTnYKYm1RZ1UzUnlaV1YwTVE0d0RBWURWUVFSRXdVNU5ERXdOVEVYTUJVR0ExVUVDaE1PU0dGemFHbERiM0p3SUVsdQpZeTR4UURBK0JnTlZCQU1UTjBOdmJuTjFiQ0JCWjJWdWRDQkRRU0F6TURnM01ETTJNVFkwTmpjeU5qVTVNelUzCk1qWXhORGt3TWpjNU5EUTVOelV4TnpFNE1EUXdXVEFUQmdjcWhrak9QUUlCQmdncWhrak9QUU1CQndOQ0FBUVkKQ0lCVHliN3RPMFhnQW55VVBkWHdCRWRJOGRvcUZ3bE96bVdVSU1oYUNlMmdxektpQWduODJJTkNWbzRzM1NmSApOVGpQdEs1UlEySEQ3WFlPeDByS28zc3dlVEFPQmdOVkhROEJBZjhFQkFNQ0FZWXdEd1lEVlIwVEFRSC9CQVV3CkF3RUIvekFwQmdOVkhRNEVJZ1FnM1ZRK1dqUHpQZmxxRkI5eW1rdStFSGtWVmpyc0l5OCtyUjd5T3dXd3ZVY3cKS3dZRFZSMGpCQ1F3SW9BZzNWUStXalB6UGZscUZCOXlta3UrRUhrVlZqcnNJeTgrclI3eU93V3d2VWN3Q2dZSQpLb1pJemowRUF3SURTQUF3UlFJZ1pNcHB5UjZLcHhtbkMrSjJQSE5Pd1RWNWR3UnM3WjBKS2R1cGI0TFVTSUVDCklRQ0JpVzBUNEdJZE5nVHdMcUJUelRZN09KUHNFTEt2cjlFeFVoSXAybFJKMkE9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
  kind: Secret
  metadata:
    annotations:
      meta.helm.sh/release-name: as-jcs
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2021-11-29T23:57:16Z"
    labels:
      app.kubernetes.io/managed-by: Helm
    name: vault-tls
    namespace: as-jcs
    resourceVersion: "666981860"
    selfLink: /api/v1/namespaces/as-jcs/secrets/vault-tls
    uid: 1b1fdafd-d648-4b52-8428-e2db50fbab75
  type: Opaque
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
[20:January:2022:05:30:39]:(corp_us-ashburn-1_dataplane):~
○ $
[20:January:2022:05:35:04]:(corp_us-ashburn-1_dataplane):~
○ $ kubectl cluster-info
Kubernetes control plane is running at https://130.35.146.120:6443
CoreDNS is running at https://130.35.146.120:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
[20:January:2022:05:35:11]:(corp_us-ashburn-1_dataplane):~
○ $ kubectl get componentstatus
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {"health":"true"}
[20:January:2022:05:35:30]:(corp_us-ashburn-1_dataplane):~
○ $ kubectl get events as-jcs
Error from server (NotFound): events "as-jcs" not found
[20:January:2022:05:36:07]:(corp_us-ashburn-1_dataplane):~
○ $ kubectl get events -n as-jcs
LAST SEEN   TYPE      REASON              OBJECT                                             MESSAGE
52m         Warning   FailedCreate        replicaset/echo-6677859554                         Error creating: pods "echo-6677859554-gfkkr" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
52m         Warning   FailedCreate        replicaset/echo-6677859554                         Error creating: pods "echo-6677859554-jf4dd" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
52m         Warning   FailedCreate        replicaset/echo-6677859554                         Error creating: pods "echo-6677859554-g6nln" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
52m         Warning   FailedCreate        replicaset/echo-6677859554                         Error creating: pods "echo-6677859554-6c7sb" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
52m         Warning   FailedCreate        replicaset/echo-6677859554                         Error creating: pods "echo-6677859554-xnrnr" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
52m         Warning   FailedCreate        replicaset/echo-6677859554                         Error creating: pods "echo-6677859554-7kl9j" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
52m         Warning   FailedCreate        replicaset/echo-6677859554                         Error creating: pods "echo-6677859554-sdddt" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
52m         Warning   FailedCreate        replicaset/echo-6677859554                         Error creating: pods "echo-6677859554-l8bhb" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
52m         Warning   FailedCreate        replicaset/echo-6677859554                         Error creating: pods "echo-6677859554-jhz4n" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
41m         Warning   FailedCreate        replicaset/echo-6677859554                         (combined from similar events): Error creating: pods "echo-6677859554-v88rn" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
30m         Warning   FailedCreate        replicaset/echo-6677859554                         Error creating: pods "echo-6677859554-l85mh" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
13m         Warning   FailedCreate        replicaset/echo-6677859554                         Error creating: pods "echo-6677859554-wqthn" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
49m         Normal    Scheduled           pod/echo-769c9d8c54-tjvkp                          Successfully assigned as-jcs/echo-769c9d8c54-tjvkp to 10.11.8.46
4m9s        Warning   ErrImageNeverPull   pod/echo-769c9d8c54-tjvkp                          Container image "echo:latest" is not present with pull policy of Never
47m         Warning   Failed              pod/echo-769c9d8c54-tjvkp                          Error: ErrImageNeverPull
49m         Normal    SuccessfulCreate    replicaset/echo-769c9d8c54                         Created pod: echo-769c9d8c54-tjvkp
52m         Normal    ScalingReplicaSet   deployment/echo                                    Scaled up replica set echo-6677859554 to 1
49m         Normal    ScalingReplicaSet   deployment/echo                                    Scaled up replica set echo-769c9d8c54 to 1
3m23s       Warning   FailedMount         pod/who-internal-who-deployment-869d6dd88d-kt955   MountVolume.SetUp failed for volume "who-config" : configmap "who-internal-config" not found
[20:January:2022:05:36:18]:(corp_us-ashburn-1_dataplane):~
○ $ kgpn as-jcs
NAME                                           READY   STATUS              RESTARTS   AGE    IP              NODE          NOMINATED NODE   READINESS GATES
echo-769c9d8c54-tjvkp                          0/1     ErrImageNeverPull   0          50m    10.245.11.112   10.11.8.46    <none>           <none>
who-internal-who-deployment-869d6dd88d-kt955   2/2     Running             317        5d6h   10.245.18.30    10.11.9.160   <none>           <none>
[20:January:2022:05:37:28]:(corp_us-ashburn-1_dataplane):~
○ $ hln as-jcs
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
NAME  NAMESPACE REVISION  UPDATED STATUS  CHART APP VERSION
[20:January:2022:05:37:51]:(corp_us-ashburn-1_dataplane):~
○ $
[20:January:2022:07:06:50]:(corp_us-ashburn-1_dataplane):~
○ $ kgpn as-jcs
NAME                                           READY   STATUS              RESTARTS   AGE    IP              NODE          NOMINATED NODE   READINESS GATES
echo-769c9d8c54-tjvkp                          0/1     ErrImageNeverPull   0          175m   10.245.11.112   10.11.8.46    <none>           <none>
who-internal-who-deployment-869d6dd88d-kt955   2/2     Running             317        5d8h   10.245.18.30    10.11.9.160   <none>           <none>
[20:January:2022:07:42:09]:(corp_us-ashburn-1_dataplane):~
○ $ kgn as-jcs events
LAST SEEN   TYPE      REASON              OBJECT                                             MESSAGE
56m         Warning   FailedCreate        replicaset/echo-6677859554                         Error creating: pods "echo-6677859554-z282k" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
39m         Warning   FailedCreate        replicaset/echo-6677859554                         Error creating: pods "echo-6677859554-kscp4" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
23m         Warning   FailedCreate        replicaset/echo-6677859554                         Error creating: pods "echo-6677859554-nzqnv" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
6m30s       Warning   FailedCreate        replicaset/echo-6677859554                         Error creating: pods "echo-6677859554-kwlj8" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
25s         Warning   ErrImageNeverPull   pod/echo-769c9d8c54-tjvkp                          Container image "echo:latest" is not present with pull policy of Never
50m         Warning   Failed              pod/echo-769c9d8c54-tjvkp                          Error: ErrImageNeverPull
5m28s       Warning   FailedMount         pod/who-internal-who-deployment-869d6dd88d-kt955   MountVolume.SetUp failed for volume "who-config" : configmap "who-internal-config" not found
[20:January:2022:07:42:27]:(corp_us-ashburn-1_dataplane):~
○ $ kgn as-jcs replicaset
NAME                                     DESIRED   CURRENT   READY   AGE
echo-6677859554                          1         0         0       178m
echo-769c9d8c54                          1         1         0       175m
who-internal-who-deployment-869d6dd88d   1         1         1       5d8h
[20:January:2022:07:42:45]:(corp_us-ashburn-1_dataplane):~
○ $ kgpn as-jcs -o wide
NAME                                           READY   STATUS              RESTARTS   AGE    IP              NODE          NOMINATED NODE   READINESS GATES
echo-769c9d8c54-tjvkp                          0/1     ErrImageNeverPull   0          176m   10.245.11.112   10.11.8.46    <none>           <none>
who-internal-who-deployment-869d6dd88d-kt955   2/2     Running             317        5d8h   10.245.18.30    10.11.9.160   <none>           <none>
[20:January:2022:07:43:05]:(corp_us-ashburn-1_dataplane):~
○ $ kgno
NAME          STATUS                     ROLES   AGE     VERSION
10.11.8.110   Ready,SchedulingDisabled   node    63d     v1.19.12
10.11.8.115   Ready                      node    63d     v1.19.12
10.11.8.138   Ready                      node    63d     v1.19.12
10.11.8.139   Ready                      node    63d     v1.19.12
10.11.8.142   Ready                      node    63d     v1.19.12
10.11.8.148   Ready                      node    63d     v1.19.12
10.11.8.160   Ready                      node    63d     v1.19.12
10.11.8.172   Ready                      node    63d     v1.19.12
10.11.8.174   Ready                      node    63d     v1.19.12
10.11.8.194   Ready                      node    63d     v1.19.12
10.11.8.203   Ready                      node    63d     v1.19.12
10.11.8.222   Ready                      node    63d     v1.19.12
10.11.8.225   Ready                      node    63d     v1.19.12
10.11.8.229   Ready                      node    63d     v1.19.12
10.11.8.232   Ready                      node    63d     v1.19.12
10.11.8.250   Ready                      node    63d     v1.19.12
10.11.8.31    Ready                      node    7d13h   v1.19.12
10.11.8.39    Ready                      node    63d     v1.19.12
10.11.8.46    Ready                      node    63d     v1.19.12
10.11.8.48    Ready                      node    7d16h   v1.19.12
10.11.8.60    Ready                      node    7d13h   v1.19.12
10.11.8.67    Ready                      node    63d     v1.19.12
10.11.8.68    Ready                      node    63d     v1.19.12
10.11.8.71    Ready                      node    63d     v1.19.12
10.11.8.72    Ready                      node    63d     v1.19.12
10.11.8.76    Ready                      node    63d     v1.19.12
10.11.8.80    Ready                      node    63d     v1.19.12
10.11.8.82    Ready                      node    63d     v1.19.12
10.11.8.94    Ready                      node    63d     v1.19.12
10.11.9.1     Ready                      node    63d     v1.19.12
10.11.9.104   Ready                      node    63d     v1.19.12
10.11.9.113   Ready                      node    7m26s   v1.19.12
10.11.9.124   Ready                      node    63d     v1.19.12
10.11.9.127   Ready                      node    63d     v1.19.12
10.11.9.13    Ready                      node    63d     v1.19.12
10.11.9.135   Ready                      node    63d     v1.19.12
10.11.9.136   Ready                      node    63d     v1.19.12
10.11.9.14    Ready                      node    63d     v1.19.12
10.11.9.141   Ready                      node    63d     v1.19.12
10.11.9.145   Ready                      node    63d     v1.19.12
10.11.9.157   Ready                      node    63d     v1.19.12
10.11.9.16    Ready                      node    63d     v1.19.12
10.11.9.160   Ready                      node    63d     v1.19.12
10.11.9.182   Ready                      node    63d     v1.19.12
10.11.9.195   Ready                      node    23m     v1.19.12
10.11.9.208   Ready                      node    63d     v1.19.12
10.11.9.23    Ready                      node    63d     v1.19.12
10.11.9.238   Ready                      node    63d     v1.19.12
10.11.9.37    Ready                      node    63d     v1.19.12
10.11.9.49    Ready                      node    63d     v1.19.12
10.11.9.56    Ready                      node    7d13h   v1.19.12
10.11.9.58    Ready                      node    7d13h   v1.19.12
10.11.9.62    Ready                      node    7d13h   v1.19.12
10.11.9.78    Ready                      node    63d     v1.19.12
10.11.9.84    Ready                      node    63d     v1.19.12
10.11.9.9     Ready                      node    63d     v1.19.12
10.11.9.91    Ready                      node    63d     v1.19.12
10.11.9.92    Ready                      node    63d     v1.19.12
10.11.9.93    Ready                      node    63d     v1.19.12
10.11.9.95    Ready                      node    63d     v1.19.12
[20:January:2022:07:43:15]:(corp_us-ashburn-1_dataplane):~
○ $ kgpn as-ciat
NAME                                           READY   STATUS    RESTARTS   AGE     IP              NODE          NOMINATED NODE   READINESS GATES
lorax-analyticsmanager-75d985dfdf-6jmjm        2/2     Running   0          7h45m   10.245.14.74    10.11.9.104   <none>           <none>
lorax-analyticsmanager-75d985dfdf-mmxrw        2/2     Running   0          7h45m   10.245.13.192   10.11.9.145   <none>           <none>
lorax-analyticsmanager-75d985dfdf-zfr7b        2/2     Running   0          7h45m   10.245.20.140   10.11.8.225   <none>           <none>
lorax-cachehost-647b6cf7b4-4v6n7               2/2     Running   0          6h14m   10.245.13.197   10.11.9.145   <none>           <none>
lorax-cachehost-647b6cf7b4-jsqlv               2/2     Running   0          6h14m   10.245.27.178   10.11.8.60    <none>           <none>
lorax-cachehost-6c8948bf68-8gmdf               2/2     Running   0          7h45m   10.245.19.21    10.11.9.238   <none>           <none>
lorax-cachehost-6c8948bf68-mzvx7               2/2     Running   0          7h45m   10.245.26.8     10.11.8.67    <none>           <none>
lorax-cachehost-6c8948bf68-vf26t               2/2     Running   0          7h45m   10.245.15.225   10.11.9.78    <none>           <none>
lorax-commandmanager-586d58947d-2h89f          2/2     Running   0          7h45m   10.245.28.203   10.11.9.56    <none>           <none>
lorax-commandmanager-586d58947d-2rcmp          2/2     Running   0          7h45m   10.245.11.175   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-c4482          2/2     Running   0          7h45m   10.245.27.176   10.11.8.60    <none>           <none>
lorax-contextmanager-67956c7584-c4xkw          2/2     Running   0          7h45m   10.245.11.177   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-h82q5          2/2     Running   0          7h45m   10.245.3.92     10.11.8.232   <none>           <none>
lorax-customizationmanager-657bbf9586-h242p    2/2     Running   0          7h45m   10.245.16.201   10.11.8.203   <none>           <none>
lorax-customizationmanager-657bbf9586-rsbqg    2/2     Running   0          7h45m   10.245.29.214   10.11.9.62    <none>           <none>
lorax-elementmanager-7f74646f4d-n4cqx          2/2     Running   0          7h45m   10.245.28.39    10.11.8.31    <none>           <none>
lorax-eventmanager-759c5b9484-ch28f            2/2     Running   0          7h45m   10.245.15.226   10.11.9.78    <none>           <none>
lorax-eventworker-7c5b779bc-z4bb5              2/2     Running   0          7h45m   10.245.22.102   10.11.8.250   <none>           <none>
lorax-queuemanager-769996b97b-z4kqc            2/2     Running   0          7h45m   10.245.16.24    10.11.8.222   <none>           <none>
lorax-realtimedatamanager-866cd5886b-qq54f     2/2     Running   0          7h45m   10.245.29.213   10.11.9.62    <none>           <none>
lorax-reportdesignmanager-5c88cb777d-sxlx9     2/2     Running   0          7h45m   10.245.20.94    10.11.8.160   <none>           <none>
lorax-reportpublishmanager-58445646c4-9ztc7    2/2     Running   0          7h45m   10.245.11.176   10.11.9.208   <none>           <none>
lorax-searchmanager-7d97d6dfcc-gn4zg           2/2     Running   0          7h45m   10.245.29.212   10.11.9.62    <none>           <none>
lorax-searchmanager-7d97d6dfcc-q2x5t           2/2     Running   0          7h45m   10.245.14.73    10.11.9.104   <none>           <none>
lorax-securityservice-b9bb8695d-t7bz5          2/2     Running   0          7h45m   10.245.12.189   10.11.8.194   <none>           <none>
who-internal-who-deployment-65d44d6f4c-fqhbs   2/2     Running   0          7h44m   10.245.25.112   10.11.8.142   <none>           <none>
who-internal-who-deployment-65d44d6f4c-w8lnq   2/2     Running   0          7h44m   10.245.11.179   10.11.9.208   <none>           <none>
who-internal-who-deployment-65d44d6f4c-xljbh   2/2     Running   0          7h45m   10.245.16.199   10.11.8.203   <none>           <none>
who-who-deployment-576cd44f87-4dr8c            2/2     Running   0          7h44m   10.245.9.42     10.11.9.136   <none>           <none>
who-who-deployment-576cd44f87-lhb49            2/2     Running   0          7h45m   10.245.16.200   10.11.8.203   <none>           <none>
who-who-deployment-576cd44f87-lkhrc            2/2     Running   0          7h43m   10.245.23.136   10.11.8.174   <none>           <none>
[20:January:2022:07:44:07]:(corp_us-ashburn-1_dataplane):~
○ $ kgn as-ciat replicaset
NAME                                     DESIRED   CURRENT   READY   AGE
horton-horton-deployment-575597484d      3         0         0       3h46m
horton-horton-deployment-5cb5d5847b      3         0         0       46m
horton-horton-deployment-75bf7f55c4      3         0         0       5h16m
horton-horton-deployment-7666bdd65b      3         0         0       4h46m
horton-horton-deployment-77dc74fcf6      3         0         0       106m
horton-horton-deployment-7956dcf558      3         0         0       135m
horton-horton-deployment-7b44565c7d      3         0         0       166m
horton-horton-deployment-7fdd67cb74      3         0         0       76m
horton-horton-deployment-8557865c96      3         0         0       16m
horton-horton-deployment-859d67c6dd      3         0         0       6h15m
horton-horton-deployment-b4cc54648       3         0         0       4h16m
horton-horton-deployment-c94fb56d4       3         0         0       3h16m
horton-horton-deployment-cf96b5f96       3         0         0       5h46m
lorax-analyticsmanager-54bc6d977         3         0         0       106m
lorax-analyticsmanager-54cddc4b7c        3         0         0       136m
lorax-analyticsmanager-574b54fdcc        3         0         0       3h16m
lorax-analyticsmanager-5845b594f7        3         0         0       3h46m
lorax-analyticsmanager-6467b86457        3         0         0       165m
lorax-analyticsmanager-649b9ff5c         3         0         0       76m
lorax-analyticsmanager-6b84676cfc        3         0         0       6h15m
lorax-analyticsmanager-6bf55f567c        3         0         0       4h16m
lorax-analyticsmanager-6c8f878f7         3         0         0       4h46m
lorax-analyticsmanager-7464749dd9        3         0         0       15m
lorax-analyticsmanager-75c549d849        3         0         0       5h16m
lorax-analyticsmanager-75d985dfdf        3         3         3       7h46m
lorax-analyticsmanager-7fb55ff7fc        3         0         0       46m
lorax-analyticsmanager-f6bbcd5fc         3         0         0       5h46m
lorax-cachehost-5656bcff96               3         0         0       76m
lorax-cachehost-5c5455dcd4               3         0         0       3h16m
lorax-cachehost-647b6cf7b4               3         2         2       6h15m
lorax-cachehost-6b99bd7cd4               3         0         0       136m
lorax-cachehost-6c8948bf68               3         3         3       7h46m
lorax-cachehost-6dd4cd6fc6               3         0         0       15m
lorax-cachehost-6fcc6d6bf6               3         0         0       46m
lorax-cachehost-7456cd54dd               3         0         0       5h46m
lorax-cachehost-7566cbbcb                3         0         0       4h46m
lorax-cachehost-7cbc6dd864               3         0         0       4h16m
lorax-cachehost-7cd66b4dbd               3         0         0       165m
lorax-cachehost-7f5b5c8844               3         0         0       3h46m
lorax-cachehost-85457b4f7b               3         0         0       106m
lorax-cachehost-99b986d84                3         0         0       5h16m
lorax-commandmanager-5449fc7644          2         0         0       76m
lorax-commandmanager-54784c4744          2         0         0       5h16m
lorax-commandmanager-55557f49f4          2         0         0       4h46m
lorax-commandmanager-56b5bb78c8          2         0         0       3h16m
lorax-commandmanager-586d58947d          2         2         2       7h46m
lorax-commandmanager-5db8948f7d          2         0         0       106m
lorax-commandmanager-5ddf8f65c4          2         0         0       46m
lorax-commandmanager-5f49868c58          2         0         0       6h15m
lorax-commandmanager-764fc878fb          2         0         0       136m
lorax-commandmanager-79c8bb6b6           2         0         0       15m
lorax-commandmanager-7ff6dd5f66          2         0         0       5h46m
lorax-commandmanager-84c6d99f8           2         0         0       4h16m
lorax-commandmanager-8645779974          2         0         0       3h46m
lorax-commandmanager-d87887f76           2         0         0       165m
lorax-contextmanager-5bb7cf754           3         0         0       165m
lorax-contextmanager-5d6b544f46          3         0         0       5h16m
lorax-contextmanager-676d9f4b96          3         0         0       6h15m
lorax-contextmanager-67956c7584          3         3         3       7h46m
lorax-contextmanager-6b847c88b           3         0         0       76m
lorax-contextmanager-6c58b5fbcb          3         0         0       3h16m
lorax-contextmanager-6d6d7594bd          3         0         0       3h46m
lorax-contextmanager-6dc9f7cf84          3         0         0       106m
lorax-contextmanager-6fb65d9f4d          3         0         0       5h46m
lorax-contextmanager-754fc85d44          3         0         0       4h16m
lorax-contextmanager-7d6c885588          3         0         0       15m
lorax-contextmanager-d5dd69566           3         0         0       136m
lorax-contextmanager-f747f598d           3         0         0       4h46m
lorax-contextmanager-f7dd6d88d           3         0         0       46m
lorax-customizationmanager-54bd59d66     2         0         0       3h16m
lorax-customizationmanager-6486f894d4    2         0         0       165m
lorax-customizationmanager-657bbf9586    2         2         2       7h46m
lorax-customizationmanager-6cb5947b6     2         0         0       3h46m
lorax-customizationmanager-7566984c4     2         0         0       15m
lorax-customizationmanager-76c587676d    2         0         0       5h16m
lorax-customizationmanager-7b5c9fd99d    2         0         0       4h16m
lorax-customizationmanager-7ccf74fbd4    2         0         0       136m
lorax-customizationmanager-8474f464b6    2         0         0       46m
lorax-customizationmanager-84f9b9f66d    2         0         0       76m
lorax-customizationmanager-86b9c84b5d    2         0         0       5h46m
lorax-customizationmanager-8788f7f66     2         0         0       6h15m
lorax-customizationmanager-bb8d7f986     2         0         0       4h46m
lorax-customizationmanager-cfbf8558      2         0         0       106m
lorax-elementmanager-5b6bfc5664          1         0         0       106m
lorax-elementmanager-6dd86c7dcb          1         0         0       6h15m
lorax-elementmanager-765f8845c8          1         0         0       165m
lorax-elementmanager-777b868f84          1         0         0       15m
lorax-elementmanager-79cfbcf654          1         0         0       136m
lorax-elementmanager-7c4fc5c74d          1         0         0       3h46m
lorax-elementmanager-7f44848cfb          1         0         0       4h46m
lorax-elementmanager-7f74646f4d          1         1         1       7h46m
lorax-elementmanager-84bbf749bd          1         0         0       46m
lorax-elementmanager-85575658b8          1         0         0       4h16m
lorax-elementmanager-868c7dbb56          1         0         0       5h46m
lorax-elementmanager-b65cb6dc4           1         0         0       76m
lorax-elementmanager-cd8895564           1         0         0       3h16m
lorax-elementmanager-df8dc55db           1         0         0       5h16m
lorax-eventmanager-5b59b586fb            2         0         0       106m
lorax-eventmanager-679fd7fdd6            2         0         0       136m
lorax-eventmanager-759c5b9484            2         1         1       7h46m
lorax-eventmanager-7755588b84            2         0         0       5h46m
lorax-eventmanager-795bb876b8            2         0         0       3h16m
lorax-eventmanager-79849f6bb             2         0         0       165m
lorax-eventmanager-7c8946c76d            2         0         0       4h46m
lorax-eventmanager-7fd6c57c6d            2         0         0       15m
lorax-eventmanager-968787676             2         0         0       4h16m
lorax-eventmanager-9ff868d               2         0         0       6h15m
lorax-eventmanager-b895c6d44             2         0         0       46m
lorax-eventmanager-bcb667494             2         0         0       76m
lorax-eventmanager-cb6f8df96             2         0         0       3h46m
lorax-eventmanager-d7c568f86             2         0         0       5h16m
lorax-eventworker-5b788d465f             1         0         0       15m
lorax-eventworker-5dfcc9c6bf             1         0         0       76m
lorax-eventworker-6876c65579             1         0         0       4h16m
lorax-eventworker-68c5d5f699             1         0         0       46m
lorax-eventworker-69db9b44b7             1         0         0       6h15m
lorax-eventworker-766699cdf              1         0         0       165m
lorax-eventworker-78bdc448bf             1         0         0       5h16m
lorax-eventworker-79fbcf4895             1         0         0       3h16m
lorax-eventworker-7c5b779bc              1         1         1       7h46m
lorax-eventworker-84454db699             1         0         0       106m
lorax-eventworker-8488fb5499             1         0         0       3h46m
lorax-eventworker-86d9655577             1         0         0       136m
lorax-eventworker-8cc65fd4c              1         0         0       4h46m
lorax-eventworker-d68455667              1         0         0       5h46m
lorax-queuemanager-5644b9fcb4            1         0         0       4h46m
lorax-queuemanager-58bf767f6d            1         0         0       3h16m
lorax-queuemanager-59b6d8b6c8            1         0         0       76m
lorax-queuemanager-59d7d8fc4d            1         0         0       6h15m
lorax-queuemanager-5cd5c79d56            1         0         0       106m
lorax-queuemanager-5f4fdf898d            1         0         0       3h46m
lorax-queuemanager-66f97c9d58            1         0         0       165m
lorax-queuemanager-6f45494dc4            1         0         0       136m
lorax-queuemanager-769996b97b            1         1         1       7h46m
lorax-queuemanager-77b4fb                1         0         0       15m
lorax-queuemanager-79c777b444            1         0         0       4h16m
lorax-queuemanager-bb49dc7c6             1         0         0       5h46m
lorax-queuemanager-cb775f484             1         0         0       46m
lorax-queuemanager-fc6fbbc78             1         0         0       5h16m
lorax-realtimedatamanager-59b9cf5654     1         0         0       5h16m
lorax-realtimedatamanager-5cf45d597d     1         0         0       46m
lorax-realtimedatamanager-5db6b5974      1         0         0       15m
lorax-realtimedatamanager-658bd645f4     1         0         0       5h46m
lorax-realtimedatamanager-65c955c86      1         0         0       165m
lorax-realtimedatamanager-697dccddc4     1         0         0       106m
lorax-realtimedatamanager-6fd8c8599d     1         0         0       136m
lorax-realtimedatamanager-745fcbf486     1         0         0       4h46m
lorax-realtimedatamanager-7567b876b6     1         0         0       6h15m
lorax-realtimedatamanager-7c7c8775c4     1         0         0       3h16m
lorax-realtimedatamanager-7cd4d9f8b      1         0         0       76m
lorax-realtimedatamanager-7f67c866b      1         0         0       4h16m
lorax-realtimedatamanager-866cd5886b     1         1         1       7h46m
lorax-realtimedatamanager-86d957ccc8     1         0         0       3h46m
lorax-reportdesignmanager-558b94946b     1         0         0       4h46m
lorax-reportdesignmanager-56465c769d     1         0         0       5h46m
lorax-reportdesignmanager-5c88cb777d     1         1         1       7h46m
lorax-reportdesignmanager-5f7d7cd974     1         0         0       6h15m
lorax-reportdesignmanager-669965954b     1         0         0       3h16m
lorax-reportdesignmanager-6744b65cc4     1         0         0       3h46m
lorax-reportdesignmanager-69b5f89b58     1         0         0       46m
lorax-reportdesignmanager-6c874c8bd6     1         0         0       106m
lorax-reportdesignmanager-6cd9df9b54     1         0         0       5h16m
lorax-reportdesignmanager-746cb6cc46     1         0         0       76m
lorax-reportdesignmanager-7668f54b       1         0         0       136m
lorax-reportdesignmanager-7998f7595d     1         0         0       15m
lorax-reportdesignmanager-7d67f9fb98     1         0         0       4h16m
lorax-reportdesignmanager-848c78786      1         0         0       165m
lorax-reportpublishmanager-58445646c4    1         1         1       7h46m
lorax-reportpublishmanager-6779d8757d    1         0         0       3h16m
lorax-reportpublishmanager-6867c97d86    1         0         0       4h16m
lorax-reportpublishmanager-68d67bc598    1         0         0       106m
lorax-reportpublishmanager-6b5b9cc74     1         0         0       46m
lorax-reportpublishmanager-6d88fc458b    1         0         0       3h46m
lorax-reportpublishmanager-6f8dd75d64    1         0         0       5h16m
lorax-reportpublishmanager-6ff7947fc4    1         0         0       6h15m
lorax-reportpublishmanager-79bc79d75d    1         0         0       4h46m
lorax-reportpublishmanager-79c6c74b4d    1         0         0       136m
lorax-reportpublishmanager-7cb6b6f458    1         0         0       15m
lorax-reportpublishmanager-85fcb8558b    1         0         0       5h46m
lorax-reportpublishmanager-ddcc6b64      1         0         0       165m
lorax-reportpublishmanager-f75747b5b     1         0         0       76m
lorax-searchmanager-5588bf669c           2         0         0       4h46m
lorax-searchmanager-57f9fc4cb5           2         0         0       76m
lorax-searchmanager-57fd4db8f            2         0         0       46m
lorax-searchmanager-595f549689           2         0         0       165m
lorax-searchmanager-5c4d488449           2         0         0       6h15m
lorax-searchmanager-5cbd688685           2         0         0       15m
lorax-searchmanager-5d75f85465           2         0         0       5h46m
lorax-searchmanager-5f95bf5bf            2         0         0       5h16m
lorax-searchmanager-79c6b8578f           2         0         0       106m
lorax-searchmanager-7d88df64bf           2         0         0       136m
lorax-searchmanager-7d97d6dfcc           2         2         2       7h46m
lorax-searchmanager-8475dfbb49           2         0         0       3h46m
lorax-searchmanager-cf6668b77            2         0         0       3h16m
lorax-searchmanager-f76cdcfcc            2         0         0       4h16m
lorax-securityservice-55c498b866         2         0         0       6h15m
lorax-securityservice-5b9b48c4           2         0         0       4h46m
lorax-securityservice-5d76cfb66          2         0         0       5h46m
lorax-securityservice-64757d4f7d         2         0         0       4h16m
lorax-securityservice-66958cb778         2         0         0       15m
lorax-securityservice-66b78457cd         2         0         0       5h16m
lorax-securityservice-67c885b654         2         0         0       3h16m
lorax-securityservice-695b757654         2         0         0       165m
lorax-securityservice-6b65d9847d         2         0         0       3h46m
lorax-securityservice-7798cf45fd         2         0         0       106m
lorax-securityservice-78487b8f94         2         0         0       136m
lorax-securityservice-86b7485fb6         2         0         0       76m
lorax-securityservice-b9bb8695d          2         1         1       7h46m
lorax-securityservice-c87b5d4c8          2         0         0       46m
who-internal-who-deployment-5844c7df47   0         0         0       106m
who-internal-who-deployment-65d44d6f4c   3         3         3       7h46m
who-internal-who-deployment-66d59bc96f   0         0         0       12h
who-internal-who-deployment-679bbc7c47   0         0         0       4h46m
who-internal-who-deployment-67d6d8c44b   0         0         0       75m
who-internal-who-deployment-69f7fb4dfb   0         0         0       136m
who-internal-who-deployment-777f4b78f4   0         0         0       10h
who-internal-who-deployment-788ffcc766   0         0         0       165m
who-internal-who-deployment-7bcbd746c    0         0         0       6h15m
who-internal-who-deployment-7bd5c447d9   0         0         0       46m
who-internal-who-deployment-7cb74474d7   1         0         0       16m
who-internal-who-deployment-7cdfb8c485   0         0         0       3h16m
who-internal-who-deployment-7fffbcf677   0         0         0       4h16m
who-internal-who-deployment-85586746c8   0         0         0       5h16m
who-internal-who-deployment-8575746b4c   0         0         0       5h46m
who-internal-who-deployment-8b79bbcb7    0         0         0       9h
who-internal-who-deployment-c79b6986c    0         0         0       3h46m
who-who-deployment-576cd44f87            3         3         3       7h46m
who-who-deployment-58b5b58c95            0         0         0       5h46m
who-who-deployment-5bc7c8bc9c            0         0         0       3h16m
who-who-deployment-6d5c44b48c            0         0         0       9h
who-who-deployment-6d98d96f8d            0         0         0       166m
who-who-deployment-6dc744bbc             0         0         0       3h46m
who-who-deployment-7458567d9f            0         0         0       4h46m
who-who-deployment-754d8d67b9            0         0         0       4h16m
who-who-deployment-795cbbd4d4            0         0         0       10h
who-who-deployment-7b787769cb            0         0         0       5h16m
who-who-deployment-7f57b5cd59            0         0         0       106m
who-who-deployment-7ff986854f            0         0         0       46m
who-who-deployment-84f4b97998            0         0         0       136m
who-who-deployment-c754dcc9              1         0         0       16m
who-who-deployment-d5cdfcf59             0         0         0       6h15m
who-who-deployment-d7b89dcb              0         0         0       76m
who-who-deployment-dd789f457             0         0         0       12h
[20:January:2022:07:44:58]:(corp_us-ashburn-1_dataplane):~
○ $ kgn as-ciat events
LAST SEEN   TYPE     REASON   OBJECT                                                  MESSAGE
3m56s       Normal   Info     service/as-ciat-agentbrowserui-20220120-06563138-zxk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-agentbrowserui-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111040
    uid: b750f47f-102f-4739-9689-4c9db95c28cb
}: Deleted service monitor
3m56s       Normal   Info     service/as-ciat-agentbrowserui-20220120-06563138-zxk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-agentbrowserui-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111040
    uid: b750f47f-102f-4739-9689-4c9db95c28cb
}: Received delete service event
16m         Normal   Info     service/as-ciat-agentbrowserui-20220120-06563138-zxk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-agentbrowserui-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065563
    uid: b750f47f-102f-4739-9689-4c9db95c28cb
}: Received add service event
16m         Normal   Info     service/as-ciat-agentbrowserui-20220120-06563138-zxk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-agentbrowserui-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065563
    uid: b750f47f-102f-4739-9689-4c9db95c28cb
}: Created service monitor
3m55s       Normal   Info     service/as-ciat-agentbrowserui-20220120-07263146-unk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-agentbrowserui-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770114731
    uid: 512b8364-8318-47c3-a5e4-70065810cd62
}: Received add service event
3m55s       Normal   Info     service/as-ciat-agentbrowserui-20220120-07263146-unk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-agentbrowserui-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770114731
    uid: 512b8364-8318-47c3-a5e4-70065810cd62
}: Created service monitor
3m56s       Normal   Info     service/as-ciat-analyticsmanager-20220120-06563138-zxk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-analyticsmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111069
    uid: 2ee613cc-06db-4fc0-ac6d-c2c3ef9d04f2
}: Deleted service monitor
3m56s       Normal   Info     service/as-ciat-analyticsmanager-20220120-06563138-zxk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-analyticsmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111069
    uid: 2ee613cc-06db-4fc0-ac6d-c2c3ef9d04f2
}: Received delete service event
10m         Normal   Info     service/as-ciat-analyticsmanager-20220120-06563138-zxk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-analyticsmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065284
    uid: 2ee613cc-06db-4fc0-ac6d-c2c3ef9d04f2
}: Created service monitor
10m         Normal   Info     service/as-ciat-analyticsmanager-20220120-06563138-zxk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-analyticsmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065284
    uid: 2ee613cc-06db-4fc0-ac6d-c2c3ef9d04f2
}: Received add service event
3m55s       Normal   Info     service/as-ciat-analyticsmanager-20220120-07263146-unk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-analyticsmanager-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115643
    uid: e437812c-ecbe-4df1-9c05-038f4501ba4d
}: Received add service event
3m55s       Normal   Info     service/as-ciat-analyticsmanager-20220120-07263146-unk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-analyticsmanager-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115643
    uid: e437812c-ecbe-4df1-9c05-038f4501ba4d
}: Created service monitor
3m56s       Normal   Info     service/as-ciat-cachehost-20220120-06563138-zxk4          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-cachehost-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111067
    uid: c49a4d7b-d8d0-4cbf-885a-de0dda633093
}: Received delete service event
17m         Normal   Info     service/as-ciat-cachehost-20220120-06563138-zxk4          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-cachehost-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065315
    uid: c49a4d7b-d8d0-4cbf-885a-de0dda633093
}: Received add service event
3m56s       Normal   Info     service/as-ciat-cachehost-20220120-06563138-zxk4          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-cachehost-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111067
    uid: c49a4d7b-d8d0-4cbf-885a-de0dda633093
}: Deleted service monitor
17m         Normal   Info     service/as-ciat-cachehost-20220120-06563138-zxk4          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-cachehost-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065315
    uid: c49a4d7b-d8d0-4cbf-885a-de0dda633093
}: Created service monitor
3m55s       Normal   Info     service/as-ciat-cachehost-20220120-07263146-unk4          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-cachehost-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115651
    uid: 1c4a3e88-9b58-4a9a-b5f0-53ff6e4a1b9a
}: Created service monitor
3m55s       Normal   Info     service/as-ciat-cachehost-20220120-07263146-unk4          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-cachehost-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115651
    uid: 1c4a3e88-9b58-4a9a-b5f0-53ff6e4a1b9a
}: Received add service event
3m56s       Normal   Info     service/as-ciat-commandmanager-20220120-06563138-zxk4     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-commandmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111062
    uid: 3d01008c-3f49-44b1-9b5a-554af8a4dd72
}: Received delete service event
13m         Normal   Info     service/as-ciat-commandmanager-20220120-06563138-zxk4     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-commandmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065271
    uid: 3d01008c-3f49-44b1-9b5a-554af8a4dd72
}: Received add service event
3m56s       Normal   Info     service/as-ciat-commandmanager-20220120-06563138-zxk4     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-commandmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111062
    uid: 3d01008c-3f49-44b1-9b5a-554af8a4dd72
}: Deleted service monitor
13m         Normal   Info     service/as-ciat-commandmanager-20220120-06563138-zxk4     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-commandmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065271
    uid: 3d01008c-3f49-44b1-9b5a-554af8a4dd72
}: Created service monitor
3m55s       Normal   Info     service/as-ciat-commandmanager-20220120-07263146-unk4     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-commandmanager-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115637
    uid: 2d844eec-2aa3-4584-9158-b2f65ef39674
}: Created service monitor
3m55s       Normal   Info     service/as-ciat-commandmanager-20220120-07263146-unk4     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-commandmanager-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115637
    uid: 2d844eec-2aa3-4584-9158-b2f65ef39674
}: Received add service event
22m         Normal   Info     service/as-ciat-contextmanager-20220120-06563138-zxk4     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-contextmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065272
    uid: 8355ba1f-0fbc-48c3-b83c-0a043d73d151
}: Received add service event
3m56s       Normal   Info     service/as-ciat-contextmanager-20220120-06563138-zxk4     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-contextmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111071
    uid: 8355ba1f-0fbc-48c3-b83c-0a043d73d151
}: Received delete service event
22m         Normal   Info     service/as-ciat-contextmanager-20220120-06563138-zxk4     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-contextmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065272
    uid: 8355ba1f-0fbc-48c3-b83c-0a043d73d151
}: Created service monitor
3m56s       Normal   Info     service/as-ciat-contextmanager-20220120-06563138-zxk4     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-contextmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111071
    uid: 8355ba1f-0fbc-48c3-b83c-0a043d73d151
}: Deleted service monitor
3m55s       Normal   Info     service/as-ciat-contextmanager-20220120-07263146-unk4     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-contextmanager-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115640
    uid: 9243158b-ace3-453d-83e9-cefd14a80e0f
}: Received add service event
3m55s       Normal   Info     service/as-ciat-contextmanager-20220120-07263146-unk4     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-contextmanager-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115640
    uid: 9243158b-ace3-453d-83e9-cefd14a80e0f
}: Created service monitor
15m         Normal   Info     service/as-ciat-customizationmanager-20220120-06563138-zxk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-customizationmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065311
    uid: 74bf3372-cf6b-48fe-9772-988d2ed61d84
}: Created service monitor
15m         Normal   Info     service/as-ciat-customizationmanager-20220120-06563138-zxk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-customizationmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065311
    uid: 74bf3372-cf6b-48fe-9772-988d2ed61d84
}: Received add service event
3m56s       Normal   Info     service/as-ciat-customizationmanager-20220120-06563138-zxk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-customizationmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111063
    uid: 74bf3372-cf6b-48fe-9772-988d2ed61d84
}: Deleted service monitor
3m56s       Normal   Info     service/as-ciat-customizationmanager-20220120-06563138-zxk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-customizationmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111063
    uid: 74bf3372-cf6b-48fe-9772-988d2ed61d84
}: Received delete service event
3m55s       Normal   Info     service/as-ciat-customizationmanager-20220120-07263146-unk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-customizationmanager-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115649
    uid: f528a506-b7b4-4f5c-8c30-693e5737c10b
}: Created service monitor
3m55s       Normal   Info     service/as-ciat-customizationmanager-20220120-07263146-unk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-customizationmanager-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115649
    uid: f528a506-b7b4-4f5c-8c30-693e5737c10b
}: Received add service event
3m56s       Normal   Info     service/as-ciat-elementmanager-20220120-06563138-zxk4         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-elementmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111076
    uid: 8105d412-6386-4b8b-ad83-ccee0e6b9944
}: Received delete service event
9m6s        Normal   Info     service/as-ciat-elementmanager-20220120-06563138-zxk4         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-elementmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065304
    uid: 8105d412-6386-4b8b-ad83-ccee0e6b9944
}: Received add service event
3m56s       Normal   Info     service/as-ciat-elementmanager-20220120-06563138-zxk4         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-elementmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111076
    uid: 8105d412-6386-4b8b-ad83-ccee0e6b9944
}: Deleted service monitor
9m6s        Normal   Info     service/as-ciat-elementmanager-20220120-06563138-zxk4         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-elementmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065304
    uid: 8105d412-6386-4b8b-ad83-ccee0e6b9944
}: Created service monitor
3m55s       Normal   Info     service/as-ciat-elementmanager-20220120-07263146-unk4         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-elementmanager-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115672
    uid: a6ccdb1b-40b9-4a8f-ab37-41e83196cc61
}: Received add service event
3m55s       Normal   Info     service/as-ciat-elementmanager-20220120-07263146-unk4         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-elementmanager-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115672
    uid: a6ccdb1b-40b9-4a8f-ab37-41e83196cc61
}: Created service monitor
3m56s       Normal   Info     service/as-ciat-eventmanager-20220120-06563138-zxk4           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111066
    uid: 2e6d3d56-237b-4c21-a236-1e8f2f78e4a7
}: Deleted service monitor
9m49s       Normal   Info     service/as-ciat-eventmanager-20220120-06563138-zxk4           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065268
    uid: 2e6d3d56-237b-4c21-a236-1e8f2f78e4a7
}: Created service monitor
3m56s       Normal   Info     service/as-ciat-eventmanager-20220120-06563138-zxk4           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111066
    uid: 2e6d3d56-237b-4c21-a236-1e8f2f78e4a7
}: Received delete service event
9m49s       Normal   Info     service/as-ciat-eventmanager-20220120-06563138-zxk4           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065268
    uid: 2e6d3d56-237b-4c21-a236-1e8f2f78e4a7
}: Received add service event
3m55s       Normal   Info     service/as-ciat-eventmanager-20220120-07263146-unk4           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventmanager-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115645
    uid: a1bb47b0-6dd7-4fac-9527-3726bd802d02
}: Received add service event
3m55s       Normal   Info     service/as-ciat-eventmanager-20220120-07263146-unk4           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventmanager-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115645
    uid: a1bb47b0-6dd7-4fac-9527-3726bd802d02
}: Created service monitor
25m         Normal   Info     service/as-ciat-eventworker-20220120-06563138-zxk4            Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventworker-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065307
    uid: cb1a987b-3817-4718-b00c-715879f2ff83
}: Created service monitor
25m         Normal   Info     service/as-ciat-eventworker-20220120-06563138-zxk4            Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventworker-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065307
    uid: cb1a987b-3817-4718-b00c-715879f2ff83
}: Received add service event
3m56s       Normal   Info     service/as-ciat-eventworker-20220120-06563138-zxk4            Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventworker-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111074
    uid: cb1a987b-3817-4718-b00c-715879f2ff83
}: Received delete service event
3m56s       Normal   Info     service/as-ciat-eventworker-20220120-06563138-zxk4            Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventworker-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111074
    uid: cb1a987b-3817-4718-b00c-715879f2ff83
}: Deleted service monitor
3m55s       Normal   Info     service/as-ciat-eventworker-20220120-07263146-unk4            Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventworker-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115653
    uid: 6a418d3e-3b83-4eae-9e32-898c92cf6a31
}: Received add service event
3m55s       Normal   Info     service/as-ciat-eventworker-20220120-07263146-unk4            Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventworker-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115653
    uid: 6a418d3e-3b83-4eae-9e32-898c92cf6a31
}: Created service monitor
25m         Normal   Info     service/as-ciat-queuemanager-20220120-06563138-zxk4           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-queuemanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065279
    uid: c5ccd22a-0871-468f-b1a0-d3af57cb6188
}: Created service monitor
25m         Normal   Info     service/as-ciat-queuemanager-20220120-06563138-zxk4           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-queuemanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065279
    uid: c5ccd22a-0871-468f-b1a0-d3af57cb6188
}: Received add service event
3m56s       Normal   Info     service/as-ciat-queuemanager-20220120-06563138-zxk4           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-queuemanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111070
    uid: c5ccd22a-0871-468f-b1a0-d3af57cb6188
}: Deleted service monitor
3m56s       Normal   Info     service/as-ciat-queuemanager-20220120-06563138-zxk4           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-queuemanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111070
    uid: c5ccd22a-0871-468f-b1a0-d3af57cb6188
}: Received delete service event
3m55s       Normal   Info     service/as-ciat-queuemanager-20220120-07263146-unk4           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-queuemanager-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115685
    uid: 234a2a49-182b-40c1-b7c2-e58eaf2fabc2
}: Received add service event
3m55s       Normal   Info     service/as-ciat-queuemanager-20220120-07263146-unk4           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-queuemanager-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115685
    uid: 234a2a49-182b-40c1-b7c2-e58eaf2fabc2
}: Created service monitor
3m56s       Normal   Info     service/as-ciat-realtimedatamanager-20220120-06563138-zxk4    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-realtimedatamanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111068
    uid: e935191f-2276-4006-944e-bbafc0aa6bdc
}: Deleted service monitor
11m         Normal   Info     service/as-ciat-realtimedatamanager-20220120-06563138-zxk4    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-realtimedatamanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065275
    uid: e935191f-2276-4006-944e-bbafc0aa6bdc
}: Created service monitor
11m         Normal   Info     service/as-ciat-realtimedatamanager-20220120-06563138-zxk4    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-realtimedatamanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065275
    uid: e935191f-2276-4006-944e-bbafc0aa6bdc
}: Received add service event
3m56s       Normal   Info     service/as-ciat-realtimedatamanager-20220120-06563138-zxk4    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-realtimedatamanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111068
    uid: e935191f-2276-4006-944e-bbafc0aa6bdc
}: Received delete service event
3m55s       Normal   Info     service/as-ciat-realtimedatamanager-20220120-07263146-unk4    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-realtimedatamanager-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115638
    uid: e53809e7-544a-4fe0-adfe-53937d4edb51
}: Created service monitor
3m55s       Normal   Info     service/as-ciat-realtimedatamanager-20220120-07263146-unk4    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-realtimedatamanager-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115638
    uid: e53809e7-544a-4fe0-adfe-53937d4edb51
}: Received add service event
3m56s       Normal   Info     service/as-ciat-reportdesignmanager-20220120-06563138-zxk4    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportdesignmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111065
    uid: 74e59ef2-638a-468e-b598-a8ace16bfc38
}: Received delete service event
15m         Normal   Info     service/as-ciat-reportdesignmanager-20220120-06563138-zxk4    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportdesignmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065320
    uid: 74e59ef2-638a-468e-b598-a8ace16bfc38
}: Created service monitor
15m         Normal   Info     service/as-ciat-reportdesignmanager-20220120-06563138-zxk4    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportdesignmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065320
    uid: 74e59ef2-638a-468e-b598-a8ace16bfc38
}: Received add service event
3m56s       Normal   Info     service/as-ciat-reportdesignmanager-20220120-06563138-zxk4    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportdesignmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111065
    uid: 74e59ef2-638a-468e-b598-a8ace16bfc38
}: Deleted service monitor
3m55s       Normal   Info     service/as-ciat-reportdesignmanager-20220120-07263146-unk4    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportdesignmanager-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115647
    uid: 432030d4-6773-4ffc-989d-f9c8623b9ae4
}: Created service monitor
3m55s       Normal   Info     service/as-ciat-reportdesignmanager-20220120-07263146-unk4    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportdesignmanager-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115647
    uid: 432030d4-6773-4ffc-989d-f9c8623b9ae4
}: Received add service event
11m         Normal   Info     service/as-ciat-reportpublishmanager-20220120-06563138-zxk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportpublishmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065295
    uid: 7cdf755f-d9b4-4489-b967-cbf93dea3023
}: Received add service event
11m         Normal   Info     service/as-ciat-reportpublishmanager-20220120-06563138-zxk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportpublishmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065295
    uid: 7cdf755f-d9b4-4489-b967-cbf93dea3023
}: Created service monitor
3m56s       Normal   Info     service/as-ciat-reportpublishmanager-20220120-06563138-zxk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportpublishmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111075
    uid: 7cdf755f-d9b4-4489-b967-cbf93dea3023
}: Received delete service event
3m56s       Normal   Info     service/as-ciat-reportpublishmanager-20220120-06563138-zxk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportpublishmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111075
    uid: 7cdf755f-d9b4-4489-b967-cbf93dea3023
}: Deleted service monitor
3m55s       Normal   Info     service/as-ciat-reportpublishmanager-20220120-07263146-unk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportpublishmanager-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115658
    uid: b550ee0a-5fb1-4915-b487-40898978e375
}: Created service monitor
3m55s       Normal   Info     service/as-ciat-reportpublishmanager-20220120-07263146-unk4   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportpublishmanager-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115658
    uid: b550ee0a-5fb1-4915-b487-40898978e375
}: Received add service event
22m         Normal   Info     service/as-ciat-searchmanager-20220120-06563138-zxk4          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-searchmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065293
    uid: 397a9aff-e250-48e7-a4ff-94d58867227a
}: Created service monitor
22m         Normal   Info     service/as-ciat-searchmanager-20220120-06563138-zxk4          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-searchmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065293
    uid: 397a9aff-e250-48e7-a4ff-94d58867227a
}: Received add service event
3m56s       Normal   Info     service/as-ciat-searchmanager-20220120-06563138-zxk4          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-searchmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111064
    uid: 397a9aff-e250-48e7-a4ff-94d58867227a
}: Received delete service event
3m56s       Normal   Info     service/as-ciat-searchmanager-20220120-06563138-zxk4          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-searchmanager-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111064
    uid: 397a9aff-e250-48e7-a4ff-94d58867227a
}: Deleted service monitor
3m55s       Normal   Info     service/as-ciat-searchmanager-20220120-07263146-unk4          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-searchmanager-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115634
    uid: 1c8eb0a0-fa70-4e74-afb1-fd90242da066
}: Created service monitor
3m55s       Normal   Info     service/as-ciat-searchmanager-20220120-07263146-unk4          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-searchmanager-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115634
    uid: 1c8eb0a0-fa70-4e74-afb1-fd90242da066
}: Received add service event
3m56s       Normal   Info     service/as-ciat-securityservice-20220120-06563138-zxk4        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-securityservice-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111072
    uid: 6c041a6b-a953-4ba0-a5a7-c7605ccb98a0
}: Received delete service event
3m56s       Normal   Info     service/as-ciat-securityservice-20220120-06563138-zxk4        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-securityservice-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770111072
    uid: 6c041a6b-a953-4ba0-a5a7-c7605ccb98a0
}: Deleted service monitor
11m         Normal   Info     service/as-ciat-securityservice-20220120-06563138-zxk4        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-securityservice-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065326
    uid: 6c041a6b-a953-4ba0-a5a7-c7605ccb98a0
}: Received add service event
11m         Normal   Info     service/as-ciat-securityservice-20220120-06563138-zxk4        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-securityservice-20220120-06563138-zxk4
    namespace: as-ciat
    resourceVersion: 770065326
    uid: 6c041a6b-a953-4ba0-a5a7-c7605ccb98a0
}: Created service monitor
3m55s       Normal   Info     service/as-ciat-securityservice-20220120-07263146-unk4        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-securityservice-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115655
    uid: c2f64bb6-d9d7-48ce-b114-18c73986bc1e
}: Created service monitor
3m55s       Normal   Info     service/as-ciat-securityservice-20220120-07263146-unk4        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-securityservice-20220120-07263146-unk4
    namespace: as-ciat
    resourceVersion: 770115655
    uid: c2f64bb6-d9d7-48ce-b114-18c73986bc1e
}: Received add service event
54m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/horton-autoscaler                       unable to get metrics for resource memory: no metrics returned from resource metrics API
19m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/horton-autoscaler                       unable to get metrics for resource memory: no metrics returned from resource metrics API
34m         Warning   FailedGetPodsMetric            horizontalpodautoscaler/horton-autoscaler                       unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
34m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/horton-autoscaler                       unable to get metrics for resource cpu: no metrics returned from resource metrics API
36m         Warning   FailedComputeMetricsReplicas   horizontalpodautoscaler/horton-autoscaler                       invalid metrics (3 invalid out of 3), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API
4m29s       Warning   FailedGetResourceMetric        horizontalpodautoscaler/horton-autoscaler                       unable to get metrics for resource memory: no metrics returned from resource metrics API
4m29s       Warning   FailedGetPodsMetric            horizontalpodautoscaler/horton-autoscaler                       unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
4m29s       Warning   FailedGetResourceMetric        horizontalpodautoscaler/horton-autoscaler                       unable to get metrics for resource cpu: no metrics returned from resource metrics API
6m20s       Warning   FailedComputeMetricsReplicas   horizontalpodautoscaler/horton-autoscaler                       invalid metrics (3 invalid out of 3), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API
54m         Warning   FailedCreate                   replicaset/horton-horton-deployment-575597484d                  Error creating: pods "horton-horton-deployment-575597484d-5gwdh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/horton-horton-deployment-575597484d                  Error creating: pods "horton-horton-deployment-575597484d-xhqxh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/horton-horton-deployment-575597484d                  Error creating: pods "horton-horton-deployment-575597484d-6tpm4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m45s       Warning   FailedCreate                   replicaset/horton-horton-deployment-575597484d                  Error creating: pods "horton-horton-deployment-575597484d-8x52g" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/horton-horton-deployment-5cb5d5847b                  Error creating: pods "horton-horton-deployment-5cb5d5847b-5nx9f" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/horton-horton-deployment-5cb5d5847b                  Error creating: pods "horton-horton-deployment-5cb5d5847b-9j7xh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/horton-horton-deployment-5cb5d5847b                  Error creating: pods "horton-horton-deployment-5cb5d5847b-92mcb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/horton-horton-deployment-5cb5d5847b                  Error creating: pods "horton-horton-deployment-5cb5d5847b-bj6hw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/horton-horton-deployment-5cb5d5847b                  Error creating: pods "horton-horton-deployment-5cb5d5847b-mv7nn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/horton-horton-deployment-5cb5d5847b                  Error creating: pods "horton-horton-deployment-5cb5d5847b-f72zp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/horton-horton-deployment-5cb5d5847b                  Error creating: pods "horton-horton-deployment-5cb5d5847b-m8s6n" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/horton-horton-deployment-5cb5d5847b                  Error creating: pods "horton-horton-deployment-5cb5d5847b-9bjs5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/horton-horton-deployment-5cb5d5847b                  Error creating: pods "horton-horton-deployment-5cb5d5847b-n9xmh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
35m         Warning   FailedCreate                   replicaset/horton-horton-deployment-5cb5d5847b                  (combined from similar events): Error creating: pods "horton-horton-deployment-5cb5d5847b-xwpnw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
25m         Warning   FailedCreate                   replicaset/horton-horton-deployment-5cb5d5847b                  Error creating: pods "horton-horton-deployment-5cb5d5847b-cfm87" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m23s       Warning   FailedCreate                   replicaset/horton-horton-deployment-5cb5d5847b                  Error creating: pods "horton-horton-deployment-5cb5d5847b-xzchc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
45m         Warning   FailedCreate                   replicaset/horton-horton-deployment-75bf7f55c4                  Error creating: pods "horton-horton-deployment-75bf7f55c4-pzvkp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/horton-horton-deployment-75bf7f55c4                  Error creating: pods "horton-horton-deployment-75bf7f55c4-lsd7f" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
12m         Warning   FailedCreate                   replicaset/horton-horton-deployment-75bf7f55c4                  Error creating: pods "horton-horton-deployment-75bf7f55c4-6qlc9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/horton-horton-deployment-7666bdd65b                  Error creating: pods "horton-horton-deployment-7666bdd65b-r7dvh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/horton-horton-deployment-7666bdd65b                  Error creating: pods "horton-horton-deployment-7666bdd65b-fg66h" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
15m         Warning   FailedCreate                   replicaset/horton-horton-deployment-7666bdd65b                  Error creating: pods "horton-horton-deployment-7666bdd65b-wbmbz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/horton-horton-deployment-77dc74fcf6                  Error creating: pods "horton-horton-deployment-77dc74fcf6-b6z2w" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
35m         Warning   FailedCreate                   replicaset/horton-horton-deployment-77dc74fcf6                  Error creating: pods "horton-horton-deployment-77dc74fcf6-4tzgv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/horton-horton-deployment-77dc74fcf6                  Error creating: pods "horton-horton-deployment-77dc74fcf6-p4xmw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
115s        Warning   FailedCreate                   replicaset/horton-horton-deployment-77dc74fcf6                  Error creating: pods "horton-horton-deployment-77dc74fcf6-9nnxc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/horton-horton-deployment-7956dcf558                  Error creating: pods "horton-horton-deployment-7956dcf558-d8wpd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/horton-horton-deployment-7956dcf558                  Error creating: pods "horton-horton-deployment-7956dcf558-gvm8c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate                   replicaset/horton-horton-deployment-7956dcf558                  Error creating: pods "horton-horton-deployment-7956dcf558-hwtns" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/horton-horton-deployment-7b44565c7d                  Error creating: pods "horton-horton-deployment-7b44565c7d-422n6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/horton-horton-deployment-7b44565c7d                  Error creating: pods "horton-horton-deployment-7b44565c7d-929dv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/horton-horton-deployment-7b44565c7d                  Error creating: pods "horton-horton-deployment-7b44565c7d-qjwmv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
55m         Warning   FailedCreate                   replicaset/horton-horton-deployment-7fdd67cb74                  Error creating: pods "horton-horton-deployment-7fdd67cb74-gv7cp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/horton-horton-deployment-7fdd67cb74                  Error creating: pods "horton-horton-deployment-7fdd67cb74-bpfrk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/horton-horton-deployment-7fdd67cb74                  Error creating: pods "horton-horton-deployment-7fdd67cb74-jd5sg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m58s       Warning   FailedCreate                   replicaset/horton-horton-deployment-7fdd67cb74                  Error creating: pods "horton-horton-deployment-7fdd67cb74-7dtxz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/horton-horton-deployment-8557865c96                  Error creating: pods "horton-horton-deployment-8557865c96-gm284" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/horton-horton-deployment-8557865c96                  Error creating: pods "horton-horton-deployment-8557865c96-96jqx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/horton-horton-deployment-8557865c96                  Error creating: pods "horton-horton-deployment-8557865c96-cg8f4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/horton-horton-deployment-8557865c96                  Error creating: pods "horton-horton-deployment-8557865c96-krbtl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/horton-horton-deployment-8557865c96                  Error creating: pods "horton-horton-deployment-8557865c96-t4nww" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/horton-horton-deployment-8557865c96                  Error creating: pods "horton-horton-deployment-8557865c96-wnpgm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/horton-horton-deployment-8557865c96                  Error creating: pods "horton-horton-deployment-8557865c96-p8jxt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/horton-horton-deployment-8557865c96                  Error creating: pods "horton-horton-deployment-8557865c96-94bm2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/horton-horton-deployment-8557865c96                  Error creating: pods "horton-horton-deployment-8557865c96-8bwpt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
6m14s       Warning   FailedCreate                   replicaset/horton-horton-deployment-8557865c96                  (combined from similar events): Error creating: pods "horton-horton-deployment-8557865c96-66cpc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
45m         Warning   FailedCreate                   replicaset/horton-horton-deployment-859d67c6dd                  Error creating: pods "horton-horton-deployment-859d67c6dd-pvhzm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/horton-horton-deployment-859d67c6dd                  Error creating: pods "horton-horton-deployment-859d67c6dd-ljxm7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
12m         Warning   FailedCreate                   replicaset/horton-horton-deployment-859d67c6dd                  Error creating: pods "horton-horton-deployment-859d67c6dd-cml2n" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/horton-horton-deployment-b4cc54648                   Error creating: pods "horton-horton-deployment-b4cc54648-hcxfq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate                   replicaset/horton-horton-deployment-b4cc54648                   Error creating: pods "horton-horton-deployment-b4cc54648-mp62j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/horton-horton-deployment-b4cc54648                   Error creating: pods "horton-horton-deployment-b4cc54648-7xktv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
92s         Warning   FailedCreate                   replicaset/horton-horton-deployment-b4cc54648                   Error creating: pods "horton-horton-deployment-b4cc54648-pjgx4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/horton-horton-deployment-c94fb56d4                   Error creating: pods "horton-horton-deployment-c94fb56d4-t5ss2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/horton-horton-deployment-c94fb56d4                   Error creating: pods "horton-horton-deployment-c94fb56d4-qhshn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
25m         Warning   FailedCreate                   replicaset/horton-horton-deployment-c94fb56d4                   Error creating: pods "horton-horton-deployment-c94fb56d4-chvjz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m39s       Warning   FailedCreate                   replicaset/horton-horton-deployment-c94fb56d4                   Error creating: pods "horton-horton-deployment-c94fb56d4-fm67k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/horton-horton-deployment-cf96b5f96                   Error creating: pods "horton-horton-deployment-cf96b5f96-2xgx5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/horton-horton-deployment-cf96b5f96                   Error creating: pods "horton-horton-deployment-cf96b5f96-2qdt4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
25m         Warning   FailedCreate                   replicaset/horton-horton-deployment-cf96b5f96                   Error creating: pods "horton-horton-deployment-cf96b5f96-xpt4r" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m32s       Warning   FailedCreate                   replicaset/horton-horton-deployment-cf96b5f96                   Error creating: pods "horton-horton-deployment-cf96b5f96-f7t6t" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Normal    ScalingReplicaSet              deployment/horton-horton-deployment                             Scaled up replica set horton-horton-deployment-5cb5d5847b to 3
17m         Normal    ScalingReplicaSet              deployment/horton-horton-deployment                             Scaled up replica set horton-horton-deployment-8557865c96 to 3
51m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-54bc6d977                     Error creating: pods "lorax-analyticsmanager-54bc6d977-fq6sb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
35m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-54bc6d977                     Error creating: pods "lorax-analyticsmanager-54bc6d977-fhzz5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-54bc6d977                     Error creating: pods "lorax-analyticsmanager-54bc6d977-q7bqh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
109s        Warning   FailedCreate                   replicaset/lorax-analyticsmanager-54bc6d977                     Error creating: pods "lorax-analyticsmanager-54bc6d977-bgg47" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-54cddc4b7c                    Error creating: pods "lorax-analyticsmanager-54cddc4b7c-2k2w2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-54cddc4b7c                    Error creating: pods "lorax-analyticsmanager-54cddc4b7c-dsvh7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
15m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-54cddc4b7c                    Error creating: pods "lorax-analyticsmanager-54cddc4b7c-c4jmq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-574b54fdcc                    Error creating: pods "lorax-analyticsmanager-574b54fdcc-mxstr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-574b54fdcc                    Error creating: pods "lorax-analyticsmanager-574b54fdcc-fpm8t" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-574b54fdcc                    Error creating: pods "lorax-analyticsmanager-574b54fdcc-d66xz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m9s        Warning   FailedCreate                   replicaset/lorax-analyticsmanager-574b54fdcc                    Error creating: pods "lorax-analyticsmanager-574b54fdcc-2vp7q" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-5845b594f7                    Error creating: pods "lorax-analyticsmanager-5845b594f7-9wgtx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-5845b594f7                    Error creating: pods "lorax-analyticsmanager-5845b594f7-c85q7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-5845b594f7                    Error creating: pods "lorax-analyticsmanager-5845b594f7-j97mh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m39s       Warning   FailedCreate                   replicaset/lorax-analyticsmanager-5845b594f7                    Error creating: pods "lorax-analyticsmanager-5845b594f7-9pxgt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-6467b86457                    Error creating: pods "lorax-analyticsmanager-6467b86457-pj9d5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-6467b86457                    Error creating: pods "lorax-analyticsmanager-6467b86457-j2vwg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-6467b86457                    Error creating: pods "lorax-analyticsmanager-6467b86457-q99bw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
55m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-649b9ff5c                     Error creating: pods "lorax-analyticsmanager-649b9ff5c-l2tjs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-649b9ff5c                     Error creating: pods "lorax-analyticsmanager-649b9ff5c-df4r4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-649b9ff5c                     Error creating: pods "lorax-analyticsmanager-649b9ff5c-572gw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
5m          Warning   FailedCreate                   replicaset/lorax-analyticsmanager-649b9ff5c                     Error creating: pods "lorax-analyticsmanager-649b9ff5c-gjfzc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
45m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-6b84676cfc                    Error creating: pods "lorax-analyticsmanager-6b84676cfc-kx8w4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-6b84676cfc                    Error creating: pods "lorax-analyticsmanager-6b84676cfc-r4ccc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
12m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-6b84676cfc                    Error creating: pods "lorax-analyticsmanager-6b84676cfc-vjsdt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-6bf55f567c                    Error creating: pods "lorax-analyticsmanager-6bf55f567c-6wdmk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-6bf55f567c                    Error creating: pods "lorax-analyticsmanager-6bf55f567c-6m2qh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-6bf55f567c                    Error creating: pods "lorax-analyticsmanager-6bf55f567c-jchn8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
88s         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-6bf55f567c                    Error creating: pods "lorax-analyticsmanager-6bf55f567c-f2vp8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-6c8f878f7                     Error creating: pods "lorax-analyticsmanager-6c8f878f7-lbrkc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-6c8f878f7                     Error creating: pods "lorax-analyticsmanager-6c8f878f7-f9lwk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-6c8f878f7                     Error creating: pods "lorax-analyticsmanager-6c8f878f7-drpj2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-7464749dd9                    Error creating: pods "lorax-analyticsmanager-7464749dd9-fzp52" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-7464749dd9                    Error creating: pods "lorax-analyticsmanager-7464749dd9-2gqlx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-7464749dd9                    Error creating: pods "lorax-analyticsmanager-7464749dd9-twkmv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-7464749dd9                    Error creating: pods "lorax-analyticsmanager-7464749dd9-b5hhn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-7464749dd9                    Error creating: pods "lorax-analyticsmanager-7464749dd9-mr9lh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-7464749dd9                    Error creating: pods "lorax-analyticsmanager-7464749dd9-bjcq4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-7464749dd9                    Error creating: pods "lorax-analyticsmanager-7464749dd9-xm48r" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-7464749dd9                    Error creating: pods "lorax-analyticsmanager-7464749dd9-tt724" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-7464749dd9                    Error creating: pods "lorax-analyticsmanager-7464749dd9-mrvm2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
5m33s       Warning   FailedCreate                   replicaset/lorax-analyticsmanager-7464749dd9                    (combined from similar events): Error creating: pods "lorax-analyticsmanager-7464749dd9-dd687" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-75c549d849                    Error creating: pods "lorax-analyticsmanager-75c549d849-kvj9p" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-75c549d849                    Error creating: pods "lorax-analyticsmanager-75c549d849-whwbf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-75c549d849                    Error creating: pods "lorax-analyticsmanager-75c549d849-742j4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedMount                    pod/lorax-analyticsmanager-75d985dfdf-6jmjm                     MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
17m         Warning   FailedMount                    pod/lorax-analyticsmanager-75d985dfdf-mmxrw                     MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
18m         Warning   FailedMount                    pod/lorax-analyticsmanager-75d985dfdf-zfr7b                     MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
47m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-7fb55ff7fc                    Error creating: pods "lorax-analyticsmanager-7fb55ff7fc-rpbn6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-7fb55ff7fc                    Error creating: pods "lorax-analyticsmanager-7fb55ff7fc-rdf4l" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-7fb55ff7fc                    Error creating: pods "lorax-analyticsmanager-7fb55ff7fc-7r4lt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-7fb55ff7fc                    Error creating: pods "lorax-analyticsmanager-7fb55ff7fc-cxz8f" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-7fb55ff7fc                    Error creating: pods "lorax-analyticsmanager-7fb55ff7fc-hkk84" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-7fb55ff7fc                    Error creating: pods "lorax-analyticsmanager-7fb55ff7fc-bmhl4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-7fb55ff7fc                    Error creating: pods "lorax-analyticsmanager-7fb55ff7fc-s6897" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-7fb55ff7fc                    Error creating: pods "lorax-analyticsmanager-7fb55ff7fc-g2c5p" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-7fb55ff7fc                    Error creating: pods "lorax-analyticsmanager-7fb55ff7fc-wxp4r" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-7fb55ff7fc                    (combined from similar events): Error creating: pods "lorax-analyticsmanager-7fb55ff7fc-6pr82" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
25m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-7fb55ff7fc                    Error creating: pods "lorax-analyticsmanager-7fb55ff7fc-4sz4x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m25s       Warning   FailedCreate                   replicaset/lorax-analyticsmanager-7fb55ff7fc                    Error creating: pods "lorax-analyticsmanager-7fb55ff7fc-rqm8v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-analyticsmanager-autoscaler       unable to get metrics for resource memory: no metrics returned from resource metrics API
24m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-analyticsmanager-autoscaler       unable to get metrics for resource memory: no metrics returned from resource metrics API
33m         Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-analyticsmanager-autoscaler       unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
33m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-analyticsmanager-autoscaler       unable to get metrics for resource cpu: no metrics returned from resource metrics API
35m         Warning   FailedComputeMetricsReplicas   horizontalpodautoscaler/lorax-analyticsmanager-autoscaler       invalid metrics (3 invalid out of 3), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API
4m15s       Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-analyticsmanager-autoscaler       unable to get metrics for resource memory: no metrics returned from resource metrics API
4m15s       Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-analyticsmanager-autoscaler       unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
4m15s       Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-analyticsmanager-autoscaler       unable to get metrics for resource cpu: no metrics returned from resource metrics API
6m6s        Warning   FailedComputeMetricsReplicas   horizontalpodautoscaler/lorax-analyticsmanager-autoscaler       invalid metrics (3 invalid out of 3), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API
58m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-f6bbcd5fc                     Error creating: pods "lorax-analyticsmanager-f6bbcd5fc-nttxn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-f6bbcd5fc                     Error creating: pods "lorax-analyticsmanager-f6bbcd5fc-d85k6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-analyticsmanager-f6bbcd5fc                     Error creating: pods "lorax-analyticsmanager-f6bbcd5fc-vp8mx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m3s        Warning   FailedCreate                   replicaset/lorax-analyticsmanager-f6bbcd5fc                     Error creating: pods "lorax-analyticsmanager-f6bbcd5fc-rghsl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Normal    ScalingReplicaSet              deployment/lorax-analyticsmanager                               Scaled up replica set lorax-analyticsmanager-7fb55ff7fc to 3
16m         Normal    ScalingReplicaSet              deployment/lorax-analyticsmanager                               Scaled up replica set lorax-analyticsmanager-7464749dd9 to 3
55m         Warning   FailedCreate                   replicaset/lorax-cachehost-5656bcff96                           Error creating: pods "lorax-cachehost-5656bcff96-dh872" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/lorax-cachehost-5656bcff96                           Error creating: pods "lorax-cachehost-5656bcff96-c4s6p" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-cachehost-5656bcff96                           Error creating: pods "lorax-cachehost-5656bcff96-g647r" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
5m5s        Warning   FailedCreate                   replicaset/lorax-cachehost-5656bcff96                           Error creating: pods "lorax-cachehost-5656bcff96-9pgrc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-cachehost-5c5455dcd4                           Error creating: pods "lorax-cachehost-5c5455dcd4-9tb28" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-cachehost-5c5455dcd4                           Error creating: pods "lorax-cachehost-5c5455dcd4-4b2pn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-cachehost-5c5455dcd4                           Error creating: pods "lorax-cachehost-5c5455dcd4-sjx59" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m13s       Warning   FailedCreate                   replicaset/lorax-cachehost-5c5455dcd4                           Error creating: pods "lorax-cachehost-5c5455dcd4-sthn5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedMount                    pod/lorax-cachehost-647b6cf7b4-4v6n7                            MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
18m         Warning   FailedMount                    pod/lorax-cachehost-647b6cf7b4-jsqlv                            MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
45m         Warning   FailedCreate                   replicaset/lorax-cachehost-647b6cf7b4                           Error creating: pods "lorax-cachehost-647b6cf7b4-sl9bj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-cachehost-647b6cf7b4                           Error creating: pods "lorax-cachehost-647b6cf7b4-k4qfm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
12m         Warning   FailedCreate                   replicaset/lorax-cachehost-647b6cf7b4                           Error creating: pods "lorax-cachehost-647b6cf7b4-rtz66" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-cachehost-6b99bd7cd4                           Error creating: pods "lorax-cachehost-6b99bd7cd4-hjg76" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-cachehost-6b99bd7cd4                           Error creating: pods "lorax-cachehost-6b99bd7cd4-nvlfs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
15m         Warning   FailedCreate                   replicaset/lorax-cachehost-6b99bd7cd4                           Error creating: pods "lorax-cachehost-6b99bd7cd4-2njcz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedMount                    pod/lorax-cachehost-6c8948bf68-8gmdf                            MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
18m         Warning   FailedMount                    pod/lorax-cachehost-6c8948bf68-mzvx7                            MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
17m         Warning   FailedMount                    pod/lorax-cachehost-6c8948bf68-vf26t                            MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
16m         Warning   FailedCreate                   replicaset/lorax-cachehost-6dd4cd6fc6                           Error creating: pods "lorax-cachehost-6dd4cd6fc6-2hbwp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-cachehost-6dd4cd6fc6                           Error creating: pods "lorax-cachehost-6dd4cd6fc6-99gtc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-cachehost-6dd4cd6fc6                           Error creating: pods "lorax-cachehost-6dd4cd6fc6-vwz9k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-cachehost-6dd4cd6fc6                           Error creating: pods "lorax-cachehost-6dd4cd6fc6-bv2bh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-cachehost-6dd4cd6fc6                           Error creating: pods "lorax-cachehost-6dd4cd6fc6-bbmnv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-cachehost-6dd4cd6fc6                           Error creating: pods "lorax-cachehost-6dd4cd6fc6-2pzzl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-cachehost-6dd4cd6fc6                           Error creating: pods "lorax-cachehost-6dd4cd6fc6-fq5c5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-cachehost-6dd4cd6fc6                           Error creating: pods "lorax-cachehost-6dd4cd6fc6-4stmp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-cachehost-6dd4cd6fc6                           Error creating: pods "lorax-cachehost-6dd4cd6fc6-m6vj9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
5m35s       Warning   FailedCreate                   replicaset/lorax-cachehost-6dd4cd6fc6                           (combined from similar events): Error creating: pods "lorax-cachehost-6dd4cd6fc6-cj8qg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-cachehost-6fcc6d6bf6                           Error creating: pods "lorax-cachehost-6fcc6d6bf6-cqjhb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-cachehost-6fcc6d6bf6                           Error creating: pods "lorax-cachehost-6fcc6d6bf6-8fpcv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-cachehost-6fcc6d6bf6                           Error creating: pods "lorax-cachehost-6fcc6d6bf6-ktnkq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-cachehost-6fcc6d6bf6                           Error creating: pods "lorax-cachehost-6fcc6d6bf6-znb4f" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-cachehost-6fcc6d6bf6                           Error creating: pods "lorax-cachehost-6fcc6d6bf6-xpz7g" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-cachehost-6fcc6d6bf6                           Error creating: pods "lorax-cachehost-6fcc6d6bf6-mhwkh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-cachehost-6fcc6d6bf6                           Error creating: pods "lorax-cachehost-6fcc6d6bf6-kn4h5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-cachehost-6fcc6d6bf6                           Error creating: pods "lorax-cachehost-6fcc6d6bf6-znrq9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-cachehost-6fcc6d6bf6                           Error creating: pods "lorax-cachehost-6fcc6d6bf6-x6v7p" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate                   replicaset/lorax-cachehost-6fcc6d6bf6                           (combined from similar events): Error creating: pods "lorax-cachehost-6fcc6d6bf6-fgb9k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
25m         Warning   FailedCreate                   replicaset/lorax-cachehost-6fcc6d6bf6                           Error creating: pods "lorax-cachehost-6fcc6d6bf6-g5w72" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m24s       Warning   FailedCreate                   replicaset/lorax-cachehost-6fcc6d6bf6                           Error creating: pods "lorax-cachehost-6fcc6d6bf6-lvtlk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-cachehost-7456cd54dd                           Error creating: pods "lorax-cachehost-7456cd54dd-5lht9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-cachehost-7456cd54dd                           Error creating: pods "lorax-cachehost-7456cd54dd-g25pn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-cachehost-7456cd54dd                           Error creating: pods "lorax-cachehost-7456cd54dd-x4kgx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m11s       Warning   FailedCreate                   replicaset/lorax-cachehost-7456cd54dd                           Error creating: pods "lorax-cachehost-7456cd54dd-7mknf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-cachehost-7566cbbcb                            Error creating: pods "lorax-cachehost-7566cbbcb-8dndk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-cachehost-7566cbbcb                            Error creating: pods "lorax-cachehost-7566cbbcb-jhtrt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate                   replicaset/lorax-cachehost-7566cbbcb                            Error creating: pods "lorax-cachehost-7566cbbcb-m4wqc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-cachehost-7cbc6dd864                           Error creating: pods "lorax-cachehost-7cbc6dd864-ctgvn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate                   replicaset/lorax-cachehost-7cbc6dd864                           Error creating: pods "lorax-cachehost-7cbc6dd864-dh44p" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-cachehost-7cbc6dd864                           Error creating: pods "lorax-cachehost-7cbc6dd864-7jfgd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
87s         Warning   FailedCreate                   replicaset/lorax-cachehost-7cbc6dd864                           Error creating: pods "lorax-cachehost-7cbc6dd864-nrgx8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-cachehost-7cd66b4dbd                           Error creating: pods "lorax-cachehost-7cd66b4dbd-qwc67" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate                   replicaset/lorax-cachehost-7cd66b4dbd                           Error creating: pods "lorax-cachehost-7cd66b4dbd-ccvsc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate                   replicaset/lorax-cachehost-7cd66b4dbd                           Error creating: pods "lorax-cachehost-7cd66b4dbd-59vjt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate                   replicaset/lorax-cachehost-7f5b5c8844                           Error creating: pods "lorax-cachehost-7f5b5c8844-7sg65" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/lorax-cachehost-7f5b5c8844                           Error creating: pods "lorax-cachehost-7f5b5c8844-gflsj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-cachehost-7f5b5c8844                           Error creating: pods "lorax-cachehost-7f5b5c8844-kvs49" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m39s       Warning   FailedCreate                   replicaset/lorax-cachehost-7f5b5c8844                           Error creating: pods "lorax-cachehost-7f5b5c8844-rv2z9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-cachehost-85457b4f7b                           Error creating: pods "lorax-cachehost-85457b4f7b-7mcnz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
35m         Warning   FailedCreate                   replicaset/lorax-cachehost-85457b4f7b                           Error creating: pods "lorax-cachehost-85457b4f7b-r757j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-cachehost-85457b4f7b                           Error creating: pods "lorax-cachehost-85457b4f7b-2sbzw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
113s        Warning   FailedCreate                   replicaset/lorax-cachehost-85457b4f7b                           Error creating: pods "lorax-cachehost-85457b4f7b-ckhwt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-cachehost-99b986d84                            Error creating: pods "lorax-cachehost-99b986d84-fz4bt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-cachehost-99b986d84                            Error creating: pods "lorax-cachehost-99b986d84-x4x95" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-cachehost-99b986d84                            Error creating: pods "lorax-cachehost-99b986d84-mlzw2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-cachehost-autoscaler              unable to get metrics for resource memory: no metrics returned from resource metrics API
24m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-cachehost-autoscaler              unable to get metrics for resource memory: no metrics returned from resource metrics API
30m         Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-cachehost-autoscaler              unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
30m         Warning   FailedComputeMetricsReplicas   horizontalpodautoscaler/lorax-cachehost-autoscaler              invalid metrics (2 invalid out of 2), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API
115s        Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-cachehost-autoscaler              unable to get metrics for resource memory: no metrics returned from resource metrics API
114s        Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-cachehost-autoscaler              unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
114s        Warning   FailedComputeMetricsReplicas   horizontalpodautoscaler/lorax-cachehost-autoscaler              invalid metrics (2 invalid out of 2), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API
47m         Normal    ScalingReplicaSet              deployment/lorax-cachehost                                      Scaled up replica set lorax-cachehost-6fcc6d6bf6 to 3
16m         Normal    ScalingReplicaSet              deployment/lorax-cachehost                                      Scaled up replica set lorax-cachehost-6dd4cd6fc6 to 3
55m         Warning   FailedCreate                   replicaset/lorax-commandmanager-5449fc7644                      Error creating: pods "lorax-commandmanager-5449fc7644-dj892" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/lorax-commandmanager-5449fc7644                      Error creating: pods "lorax-commandmanager-5449fc7644-4r8hs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-commandmanager-5449fc7644                      Error creating: pods "lorax-commandmanager-5449fc7644-9dt8q" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
5m7s        Warning   FailedCreate                   replicaset/lorax-commandmanager-5449fc7644                      Error creating: pods "lorax-commandmanager-5449fc7644-4pn5l" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-commandmanager-54784c4744                      Error creating: pods "lorax-commandmanager-54784c4744-8fzs4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-commandmanager-54784c4744                      Error creating: pods "lorax-commandmanager-54784c4744-n4cvt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-commandmanager-54784c4744                      Error creating: pods "lorax-commandmanager-54784c4744-jbdl9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-commandmanager-55557f49f4                      Error creating: pods "lorax-commandmanager-55557f49f4-7fhmp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-commandmanager-55557f49f4                      Error creating: pods "lorax-commandmanager-55557f49f4-dmpw2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate                   replicaset/lorax-commandmanager-55557f49f4                      Error creating: pods "lorax-commandmanager-55557f49f4-9rl8l" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-commandmanager-56b5bb78c8                      Error creating: pods "lorax-commandmanager-56b5bb78c8-5wr97" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-commandmanager-56b5bb78c8                      Error creating: pods "lorax-commandmanager-56b5bb78c8-pm5gh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-commandmanager-56b5bb78c8                      Error creating: pods "lorax-commandmanager-56b5bb78c8-k7dkn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m12s       Warning   FailedCreate                   replicaset/lorax-commandmanager-56b5bb78c8                      Error creating: pods "lorax-commandmanager-56b5bb78c8-m74df" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedMount                    pod/lorax-commandmanager-586d58947d-2h89f                       MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
17m         Warning   FailedMount                    pod/lorax-commandmanager-586d58947d-2rcmp                       MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
51m         Warning   FailedCreate                   replicaset/lorax-commandmanager-5db8948f7d                      Error creating: pods "lorax-commandmanager-5db8948f7d-wjhcx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
35m         Warning   FailedCreate                   replicaset/lorax-commandmanager-5db8948f7d                      Error creating: pods "lorax-commandmanager-5db8948f7d-8rr4j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-commandmanager-5db8948f7d                      Error creating: pods "lorax-commandmanager-5db8948f7d-mnqd6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
113s        Warning   FailedCreate                   replicaset/lorax-commandmanager-5db8948f7d                      Error creating: pods "lorax-commandmanager-5db8948f7d-wnd8c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-commandmanager-5ddf8f65c4                      Error creating: pods "lorax-commandmanager-5ddf8f65c4-tqzxh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-commandmanager-5ddf8f65c4                      Error creating: pods "lorax-commandmanager-5ddf8f65c4-rvqs8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-commandmanager-5ddf8f65c4                      Error creating: pods "lorax-commandmanager-5ddf8f65c4-j29l2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-commandmanager-5ddf8f65c4                      Error creating: pods "lorax-commandmanager-5ddf8f65c4-2pcfq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-commandmanager-5ddf8f65c4                      Error creating: pods "lorax-commandmanager-5ddf8f65c4-qwpb8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-commandmanager-5ddf8f65c4                      Error creating: pods "lorax-commandmanager-5ddf8f65c4-pzzhv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-commandmanager-5ddf8f65c4                      Error creating: pods "lorax-commandmanager-5ddf8f65c4-6m296" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-commandmanager-5ddf8f65c4                      Error creating: pods "lorax-commandmanager-5ddf8f65c4-rf882" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-commandmanager-5ddf8f65c4                      Error creating: pods "lorax-commandmanager-5ddf8f65c4-zl4cm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
35m         Warning   FailedCreate                   replicaset/lorax-commandmanager-5ddf8f65c4                      (combined from similar events): Error creating: pods "lorax-commandmanager-5ddf8f65c4-7pkd8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
25m         Warning   FailedCreate                   replicaset/lorax-commandmanager-5ddf8f65c4                      Error creating: pods "lorax-commandmanager-5ddf8f65c4-4p44m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m23s       Warning   FailedCreate                   replicaset/lorax-commandmanager-5ddf8f65c4                      Error creating: pods "lorax-commandmanager-5ddf8f65c4-dnh6x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
45m         Warning   FailedCreate                   replicaset/lorax-commandmanager-5f49868c58                      Error creating: pods "lorax-commandmanager-5f49868c58-zfgfr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-commandmanager-5f49868c58                      Error creating: pods "lorax-commandmanager-5f49868c58-nzf6v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
12m         Warning   FailedCreate                   replicaset/lorax-commandmanager-5f49868c58                      Error creating: pods "lorax-commandmanager-5f49868c58-b9979" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-commandmanager-764fc878fb                      Error creating: pods "lorax-commandmanager-764fc878fb-lmtwh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-commandmanager-764fc878fb                      Error creating: pods "lorax-commandmanager-764fc878fb-h4chw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
15m         Warning   FailedCreate                   replicaset/lorax-commandmanager-764fc878fb                      Error creating: pods "lorax-commandmanager-764fc878fb-9kkz5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-commandmanager-79c8bb6b6                       Error creating: pods "lorax-commandmanager-79c8bb6b6-ht6ds" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-commandmanager-79c8bb6b6                       Error creating: pods "lorax-commandmanager-79c8bb6b6-98l2t" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-commandmanager-79c8bb6b6                       Error creating: pods "lorax-commandmanager-79c8bb6b6-vfm59" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-commandmanager-79c8bb6b6                       Error creating: pods "lorax-commandmanager-79c8bb6b6-2s7tp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-commandmanager-79c8bb6b6                       Error creating: pods "lorax-commandmanager-79c8bb6b6-9x6bk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-commandmanager-79c8bb6b6                       Error creating: pods "lorax-commandmanager-79c8bb6b6-km68b" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-commandmanager-79c8bb6b6                       Error creating: pods "lorax-commandmanager-79c8bb6b6-d49zh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-commandmanager-79c8bb6b6                       Error creating: pods "lorax-commandmanager-79c8bb6b6-d6fmr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-commandmanager-79c8bb6b6                       Error creating: pods "lorax-commandmanager-79c8bb6b6-tcqnn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
5m32s       Warning   FailedCreate                   replicaset/lorax-commandmanager-79c8bb6b6                       (combined from similar events): Error creating: pods "lorax-commandmanager-79c8bb6b6-kwkvl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-commandmanager-7ff6dd5f66                      Error creating: pods "lorax-commandmanager-7ff6dd5f66-6wv9x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate                   replicaset/lorax-commandmanager-7ff6dd5f66                      Error creating: pods "lorax-commandmanager-7ff6dd5f66-cxjd4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-commandmanager-7ff6dd5f66                      Error creating: pods "lorax-commandmanager-7ff6dd5f66-8tsjp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m31s       Warning   FailedCreate                   replicaset/lorax-commandmanager-7ff6dd5f66                      Error creating: pods "lorax-commandmanager-7ff6dd5f66-x4rf5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-commandmanager-84c6d99f8                       Error creating: pods "lorax-commandmanager-84c6d99f8-w2r7j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate                   replicaset/lorax-commandmanager-84c6d99f8                       Error creating: pods "lorax-commandmanager-84c6d99f8-tzp44" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-commandmanager-84c6d99f8                       Error creating: pods "lorax-commandmanager-84c6d99f8-vkmsp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
88s         Warning   FailedCreate                   replicaset/lorax-commandmanager-84c6d99f8                       Error creating: pods "lorax-commandmanager-84c6d99f8-4f69m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate                   replicaset/lorax-commandmanager-8645779974                      Error creating: pods "lorax-commandmanager-8645779974-5lvzr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate                   replicaset/lorax-commandmanager-8645779974                      Error creating: pods "lorax-commandmanager-8645779974-xqllh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-commandmanager-8645779974                      Error creating: pods "lorax-commandmanager-8645779974-v679j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m37s       Warning   FailedCreate                   replicaset/lorax-commandmanager-8645779974                      Error creating: pods "lorax-commandmanager-8645779974-6nwf9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-commandmanager-autoscaler         unable to get metrics for resource memory: no metrics returned from resource metrics API
19m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-commandmanager-autoscaler         unable to get metrics for resource memory: no metrics returned from resource metrics API
34m         Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-commandmanager-autoscaler         unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
34m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-commandmanager-autoscaler         unable to get metrics for resource cpu: no metrics returned from resource metrics API
36m         Warning   FailedComputeMetricsReplicas   horizontalpodautoscaler/lorax-commandmanager-autoscaler         invalid metrics (3 invalid out of 3), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API
4m32s       Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-commandmanager-autoscaler         unable to get metrics for resource memory: no metrics returned from resource metrics API
4m32s       Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-commandmanager-autoscaler         unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
4m32s       Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-commandmanager-autoscaler         unable to get metrics for resource cpu: no metrics returned from resource metrics API
6m20s       Warning   FailedComputeMetricsReplicas   horizontalpodautoscaler/lorax-commandmanager-autoscaler         invalid metrics (3 invalid out of 3), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API
44m         Warning   FailedCreate                   replicaset/lorax-commandmanager-d87887f76                       Error creating: pods "lorax-commandmanager-d87887f76-2ls4c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate                   replicaset/lorax-commandmanager-d87887f76                       Error creating: pods "lorax-commandmanager-d87887f76-hxpz5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate                   replicaset/lorax-commandmanager-d87887f76                       Error creating: pods "lorax-commandmanager-d87887f76-gdbqk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Normal    ScalingReplicaSet              deployment/lorax-commandmanager                                 Scaled up replica set lorax-commandmanager-5ddf8f65c4 to 2
16m         Normal    ScalingReplicaSet              deployment/lorax-commandmanager                                 Scaled up replica set lorax-commandmanager-79c8bb6b6 to 2
44m         Warning   FailedCreate                   replicaset/lorax-contextmanager-5bb7cf754                       Error creating: pods "lorax-contextmanager-5bb7cf754-mwnn7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate                   replicaset/lorax-contextmanager-5bb7cf754                       Error creating: pods "lorax-contextmanager-5bb7cf754-9wsmh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate                   replicaset/lorax-contextmanager-5bb7cf754                       Error creating: pods "lorax-contextmanager-5bb7cf754-tl4jl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-contextmanager-5d6b544f46                      Error creating: pods "lorax-contextmanager-5d6b544f46-gtrdp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-contextmanager-5d6b544f46                      Error creating: pods "lorax-contextmanager-5d6b544f46-q4bjl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-contextmanager-5d6b544f46                      Error creating: pods "lorax-contextmanager-5d6b544f46-lnzmt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
45m         Warning   FailedCreate                   replicaset/lorax-contextmanager-676d9f4b96                      Error creating: pods "lorax-contextmanager-676d9f4b96-bckrp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-contextmanager-676d9f4b96                      Error creating: pods "lorax-contextmanager-676d9f4b96-xblnt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
12m         Warning   FailedCreate                   replicaset/lorax-contextmanager-676d9f4b96                      Error creating: pods "lorax-contextmanager-676d9f4b96-n8tb7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedMount                    pod/lorax-contextmanager-67956c7584-c4482                       MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
17m         Warning   FailedMount                    pod/lorax-contextmanager-67956c7584-c4xkw                       MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
17m         Warning   FailedMount                    pod/lorax-contextmanager-67956c7584-h82q5                       MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
54m         Warning   FailedCreate                   replicaset/lorax-contextmanager-6b847c88b                       Error creating: pods "lorax-contextmanager-6b847c88b-7brlr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/lorax-contextmanager-6b847c88b                       Error creating: pods "lorax-contextmanager-6b847c88b-4m754" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-contextmanager-6b847c88b                       Error creating: pods "lorax-contextmanager-6b847c88b-nkvrv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m57s       Warning   FailedCreate                   replicaset/lorax-contextmanager-6b847c88b                       Error creating: pods "lorax-contextmanager-6b847c88b-qmqx9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-contextmanager-6c58b5fbcb                      Error creating: pods "lorax-contextmanager-6c58b5fbcb-psqxk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-contextmanager-6c58b5fbcb                      Error creating: pods "lorax-contextmanager-6c58b5fbcb-2ncbb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-contextmanager-6c58b5fbcb                      Error creating: pods "lorax-contextmanager-6c58b5fbcb-pgqwk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m9s        Warning   FailedCreate                   replicaset/lorax-contextmanager-6c58b5fbcb                      Error creating: pods "lorax-contextmanager-6c58b5fbcb-m2xcv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate                   replicaset/lorax-contextmanager-6d6d7594bd                      Error creating: pods "lorax-contextmanager-6d6d7594bd-45w86" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/lorax-contextmanager-6d6d7594bd                      Error creating: pods "lorax-contextmanager-6d6d7594bd-fbd4b" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-contextmanager-6d6d7594bd                      Error creating: pods "lorax-contextmanager-6d6d7594bd-8jnzt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m40s       Warning   FailedCreate                   replicaset/lorax-contextmanager-6d6d7594bd                      Error creating: pods "lorax-contextmanager-6d6d7594bd-vxdzc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-contextmanager-6dc9f7cf84                      Error creating: pods "lorax-contextmanager-6dc9f7cf84-xc2lb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
35m         Warning   FailedCreate                   replicaset/lorax-contextmanager-6dc9f7cf84                      Error creating: pods "lorax-contextmanager-6dc9f7cf84-f58s9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-contextmanager-6dc9f7cf84                      Error creating: pods "lorax-contextmanager-6dc9f7cf84-9zdbp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
111s        Warning   FailedCreate                   replicaset/lorax-contextmanager-6dc9f7cf84                      Error creating: pods "lorax-contextmanager-6dc9f7cf84-svcf4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-contextmanager-6fb65d9f4d                      Error creating: pods "lorax-contextmanager-6fb65d9f4d-lh8qp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-contextmanager-6fb65d9f4d                      Error creating: pods "lorax-contextmanager-6fb65d9f4d-6jgqf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-contextmanager-6fb65d9f4d                      Error creating: pods "lorax-contextmanager-6fb65d9f4d-k9h52" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m4s        Warning   FailedCreate                   replicaset/lorax-contextmanager-6fb65d9f4d                      Error creating: pods "lorax-contextmanager-6fb65d9f4d-7dvfb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-contextmanager-754fc85d44                      Error creating: pods "lorax-contextmanager-754fc85d44-cn28q" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate                   replicaset/lorax-contextmanager-754fc85d44                      Error creating: pods "lorax-contextmanager-754fc85d44-fp7wf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-contextmanager-754fc85d44                      Error creating: pods "lorax-contextmanager-754fc85d44-thljf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
87s         Warning   FailedCreate                   replicaset/lorax-contextmanager-754fc85d44                      Error creating: pods "lorax-contextmanager-754fc85d44-f96rl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-contextmanager-7d6c885588                      Error creating: pods "lorax-contextmanager-7d6c885588-cjm97" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-contextmanager-7d6c885588                      Error creating: pods "lorax-contextmanager-7d6c885588-4qqgw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-contextmanager-7d6c885588                      Error creating: pods "lorax-contextmanager-7d6c885588-hhfsm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-contextmanager-7d6c885588                      Error creating: pods "lorax-contextmanager-7d6c885588-69cs4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-contextmanager-7d6c885588                      Error creating: pods "lorax-contextmanager-7d6c885588-xhlx5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-contextmanager-7d6c885588                      Error creating: pods "lorax-contextmanager-7d6c885588-v2mgg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-contextmanager-7d6c885588                      Error creating: pods "lorax-contextmanager-7d6c885588-p96sq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-contextmanager-7d6c885588                      Error creating: pods "lorax-contextmanager-7d6c885588-fjwzm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-contextmanager-7d6c885588                      Error creating: pods "lorax-contextmanager-7d6c885588-mt75w" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
5m35s       Warning   FailedCreate                   replicaset/lorax-contextmanager-7d6c885588                      (combined from similar events): Error creating: pods "lorax-contextmanager-7d6c885588-krcz4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-contextmanager-autoscaler         unable to get metrics for resource memory: no metrics returned from resource metrics API
24m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-contextmanager-autoscaler         unable to get metrics for resource memory: no metrics returned from resource metrics API
33m         Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-contextmanager-autoscaler         unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
33m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-contextmanager-autoscaler         unable to get metrics for resource cpu: no metrics returned from resource metrics API
35m         Warning   FailedComputeMetricsReplicas   horizontalpodautoscaler/lorax-contextmanager-autoscaler         invalid metrics (3 invalid out of 3), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API
4m15s       Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-contextmanager-autoscaler         unable to get metrics for resource memory: no metrics returned from resource metrics API
4m15s       Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-contextmanager-autoscaler         unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
4m15s       Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-contextmanager-autoscaler         unable to get metrics for resource cpu: no metrics returned from resource metrics API
6m6s        Warning   FailedComputeMetricsReplicas   horizontalpodautoscaler/lorax-contextmanager-autoscaler         invalid metrics (3 invalid out of 3), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API
48m         Warning   FailedCreate                   replicaset/lorax-contextmanager-d5dd69566                       Error creating: pods "lorax-contextmanager-d5dd69566-7nv2d" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-contextmanager-d5dd69566                       Error creating: pods "lorax-contextmanager-d5dd69566-lgp72" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
15m         Warning   FailedCreate                   replicaset/lorax-contextmanager-d5dd69566                       Error creating: pods "lorax-contextmanager-d5dd69566-m9cjh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-contextmanager-f747f598d                       Error creating: pods "lorax-contextmanager-f747f598d-sjnv5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-contextmanager-f747f598d                       Error creating: pods "lorax-contextmanager-f747f598d-mvfk4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate                   replicaset/lorax-contextmanager-f747f598d                       Error creating: pods "lorax-contextmanager-f747f598d-988mj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-contextmanager-f7dd6d88d                       Error creating: pods "lorax-contextmanager-f7dd6d88d-xdrg7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-contextmanager-f7dd6d88d                       Error creating: pods "lorax-contextmanager-f7dd6d88d-6zh9x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-contextmanager-f7dd6d88d                       Error creating: pods "lorax-contextmanager-f7dd6d88d-wqztl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-contextmanager-f7dd6d88d                       Error creating: pods "lorax-contextmanager-f7dd6d88d-9zvt6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-contextmanager-f7dd6d88d                       Error creating: pods "lorax-contextmanager-f7dd6d88d-ccjfc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-contextmanager-f7dd6d88d                       Error creating: pods "lorax-contextmanager-f7dd6d88d-fvvtk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-contextmanager-f7dd6d88d                       Error creating: pods "lorax-contextmanager-f7dd6d88d-5k6nf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-contextmanager-f7dd6d88d                       Error creating: pods "lorax-contextmanager-f7dd6d88d-8vkpk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-contextmanager-f7dd6d88d                       Error creating: pods "lorax-contextmanager-f7dd6d88d-6dr25" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate                   replicaset/lorax-contextmanager-f7dd6d88d                       (combined from similar events): Error creating: pods "lorax-contextmanager-f7dd6d88d-f5xbc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
25m         Warning   FailedCreate                   replicaset/lorax-contextmanager-f7dd6d88d                       Error creating: pods "lorax-contextmanager-f7dd6d88d-vd2fq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m24s       Warning   FailedCreate                   replicaset/lorax-contextmanager-f7dd6d88d                       Error creating: pods "lorax-contextmanager-f7dd6d88d-w7f6t" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Normal    ScalingReplicaSet              deployment/lorax-contextmanager                                 Scaled up replica set lorax-contextmanager-f7dd6d88d to 3
16m         Normal    ScalingReplicaSet              deployment/lorax-contextmanager                                 Scaled up replica set lorax-contextmanager-7d6c885588 to 3
58m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-54bd59d66                 Error creating: pods "lorax-customizationmanager-54bd59d66-tv4bd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-54bd59d66                 Error creating: pods "lorax-customizationmanager-54bd59d66-h7x4h" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-54bd59d66                 Error creating: pods "lorax-customizationmanager-54bd59d66-x2bkf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m7s        Warning   FailedCreate                   replicaset/lorax-customizationmanager-54bd59d66                 Error creating: pods "lorax-customizationmanager-54bd59d66-27z6t" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-6486f894d4                Error creating: pods "lorax-customizationmanager-6486f894d4-m4bj6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-6486f894d4                Error creating: pods "lorax-customizationmanager-6486f894d4-qbmng" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-6486f894d4                Error creating: pods "lorax-customizationmanager-6486f894d4-gdlx6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedMount                    pod/lorax-customizationmanager-657bbf9586-h242p                 MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
18m         Warning   FailedMount                    pod/lorax-customizationmanager-657bbf9586-rsbqg                 MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
54m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-6cb5947b6                 Error creating: pods "lorax-customizationmanager-6cb5947b6-w9wgd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-6cb5947b6                 Error creating: pods "lorax-customizationmanager-6cb5947b6-dvjrn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-6cb5947b6                 Error creating: pods "lorax-customizationmanager-6cb5947b6-8brmw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m3s        Warning   FailedCreate                   replicaset/lorax-customizationmanager-6cb5947b6                 Error creating: pods "lorax-customizationmanager-6cb5947b6-6z6gl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-7566984c4                 Error creating: pods "lorax-customizationmanager-7566984c4-bmtss" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-7566984c4                 Error creating: pods "lorax-customizationmanager-7566984c4-dcwc2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-7566984c4                 Error creating: pods "lorax-customizationmanager-7566984c4-cccvx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-7566984c4                 Error creating: pods "lorax-customizationmanager-7566984c4-29qqw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-7566984c4                 Error creating: pods "lorax-customizationmanager-7566984c4-6slrw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-7566984c4                 Error creating: pods "lorax-customizationmanager-7566984c4-twcch" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-7566984c4                 Error creating: pods "lorax-customizationmanager-7566984c4-qx89k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-7566984c4                 Error creating: pods "lorax-customizationmanager-7566984c4-kqgwh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-7566984c4                 Error creating: pods "lorax-customizationmanager-7566984c4-8pmms" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
5m33s       Warning   FailedCreate                   replicaset/lorax-customizationmanager-7566984c4                 (combined from similar events): Error creating: pods "lorax-customizationmanager-7566984c4-zrlcq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-76c587676d                Error creating: pods "lorax-customizationmanager-76c587676d-jtn4p" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-76c587676d                Error creating: pods "lorax-customizationmanager-76c587676d-qnzvh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-76c587676d                Error creating: pods "lorax-customizationmanager-76c587676d-s896z" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-7b5c9fd99d                Error creating: pods "lorax-customizationmanager-7b5c9fd99d-7z2rj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-7b5c9fd99d                Error creating: pods "lorax-customizationmanager-7b5c9fd99d-7b62q" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-7b5c9fd99d                Error creating: pods "lorax-customizationmanager-7b5c9fd99d-bz6rc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
87s         Warning   FailedCreate                   replicaset/lorax-customizationmanager-7b5c9fd99d                Error creating: pods "lorax-customizationmanager-7b5c9fd99d-rfh7v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-7ccf74fbd4                Error creating: pods "lorax-customizationmanager-7ccf74fbd4-dwjlb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-7ccf74fbd4                Error creating: pods "lorax-customizationmanager-7ccf74fbd4-j8ldz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
15m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-7ccf74fbd4                Error creating: pods "lorax-customizationmanager-7ccf74fbd4-8z5l5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-8474f464b6                Error creating: pods "lorax-customizationmanager-8474f464b6-6k22v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-8474f464b6                Error creating: pods "lorax-customizationmanager-8474f464b6-pd8tp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-8474f464b6                Error creating: pods "lorax-customizationmanager-8474f464b6-chkk9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-8474f464b6                Error creating: pods "lorax-customizationmanager-8474f464b6-vmd6m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-8474f464b6                Error creating: pods "lorax-customizationmanager-8474f464b6-pvh6m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-8474f464b6                Error creating: pods "lorax-customizationmanager-8474f464b6-bs7k9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-8474f464b6                Error creating: pods "lorax-customizationmanager-8474f464b6-mqwhp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-8474f464b6                Error creating: pods "lorax-customizationmanager-8474f464b6-xd9bt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-8474f464b6                Error creating: pods "lorax-customizationmanager-8474f464b6-n22zz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
35m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-8474f464b6                (combined from similar events): Error creating: pods "lorax-customizationmanager-8474f464b6-jw8c6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
25m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-8474f464b6                Error creating: pods "lorax-customizationmanager-8474f464b6-4vpd7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m23s       Warning   FailedCreate                   replicaset/lorax-customizationmanager-8474f464b6                Error creating: pods "lorax-customizationmanager-8474f464b6-2q7mk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
55m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-84f9b9f66d                Error creating: pods "lorax-customizationmanager-84f9b9f66d-wwrtp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-84f9b9f66d                Error creating: pods "lorax-customizationmanager-84f9b9f66d-n4lsx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-84f9b9f66d                Error creating: pods "lorax-customizationmanager-84f9b9f66d-j4pm6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m58s       Warning   FailedCreate                   replicaset/lorax-customizationmanager-84f9b9f66d                Error creating: pods "lorax-customizationmanager-84f9b9f66d-k6rdt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-86b9c84b5d                Error creating: pods "lorax-customizationmanager-86b9c84b5d-jmbdm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-86b9c84b5d                Error creating: pods "lorax-customizationmanager-86b9c84b5d-ngqqc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-86b9c84b5d                Error creating: pods "lorax-customizationmanager-86b9c84b5d-gc5nl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m56s       Warning   FailedCreate                   replicaset/lorax-customizationmanager-86b9c84b5d                Error creating: pods "lorax-customizationmanager-86b9c84b5d-wvgpv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
45m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-8788f7f66                 Error creating: pods "lorax-customizationmanager-8788f7f66-h7qnt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-8788f7f66                 Error creating: pods "lorax-customizationmanager-8788f7f66-v74mf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
12m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-8788f7f66                 Error creating: pods "lorax-customizationmanager-8788f7f66-n7khl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-customizationmanager-autoscaler   unable to get metrics for resource memory: no metrics returned from resource metrics API
19m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-customizationmanager-autoscaler   unable to get metrics for resource memory: no metrics returned from resource metrics API
34m         Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-customizationmanager-autoscaler   unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
34m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-customizationmanager-autoscaler   unable to get metrics for resource cpu: no metrics returned from resource metrics API
36m         Warning   FailedComputeMetricsReplicas   horizontalpodautoscaler/lorax-customizationmanager-autoscaler   invalid metrics (3 invalid out of 3), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API
4m26s       Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-customizationmanager-autoscaler   unable to get metrics for resource memory: no metrics returned from resource metrics API
4m26s       Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-customizationmanager-autoscaler   unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
4m26s       Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-customizationmanager-autoscaler   unable to get metrics for resource cpu: no metrics returned from resource metrics API
6m18s       Warning   FailedComputeMetricsReplicas   horizontalpodautoscaler/lorax-customizationmanager-autoscaler   invalid metrics (3 invalid out of 3), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API
48m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-bb8d7f986                 Error creating: pods "lorax-customizationmanager-bb8d7f986-jk2fg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-bb8d7f986                 Error creating: pods "lorax-customizationmanager-bb8d7f986-5nrzx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-bb8d7f986                 Error creating: pods "lorax-customizationmanager-bb8d7f986-vsxp7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-cfbf8558                  Error creating: pods "lorax-customizationmanager-cfbf8558-5kjw7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
35m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-cfbf8558                  Error creating: pods "lorax-customizationmanager-cfbf8558-wkpbp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-customizationmanager-cfbf8558                  Error creating: pods "lorax-customizationmanager-cfbf8558-w9j6g" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
111s        Warning   FailedCreate                   replicaset/lorax-customizationmanager-cfbf8558                  Error creating: pods "lorax-customizationmanager-cfbf8558-7db45" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Normal    ScalingReplicaSet              deployment/lorax-customizationmanager                           Scaled up replica set lorax-customizationmanager-8474f464b6 to 2
16m         Normal    ScalingReplicaSet              deployment/lorax-customizationmanager                           Scaled up replica set lorax-customizationmanager-7566984c4 to 2
51m         Warning   FailedCreate                   replicaset/lorax-elementmanager-5b6bfc5664                      Error creating: pods "lorax-elementmanager-5b6bfc5664-hx9fc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
35m         Warning   FailedCreate                   replicaset/lorax-elementmanager-5b6bfc5664                      Error creating: pods "lorax-elementmanager-5b6bfc5664-g49q9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-elementmanager-5b6bfc5664                      Error creating: pods "lorax-elementmanager-5b6bfc5664-gqfjk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
114s        Warning   FailedCreate                   replicaset/lorax-elementmanager-5b6bfc5664                      Error creating: pods "lorax-elementmanager-5b6bfc5664-ggg9q" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
45m         Warning   FailedCreate                   replicaset/lorax-elementmanager-6dd86c7dcb                      Error creating: pods "lorax-elementmanager-6dd86c7dcb-rc455" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-elementmanager-6dd86c7dcb                      Error creating: pods "lorax-elementmanager-6dd86c7dcb-8gvxk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
12m         Warning   FailedCreate                   replicaset/lorax-elementmanager-6dd86c7dcb                      Error creating: pods "lorax-elementmanager-6dd86c7dcb-vlrrb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-elementmanager-765f8845c8                      Error creating: pods "lorax-elementmanager-765f8845c8-msnwt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate                   replicaset/lorax-elementmanager-765f8845c8                      Error creating: pods "lorax-elementmanager-765f8845c8-flcc2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate                   replicaset/lorax-elementmanager-765f8845c8                      Error creating: pods "lorax-elementmanager-765f8845c8-hfwdd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-elementmanager-777b868f84                      Error creating: pods "lorax-elementmanager-777b868f84-l56fj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-elementmanager-777b868f84                      Error creating: pods "lorax-elementmanager-777b868f84-tmv89" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-elementmanager-777b868f84                      Error creating: pods "lorax-elementmanager-777b868f84-bs8rr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-elementmanager-777b868f84                      Error creating: pods "lorax-elementmanager-777b868f84-2pjcs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-elementmanager-777b868f84                      Error creating: pods "lorax-elementmanager-777b868f84-zbjrk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-elementmanager-777b868f84                      Error creating: pods "lorax-elementmanager-777b868f84-jd27s" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-elementmanager-777b868f84                      Error creating: pods "lorax-elementmanager-777b868f84-4jbrb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-elementmanager-777b868f84                      Error creating: pods "lorax-elementmanager-777b868f84-lffcf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-elementmanager-777b868f84                      Error creating: pods "lorax-elementmanager-777b868f84-ftg4l" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
5m37s       Warning   FailedCreate                   replicaset/lorax-elementmanager-777b868f84                      (combined from similar events): Error creating: pods "lorax-elementmanager-777b868f84-2bvcp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-elementmanager-79cfbcf654                      Error creating: pods "lorax-elementmanager-79cfbcf654-dn6xr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-elementmanager-79cfbcf654                      Error creating: pods "lorax-elementmanager-79cfbcf654-h6fk9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
15m         Warning   FailedCreate                   replicaset/lorax-elementmanager-79cfbcf654                      Error creating: pods "lorax-elementmanager-79cfbcf654-gzxhx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate                   replicaset/lorax-elementmanager-7c4fc5c74d                      Error creating: pods "lorax-elementmanager-7c4fc5c74d-xkt9x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate                   replicaset/lorax-elementmanager-7c4fc5c74d                      Error creating: pods "lorax-elementmanager-7c4fc5c74d-zt7sx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-elementmanager-7c4fc5c74d                      Error creating: pods "lorax-elementmanager-7c4fc5c74d-px588" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m37s       Warning   FailedCreate                   replicaset/lorax-elementmanager-7c4fc5c74d                      Error creating: pods "lorax-elementmanager-7c4fc5c74d-gfczv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-elementmanager-7f44848cfb                      Error creating: pods "lorax-elementmanager-7f44848cfb-694jm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-elementmanager-7f44848cfb                      Error creating: pods "lorax-elementmanager-7f44848cfb-mz4mw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate                   replicaset/lorax-elementmanager-7f44848cfb                      Error creating: pods "lorax-elementmanager-7f44848cfb-v7x7b" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedMount                    pod/lorax-elementmanager-7f74646f4d-n4cqx                       MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
47m         Warning   FailedCreate                   replicaset/lorax-elementmanager-84bbf749bd                      Error creating: pods "lorax-elementmanager-84bbf749bd-8c9ct" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-elementmanager-84bbf749bd                      Error creating: pods "lorax-elementmanager-84bbf749bd-tgn4c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-elementmanager-84bbf749bd                      Error creating: pods "lorax-elementmanager-84bbf749bd-6lslw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-elementmanager-84bbf749bd                      Error creating: pods "lorax-elementmanager-84bbf749bd-q2w84" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-elementmanager-84bbf749bd                      Error creating: pods "lorax-elementmanager-84bbf749bd-2xxr2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-elementmanager-84bbf749bd                      Error creating: pods "lorax-elementmanager-84bbf749bd-cx768" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-elementmanager-84bbf749bd                      Error creating: pods "lorax-elementmanager-84bbf749bd-n9gzx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-elementmanager-84bbf749bd                      Error creating: pods "lorax-elementmanager-84bbf749bd-kprwt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-elementmanager-84bbf749bd                      Error creating: pods "lorax-elementmanager-84bbf749bd-cf56p" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate                   replicaset/lorax-elementmanager-84bbf749bd                      (combined from similar events): Error creating: pods "lorax-elementmanager-84bbf749bd-tvss8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
25m         Warning   FailedCreate                   replicaset/lorax-elementmanager-84bbf749bd                      Error creating: pods "lorax-elementmanager-84bbf749bd-gfl9s" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m24s       Warning   FailedCreate                   replicaset/lorax-elementmanager-84bbf749bd                      Error creating: pods "lorax-elementmanager-84bbf749bd-hrmcv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-elementmanager-85575658b8                      Error creating: pods "lorax-elementmanager-85575658b8-g7spk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate                   replicaset/lorax-elementmanager-85575658b8                      Error creating: pods "lorax-elementmanager-85575658b8-rgdhp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-elementmanager-85575658b8                      Error creating: pods "lorax-elementmanager-85575658b8-bzkk9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
89s         Warning   FailedCreate                   replicaset/lorax-elementmanager-85575658b8                      Error creating: pods "lorax-elementmanager-85575658b8-vc7kn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-elementmanager-868c7dbb56                      Error creating: pods "lorax-elementmanager-868c7dbb56-4rg9z" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-elementmanager-868c7dbb56                      Error creating: pods "lorax-elementmanager-868c7dbb56-8glbf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-elementmanager-868c7dbb56                      Error creating: pods "lorax-elementmanager-868c7dbb56-k89x4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m56s       Warning   FailedCreate                   replicaset/lorax-elementmanager-868c7dbb56                      Error creating: pods "lorax-elementmanager-868c7dbb56-bhqmb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
55m         Warning   FailedCreate                   replicaset/lorax-elementmanager-b65cb6dc4                       Error creating: pods "lorax-elementmanager-b65cb6dc4-hz4xn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/lorax-elementmanager-b65cb6dc4                       Error creating: pods "lorax-elementmanager-b65cb6dc4-t8svp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-elementmanager-b65cb6dc4                       Error creating: pods "lorax-elementmanager-b65cb6dc4-jv2tj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m59s       Warning   FailedCreate                   replicaset/lorax-elementmanager-b65cb6dc4                       Error creating: pods "lorax-elementmanager-b65cb6dc4-wltfz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-elementmanager-cd8895564                       Error creating: pods "lorax-elementmanager-cd8895564-c8g2j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-elementmanager-cd8895564                       Error creating: pods "lorax-elementmanager-cd8895564-7tjp4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-elementmanager-cd8895564                       Error creating: pods "lorax-elementmanager-cd8895564-wpl7t" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m12s       Warning   FailedCreate                   replicaset/lorax-elementmanager-cd8895564                       Error creating: pods "lorax-elementmanager-cd8895564-bb5kb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-elementmanager-df8dc55db                       Error creating: pods "lorax-elementmanager-df8dc55db-6rst8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-elementmanager-df8dc55db                       Error creating: pods "lorax-elementmanager-df8dc55db-vv6tp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-elementmanager-df8dc55db                       Error creating: pods "lorax-elementmanager-df8dc55db-jvl7j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Normal    ScalingReplicaSet              deployment/lorax-elementmanager                                 Scaled up replica set lorax-elementmanager-84bbf749bd to 1
16m         Normal    ScalingReplicaSet              deployment/lorax-elementmanager                                 Scaled up replica set lorax-elementmanager-777b868f84 to 1
51m         Warning   FailedCreate                   replicaset/lorax-eventmanager-5b59b586fb                        Error creating: pods "lorax-eventmanager-5b59b586fb-2pspv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
35m         Warning   FailedCreate                   replicaset/lorax-eventmanager-5b59b586fb                        Error creating: pods "lorax-eventmanager-5b59b586fb-5q9qf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-eventmanager-5b59b586fb                        Error creating: pods "lorax-eventmanager-5b59b586fb-46jdx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
112s        Warning   FailedCreate                   replicaset/lorax-eventmanager-5b59b586fb                        Error creating: pods "lorax-eventmanager-5b59b586fb-22rmw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-eventmanager-679fd7fdd6                        Error creating: pods "lorax-eventmanager-679fd7fdd6-r4b7x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-eventmanager-679fd7fdd6                        Error creating: pods "lorax-eventmanager-679fd7fdd6-6sb4n" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
15m         Warning   FailedCreate                   replicaset/lorax-eventmanager-679fd7fdd6                        Error creating: pods "lorax-eventmanager-679fd7fdd6-mmgws" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedMount                    pod/lorax-eventmanager-759c5b9484-ch28f                         MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
18m         Warning   FailedMount                    pod/lorax-eventmanager-759c5b9484-xfl42                         MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
4m2s        Normal    Killing                        pod/lorax-eventmanager-759c5b9484-xfl42                         Stopping container vault-agent
4m2s        Normal    Killing                        pod/lorax-eventmanager-759c5b9484-xfl42                         Stopping container server
4m1s        Warning   FailedCreate                   replicaset/lorax-eventmanager-759c5b9484                        Error creating: pods "lorax-eventmanager-759c5b9484-rf52t" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m59s       Warning   FailedCreate                   replicaset/lorax-eventmanager-759c5b9484                        Error creating: pods "lorax-eventmanager-759c5b9484-hg9mf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m58s       Warning   FailedCreate                   replicaset/lorax-eventmanager-759c5b9484                        Error creating: pods "lorax-eventmanager-759c5b9484-pl556" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m56s       Warning   FailedCreate                   replicaset/lorax-eventmanager-759c5b9484                        Error creating: pods "lorax-eventmanager-759c5b9484-fw2vw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m54s       Warning   FailedCreate                   replicaset/lorax-eventmanager-759c5b9484                        Error creating: pods "lorax-eventmanager-759c5b9484-c6rzw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m51s       Warning   FailedCreate                   replicaset/lorax-eventmanager-759c5b9484                        Error creating: pods "lorax-eventmanager-759c5b9484-ddl6q" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m47s       Warning   FailedCreate                   replicaset/lorax-eventmanager-759c5b9484                        Error creating: pods "lorax-eventmanager-759c5b9484-54fwp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m46s       Warning   FailedCreate                   replicaset/lorax-eventmanager-759c5b9484                        Error creating: pods "lorax-eventmanager-759c5b9484-hmwkp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m44s       Warning   FailedCreate                   replicaset/lorax-eventmanager-759c5b9484                        Error creating: pods "lorax-eventmanager-759c5b9484-45486" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
2m1s        Warning   FailedCreate                   replicaset/lorax-eventmanager-759c5b9484                        (combined from similar events): Error creating: pods "lorax-eventmanager-759c5b9484-jq2fz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-eventmanager-7755588b84                        Error creating: pods "lorax-eventmanager-7755588b84-sp45w" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-eventmanager-7755588b84                        Error creating: pods "lorax-eventmanager-7755588b84-pxbn9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-eventmanager-7755588b84                        Error creating: pods "lorax-eventmanager-7755588b84-mtmnx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m4s        Warning   FailedCreate                   replicaset/lorax-eventmanager-7755588b84                        Error creating: pods "lorax-eventmanager-7755588b84-t94ld" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-eventmanager-795bb876b8                        Error creating: pods "lorax-eventmanager-795bb876b8-s8vj4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-eventmanager-795bb876b8                        Error creating: pods "lorax-eventmanager-795bb876b8-hhfg6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-eventmanager-795bb876b8                        Error creating: pods "lorax-eventmanager-795bb876b8-lt8qk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m12s       Warning   FailedCreate                   replicaset/lorax-eventmanager-795bb876b8                        Error creating: pods "lorax-eventmanager-795bb876b8-86xjz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-eventmanager-79849f6bb                         Error creating: pods "lorax-eventmanager-79849f6bb-lxgw2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate                   replicaset/lorax-eventmanager-79849f6bb                         Error creating: pods "lorax-eventmanager-79849f6bb-725hf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-eventmanager-79849f6bb                         Error creating: pods "lorax-eventmanager-79849f6bb-6n5c9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-eventmanager-7c8946c76d                        Error creating: pods "lorax-eventmanager-7c8946c76d-njlkf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-eventmanager-7c8946c76d                        Error creating: pods "lorax-eventmanager-7c8946c76d-4gxrv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate                   replicaset/lorax-eventmanager-7c8946c76d                        Error creating: pods "lorax-eventmanager-7c8946c76d-hp2h4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-eventmanager-7fd6c57c6d                        Error creating: pods "lorax-eventmanager-7fd6c57c6d-m445w" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-eventmanager-7fd6c57c6d                        Error creating: pods "lorax-eventmanager-7fd6c57c6d-wgc2d" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-eventmanager-7fd6c57c6d                        Error creating: pods "lorax-eventmanager-7fd6c57c6d-k6tzk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-eventmanager-7fd6c57c6d                        Error creating: pods "lorax-eventmanager-7fd6c57c6d-nmgdz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-eventmanager-7fd6c57c6d                        Error creating: pods "lorax-eventmanager-7fd6c57c6d-srdp8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-eventmanager-7fd6c57c6d                        Error creating: pods "lorax-eventmanager-7fd6c57c6d-zsv49" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-eventmanager-7fd6c57c6d                        Error creating: pods "lorax-eventmanager-7fd6c57c6d-9lcgc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-eventmanager-7fd6c57c6d                        Error creating: pods "lorax-eventmanager-7fd6c57c6d-d4mnr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-eventmanager-7fd6c57c6d                        Error creating: pods "lorax-eventmanager-7fd6c57c6d-9md9x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
5m35s       Warning   FailedCreate                   replicaset/lorax-eventmanager-7fd6c57c6d                        (combined from similar events): Error creating: pods "lorax-eventmanager-7fd6c57c6d-wbspt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-eventmanager-968787676                         Error creating: pods "lorax-eventmanager-968787676-xvwbq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate                   replicaset/lorax-eventmanager-968787676                         Error creating: pods "lorax-eventmanager-968787676-wpd22" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-eventmanager-968787676                         Error creating: pods "lorax-eventmanager-968787676-v4fr8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
89s         Warning   FailedCreate                   replicaset/lorax-eventmanager-968787676                         Error creating: pods "lorax-eventmanager-968787676-dr79f" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
45m         Warning   FailedCreate                   replicaset/lorax-eventmanager-9ff868d                           Error creating: pods "lorax-eventmanager-9ff868d-j9z76" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-eventmanager-9ff868d                           Error creating: pods "lorax-eventmanager-9ff868d-qw948" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-eventmanager-9ff868d                           Error creating: pods "lorax-eventmanager-9ff868d-q6mq8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-eventmanager-autoscaler           unable to get metrics for resource memory: no metrics returned from resource metrics API
20m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-eventmanager-autoscaler           unable to get metrics for resource memory: no metrics returned from resource metrics API
33m         Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-eventmanager-autoscaler           unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
33m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-eventmanager-autoscaler           unable to get metrics for resource cpu: no metrics returned from resource metrics API
35m         Warning   FailedComputeMetricsReplicas   horizontalpodautoscaler/lorax-eventmanager-autoscaler           invalid metrics (3 invalid out of 3), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API
4m16s       Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-eventmanager-autoscaler           unable to get metrics for resource memory: no metrics returned from resource metrics API
4m16s       Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-eventmanager-autoscaler           unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
4m16s       Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-eventmanager-autoscaler           unable to get metrics for resource cpu: no metrics returned from resource metrics API
6m7s        Warning   FailedComputeMetricsReplicas   horizontalpodautoscaler/lorax-eventmanager-autoscaler           invalid metrics (3 invalid out of 3), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API
47m         Warning   FailedCreate                   replicaset/lorax-eventmanager-b895c6d44                         Error creating: pods "lorax-eventmanager-b895c6d44-42vd7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-eventmanager-b895c6d44                         Error creating: pods "lorax-eventmanager-b895c6d44-hkv44" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-eventmanager-b895c6d44                         Error creating: pods "lorax-eventmanager-b895c6d44-qmv5c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-eventmanager-b895c6d44                         Error creating: pods "lorax-eventmanager-b895c6d44-tw8n5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-eventmanager-b895c6d44                         Error creating: pods "lorax-eventmanager-b895c6d44-xm7rq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-eventmanager-b895c6d44                         Error creating: pods "lorax-eventmanager-b895c6d44-c4dj2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-eventmanager-b895c6d44                         Error creating: pods "lorax-eventmanager-b895c6d44-vpcwt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-eventmanager-b895c6d44                         Error creating: pods "lorax-eventmanager-b895c6d44-znrrs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-eventmanager-b895c6d44                         Error creating: pods "lorax-eventmanager-b895c6d44-f4ksq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
35m         Warning   FailedCreate                   replicaset/lorax-eventmanager-b895c6d44                         (combined from similar events): Error creating: pods "lorax-eventmanager-b895c6d44-tgcw8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
25m         Warning   FailedCreate                   replicaset/lorax-eventmanager-b895c6d44                         Error creating: pods "lorax-eventmanager-b895c6d44-2jg9q" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m23s       Warning   FailedCreate                   replicaset/lorax-eventmanager-b895c6d44                         Error creating: pods "lorax-eventmanager-b895c6d44-pfsnr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
55m         Warning   FailedCreate                   replicaset/lorax-eventmanager-bcb667494                         Error creating: pods "lorax-eventmanager-bcb667494-bj4q9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/lorax-eventmanager-bcb667494                         Error creating: pods "lorax-eventmanager-bcb667494-tm7c2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-eventmanager-bcb667494                         Error creating: pods "lorax-eventmanager-bcb667494-pdvsk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
5m8s        Warning   FailedCreate                   replicaset/lorax-eventmanager-bcb667494                         Error creating: pods "lorax-eventmanager-bcb667494-brn8h" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate                   replicaset/lorax-eventmanager-cb6f8df96                         Error creating: pods "lorax-eventmanager-cb6f8df96-lqzqq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate                   replicaset/lorax-eventmanager-cb6f8df96                         Error creating: pods "lorax-eventmanager-cb6f8df96-5tqp7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-eventmanager-cb6f8df96                         Error creating: pods "lorax-eventmanager-cb6f8df96-rmznx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m34s       Warning   FailedCreate                   replicaset/lorax-eventmanager-cb6f8df96                         Error creating: pods "lorax-eventmanager-cb6f8df96-pblq2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-eventmanager-d7c568f86                         Error creating: pods "lorax-eventmanager-d7c568f86-7g2b4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-eventmanager-d7c568f86                         Error creating: pods "lorax-eventmanager-d7c568f86-tlzpq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-eventmanager-d7c568f86                         Error creating: pods "lorax-eventmanager-d7c568f86-psrbp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Normal    ScalingReplicaSet              deployment/lorax-eventmanager                                   Scaled up replica set lorax-eventmanager-b895c6d44 to 2
16m         Normal    ScalingReplicaSet              deployment/lorax-eventmanager                                   Scaled up replica set lorax-eventmanager-7fd6c57c6d to 2
16m         Warning   FailedCreate                   replicaset/lorax-eventworker-5b788d465f                         Error creating: pods "lorax-eventworker-5b788d465f-tc7th" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-eventworker-5b788d465f                         Error creating: pods "lorax-eventworker-5b788d465f-pf9td" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-eventworker-5b788d465f                         Error creating: pods "lorax-eventworker-5b788d465f-js2gs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-eventworker-5b788d465f                         Error creating: pods "lorax-eventworker-5b788d465f-gdwhk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-eventworker-5b788d465f                         Error creating: pods "lorax-eventworker-5b788d465f-8jbhr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-eventworker-5b788d465f                         Error creating: pods "lorax-eventworker-5b788d465f-gv6tg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-eventworker-5b788d465f                         Error creating: pods "lorax-eventworker-5b788d465f-wtrc6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-eventworker-5b788d465f                         Error creating: pods "lorax-eventworker-5b788d465f-8bjxx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-eventworker-5b788d465f                         Error creating: pods "lorax-eventworker-5b788d465f-b75bv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
5m33s       Warning   FailedCreate                   replicaset/lorax-eventworker-5b788d465f                         (combined from similar events): Error creating: pods "lorax-eventworker-5b788d465f-7bfvp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
55m         Warning   FailedCreate                   replicaset/lorax-eventworker-5dfcc9c6bf                         Error creating: pods "lorax-eventworker-5dfcc9c6bf-dxkvg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/lorax-eventworker-5dfcc9c6bf                         Error creating: pods "lorax-eventworker-5dfcc9c6bf-5pwkq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-eventworker-5dfcc9c6bf                         Error creating: pods "lorax-eventworker-5dfcc9c6bf-qcbnx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
5m8s        Warning   FailedCreate                   replicaset/lorax-eventworker-5dfcc9c6bf                         Error creating: pods "lorax-eventworker-5dfcc9c6bf-4rl8p" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-eventworker-6876c65579                         Error creating: pods "lorax-eventworker-6876c65579-42b2b" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate                   replicaset/lorax-eventworker-6876c65579                         Error creating: pods "lorax-eventworker-6876c65579-wtttt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-eventworker-6876c65579                         Error creating: pods "lorax-eventworker-6876c65579-v4r8k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
87s         Warning   FailedCreate                   replicaset/lorax-eventworker-6876c65579                         Error creating: pods "lorax-eventworker-6876c65579-v5rb4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-eventworker-68c5d5f699                         Error creating: pods "lorax-eventworker-68c5d5f699-mn4lp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-eventworker-68c5d5f699                         Error creating: pods "lorax-eventworker-68c5d5f699-j59st" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-eventworker-68c5d5f699                         Error creating: pods "lorax-eventworker-68c5d5f699-96hf4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-eventworker-68c5d5f699                         Error creating: pods "lorax-eventworker-68c5d5f699-hfmmk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-eventworker-68c5d5f699                         Error creating: pods "lorax-eventworker-68c5d5f699-6g49n" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-eventworker-68c5d5f699                         Error creating: pods "lorax-eventworker-68c5d5f699-bqd6j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-eventworker-68c5d5f699                         Error creating: pods "lorax-eventworker-68c5d5f699-twb59" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-eventworker-68c5d5f699                         Error creating: pods "lorax-eventworker-68c5d5f699-7lbjb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-eventworker-68c5d5f699                         Error creating: pods "lorax-eventworker-68c5d5f699-csqm4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate                   replicaset/lorax-eventworker-68c5d5f699                         (combined from similar events): Error creating: pods "lorax-eventworker-68c5d5f699-whqfv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
25m         Warning   FailedCreate                   replicaset/lorax-eventworker-68c5d5f699                         Error creating: pods "lorax-eventworker-68c5d5f699-8bxqf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m26s       Warning   FailedCreate                   replicaset/lorax-eventworker-68c5d5f699                         Error creating: pods "lorax-eventworker-68c5d5f699-c9wqr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
45m         Warning   FailedCreate                   replicaset/lorax-eventworker-69db9b44b7                         Error creating: pods "lorax-eventworker-69db9b44b7-2phh2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-eventworker-69db9b44b7                         Error creating: pods "lorax-eventworker-69db9b44b7-w98md" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
12m         Warning   FailedCreate                   replicaset/lorax-eventworker-69db9b44b7                         Error creating: pods "lorax-eventworker-69db9b44b7-4wq5v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-eventworker-766699cdf                          Error creating: pods "lorax-eventworker-766699cdf-n89fz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate                   replicaset/lorax-eventworker-766699cdf                          Error creating: pods "lorax-eventworker-766699cdf-5xcdc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate                   replicaset/lorax-eventworker-766699cdf                          Error creating: pods "lorax-eventworker-766699cdf-s96t5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-eventworker-78bdc448bf                         Error creating: pods "lorax-eventworker-78bdc448bf-t8g8t" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-eventworker-78bdc448bf                         Error creating: pods "lorax-eventworker-78bdc448bf-mmp2k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-eventworker-78bdc448bf                         Error creating: pods "lorax-eventworker-78bdc448bf-pr86q" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-eventworker-79fbcf4895                         Error creating: pods "lorax-eventworker-79fbcf4895-fb7p9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-eventworker-79fbcf4895                         Error creating: pods "lorax-eventworker-79fbcf4895-tnzq7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-eventworker-79fbcf4895                         Error creating: pods "lorax-eventworker-79fbcf4895-96r87" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m13s       Warning   FailedCreate                   replicaset/lorax-eventworker-79fbcf4895                         Error creating: pods "lorax-eventworker-79fbcf4895-8fq4r" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedMount                    pod/lorax-eventworker-7c5b779bc-z4bb5                           MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
51m         Warning   FailedCreate                   replicaset/lorax-eventworker-84454db699                         Error creating: pods "lorax-eventworker-84454db699-jw7v2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
35m         Warning   FailedCreate                   replicaset/lorax-eventworker-84454db699                         Error creating: pods "lorax-eventworker-84454db699-nphk5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-eventworker-84454db699                         Error creating: pods "lorax-eventworker-84454db699-77k9g" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
111s        Warning   FailedCreate                   replicaset/lorax-eventworker-84454db699                         Error creating: pods "lorax-eventworker-84454db699-ld42f" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate                   replicaset/lorax-eventworker-8488fb5499                         Error creating: pods "lorax-eventworker-8488fb5499-qt9jq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate                   replicaset/lorax-eventworker-8488fb5499                         Error creating: pods "lorax-eventworker-8488fb5499-w92px" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate                   replicaset/lorax-eventworker-8488fb5499                         Error creating: pods "lorax-eventworker-8488fb5499-qlnwn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m4s        Warning   FailedCreate                   replicaset/lorax-eventworker-8488fb5499                         Error creating: pods "lorax-eventworker-8488fb5499-fl47p" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-eventworker-86d9655577                         Error creating: pods "lorax-eventworker-86d9655577-qrsd8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-eventworker-86d9655577                         Error creating: pods "lorax-eventworker-86d9655577-lt985" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
15m         Warning   FailedCreate                   replicaset/lorax-eventworker-86d9655577                         Error creating: pods "lorax-eventworker-86d9655577-r4mzr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-eventworker-8cc65fd4c                          Error creating: pods "lorax-eventworker-8cc65fd4c-rj87p" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-eventworker-8cc65fd4c                          Error creating: pods "lorax-eventworker-8cc65fd4c-jzs5v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate                   replicaset/lorax-eventworker-8cc65fd4c                          Error creating: pods "lorax-eventworker-8cc65fd4c-8hsf2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-eventworker-d68455667                          Error creating: pods "lorax-eventworker-d68455667-trrpm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-eventworker-d68455667                          Error creating: pods "lorax-eventworker-d68455667-75j42" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-eventworker-d68455667                          Error creating: pods "lorax-eventworker-d68455667-q44dw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m6s        Warning   FailedCreate                   replicaset/lorax-eventworker-d68455667                          Error creating: pods "lorax-eventworker-d68455667-d7jzp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Normal    ScalingReplicaSet              deployment/lorax-eventworker                                    Scaled up replica set lorax-eventworker-68c5d5f699 to 1
16m         Normal    ScalingReplicaSet              deployment/lorax-eventworker                                    Scaled up replica set lorax-eventworker-5b788d465f to 1
26m         Normal    Info                           configmap/lorax-lorax-dashboards                                Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-lorax-dashboards
    namespace: as-ciat
    resourceVersion: 770065263
    uid: daec5765-7483-423c-8a84-e9e1804a3af1
}: Pushed add dashboard to kafka
47m         Normal    Info                           configmap/lorax-lorax-dashboards                                Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-lorax-dashboards
    namespace: as-ciat
    resourceVersion: 770065263
    uid: daec5765-7483-423c-8a84-e9e1804a3af1
}: Pushed add dashboard to kafka
49m         Normal    Info                           configmap/lorax-lorax-dashboards                                Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-lorax-dashboards
    namespace: as-ciat
    resourceVersion: 770062558
    uid: a09b597e-0c31-491f-ab0b-2ee44e47da97
}: Pushed delete dashboard to kafka
16m         Normal    Info                           configmap/lorax-lorax-dashboards                                Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-lorax-dashboards
    namespace: as-ciat
    resourceVersion: 770115628
    uid: c8a2d11c-2a53-4c0d-8a10-b1a879be9acb
}: Pushed add dashboard to kafka
16m         Normal    Info                           configmap/lorax-lorax-dashboards                                Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-lorax-dashboards
    namespace: as-ciat
    resourceVersion: 770115628
    uid: c8a2d11c-2a53-4c0d-8a10-b1a879be9acb
}: Received add dashboard event
47m         Normal    Info                           configmap/lorax-lorax-dashboards                                Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-lorax-dashboards
    namespace: as-ciat
    resourceVersion: 770065263
    uid: daec5765-7483-423c-8a84-e9e1804a3af1
}: Received add dashboard event
49m         Normal    Info                           configmap/lorax-lorax-dashboards                                Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-lorax-dashboards
    namespace: as-ciat
    resourceVersion: 770062558
    uid: a09b597e-0c31-491f-ab0b-2ee44e47da97
}: Received remove dashboard event
19m         Normal    Info                           configmap/lorax-lorax-dashboards                                Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-lorax-dashboards
    namespace: as-ciat
    resourceVersion: 770111133
    uid: daec5765-7483-423c-8a84-e9e1804a3af1
}: Received remove dashboard event
19m         Normal    Info                           configmap/lorax-lorax-dashboards                                Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-lorax-dashboards
    namespace: as-ciat
    resourceVersion: 770111133
    uid: daec5765-7483-423c-8a84-e9e1804a3af1
}: Pushed delete dashboard to kafka
26m         Normal    Info                           configmap/lorax-lorax-dashboards                                Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-lorax-dashboards
    namespace: as-ciat
    resourceVersion: 770065263
    uid: daec5765-7483-423c-8a84-e9e1804a3af1
}: Received add dashboard event
26m         Normal    Info                           configmap/lorax-prometheus-shared-rules                         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-prometheus-shared-rules
    namespace: as-ciat
    resourceVersion: 770065261
    uid: d0957656-3964-4483-b655-a3b29766ef8d
}: Received add rule event
19m         Normal    Info                           configmap/lorax-prometheus-shared-rules                         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-prometheus-shared-rules
    namespace: as-ciat
    resourceVersion: 770111132
    uid: d0957656-3964-4483-b655-a3b29766ef8d
}: Received delete event
16m         Normal    Info                           configmap/lorax-prometheus-shared-rules                         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-prometheus-shared-rules
    namespace: as-ciat
    resourceVersion: 770115626
    uid: 0d721fee-3180-4968-ada5-5a189703951d
}: Received add rule event
26m         Normal    Info                           configmap/lorax-prometheus-shared-rules                         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-prometheus-shared-rules
    namespace: as-ciat
    resourceVersion: 770065261
    uid: d0957656-3964-4483-b655-a3b29766ef8d
}: Pushed rule to kafka
19m         Normal    Info                           configmap/lorax-prometheus-shared-rules                         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-prometheus-shared-rules
    namespace: as-ciat
    resourceVersion: 770111132
    uid: d0957656-3964-4483-b655-a3b29766ef8d
}: Pushed delete rule to kafka
49m         Normal    Info                           configmap/lorax-prometheus-shared-rules                         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-prometheus-shared-rules
    namespace: as-ciat
    resourceVersion: 770062557
    uid: 754fc132-ea48-4f31-a1a7-28dc112550d8
}: Pushed delete rule to kafka
47m         Normal    Info                           configmap/lorax-prometheus-shared-rules                         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-prometheus-shared-rules
    namespace: as-ciat
    resourceVersion: 770065261
    uid: d0957656-3964-4483-b655-a3b29766ef8d
}: Received add rule event
47m         Normal    Info                           configmap/lorax-prometheus-shared-rules                         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-prometheus-shared-rules
    namespace: as-ciat
    resourceVersion: 770065261
    uid: d0957656-3964-4483-b655-a3b29766ef8d
}: Pushed rule to kafka
49m         Normal    Info                           configmap/lorax-prometheus-shared-rules                         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-prometheus-shared-rules
    namespace: as-ciat
    resourceVersion: 770062557
    uid: 754fc132-ea48-4f31-a1a7-28dc112550d8
}: Received delete event
16m         Normal    Info                           configmap/lorax-prometheus-shared-rules                         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-prometheus-shared-rules
    namespace: as-ciat
    resourceVersion: 770115626
    uid: 0d721fee-3180-4968-ada5-5a189703951d
}: Pushed rule to kafka
48m         Warning   FailedCreate                   replicaset/lorax-queuemanager-5644b9fcb4                        Error creating: pods "lorax-queuemanager-5644b9fcb4-96qdt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-queuemanager-5644b9fcb4                        Error creating: pods "lorax-queuemanager-5644b9fcb4-t76r6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate                   replicaset/lorax-queuemanager-5644b9fcb4                        Error creating: pods "lorax-queuemanager-5644b9fcb4-zqsx4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-queuemanager-58bf767f6d                        Error creating: pods "lorax-queuemanager-58bf767f6d-85pdz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-queuemanager-58bf767f6d                        Error creating: pods "lorax-queuemanager-58bf767f6d-vgrvl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-queuemanager-58bf767f6d                        Error creating: pods "lorax-queuemanager-58bf767f6d-p9zhq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m15s       Warning   FailedCreate                   replicaset/lorax-queuemanager-58bf767f6d                        Error creating: pods "lorax-queuemanager-58bf767f6d-xrmr9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate                   replicaset/lorax-queuemanager-59b6d8b6c8                        Error creating: pods "lorax-queuemanager-59b6d8b6c8-4spdd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/lorax-queuemanager-59b6d8b6c8                        Error creating: pods "lorax-queuemanager-59b6d8b6c8-qxhqn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-queuemanager-59b6d8b6c8                        Error creating: pods "lorax-queuemanager-59b6d8b6c8-9nzhh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m58s       Warning   FailedCreate                   replicaset/lorax-queuemanager-59b6d8b6c8                        Error creating: pods "lorax-queuemanager-59b6d8b6c8-hmcmk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
45m         Warning   FailedCreate                   replicaset/lorax-queuemanager-59d7d8fc4d                        Error creating: pods "lorax-queuemanager-59d7d8fc4d-fvqtl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-queuemanager-59d7d8fc4d                        Error creating: pods "lorax-queuemanager-59d7d8fc4d-bmlqf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
12m         Warning   FailedCreate                   replicaset/lorax-queuemanager-59d7d8fc4d                        Error creating: pods "lorax-queuemanager-59d7d8fc4d-npb65" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-queuemanager-5cd5c79d56                        Error creating: pods "lorax-queuemanager-5cd5c79d56-2gqqg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
35m         Warning   FailedCreate                   replicaset/lorax-queuemanager-5cd5c79d56                        Error creating: pods "lorax-queuemanager-5cd5c79d56-pkpqt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-queuemanager-5cd5c79d56                        Error creating: pods "lorax-queuemanager-5cd5c79d56-97d5m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
113s        Warning   FailedCreate                   replicaset/lorax-queuemanager-5cd5c79d56                        Error creating: pods "lorax-queuemanager-5cd5c79d56-gsm22" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate                   replicaset/lorax-queuemanager-5f4fdf898d                        Error creating: pods "lorax-queuemanager-5f4fdf898d-g8d8k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/lorax-queuemanager-5f4fdf898d                        Error creating: pods "lorax-queuemanager-5f4fdf898d-8rqkr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-queuemanager-5f4fdf898d                        Error creating: pods "lorax-queuemanager-5f4fdf898d-ftc2m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m39s       Warning   FailedCreate                   replicaset/lorax-queuemanager-5f4fdf898d                        Error creating: pods "lorax-queuemanager-5f4fdf898d-7fddx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-queuemanager-66f97c9d58                        Error creating: pods "lorax-queuemanager-66f97c9d58-kc6hj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate                   replicaset/lorax-queuemanager-66f97c9d58                        Error creating: pods "lorax-queuemanager-66f97c9d58-wpwsb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-queuemanager-66f97c9d58                        Error creating: pods "lorax-queuemanager-66f97c9d58-rswq7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-queuemanager-6f45494dc4                        Error creating: pods "lorax-queuemanager-6f45494dc4-gnvsc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-queuemanager-6f45494dc4                        Error creating: pods "lorax-queuemanager-6f45494dc4-hf9br" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
15m         Warning   FailedCreate                   replicaset/lorax-queuemanager-6f45494dc4                        Error creating: pods "lorax-queuemanager-6f45494dc4-tvt5k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedMount                    pod/lorax-queuemanager-769996b97b-z4kqc                         MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
16m         Warning   FailedCreate                   replicaset/lorax-queuemanager-77b4fb                            Error creating: pods "lorax-queuemanager-77b4fb-vlqtg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-queuemanager-77b4fb                            Error creating: pods "lorax-queuemanager-77b4fb-2wtfb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-queuemanager-77b4fb                            Error creating: pods "lorax-queuemanager-77b4fb-6m28b" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-queuemanager-77b4fb                            Error creating: pods "lorax-queuemanager-77b4fb-8cwx5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-queuemanager-77b4fb                            Error creating: pods "lorax-queuemanager-77b4fb-xrx4w" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-queuemanager-77b4fb                            Error creating: pods "lorax-queuemanager-77b4fb-nw6cw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-queuemanager-77b4fb                            Error creating: pods "lorax-queuemanager-77b4fb-sv2bj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-queuemanager-77b4fb                            Error creating: pods "lorax-queuemanager-77b4fb-ldzg8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-queuemanager-77b4fb                            Error creating: pods "lorax-queuemanager-77b4fb-xphsv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
5m37s       Warning   FailedCreate                   replicaset/lorax-queuemanager-77b4fb                            (combined from similar events): Error creating: pods "lorax-queuemanager-77b4fb-pvw7l" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-queuemanager-79c777b444                        Error creating: pods "lorax-queuemanager-79c777b444-zbjzv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate                   replicaset/lorax-queuemanager-79c777b444                        Error creating: pods "lorax-queuemanager-79c777b444-jfhln" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-queuemanager-79c777b444                        Error creating: pods "lorax-queuemanager-79c777b444-kgcxq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
88s         Warning   FailedCreate                   replicaset/lorax-queuemanager-79c777b444                        Error creating: pods "lorax-queuemanager-79c777b444-fsmqd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-queuemanager-bb49dc7c6                         Error creating: pods "lorax-queuemanager-bb49dc7c6-c5f5r" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-queuemanager-bb49dc7c6                         Error creating: pods "lorax-queuemanager-bb49dc7c6-g2sb2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-queuemanager-bb49dc7c6                         Error creating: pods "lorax-queuemanager-bb49dc7c6-wgcpk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m7s        Warning   FailedCreate                   replicaset/lorax-queuemanager-bb49dc7c6                         Error creating: pods "lorax-queuemanager-bb49dc7c6-4zjp7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-queuemanager-cb775f484                         Error creating: pods "lorax-queuemanager-cb775f484-p2xlc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-queuemanager-cb775f484                         Error creating: pods "lorax-queuemanager-cb775f484-pwtj2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-queuemanager-cb775f484                         Error creating: pods "lorax-queuemanager-cb775f484-ql785" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-queuemanager-cb775f484                         Error creating: pods "lorax-queuemanager-cb775f484-vsrsx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-queuemanager-cb775f484                         Error creating: pods "lorax-queuemanager-cb775f484-gdn8m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-queuemanager-cb775f484                         Error creating: pods "lorax-queuemanager-cb775f484-r6d5q" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-queuemanager-cb775f484                         Error creating: pods "lorax-queuemanager-cb775f484-9rthm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-queuemanager-cb775f484                         Error creating: pods "lorax-queuemanager-cb775f484-n897t" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-queuemanager-cb775f484                         Error creating: pods "lorax-queuemanager-cb775f484-j5tgm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate                   replicaset/lorax-queuemanager-cb775f484                         (combined from similar events): Error creating: pods "lorax-queuemanager-cb775f484-qjjf4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
25m         Warning   FailedCreate                   replicaset/lorax-queuemanager-cb775f484                         Error creating: pods "lorax-queuemanager-cb775f484-694t6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m23s       Warning   FailedCreate                   replicaset/lorax-queuemanager-cb775f484                         Error creating: pods "lorax-queuemanager-cb775f484-tbx5x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-queuemanager-fc6fbbc78                         Error creating: pods "lorax-queuemanager-fc6fbbc78-jlf4k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-queuemanager-fc6fbbc78                         Error creating: pods "lorax-queuemanager-fc6fbbc78-vzdn8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-queuemanager-fc6fbbc78                         Error creating: pods "lorax-queuemanager-fc6fbbc78-fxd27" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Normal    ScalingReplicaSet              deployment/lorax-queuemanager                                   Scaled up replica set lorax-queuemanager-cb775f484 to 1
16m         Normal    ScalingReplicaSet              deployment/lorax-queuemanager                                   Scaled up replica set lorax-queuemanager-77b4fb to 1
44m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-59b9cf5654                 Error creating: pods "lorax-realtimedatamanager-59b9cf5654-5xzf5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-59b9cf5654                 Error creating: pods "lorax-realtimedatamanager-59b9cf5654-l9xvg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-59b9cf5654                 Error creating: pods "lorax-realtimedatamanager-59b9cf5654-ddh6j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-5cf45d597d                 Error creating: pods "lorax-realtimedatamanager-5cf45d597d-pxhmk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-5cf45d597d                 Error creating: pods "lorax-realtimedatamanager-5cf45d597d-pcz5k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-5cf45d597d                 Error creating: pods "lorax-realtimedatamanager-5cf45d597d-fqq8g" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-5cf45d597d                 Error creating: pods "lorax-realtimedatamanager-5cf45d597d-8lm2g" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-5cf45d597d                 Error creating: pods "lorax-realtimedatamanager-5cf45d597d-kbvgz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-5cf45d597d                 Error creating: pods "lorax-realtimedatamanager-5cf45d597d-z2n2h" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-5cf45d597d                 Error creating: pods "lorax-realtimedatamanager-5cf45d597d-cdp6g" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-5cf45d597d                 Error creating: pods "lorax-realtimedatamanager-5cf45d597d-lcc2k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-5cf45d597d                 Error creating: pods "lorax-realtimedatamanager-5cf45d597d-vd4cb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-5cf45d597d                 (combined from similar events): Error creating: pods "lorax-realtimedatamanager-5cf45d597d-xbhsv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
25m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-5cf45d597d                 Error creating: pods "lorax-realtimedatamanager-5cf45d597d-t742f" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m24s       Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-5cf45d597d                 Error creating: pods "lorax-realtimedatamanager-5cf45d597d-2l762" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-5db6b5974                  Error creating: pods "lorax-realtimedatamanager-5db6b5974-jtfsx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-5db6b5974                  Error creating: pods "lorax-realtimedatamanager-5db6b5974-86knf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-5db6b5974                  Error creating: pods "lorax-realtimedatamanager-5db6b5974-kpf59" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-5db6b5974                  Error creating: pods "lorax-realtimedatamanager-5db6b5974-fhbkf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-5db6b5974                  Error creating: pods "lorax-realtimedatamanager-5db6b5974-z4t99" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-5db6b5974                  Error creating: pods "lorax-realtimedatamanager-5db6b5974-z6bdd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-5db6b5974                  Error creating: pods "lorax-realtimedatamanager-5db6b5974-js4st" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-5db6b5974                  Error creating: pods "lorax-realtimedatamanager-5db6b5974-b4z5n" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-5db6b5974                  Error creating: pods "lorax-realtimedatamanager-5db6b5974-fg56z" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
5m33s       Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-5db6b5974                  (combined from similar events): Error creating: pods "lorax-realtimedatamanager-5db6b5974-jdgk7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-658bd645f4                 Error creating: pods "lorax-realtimedatamanager-658bd645f4-pkm9l" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-658bd645f4                 Error creating: pods "lorax-realtimedatamanager-658bd645f4-4m5xp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-658bd645f4                 Error creating: pods "lorax-realtimedatamanager-658bd645f4-f5glv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m56s       Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-658bd645f4                 Error creating: pods "lorax-realtimedatamanager-658bd645f4-7dt6n" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-65c955c86                  Error creating: pods "lorax-realtimedatamanager-65c955c86-rvdx2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-65c955c86                  Error creating: pods "lorax-realtimedatamanager-65c955c86-htr4p" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-65c955c86                  Error creating: pods "lorax-realtimedatamanager-65c955c86-66mdh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-697dccddc4                 Error creating: pods "lorax-realtimedatamanager-697dccddc4-drj99" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
35m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-697dccddc4                 Error creating: pods "lorax-realtimedatamanager-697dccddc4-wnzvn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-697dccddc4                 Error creating: pods "lorax-realtimedatamanager-697dccddc4-8s4cj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
111s        Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-697dccddc4                 Error creating: pods "lorax-realtimedatamanager-697dccddc4-k4jpr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-6fd8c8599d                 Error creating: pods "lorax-realtimedatamanager-6fd8c8599d-24s4c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-6fd8c8599d                 Error creating: pods "lorax-realtimedatamanager-6fd8c8599d-6bflm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
15m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-6fd8c8599d                 Error creating: pods "lorax-realtimedatamanager-6fd8c8599d-2wr2z" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-745fcbf486                 Error creating: pods "lorax-realtimedatamanager-745fcbf486-g5k9v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-745fcbf486                 Error creating: pods "lorax-realtimedatamanager-745fcbf486-f7cc2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-745fcbf486                 Error creating: pods "lorax-realtimedatamanager-745fcbf486-q876h" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
45m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-7567b876b6                 Error creating: pods "lorax-realtimedatamanager-7567b876b6-5xhxd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-7567b876b6                 Error creating: pods "lorax-realtimedatamanager-7567b876b6-hvqbd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-7567b876b6                 Error creating: pods "lorax-realtimedatamanager-7567b876b6-78mh5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-7c7c8775c4                 Error creating: pods "lorax-realtimedatamanager-7c7c8775c4-r4p6l" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-7c7c8775c4                 Error creating: pods "lorax-realtimedatamanager-7c7c8775c4-6l99n" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-7c7c8775c4                 Error creating: pods "lorax-realtimedatamanager-7c7c8775c4-vxvdr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m9s        Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-7c7c8775c4                 Error creating: pods "lorax-realtimedatamanager-7c7c8775c4-87hw4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
55m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-7cd4d9f8b                  Error creating: pods "lorax-realtimedatamanager-7cd4d9f8b-nc6hc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-7cd4d9f8b                  Error creating: pods "lorax-realtimedatamanager-7cd4d9f8b-mxqts" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-7cd4d9f8b                  Error creating: pods "lorax-realtimedatamanager-7cd4d9f8b-4fsk5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m58s       Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-7cd4d9f8b                  Error creating: pods "lorax-realtimedatamanager-7cd4d9f8b-djg56" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-7f67c866b                  Error creating: pods "lorax-realtimedatamanager-7f67c866b-ttqb4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-7f67c866b                  Error creating: pods "lorax-realtimedatamanager-7f67c866b-fw7sk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-7f67c866b                  Error creating: pods "lorax-realtimedatamanager-7f67c866b-qk97k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
91s         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-7f67c866b                  Error creating: pods "lorax-realtimedatamanager-7f67c866b-pg2vn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedMount                    pod/lorax-realtimedatamanager-866cd5886b-qq54f                  MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
54m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-86d957ccc8                 Error creating: pods "lorax-realtimedatamanager-86d957ccc8-rfsfj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-86d957ccc8                 Error creating: pods "lorax-realtimedatamanager-86d957ccc8-62grw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-86d957ccc8                 Error creating: pods "lorax-realtimedatamanager-86d957ccc8-pjrd2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m39s       Warning   FailedCreate                   replicaset/lorax-realtimedatamanager-86d957ccc8                 Error creating: pods "lorax-realtimedatamanager-86d957ccc8-xzpdg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Normal    ScalingReplicaSet              deployment/lorax-realtimedatamanager                            Scaled up replica set lorax-realtimedatamanager-5cf45d597d to 1
16m         Normal    ScalingReplicaSet              deployment/lorax-realtimedatamanager                            Scaled up replica set lorax-realtimedatamanager-5db6b5974 to 1
48m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-558b94946b                 Error creating: pods "lorax-reportdesignmanager-558b94946b-hr4dt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-558b94946b                 Error creating: pods "lorax-reportdesignmanager-558b94946b-4zbs4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-558b94946b                 Error creating: pods "lorax-reportdesignmanager-558b94946b-dzg78" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-56465c769d                 Error creating: pods "lorax-reportdesignmanager-56465c769d-qvpkc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-56465c769d                 Error creating: pods "lorax-reportdesignmanager-56465c769d-m6f9r" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-56465c769d                 Error creating: pods "lorax-reportdesignmanager-56465c769d-rzpzn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m13s       Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-56465c769d                 Error creating: pods "lorax-reportdesignmanager-56465c769d-nrd2w" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedMount                    pod/lorax-reportdesignmanager-5c88cb777d-sxlx9                  MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
45m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-5f7d7cd974                 Error creating: pods "lorax-reportdesignmanager-5f7d7cd974-flz2c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-5f7d7cd974                 Error creating: pods "lorax-reportdesignmanager-5f7d7cd974-2k5fs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-5f7d7cd974                 Error creating: pods "lorax-reportdesignmanager-5f7d7cd974-5gl5t" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-669965954b                 Error creating: pods "lorax-reportdesignmanager-669965954b-wz24x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-669965954b                 Error creating: pods "lorax-reportdesignmanager-669965954b-gq7gp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-669965954b                 Error creating: pods "lorax-reportdesignmanager-669965954b-9h49n" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m11s       Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-669965954b                 Error creating: pods "lorax-reportdesignmanager-669965954b-mhf8j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-6744b65cc4                 Error creating: pods "lorax-reportdesignmanager-6744b65cc4-65p5c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-6744b65cc4                 Error creating: pods "lorax-reportdesignmanager-6744b65cc4-qkh27" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-6744b65cc4                 Error creating: pods "lorax-reportdesignmanager-6744b65cc4-hzhbb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m4s        Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-6744b65cc4                 Error creating: pods "lorax-reportdesignmanager-6744b65cc4-q5p94" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-69b5f89b58                 Error creating: pods "lorax-reportdesignmanager-69b5f89b58-8v87w" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-69b5f89b58                 Error creating: pods "lorax-reportdesignmanager-69b5f89b58-69t4j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-69b5f89b58                 Error creating: pods "lorax-reportdesignmanager-69b5f89b58-zq8w8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-69b5f89b58                 Error creating: pods "lorax-reportdesignmanager-69b5f89b58-wwn86" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-69b5f89b58                 Error creating: pods "lorax-reportdesignmanager-69b5f89b58-7zp2m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-69b5f89b58                 Error creating: pods "lorax-reportdesignmanager-69b5f89b58-xqf5k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-69b5f89b58                 Error creating: pods "lorax-reportdesignmanager-69b5f89b58-sczk7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-69b5f89b58                 Error creating: pods "lorax-reportdesignmanager-69b5f89b58-j4tmd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-69b5f89b58                 Error creating: pods "lorax-reportdesignmanager-69b5f89b58-6xjrb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-69b5f89b58                 (combined from similar events): Error creating: pods "lorax-reportdesignmanager-69b5f89b58-ttf7p" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
25m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-69b5f89b58                 Error creating: pods "lorax-reportdesignmanager-69b5f89b58-cg6jb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m25s       Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-69b5f89b58                 Error creating: pods "lorax-reportdesignmanager-69b5f89b58-msf45" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-6c874c8bd6                 Error creating: pods "lorax-reportdesignmanager-6c874c8bd6-c8kkz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
35m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-6c874c8bd6                 Error creating: pods "lorax-reportdesignmanager-6c874c8bd6-ddvqm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-6c874c8bd6                 Error creating: pods "lorax-reportdesignmanager-6c874c8bd6-h4gqs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
106s        Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-6c874c8bd6                 Error creating: pods "lorax-reportdesignmanager-6c874c8bd6-665fj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-6cd9df9b54                 Error creating: pods "lorax-reportdesignmanager-6cd9df9b54-kkz56" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-6cd9df9b54                 Error creating: pods "lorax-reportdesignmanager-6cd9df9b54-ms7h7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-6cd9df9b54                 Error creating: pods "lorax-reportdesignmanager-6cd9df9b54-pd5gj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
55m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-746cb6cc46                 Error creating: pods "lorax-reportdesignmanager-746cb6cc46-hzdzv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-746cb6cc46                 Error creating: pods "lorax-reportdesignmanager-746cb6cc46-h4bpd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-746cb6cc46                 Error creating: pods "lorax-reportdesignmanager-746cb6cc46-54dfw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
5m9s        Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-746cb6cc46                 Error creating: pods "lorax-reportdesignmanager-746cb6cc46-rtbs9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-7668f54b                   Error creating: pods "lorax-reportdesignmanager-7668f54b-r74rf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-7668f54b                   Error creating: pods "lorax-reportdesignmanager-7668f54b-pdrfg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
15m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-7668f54b                   Error creating: pods "lorax-reportdesignmanager-7668f54b-l5g6h" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-7998f7595d                 Error creating: pods "lorax-reportdesignmanager-7998f7595d-x5m7w" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-7998f7595d                 Error creating: pods "lorax-reportdesignmanager-7998f7595d-2fsnf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-7998f7595d                 Error creating: pods "lorax-reportdesignmanager-7998f7595d-fz86l" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-7998f7595d                 Error creating: pods "lorax-reportdesignmanager-7998f7595d-tms4r" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-7998f7595d                 Error creating: pods "lorax-reportdesignmanager-7998f7595d-fjmdx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-7998f7595d                 Error creating: pods "lorax-reportdesignmanager-7998f7595d-vsvgv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-7998f7595d                 Error creating: pods "lorax-reportdesignmanager-7998f7595d-kwjlx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-7998f7595d                 Error creating: pods "lorax-reportdesignmanager-7998f7595d-jpmh7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-7998f7595d                 Error creating: pods "lorax-reportdesignmanager-7998f7595d-zhwq4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
5m33s       Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-7998f7595d                 (combined from similar events): Error creating: pods "lorax-reportdesignmanager-7998f7595d-bxbbs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-7d67f9fb98                 Error creating: pods "lorax-reportdesignmanager-7d67f9fb98-f8rmq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-7d67f9fb98                 Error creating: pods "lorax-reportdesignmanager-7d67f9fb98-qvxd5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-7d67f9fb98                 Error creating: pods "lorax-reportdesignmanager-7d67f9fb98-q2twm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
90s         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-7d67f9fb98                 Error creating: pods "lorax-reportdesignmanager-7d67f9fb98-hw4xw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-848c78786                  Error creating: pods "lorax-reportdesignmanager-848c78786-xph9c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-848c78786                  Error creating: pods "lorax-reportdesignmanager-848c78786-85mvv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-reportdesignmanager-848c78786                  Error creating: pods "lorax-reportdesignmanager-848c78786-k4xjr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Normal    ScalingReplicaSet              deployment/lorax-reportdesignmanager                            Scaled up replica set lorax-reportdesignmanager-69b5f89b58 to 1
16m         Normal    ScalingReplicaSet              deployment/lorax-reportdesignmanager                            Scaled up replica set lorax-reportdesignmanager-7998f7595d to 1
18m         Warning   FailedMount                    pod/lorax-reportpublishmanager-58445646c4-9ztc7                 MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
58m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6779d8757d                Error creating: pods "lorax-reportpublishmanager-6779d8757d-rhpjx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6779d8757d                Error creating: pods "lorax-reportpublishmanager-6779d8757d-cgw8v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6779d8757d                Error creating: pods "lorax-reportpublishmanager-6779d8757d-t9kzc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m10s       Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6779d8757d                Error creating: pods "lorax-reportpublishmanager-6779d8757d-9zsn5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6867c97d86                Error creating: pods "lorax-reportpublishmanager-6867c97d86-s6z4p" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6867c97d86                Error creating: pods "lorax-reportpublishmanager-6867c97d86-ckj4x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6867c97d86                Error creating: pods "lorax-reportpublishmanager-6867c97d86-8gdl7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
91s         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6867c97d86                Error creating: pods "lorax-reportpublishmanager-6867c97d86-bxh94" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-68d67bc598                Error creating: pods "lorax-reportpublishmanager-68d67bc598-6wqv5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
35m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-68d67bc598                Error creating: pods "lorax-reportpublishmanager-68d67bc598-5kdgh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-68d67bc598                Error creating: pods "lorax-reportpublishmanager-68d67bc598-2zg5k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
112s        Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-68d67bc598                Error creating: pods "lorax-reportpublishmanager-68d67bc598-dmqzd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6b5b9cc74                 Error creating: pods "lorax-reportpublishmanager-6b5b9cc74-rkd2n" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6b5b9cc74                 Error creating: pods "lorax-reportpublishmanager-6b5b9cc74-t4jfm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6b5b9cc74                 Error creating: pods "lorax-reportpublishmanager-6b5b9cc74-sc8n6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6b5b9cc74                 Error creating: pods "lorax-reportpublishmanager-6b5b9cc74-hmg89" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6b5b9cc74                 Error creating: pods "lorax-reportpublishmanager-6b5b9cc74-f2psz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6b5b9cc74                 Error creating: pods "lorax-reportpublishmanager-6b5b9cc74-7fvwr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6b5b9cc74                 Error creating: pods "lorax-reportpublishmanager-6b5b9cc74-cgdq7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6b5b9cc74                 Error creating: pods "lorax-reportpublishmanager-6b5b9cc74-cv29g" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6b5b9cc74                 Error creating: pods "lorax-reportpublishmanager-6b5b9cc74-qrvgm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6b5b9cc74                 (combined from similar events): Error creating: pods "lorax-reportpublishmanager-6b5b9cc74-gxgzw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
25m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6b5b9cc74                 Error creating: pods "lorax-reportpublishmanager-6b5b9cc74-txr2r" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m24s       Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6b5b9cc74                 Error creating: pods "lorax-reportpublishmanager-6b5b9cc74-vsdpt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6d88fc458b                Error creating: pods "lorax-reportpublishmanager-6d88fc458b-vkkxx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6d88fc458b                Error creating: pods "lorax-reportpublishmanager-6d88fc458b-q8nbh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6d88fc458b                Error creating: pods "lorax-reportpublishmanager-6d88fc458b-w6jk7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m39s       Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6d88fc458b                Error creating: pods "lorax-reportpublishmanager-6d88fc458b-25cpk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6f8dd75d64                Error creating: pods "lorax-reportpublishmanager-6f8dd75d64-q8z7n" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6f8dd75d64                Error creating: pods "lorax-reportpublishmanager-6f8dd75d64-g648j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6f8dd75d64                Error creating: pods "lorax-reportpublishmanager-6f8dd75d64-swcgh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
45m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6ff7947fc4                Error creating: pods "lorax-reportpublishmanager-6ff7947fc4-xcwj6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6ff7947fc4                Error creating: pods "lorax-reportpublishmanager-6ff7947fc4-z6qbd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
12m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-6ff7947fc4                Error creating: pods "lorax-reportpublishmanager-6ff7947fc4-kxqqh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-79bc79d75d                Error creating: pods "lorax-reportpublishmanager-79bc79d75d-ttpfg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-79bc79d75d                Error creating: pods "lorax-reportpublishmanager-79bc79d75d-99x9v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-79bc79d75d                Error creating: pods "lorax-reportpublishmanager-79bc79d75d-gwk26" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-79c6c74b4d                Error creating: pods "lorax-reportpublishmanager-79c6c74b4d-jhlvb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-79c6c74b4d                Error creating: pods "lorax-reportpublishmanager-79c6c74b4d-9qwjt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
15m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-79c6c74b4d                Error creating: pods "lorax-reportpublishmanager-79c6c74b4d-p8jcv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-7cb6b6f458                Error creating: pods "lorax-reportpublishmanager-7cb6b6f458-7q8z8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-7cb6b6f458                Error creating: pods "lorax-reportpublishmanager-7cb6b6f458-lg9m2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-7cb6b6f458                Error creating: pods "lorax-reportpublishmanager-7cb6b6f458-gtm2t" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-7cb6b6f458                Error creating: pods "lorax-reportpublishmanager-7cb6b6f458-s5rz2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-7cb6b6f458                Error creating: pods "lorax-reportpublishmanager-7cb6b6f458-p7jpq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-7cb6b6f458                Error creating: pods "lorax-reportpublishmanager-7cb6b6f458-5zw5z" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-7cb6b6f458                Error creating: pods "lorax-reportpublishmanager-7cb6b6f458-t95fx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-7cb6b6f458                Error creating: pods "lorax-reportpublishmanager-7cb6b6f458-b6mhw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-7cb6b6f458                Error creating: pods "lorax-reportpublishmanager-7cb6b6f458-wsghq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
5m34s       Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-7cb6b6f458                (combined from similar events): Error creating: pods "lorax-reportpublishmanager-7cb6b6f458-st642" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-85fcb8558b                Error creating: pods "lorax-reportpublishmanager-85fcb8558b-r4w4v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-85fcb8558b                Error creating: pods "lorax-reportpublishmanager-85fcb8558b-frz9l" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-85fcb8558b                Error creating: pods "lorax-reportpublishmanager-85fcb8558b-2sbzn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m12s       Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-85fcb8558b                Error creating: pods "lorax-reportpublishmanager-85fcb8558b-hgsx4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-ddcc6b64                  Error creating: pods "lorax-reportpublishmanager-ddcc6b64-k8f6v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-ddcc6b64                  Error creating: pods "lorax-reportpublishmanager-ddcc6b64-qh48x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-ddcc6b64                  Error creating: pods "lorax-reportpublishmanager-ddcc6b64-wv6m5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
55m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-f75747b5b                 Error creating: pods "lorax-reportpublishmanager-f75747b5b-4ffrh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-f75747b5b                 Error creating: pods "lorax-reportpublishmanager-f75747b5b-7t5dj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-f75747b5b                 Error creating: pods "lorax-reportpublishmanager-f75747b5b-2dxxf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
5m          Warning   FailedCreate                   replicaset/lorax-reportpublishmanager-f75747b5b                 Error creating: pods "lorax-reportpublishmanager-f75747b5b-j45k5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Normal    ScalingReplicaSet              deployment/lorax-reportpublishmanager                           Scaled up replica set lorax-reportpublishmanager-6b5b9cc74 to 1
16m         Normal    ScalingReplicaSet              deployment/lorax-reportpublishmanager                           Scaled up replica set lorax-reportpublishmanager-7cb6b6f458 to 1
48m         Warning   FailedCreate                   replicaset/lorax-searchmanager-5588bf669c                       Error creating: pods "lorax-searchmanager-5588bf669c-rsdpp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-searchmanager-5588bf669c                       Error creating: pods "lorax-searchmanager-5588bf669c-2tdc6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate                   replicaset/lorax-searchmanager-5588bf669c                       Error creating: pods "lorax-searchmanager-5588bf669c-mrgj8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
55m         Warning   FailedCreate                   replicaset/lorax-searchmanager-57f9fc4cb5                       Error creating: pods "lorax-searchmanager-57f9fc4cb5-h4jqc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/lorax-searchmanager-57f9fc4cb5                       Error creating: pods "lorax-searchmanager-57f9fc4cb5-2fnj2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-searchmanager-57f9fc4cb5                       Error creating: pods "lorax-searchmanager-57f9fc4cb5-dg54m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m59s       Warning   FailedCreate                   replicaset/lorax-searchmanager-57f9fc4cb5                       Error creating: pods "lorax-searchmanager-57f9fc4cb5-f485c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-searchmanager-57fd4db8f                        Error creating: pods "lorax-searchmanager-57fd4db8f-lzstm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-searchmanager-57fd4db8f                        Error creating: pods "lorax-searchmanager-57fd4db8f-fdh5k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-searchmanager-57fd4db8f                        Error creating: pods "lorax-searchmanager-57fd4db8f-6p8ck" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-searchmanager-57fd4db8f                        Error creating: pods "lorax-searchmanager-57fd4db8f-tlb4h" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-searchmanager-57fd4db8f                        Error creating: pods "lorax-searchmanager-57fd4db8f-qj42v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-searchmanager-57fd4db8f                        Error creating: pods "lorax-searchmanager-57fd4db8f-ljbf5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-searchmanager-57fd4db8f                        Error creating: pods "lorax-searchmanager-57fd4db8f-w642z" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-searchmanager-57fd4db8f                        Error creating: pods "lorax-searchmanager-57fd4db8f-kzcln" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-searchmanager-57fd4db8f                        Error creating: pods "lorax-searchmanager-57fd4db8f-dcr2t" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate                   replicaset/lorax-searchmanager-57fd4db8f                        (combined from similar events): Error creating: pods "lorax-searchmanager-57fd4db8f-pccl2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
25m         Warning   FailedCreate                   replicaset/lorax-searchmanager-57fd4db8f                        Error creating: pods "lorax-searchmanager-57fd4db8f-ptg92" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m26s       Warning   FailedCreate                   replicaset/lorax-searchmanager-57fd4db8f                        Error creating: pods "lorax-searchmanager-57fd4db8f-vpnpt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-searchmanager-595f549689                       Error creating: pods "lorax-searchmanager-595f549689-tt59t" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate                   replicaset/lorax-searchmanager-595f549689                       Error creating: pods "lorax-searchmanager-595f549689-8zncr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-searchmanager-595f549689                       Error creating: pods "lorax-searchmanager-595f549689-kv856" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
45m         Warning   FailedCreate                   replicaset/lorax-searchmanager-5c4d488449                       Error creating: pods "lorax-searchmanager-5c4d488449-vgqk6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-searchmanager-5c4d488449                       Error creating: pods "lorax-searchmanager-5c4d488449-xtshr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
12m         Warning   FailedCreate                   replicaset/lorax-searchmanager-5c4d488449                       Error creating: pods "lorax-searchmanager-5c4d488449-vp7kb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-searchmanager-5cbd688685                       Error creating: pods "lorax-searchmanager-5cbd688685-ftjx9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-searchmanager-5cbd688685                       Error creating: pods "lorax-searchmanager-5cbd688685-czj8t" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-searchmanager-5cbd688685                       Error creating: pods "lorax-searchmanager-5cbd688685-z2sbd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-searchmanager-5cbd688685                       Error creating: pods "lorax-searchmanager-5cbd688685-cfxnt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-searchmanager-5cbd688685                       Error creating: pods "lorax-searchmanager-5cbd688685-2zjtp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-searchmanager-5cbd688685                       Error creating: pods "lorax-searchmanager-5cbd688685-j55bz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-searchmanager-5cbd688685                       Error creating: pods "lorax-searchmanager-5cbd688685-s8hkh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-searchmanager-5cbd688685                       Error creating: pods "lorax-searchmanager-5cbd688685-ff6l8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-searchmanager-5cbd688685                       Error creating: pods "lorax-searchmanager-5cbd688685-4zgzq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
5m36s       Warning   FailedCreate                   replicaset/lorax-searchmanager-5cbd688685                       (combined from similar events): Error creating: pods "lorax-searchmanager-5cbd688685-6b975" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-searchmanager-5d75f85465                       Error creating: pods "lorax-searchmanager-5d75f85465-5252w" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-searchmanager-5d75f85465                       Error creating: pods "lorax-searchmanager-5d75f85465-2jl7q" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-searchmanager-5d75f85465                       Error creating: pods "lorax-searchmanager-5d75f85465-fz59j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m5s        Warning   FailedCreate                   replicaset/lorax-searchmanager-5d75f85465                       Error creating: pods "lorax-searchmanager-5d75f85465-j79f4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-searchmanager-5f95bf5bf                        Error creating: pods "lorax-searchmanager-5f95bf5bf-tlskv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-searchmanager-5f95bf5bf                        Error creating: pods "lorax-searchmanager-5f95bf5bf-hxbs2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-searchmanager-5f95bf5bf                        Error creating: pods "lorax-searchmanager-5f95bf5bf-nnz5v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-searchmanager-79c6b8578f                       Error creating: pods "lorax-searchmanager-79c6b8578f-hkjtl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
35m         Warning   FailedCreate                   replicaset/lorax-searchmanager-79c6b8578f                       Error creating: pods "lorax-searchmanager-79c6b8578f-247tk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-searchmanager-79c6b8578f                       Error creating: pods "lorax-searchmanager-79c6b8578f-qthmn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
112s        Warning   FailedCreate                   replicaset/lorax-searchmanager-79c6b8578f                       Error creating: pods "lorax-searchmanager-79c6b8578f-pwgwg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-searchmanager-7d88df64bf                       Error creating: pods "lorax-searchmanager-7d88df64bf-kzl4m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-searchmanager-7d88df64bf                       Error creating: pods "lorax-searchmanager-7d88df64bf-gpcsb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
15m         Warning   FailedCreate                   replicaset/lorax-searchmanager-7d88df64bf                       Error creating: pods "lorax-searchmanager-7d88df64bf-qqvnr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedMount                    pod/lorax-searchmanager-7d97d6dfcc-gn4zg                        MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
17m         Warning   FailedMount                    pod/lorax-searchmanager-7d97d6dfcc-q2x5t                        MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
54m         Warning   FailedCreate                   replicaset/lorax-searchmanager-8475dfbb49                       Error creating: pods "lorax-searchmanager-8475dfbb49-9966g" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate                   replicaset/lorax-searchmanager-8475dfbb49                       Error creating: pods "lorax-searchmanager-8475dfbb49-p65st" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-searchmanager-8475dfbb49                       Error creating: pods "lorax-searchmanager-8475dfbb49-5vh6m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m37s       Warning   FailedCreate                   replicaset/lorax-searchmanager-8475dfbb49                       Error creating: pods "lorax-searchmanager-8475dfbb49-h6b66" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-searchmanager-autoscaler          unable to get metrics for resource memory: no metrics returned from resource metrics API
24m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-searchmanager-autoscaler          unable to get metrics for resource memory: no metrics returned from resource metrics API
34m         Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-searchmanager-autoscaler          unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
34m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-searchmanager-autoscaler          unable to get metrics for resource cpu: no metrics returned from resource metrics API
36m         Warning   FailedComputeMetricsReplicas   horizontalpodautoscaler/lorax-searchmanager-autoscaler          invalid metrics (3 invalid out of 3), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API
3s          Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-searchmanager-autoscaler          unable to get metrics for resource memory: no metrics returned from resource metrics API
3m50s       Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-searchmanager-autoscaler          unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
3m50s       Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-searchmanager-autoscaler          unable to get metrics for resource cpu: no metrics returned from resource metrics API
5m44s       Warning   FailedComputeMetricsReplicas   horizontalpodautoscaler/lorax-searchmanager-autoscaler          invalid metrics (3 invalid out of 3), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API
58m         Warning   FailedCreate                   replicaset/lorax-searchmanager-cf6668b77                        Error creating: pods "lorax-searchmanager-cf6668b77-gcccm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-searchmanager-cf6668b77                        Error creating: pods "lorax-searchmanager-cf6668b77-6mm9c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-searchmanager-cf6668b77                        Error creating: pods "lorax-searchmanager-cf6668b77-v4frt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m9s        Warning   FailedCreate                   replicaset/lorax-searchmanager-cf6668b77                        Error creating: pods "lorax-searchmanager-cf6668b77-6vwwp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-searchmanager-f76cdcfcc                        Error creating: pods "lorax-searchmanager-f76cdcfcc-mcx4v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate                   replicaset/lorax-searchmanager-f76cdcfcc                        Error creating: pods "lorax-searchmanager-f76cdcfcc-c4f2h" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-searchmanager-f76cdcfcc                        Error creating: pods "lorax-searchmanager-f76cdcfcc-x45vx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
88s         Warning   FailedCreate                   replicaset/lorax-searchmanager-f76cdcfcc                        Error creating: pods "lorax-searchmanager-f76cdcfcc-wmdpm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Normal    ScalingReplicaSet              deployment/lorax-searchmanager                                  Scaled up replica set lorax-searchmanager-57fd4db8f to 2
16m         Normal    ScalingReplicaSet              deployment/lorax-searchmanager                                  Scaled up replica set lorax-searchmanager-5cbd688685 to 2
45m         Warning   FailedCreate                   replicaset/lorax-securityservice-55c498b866                     Error creating: pods "lorax-securityservice-55c498b866-9t97j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-securityservice-55c498b866                     Error creating: pods "lorax-securityservice-55c498b866-np92g" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-securityservice-55c498b866                     Error creating: pods "lorax-securityservice-55c498b866-rw8jp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-securityservice-5b9b48c4                       Error creating: pods "lorax-securityservice-5b9b48c4-4v2fp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-securityservice-5b9b48c4                       Error creating: pods "lorax-securityservice-5b9b48c4-7kj7j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate                   replicaset/lorax-securityservice-5b9b48c4                       Error creating: pods "lorax-securityservice-5b9b48c4-mh9tj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-securityservice-5d76cfb66                      Error creating: pods "lorax-securityservice-5d76cfb66-p76rp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-securityservice-5d76cfb66                      Error creating: pods "lorax-securityservice-5d76cfb66-pdcgd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-securityservice-5d76cfb66                      Error creating: pods "lorax-securityservice-5d76cfb66-tz6rn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m58s       Warning   FailedCreate                   replicaset/lorax-securityservice-5d76cfb66                      Error creating: pods "lorax-securityservice-5d76cfb66-r2zhl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-securityservice-64757d4f7d                     Error creating: pods "lorax-securityservice-64757d4f7d-4t4v6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate                   replicaset/lorax-securityservice-64757d4f7d                     Error creating: pods "lorax-securityservice-64757d4f7d-45lfw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-securityservice-64757d4f7d                     Error creating: pods "lorax-securityservice-64757d4f7d-jkvt4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
90s         Warning   FailedCreate                   replicaset/lorax-securityservice-64757d4f7d                     Error creating: pods "lorax-securityservice-64757d4f7d-l875f" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-securityservice-66958cb778                     Error creating: pods "lorax-securityservice-66958cb778-nld8x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-securityservice-66958cb778                     Error creating: pods "lorax-securityservice-66958cb778-wdjtw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-securityservice-66958cb778                     Error creating: pods "lorax-securityservice-66958cb778-bpztl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-securityservice-66958cb778                     Error creating: pods "lorax-securityservice-66958cb778-sv4pb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-securityservice-66958cb778                     Error creating: pods "lorax-securityservice-66958cb778-mp4k8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-securityservice-66958cb778                     Error creating: pods "lorax-securityservice-66958cb778-cb7vs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-securityservice-66958cb778                     Error creating: pods "lorax-securityservice-66958cb778-czn7p" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-securityservice-66958cb778                     Error creating: pods "lorax-securityservice-66958cb778-6b2rn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate                   replicaset/lorax-securityservice-66958cb778                     Error creating: pods "lorax-securityservice-66958cb778-gbd49" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
5m36s       Warning   FailedCreate                   replicaset/lorax-securityservice-66958cb778                     (combined from similar events): Error creating: pods "lorax-securityservice-66958cb778-c7dsl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-securityservice-66b78457cd                     Error creating: pods "lorax-securityservice-66b78457cd-rtt4w" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28m         Warning   FailedCreate                   replicaset/lorax-securityservice-66b78457cd                     Error creating: pods "lorax-securityservice-66b78457cd-v2l7v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-securityservice-66b78457cd                     Error creating: pods "lorax-securityservice-66b78457cd-tv25c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58m         Warning   FailedCreate                   replicaset/lorax-securityservice-67c885b654                     Error creating: pods "lorax-securityservice-67c885b654-qshhb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate                   replicaset/lorax-securityservice-67c885b654                     Error creating: pods "lorax-securityservice-67c885b654-d89pr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate                   replicaset/lorax-securityservice-67c885b654                     Error creating: pods "lorax-securityservice-67c885b654-n8dfv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m14s       Warning   FailedCreate                   replicaset/lorax-securityservice-67c885b654                     Error creating: pods "lorax-securityservice-67c885b654-ftfr5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate                   replicaset/lorax-securityservice-695b757654                     Error creating: pods "lorax-securityservice-695b757654-bvmqn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate                   replicaset/lorax-securityservice-695b757654                     Error creating: pods "lorax-securityservice-695b757654-rmvgf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedCreate                   replicaset/lorax-securityservice-695b757654                     Error creating: pods "lorax-securityservice-695b757654-z6c62" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate                   replicaset/lorax-securityservice-6b65d9847d                     Error creating: pods "lorax-securityservice-6b65d9847d-lxj7n" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/lorax-securityservice-6b65d9847d                     Error creating: pods "lorax-securityservice-6b65d9847d-52c8r" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-securityservice-6b65d9847d                     Error creating: pods "lorax-securityservice-6b65d9847d-7jrpt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m39s       Warning   FailedCreate                   replicaset/lorax-securityservice-6b65d9847d                     Error creating: pods "lorax-securityservice-6b65d9847d-xvfth" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate                   replicaset/lorax-securityservice-7798cf45fd                     Error creating: pods "lorax-securityservice-7798cf45fd-pw7sm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
35m         Warning   FailedCreate                   replicaset/lorax-securityservice-7798cf45fd                     Error creating: pods "lorax-securityservice-7798cf45fd-j9cnw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate                   replicaset/lorax-securityservice-7798cf45fd                     Error creating: pods "lorax-securityservice-7798cf45fd-6hphs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
110s        Warning   FailedCreate                   replicaset/lorax-securityservice-7798cf45fd                     Error creating: pods "lorax-securityservice-7798cf45fd-6hbcs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
48m         Warning   FailedCreate                   replicaset/lorax-securityservice-78487b8f94                     Error creating: pods "lorax-securityservice-78487b8f94-4ws96" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate                   replicaset/lorax-securityservice-78487b8f94                     Error creating: pods "lorax-securityservice-78487b8f94-g7pkw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
15m         Warning   FailedCreate                   replicaset/lorax-securityservice-78487b8f94                     Error creating: pods "lorax-securityservice-78487b8f94-wdjsz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
55m         Warning   FailedCreate                   replicaset/lorax-securityservice-86b7485fb6                     Error creating: pods "lorax-securityservice-86b7485fb6-l96qd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate                   replicaset/lorax-securityservice-86b7485fb6                     Error creating: pods "lorax-securityservice-86b7485fb6-tlpm2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate                   replicaset/lorax-securityservice-86b7485fb6                     Error creating: pods "lorax-securityservice-86b7485fb6-7bng6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m59s       Warning   FailedCreate                   replicaset/lorax-securityservice-86b7485fb6                     Error creating: pods "lorax-securityservice-86b7485fb6-g8pbz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
49m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-securityservice-autoscaler        unable to get metrics for resource memory: no metrics returned from resource metrics API
19m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-securityservice-autoscaler        unable to get metrics for resource memory: no metrics returned from resource metrics API
34m         Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-securityservice-autoscaler        unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
34m         Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-securityservice-autoscaler        unable to get metrics for resource cpu: no metrics returned from resource metrics API
36m         Warning   FailedComputeMetricsReplicas   horizontalpodautoscaler/lorax-securityservice-autoscaler        invalid metrics (3 invalid out of 3), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API
4m18s       Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-securityservice-autoscaler        unable to get metrics for resource memory: no metrics returned from resource metrics API
4m18s       Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-securityservice-autoscaler        unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
4m18s       Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-securityservice-autoscaler        unable to get metrics for resource cpu: no metrics returned from resource metrics API
6m10s       Warning   FailedComputeMetricsReplicas   horizontalpodautoscaler/lorax-securityservice-autoscaler        invalid metrics (3 invalid out of 3), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API
17m         Warning   FailedMount                    pod/lorax-securityservice-b9bb8695d-6mgbq                       MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
4m1s        Normal    Killing                        pod/lorax-securityservice-b9bb8695d-6mgbq                       Stopping container vault-agent
4m1s        Normal    Killing                        pod/lorax-securityservice-b9bb8695d-6mgbq                       Stopping container server
17m         Warning   FailedMount                    pod/lorax-securityservice-b9bb8695d-t7bz5                       MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
4m          Warning   FailedCreate                   replicaset/lorax-securityservice-b9bb8695d                      Error creating: pods "lorax-securityservice-b9bb8695d-vtr5n" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m58s       Warning   FailedCreate                   replicaset/lorax-securityservice-b9bb8695d                      Error creating: pods "lorax-securityservice-b9bb8695d-qbxt5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m56s       Warning   FailedCreate                   replicaset/lorax-securityservice-b9bb8695d                      Error creating: pods "lorax-securityservice-b9bb8695d-7fpbb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m54s       Warning   FailedCreate                   replicaset/lorax-securityservice-b9bb8695d                      Error creating: pods "lorax-securityservice-b9bb8695d-qz2qz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m52s       Warning   FailedCreate                   replicaset/lorax-securityservice-b9bb8695d                      Error creating: pods "lorax-securityservice-b9bb8695d-g895v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m49s       Warning   FailedCreate                   replicaset/lorax-securityservice-b9bb8695d                      Error creating: pods "lorax-securityservice-b9bb8695d-4txzk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m46s       Warning   FailedCreate                   replicaset/lorax-securityservice-b9bb8695d                      Error creating: pods "lorax-securityservice-b9bb8695d-mk48r" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m45s       Warning   FailedCreate                   replicaset/lorax-securityservice-b9bb8695d                      Error creating: pods "lorax-securityservice-b9bb8695d-dkk67" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m43s       Warning   FailedCreate                   replicaset/lorax-securityservice-b9bb8695d                      Error creating: pods "lorax-securityservice-b9bb8695d-r9gmh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58s         Warning   FailedCreate                   replicaset/lorax-securityservice-b9bb8695d                      (combined from similar events): Error creating: pods "lorax-securityservice-b9bb8695d-srkmn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-securityservice-c87b5d4c8                      Error creating: pods "lorax-securityservice-c87b5d4c8-nkp9c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-securityservice-c87b5d4c8                      Error creating: pods "lorax-securityservice-c87b5d4c8-kj7x7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-securityservice-c87b5d4c8                      Error creating: pods "lorax-securityservice-c87b5d4c8-hnf4b" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-securityservice-c87b5d4c8                      Error creating: pods "lorax-securityservice-c87b5d4c8-6549l" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-securityservice-c87b5d4c8                      Error creating: pods "lorax-securityservice-c87b5d4c8-2kqk7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-securityservice-c87b5d4c8                      Error creating: pods "lorax-securityservice-c87b5d4c8-2w4pr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-securityservice-c87b5d4c8                      Error creating: pods "lorax-securityservice-c87b5d4c8-bs4x7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-securityservice-c87b5d4c8                      Error creating: pods "lorax-securityservice-c87b5d4c8-cllb8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/lorax-securityservice-c87b5d4c8                      Error creating: pods "lorax-securityservice-c87b5d4c8-grtq6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate                   replicaset/lorax-securityservice-c87b5d4c8                      (combined from similar events): Error creating: pods "lorax-securityservice-c87b5d4c8-ncxmm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
25m         Warning   FailedCreate                   replicaset/lorax-securityservice-c87b5d4c8                      Error creating: pods "lorax-securityservice-c87b5d4c8-2fkzc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8m25s       Warning   FailedCreate                   replicaset/lorax-securityservice-c87b5d4c8                      Error creating: pods "lorax-securityservice-c87b5d4c8-thzs4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Normal    ScalingReplicaSet              deployment/lorax-securityservice                                Scaled up replica set lorax-securityservice-c87b5d4c8 to 2
16m         Normal    ScalingReplicaSet              deployment/lorax-securityservice                                Scaled up replica set lorax-securityservice-66958cb778 to 2
37s         Warning   FailedGetPodsMetric            horizontalpodautoscaler/who-autoscaler                          unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
26m         Normal    Info                           configmap/who-dashboards                                        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: who-dashboards
    namespace: as-ciat
    resourceVersion: 751636959
    uid: 05f498ae-3014-4426-9475-927e3e8b0278
}: Received add dashboard event
26m         Normal    Info                           configmap/who-dashboards                                        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: who-dashboards
    namespace: as-ciat
    resourceVersion: 751636959
    uid: 05f498ae-3014-4426-9475-927e3e8b0278
}: Pushed add dashboard to kafka
47m         Normal    Sync                           ingress/who-ingress                                             Scheduled for sync
47m         Normal    Sync                           ingress/who-ingress                                             Scheduled for sync
47m         Normal    Sync                           ingress/who-ingress                                             Scheduled for sync
47m         Normal    Sync                           ingress/who-ingress                                             Scheduled for sync
47m         Normal    Sync                           ingress/who-ingress                                             Scheduled for sync
47m         Normal    Sync                           ingress/who-ingress                                             Scheduled for sync
47m         Normal    Sync                           ingress/who-ingress                                             Scheduled for sync
34m         Normal    Sync                           ingress/who-ingress                                             Scheduled for sync
17m         Normal    Sync                           ingress/who-ingress                                             Scheduled for sync
3m54s       Normal    Sync                           ingress/who-ingress                                             Scheduled for sync
2m7s        Warning   FailedGetPodsMetric            horizontalpodautoscaler/who-internal-autoscaler                 unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
26m         Normal    Info                           configmap/who-internal-dashboards                               Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: who-internal-dashboards
    namespace: as-ciat
    resourceVersion: 751637090
    uid: 90a8fe2f-54a9-472f-8d4f-2f648548c9cf
}: Pushed add dashboard to kafka
26m         Normal    Info                           configmap/who-internal-dashboards                               Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: who-internal-dashboards
    namespace: as-ciat
    resourceVersion: 751637090
    uid: 90a8fe2f-54a9-472f-8d4f-2f648548c9cf
}: Received add dashboard event
26m         Normal    Info                           configmap/who-internal-prometheus-k8s-rules                     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: who-internal-prometheus-k8s-rules
    namespace: as-ciat
    resourceVersion: 751367784
    uid: ad97d5ef-3d27-4d80-ac11-93eed7165c61
}: Received add rule event
26m         Normal    Info                           configmap/who-internal-prometheus-k8s-rules                     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: who-internal-prometheus-k8s-rules
    namespace: as-ciat
    resourceVersion: 751367784
    uid: ad97d5ef-3d27-4d80-ac11-93eed7165c61
}: Pushed rule to kafka
54m         Warning   FailedCreate                   replicaset/who-internal-who-deployment-67d6d8c44b               Error creating: pods "who-internal-who-deployment-67d6d8c44b-bp8vn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/who-internal-who-deployment-7bd5c447d9               Error creating: pods "who-internal-who-deployment-7bd5c447d9-7jn9n" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/who-internal-who-deployment-7bd5c447d9               Error creating: pods "who-internal-who-deployment-7bd5c447d9-4rf2j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/who-internal-who-deployment-7bd5c447d9               Error creating: pods "who-internal-who-deployment-7bd5c447d9-gj7nb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/who-internal-who-deployment-7bd5c447d9               Error creating: pods "who-internal-who-deployment-7bd5c447d9-l8jt8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/who-internal-who-deployment-7bd5c447d9               Error creating: pods "who-internal-who-deployment-7bd5c447d9-d7qgq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/who-internal-who-deployment-7bd5c447d9               Error creating: pods "who-internal-who-deployment-7bd5c447d9-jbhdp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/who-internal-who-deployment-7bd5c447d9               Error creating: pods "who-internal-who-deployment-7bd5c447d9-llvpz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/who-internal-who-deployment-7bd5c447d9               Error creating: pods "who-internal-who-deployment-7bd5c447d9-nxs4h" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/who-internal-who-deployment-7bd5c447d9               Error creating: pods "who-internal-who-deployment-7bd5c447d9-zffck" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
35m         Warning   FailedCreate                   replicaset/who-internal-who-deployment-7bd5c447d9               (combined from similar events): Error creating: pods "who-internal-who-deployment-7bd5c447d9-bzcqj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
25m         Warning   FailedCreate                   replicaset/who-internal-who-deployment-7bd5c447d9               Error creating: pods "who-internal-who-deployment-7bd5c447d9-5l7sd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/who-internal-who-deployment-7cb74474d7               Error creating: pods "who-internal-who-deployment-7cb74474d7-w7tbb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/who-internal-who-deployment-7cb74474d7               Error creating: pods "who-internal-who-deployment-7cb74474d7-2hv7x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/who-internal-who-deployment-7cb74474d7               Error creating: pods "who-internal-who-deployment-7cb74474d7-9t9nq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/who-internal-who-deployment-7cb74474d7               Error creating: pods "who-internal-who-deployment-7cb74474d7-s8fzl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/who-internal-who-deployment-7cb74474d7               Error creating: pods "who-internal-who-deployment-7cb74474d7-pjq28" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/who-internal-who-deployment-7cb74474d7               Error creating: pods "who-internal-who-deployment-7cb74474d7-ht6rw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/who-internal-who-deployment-7cb74474d7               Error creating: pods "who-internal-who-deployment-7cb74474d7-mm4lg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/who-internal-who-deployment-7cb74474d7               Error creating: pods "who-internal-who-deployment-7cb74474d7-qmc44" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/who-internal-who-deployment-7cb74474d7               Error creating: pods "who-internal-who-deployment-7cb74474d7-xgpbb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
6m6s        Warning   FailedCreate                   replicaset/who-internal-who-deployment-7cb74474d7               (combined from similar events): Error creating: pods "who-internal-who-deployment-7cb74474d7-ljndq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Normal    ScalingReplicaSet              deployment/who-internal-who-deployment                          Scaled down replica set who-internal-who-deployment-67d6d8c44b to 0
47m         Normal    ScalingReplicaSet              deployment/who-internal-who-deployment                          Scaled up replica set who-internal-who-deployment-7bd5c447d9 to 1
17m         Normal    ScalingReplicaSet              deployment/who-internal-who-deployment                          Scaled down replica set who-internal-who-deployment-7bd5c447d9 to 0
17m         Normal    ScalingReplicaSet              deployment/who-internal-who-deployment                          Scaled up replica set who-internal-who-deployment-7cb74474d7 to 1
23m         Normal    Info                           service/who-internal-who                                        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: who-internal-who
    namespace: as-ciat
    resourceVersion: 751637097
    uid: 6812769a-0c6a-4615-8002-f8cc8feff0f9
}: Created service monitor
23m         Normal    Info                           service/who-internal-who                                        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: who-internal-who
    namespace: as-ciat
    resourceVersion: 751637097
    uid: 6812769a-0c6a-4615-8002-f8cc8feff0f9
}: Received add service event
26m         Normal    Info                           configmap/who-prometheus-k8s-rules                              Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: who-prometheus-k8s-rules
    namespace: as-ciat
    resourceVersion: 714897381
    uid: 286b8003-8359-4e3e-9d95-b6ac8f5f09fa
}: Received add rule event
26m         Normal    Info                           configmap/who-prometheus-k8s-rules                              Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: who-prometheus-k8s-rules
    namespace: as-ciat
    resourceVersion: 714897381
    uid: 286b8003-8359-4e3e-9d95-b6ac8f5f09fa
}: Pushed rule to kafka
47m         Warning   FailedCreate                   replicaset/who-who-deployment-7ff986854f                        Error creating: pods "who-who-deployment-7ff986854f-9cq9k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/who-who-deployment-7ff986854f                        Error creating: pods "who-who-deployment-7ff986854f-nlmgk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/who-who-deployment-7ff986854f                        Error creating: pods "who-who-deployment-7ff986854f-nd26f" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/who-who-deployment-7ff986854f                        Error creating: pods "who-who-deployment-7ff986854f-f4fsx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/who-who-deployment-7ff986854f                        Error creating: pods "who-who-deployment-7ff986854f-bk9b5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/who-who-deployment-7ff986854f                        Error creating: pods "who-who-deployment-7ff986854f-8hqhp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/who-who-deployment-7ff986854f                        Error creating: pods "who-who-deployment-7ff986854f-dzjgd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/who-who-deployment-7ff986854f                        Error creating: pods "who-who-deployment-7ff986854f-lc4pd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate                   replicaset/who-who-deployment-7ff986854f                        Error creating: pods "who-who-deployment-7ff986854f-hv7mz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate                   replicaset/who-who-deployment-7ff986854f                        (combined from similar events): Error creating: pods "who-who-deployment-7ff986854f-8vgqh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
25m         Warning   FailedCreate                   replicaset/who-who-deployment-7ff986854f                        Error creating: pods "who-who-deployment-7ff986854f-4qbw4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/who-who-deployment-c754dcc9                          Error creating: pods "who-who-deployment-c754dcc9-j6cbf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/who-who-deployment-c754dcc9                          Error creating: pods "who-who-deployment-c754dcc9-rp2fg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/who-who-deployment-c754dcc9                          Error creating: pods "who-who-deployment-c754dcc9-gq7bm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/who-who-deployment-c754dcc9                          Error creating: pods "who-who-deployment-c754dcc9-bx7s8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/who-who-deployment-c754dcc9                          Error creating: pods "who-who-deployment-c754dcc9-mmg76" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/who-who-deployment-c754dcc9                          Error creating: pods "who-who-deployment-c754dcc9-4gm92" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/who-who-deployment-c754dcc9                          Error creating: pods "who-who-deployment-c754dcc9-n4rvn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/who-who-deployment-c754dcc9                          Error creating: pods "who-who-deployment-c754dcc9-grcln" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate                   replicaset/who-who-deployment-c754dcc9                          Error creating: pods "who-who-deployment-c754dcc9-t4kqx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
6m4s        Warning   FailedCreate                   replicaset/who-who-deployment-c754dcc9                          (combined from similar events): Error creating: pods "who-who-deployment-c754dcc9-7gpvp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate                   replicaset/who-who-deployment-d7b89dcb                          Error creating: pods "who-who-deployment-d7b89dcb-7k8w6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Normal    ScalingReplicaSet              deployment/who-who-deployment                                   Scaled down replica set who-who-deployment-d7b89dcb to 0
47m         Normal    ScalingReplicaSet              deployment/who-who-deployment                                   Scaled up replica set who-who-deployment-7ff986854f to 1
17m         Normal    ScalingReplicaSet              deployment/who-who-deployment                                   Scaled down replica set who-who-deployment-7ff986854f to 0
17m         Normal    ScalingReplicaSet              deployment/who-who-deployment                                   Scaled up replica set who-who-deployment-c754dcc9 to 1
10m         Normal    Info                           service/who-who                                                 Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: who-who
    namespace: as-ciat
    resourceVersion: 751636962
    uid: e491e275-d6d1-4e6f-aff2-01c4972b1318
}: Received add service event
10m         Normal    Info                           service/who-who                                                 Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: who-who
    namespace: as-ciat
    resourceVersion: 751636962
    uid: e491e275-d6d1-4e6f-aff2-01c4972b1318
}: Created service monitor
[20:January:2022:07:45:57]:(corp_us-ashburn-1_dataplane):~
○ $ kgn as-jcs ResourceQuota -oyaml
apiVersion: v1
items:
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    annotations:
      meta.helm.sh/release-name: as-jcs
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2021-11-29T23:57:15Z"
    labels:
      app: oracle-namespace-0.0.9
      app.kubernetes.io/managed-by: Helm
      chart: oracle-namespace-0.0.9
      heritage: Helm
      release: as-jcs
    name: as-jcs-resource-quota
    namespace: as-jcs
    resourceVersion: "769877222"
    selfLink: /api/v1/namespaces/as-jcs/resourcequotas/as-jcs-resource-quota
    uid: ba4aec9c-fad9-425e-be40-715f537454c8
  spec:
    hard:
      limits.cpu: "32"
      limits.memory: 112Gi
      pods: "50"
      requests.cpu: "16"
      requests.memory: 80Gi
  status:
    hard:
      limits.cpu: "32"
      limits.memory: 112Gi
      pods: "50"
      requests.cpu: "16"
      requests.memory: 80Gi
    used:
      limits.cpu: 2100m
      limits.memory: 4224Mi
      pods: "2"
      requests.cpu: 2100m
      requests.memory: 4224Mi
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
[20:January:2022:07:52:26]:(corp_us-ashburn-1_dataplane):~
○ $ kgpn as-jcs -o wide
NAME                                           READY   STATUS              RESTARTS   AGE    IP              NODE          NOMINATED NODE   READINESS GATES
echo-769c9d8c54-tjvkp                          0/1     ErrImageNeverPull   0          3h5m   10.245.11.112   10.11.8.46    <none>           <none>
who-internal-who-deployment-869d6dd88d-kt955   2/2     Running             317        5d9h   10.245.18.30    10.11.9.160   <none>           <none>
[20:January:2022:07:52:59]:(corp_us-ashburn-1_dataplane):~
○ $ hln as-jcs
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
NAME  NAMESPACE REVISION  UPDATED STATUS  CHART APP VERSION
[20:January:2022:07:55:24]:(corp_us-ashburn-1_dataplane):~
○ $ kdpn as-jcs echo-769c9d8c54-tjvkp
Name:         echo-769c9d8c54-tjvkp
Namespace:    as-jcs
Priority:     0
Node:         10.11.8.46/10.11.8.46
Start Time:   Thu, 20 Jan 2022 04:47:00 +0000
Labels:       app=echo
              pod-template-hash=769c9d8c54
Annotations:  kubernetes.io/psp: psp-protect-docker
Status:       Pending
IP:           10.245.11.112
IPs:
  IP:           10.245.11.112
Controlled By:  ReplicaSet/echo-769c9d8c54
Containers:
  app:
    Container ID:
    Image:          echo:latest
    Image ID:
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ErrImageNeverPull
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  2Gi
    Requests:
      cpu:        1
      memory:     2Gi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-f8w7v (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  default-token-f8w7v:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-f8w7v
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason             Age                     From     Message
  ----     ------             ----                    ----     -------
  Warning  ErrImageNeverPull  3m35s (x856 over 3h8m)  kubelet  Container image "echo:latest" is not present with pull policy of Never
[20:January:2022:07:55:36]:(corp_us-ashburn-1_dataplane):~
○ $ kdpn as-jcs who-internal-who-deployment-869d6dd88d-kt955
Name:         who-internal-who-deployment-869d6dd88d-kt955
Namespace:    as-jcs
Priority:     0
Node:         10.11.9.160/10.11.9.160
Start Time:   Fri, 14 Jan 2022 22:47:28 +0000
Labels:       app=who
              cluster=as-jcs
              isInternalDeployment=true
              pod-template-hash=869d6dd88d
Annotations:  as.osvc.oracle.com/app-version: 1dac91aaf88209fd8a62c3087480ba74ce64caba
              as.osvc.oracle.com/managed: true
              kubectl.kubernetes.io/default-container: server
              kubernetes.io/psp: psp-protect-docker
              vault.security.banzaicloud.io/log-level: warn
              vault.security.banzaicloud.io/vault-addr: https://vault.query.corp.consul:8200
              vault.security.banzaicloud.io/vault-agent-configmap: as-jcs-va-configmap
              vault.security.banzaicloud.io/vault-env-daemon: true
              vault.security.banzaicloud.io/vault-path: k8s-iad-dataplane
              vault.security.banzaicloud.io/vault-role: as-jcs_role
              vault.security.banzaicloud.io/vault-skip-verify: true
Status:       Running
IP:           10.245.18.30
IPs:
  IP:           10.245.18.30
Controlled By:  ReplicaSet/who-internal-who-deployment-869d6dd88d
Init Containers:
  copy-vault-env:
    Container ID:  docker://7ecffa14b06a0a99450179032f01a3275f0e39b185ecc71bfcf16f44a3d346a4
    Image:         docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3
    Image ID:      docker-pullable://docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env@sha256:7435d1c641c0aa0fd4faaff32ab189b29d3d909b100d36d4d90df3b79cee8ce3
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -c
      cp /usr/local/bin/vault-env /vault/
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 14 Jan 2022 22:47:29 +0000
      Finished:     Fri, 14 Jan 2022 22:47:29 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     250m
      memory:  64Mi
    Requests:
      cpu:        50m
      memory:     64Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-f8w7v (ro)
      /vault/ from vault-env (rw)
Containers:
  vault-agent:
    Container ID:  docker://b69058327f8d6616661708bd6327de75a0939b153c476928d3d3c9f2fcc7ec3d
    Image:         docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5
    Image ID:      docker-pullable://docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault@sha256:57438e97b72c05368ca06f4d5f34667e965248635718001abdea8fa11f18adad
    Port:          <none>
    Host Port:     <none>
    Args:
      agent
      -config
      /vault/config/config.hcl
    State:          Running
      Started:      Tue, 18 Jan 2022 14:29:00 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    2
      Started:      Fri, 14 Jan 2022 22:47:30 +0000
      Finished:     Tue, 18 Jan 2022 14:28:59 +0000
    Ready:          True
    Restart Count:  1
    Limits:
      cpu:     100m
      memory:  128Mi
    Requests:
      cpu:     100m
      memory:  128Mi
    Environment:
      VAULT_ADDR:         https://vault.query.corp.consul:8200
      VAULT_SKIP_VERIFY:  true
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-f8w7v (ro)
      /vault/ from vault-env (rw)
      /vault/config/config.hcl from agent-configmap (ro,path="config.hcl")
      /vault/secrets from agent-secrets (rw)
  server:
    Container ID:  docker://ed35fd02af482c373718fa68579de05ffc7297df53ef6773faa7764abfa9cd88
    Image:         iad.ocir.io/osvccorp/bui/who:master-2022.01.11-16.47.10-1dac91aa
    Image ID:      docker-pullable://iad.ocir.io/osvccorp/bui/who@sha256:76633668d4c6d9a325d7a9f44e2443ca3dc5b46dbb1bc2ffdb9702b94c90bcd1
    Port:          7001/TCP
    Host Port:     0/TCP
    Command:
      /vault/vault-env
    Args:
      /bin/sh
      -c
      java     -XX:+UnlockExperimentalVMOptions     -XX:+UseCGroupMemoryLimitForHeap     -XX:+HeapDumpOnOutOfMemoryError     -XX:HeapDumpPath=/app/logs/heap_pid%%p.hprof     -XX:+UseG1GC     -XX:MaxGCPauseMillis=250     -Dorg.oracle.fusion.service.kubernetes.namespace=${NAMESPACE}     -Dorg.oracle.fusion.service.ContextPath=/app/config     -Dorg.oracle.fusion.service.resourcePath=/app/config     -Dorg.oracle.fusion.service.hostName=0.0.0.0     -Dorg.oracle.fusion.service.portNumber=7001     -Dorg.oracle.fusion.service.consulHttpApiUrl=${CONSUL}     -Dconsul.base.address=${CONSUL}     -Dorg.oracle.fusion.service.path=${SERVICE_PATH}     -Dorg.oracle.fusion.service.devMode=${DEVMODE}     -Dorg.oracle.fusion.service.buiBasePath=${BUI_BASE_PATH}     -Dorg.oracle.fusion.service.clusterName=${CLUSTER}     -Djava.rmi.server.hostname=who     -Dlogback.configurationFile=/app/config/live/logback.xml     -Dorg.oracle.fusion.service.jolokiaRealmFile=/app/config/realm.properties     -Dorg.oracle.fusion.service.jolokiaRole=asJolokiaRole     -Duser.dir=/app     ${JAVA_OPTS}     -cp     /app/*:/app/lib/*     org.oracle.service.router.Host
    State:          Running
      Started:      Tue, 18 Jan 2022 14:32:41 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Tue, 18 Jan 2022 14:27:28 +0000
      Finished:     Tue, 18 Jan 2022 14:27:38 +0000
    Ready:          True
    Restart Count:  316
    Limits:
      cpu:     1
      memory:  2Gi
    Requests:
      cpu:      1
      memory:   2Gi
    Liveness:   http-get http://:7001/AgentWeb/ping delay=0s timeout=1s period=10s #success=1 #failure=5
    Readiness:  http-get http://:7001/AgentWeb/ready delay=0s timeout=1s period=10s #success=1 #failure=10
    Environment:
      JAVA_OPTS:                     -Dorg.oracle.fusion.service.k8s=true -Dorg.oracle.fusion.service.kubernetes.domain=cluster.local -Dorg.oracle.fusion.service.kubernetes.singleVersion=true -Dorg.oracle.fusion.service.jaeger.enabled=true -Dorg.oracle.fusion.service.forwardedForHeader=X-Original-Forwarded-For -Dorg.oracle.fusion.service.defaultVersion=1cd2ca8f -Dorg.oracle.fusion.service.consul.certPath=/consul/ca.crt -Dorg.oracle.fusion.service.vault.token-path=/vault/.vault-token -Dorg.oracle.fusion.service.vault.enabled=true -Dorg.oracle.fusion.service.vault.truststore=/vault/ca.crt -Dorg.oracle.fusion.service.vault.ssl.enabled=true
                                     -Dkafka.bootstrap.servers=bui-kafka-kafka-brokers.bui:9093 -Dkafka.security.protocol=SSL -Dkafka.auth.keystore.path=/vault/secrets/keystore.p12 -Dkafka.auth.truststore.path=/vault/secrets/truststore.p12 -Dkafka.auth.issuing.ca.path=/vault/secrets/issuing_ca.pem -Dkafka.auth.private.key.path=/vault/secrets/private_key.pem -Dkafka.auth.certificate.path=/vault/secrets/certificate.pem
                                     -Dlogging.api.kafka.enabled=false -Dschema.registry.url=https://schemaregistry-proxy-kafka.service.iad-dataplane.corp.consul -Dorg.oracle.fusion.service.who.internalOnly=true
      CLUSTER:                       as-jcs
      VERSION:                       as-jcs (v1:metadata.namespace)
      SERVICE_PATH:                  /AgentWeb
      BUI_BASE_PATH:                 as-jcs.corp.channels.ocs.oc-test.com
      DEVMODE:                       true
      NAMESPACE:                     as-jcs (v1:metadata.namespace)
      HOST_IP:                        (v1:status.hostIP)
      CONSUL:                        https://$(HOST_IP):8501
      CONSUL_TOKEN:                  vault:/cpe_consul/creds/as-jcs_consul_role#token
      JAEGER_AGENT_HOST:              (v1:status.hostIP)
      JAEGER_AGENT_PORT:             6831
      JAEGER_AGENT_PROTOCOL:         udp
      VAULT_ADDR:                    https://vault.query.corp.consul:8200
      VAULT_SKIP_VERIFY:             true
      VAULT_AUTH_METHOD:             jwt
      VAULT_PATH:                    k8s-iad-dataplane
      VAULT_ROLE:                    as-jcs_role
      VAULT_IGNORE_MISSING_SECRETS:  false
      VAULT_ENV_PASSTHROUGH:
      VAULT_JSON_LOG:                false
      VAULT_CLIENT_TIMEOUT:          10s
      VAULT_LOG_LEVEL:               warn
      VAULT_ENV_DAEMON:              true
    Mounts:
      /app/config/live from who-config (rw)
      /consul from consul-ca (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-f8w7v (ro)
      /vault/ from vault-env (rw)
      /vault/ca.crt from vault-ca (ro,path="ca.crt")
      /vault/secrets from agent-secrets (rw)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  who-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      who-internal-config
    Optional:  false
  consul-ca:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  consul-tls
    Optional:    false
  vault-ca:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  vault-tls
    Optional:    false
  default-token-f8w7v:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-f8w7v
    Optional:    false
  vault-env:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  agent-secrets:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  agent-configmap:
    Type:        ConfigMap (a volume populated by a ConfigMap)
    Name:        as-jcs-va-configmap
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason       Age                      From     Message
  ----     ------       ----                     ----     -------
  Warning  FailedMount  2m36s (x117 over 3h44m)  kubelet  MountVolume.SetUp failed for volume "who-config" : configmap "who-internal-config" not found
[20:January:2022:07:55:52]:(corp_us-ashburn-1_dataplane):~
○ $
[20:January:2022:07:56:11]:(corp_us-ashburn-1_dataplane):~
○ $ kgn as-jcs events
LAST SEEN   TYPE      REASON              OBJECT                                             MESSAGE
53m         Warning   FailedCreate        replicaset/echo-6677859554                         Error creating: pods "echo-6677859554-kscp4" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
37m         Warning   FailedCreate        replicaset/echo-6677859554                         Error creating: pods "echo-6677859554-nzqnv" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
20m         Warning   FailedCreate        replicaset/echo-6677859554                         Error creating: pods "echo-6677859554-kwlj8" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
3m40s       Warning   FailedCreate        replicaset/echo-6677859554                         Error creating: pods "echo-6677859554-xgb5v" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
4m17s       Warning   ErrImageNeverPull   pod/echo-769c9d8c54-tjvkp                          Container image "echo:latest" is not present with pull policy of Never
3m2s        Warning   FailedMount         pod/who-internal-who-deployment-869d6dd88d-kt955   MountVolume.SetUp failed for volume "who-config" : configmap "who-internal-config" not found
[20:January:2022:07:56:19]:(corp_us-ashburn-1_dataplane):~
○ $ #kgn as-ciat events
[20:January:2022:07:57:38]:(corp_us-ashburn-1_dataplane):~
○ $ hls as-ciat
bash: hls: command not found
[20:January:2022:08:04:59]:(corp_us-ashburn-1_dataplane):~
○ $ helm ls -n as-ciat
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
NAME          NAMESPACE REVISION  UPDATED                                 STATUS    CHART               APP VERSION
horton        as-ciat   1         2022-01-20 07:58:28.289381908 +0000 UTC deployed  horton-dock-22.4.10 565ff6986f4ec0511b1de2218ba3c3244ad23e75
lorax         as-ciat   1         2022-01-20 07:58:28.318276004 +0000 UTC deployed  lorax-dock-22.4.7   431ac7a500783c0421ee8721a3f7547f480becc9
who           as-ciat   475       2022-01-20 07:58:26.188549191 +0000 UTC deployed  who-master-22.4.1   49f19d6e243010f2154536e8a95e18c1c5f07b48
who-internal  as-ciat   125       2022-01-20 07:58:40.13088022 +0000 UTC  deployed  who-master-22.4.1   49f19d6e243010f2154536e8a95e18c1c5f07b48
[20:January:2022:08:05:09]:(corp_us-ashburn-1_dataplane):~
○ $ helm ls -n as-jcs
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
NAME  NAMESPACE REVISION  UPDATED STATUS  CHART APP VERSION
[20:January:2022:08:05:22]:(corp_us-ashburn-1_dataplane):~
○ $ kgpn as-jcs
NAME                                           READY   STATUS              RESTARTS   AGE     IP              NODE          NOMINATED NODE   READINESS GATES
echo-769c9d8c54-tjvkp                          0/1     ErrImageNeverPull   0          3h18m   10.245.11.112   10.11.8.46    <none>           <none>
who-internal-who-deployment-869d6dd88d-kt955   2/2     Running             317        5d9h    10.245.18.30    10.11.9.160   <none>           <none>
[20:January:2022:08:05:44]:(corp_us-ashburn-1_dataplane):~
○ $ kgpn as-ciat
NAME                                           READY   STATUS    RESTARTS   AGE     IP              NODE          NOMINATED NODE   READINESS GATES
lorax-analyticsmanager-75d985dfdf-6jmjm        2/2     Running   0          8h      10.245.14.74    10.11.9.104   <none>           <none>
lorax-analyticsmanager-75d985dfdf-mmxrw        2/2     Running   0          8h      10.245.13.192   10.11.9.145   <none>           <none>
lorax-analyticsmanager-75d985dfdf-zfr7b        2/2     Running   0          8h      10.245.20.140   10.11.8.225   <none>           <none>
lorax-cachehost-647b6cf7b4-4v6n7               2/2     Running   0          6h36m   10.245.13.197   10.11.9.145   <none>           <none>
lorax-cachehost-647b6cf7b4-jsqlv               2/2     Running   0          6h36m   10.245.27.178   10.11.8.60    <none>           <none>
lorax-cachehost-6c8948bf68-8gmdf               2/2     Running   0          8h      10.245.19.21    10.11.9.238   <none>           <none>
lorax-cachehost-6c8948bf68-mzvx7               2/2     Running   0          8h      10.245.26.8     10.11.8.67    <none>           <none>
lorax-cachehost-6c8948bf68-vf26t               2/2     Running   0          8h      10.245.15.225   10.11.9.78    <none>           <none>
lorax-commandmanager-586d58947d-2h89f          2/2     Running   0          8h      10.245.28.203   10.11.9.56    <none>           <none>
lorax-commandmanager-586d58947d-2rcmp          2/2     Running   0          8h      10.245.11.175   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-c4482          2/2     Running   0          8h      10.245.27.176   10.11.8.60    <none>           <none>
lorax-contextmanager-67956c7584-c4xkw          2/2     Running   0          8h      10.245.11.177   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-h82q5          2/2     Running   0          8h      10.245.3.92     10.11.8.232   <none>           <none>
lorax-customizationmanager-657bbf9586-h242p    2/2     Running   0          8h      10.245.16.201   10.11.8.203   <none>           <none>
lorax-customizationmanager-657bbf9586-rsbqg    2/2     Running   0          8h      10.245.29.214   10.11.9.62    <none>           <none>
lorax-elementmanager-7f74646f4d-n4cqx          2/2     Running   0          8h      10.245.28.39    10.11.8.31    <none>           <none>
lorax-eventmanager-759c5b9484-ch28f            2/2     Running   0          8h      10.245.15.226   10.11.9.78    <none>           <none>
lorax-eventworker-7c5b779bc-z4bb5              2/2     Running   0          8h      10.245.22.102   10.11.8.250   <none>           <none>
lorax-queuemanager-769996b97b-z4kqc            2/2     Running   0          8h      10.245.16.24    10.11.8.222   <none>           <none>
lorax-realtimedatamanager-866cd5886b-qq54f     2/2     Running   0          8h      10.245.29.213   10.11.9.62    <none>           <none>
lorax-reportdesignmanager-5c88cb777d-sxlx9     2/2     Running   0          8h      10.245.20.94    10.11.8.160   <none>           <none>
lorax-reportpublishmanager-58445646c4-9ztc7    2/2     Running   0          8h      10.245.11.176   10.11.9.208   <none>           <none>
lorax-searchmanager-7d97d6dfcc-gn4zg           2/2     Running   0          8h      10.245.29.212   10.11.9.62    <none>           <none>
lorax-searchmanager-7d97d6dfcc-q2x5t           2/2     Running   0          8h      10.245.14.73    10.11.9.104   <none>           <none>
lorax-securityservice-b9bb8695d-t7bz5          2/2     Running   0          8h      10.245.12.189   10.11.8.194   <none>           <none>
who-internal-who-deployment-65d44d6f4c-fqhbs   2/2     Running   0          8h      10.245.25.112   10.11.8.142   <none>           <none>
who-internal-who-deployment-65d44d6f4c-w8lnq   2/2     Running   0          8h      10.245.11.179   10.11.9.208   <none>           <none>
who-internal-who-deployment-65d44d6f4c-xljbh   2/2     Running   0          8h      10.245.16.199   10.11.8.203   <none>           <none>
who-who-deployment-576cd44f87-4dr8c            2/2     Running   0          8h      10.245.9.42     10.11.9.136   <none>           <none>
who-who-deployment-576cd44f87-lhb49            2/2     Running   0          8h      10.245.16.200   10.11.8.203   <none>           <none>
who-who-deployment-576cd44f87-lkhrc            2/2     Running   0          8h      10.245.23.136   10.11.8.174   <none>           <none>
[20:January:2022:08:05:55]:(corp_us-ashburn-1_dataplane):~
○ $ hln as-ciat
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
NAME          NAMESPACE REVISION  UPDATED                                 STATUS    CHART               APP VERSION
horton        as-ciat   1         2022-01-20 07:58:28.289381908 +0000 UTC deployed  horton-dock-22.4.10 565ff6986f4ec0511b1de2218ba3c3244ad23e75
lorax         as-ciat   1         2022-01-20 07:58:28.318276004 +0000 UTC deployed  lorax-dock-22.4.7   431ac7a500783c0421ee8721a3f7547f480becc9
who           as-ciat   475       2022-01-20 07:58:26.188549191 +0000 UTC deployed  who-master-22.4.1   49f19d6e243010f2154536e8a95e18c1c5f07b48
who-internal  as-ciat   125       2022-01-20 07:58:40.13088022 +0000 UTC  deployed  who-master-22.4.1   49f19d6e243010f2154536e8a95e18c1c5f07b48
[20:January:2022:08:06:08]:(corp_us-ashburn-1_dataplane):~
○ $ kgdpn as-ciat
NAME                          READY   UP-TO-DATE   AVAILABLE   AGE
horton-horton-deployment      0/3     0            0           8m50s
lorax-analyticsmanager        0/3     0            0           8m49s
lorax-cachehost               0/3     0            0           8m49s
lorax-commandmanager          0/2     0            0           8m49s
lorax-contextmanager          0/3     0            0           8m49s
lorax-customizationmanager    0/2     0            0           8m49s
lorax-elementmanager          0/1     0            0           8m49s
lorax-eventmanager            0/2     0            0           8m49s
lorax-eventworker             0/1     0            0           8m49s
lorax-queuemanager            0/1     0            0           8m49s
lorax-realtimedatamanager     0/1     0            0           8m49s
lorax-reportdesignmanager     0/1     0            0           8m49s
lorax-reportpublishmanager    0/1     0            0           8m49s
lorax-searchmanager           0/2     0            0           8m49s
lorax-securityservice         0/2     0            0           8m49s
who-internal-who-deployment   3/3     0            3           8d
who-who-deployment            3/3     0            3           27d
[20:January:2022:08:07:20]:(corp_us-ashburn-1_dataplane):~
○ $ kgn as-ciat services
NAME                                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
as-ciat-agentbrowserui-20220120-07563127-q8qa         ClusterIP   10.97.48.0      <none>        80/TCP              9m25s
as-ciat-analyticsmanager-20220120-07563127-q8qa       ClusterIP   10.97.91.223    <none>        80/TCP              9m24s
as-ciat-cachehost-20220120-07563127-q8qa              ClusterIP   10.97.80.66     <none>        80/TCP              9m24s
as-ciat-coherence-20220120-07563127-q8qa              ClusterIP   None            <none>        9000/TCP,9001/TCP   9m24s
as-ciat-commandmanager-20220120-07563127-q8qa         ClusterIP   10.97.20.47     <none>        80/TCP              9m24s
as-ciat-contextmanager-20220120-07563127-q8qa         ClusterIP   10.97.2.28      <none>        80/TCP              9m24s
as-ciat-customizationmanager-20220120-07563127-q8qa   ClusterIP   10.97.201.59    <none>        80/TCP              9m24s
as-ciat-elementmanager-20220120-07563127-q8qa         ClusterIP   10.97.103.234   <none>        80/TCP              9m24s
as-ciat-eventmanager-20220120-07563127-q8qa           ClusterIP   10.97.210.242   <none>        80/TCP              9m24s
as-ciat-eventworker-20220120-07563127-q8qa            ClusterIP   10.97.76.215    <none>        80/TCP              9m24s
as-ciat-queuemanager-20220120-07563127-q8qa           ClusterIP   10.97.139.22    <none>        80/TCP              9m24s
as-ciat-realtimedatamanager-20220120-07563127-q8qa    ClusterIP   10.97.16.155    <none>        80/TCP              9m24s
as-ciat-reportdesignmanager-20220120-07563127-q8qa    ClusterIP   10.97.143.248   <none>        80/TCP              9m24s
as-ciat-reportpublishmanager-20220120-07563127-q8qa   ClusterIP   10.97.157.183   <none>        80/TCP              9m24s
as-ciat-searchmanager-20220120-07563127-q8qa          ClusterIP   10.97.206.139   <none>        80/TCP              9m24s
as-ciat-securityservice-20220120-07563127-q8qa        ClusterIP   10.97.71.88     <none>        80/TCP              9m24s
who-internal-who                                      ClusterIP   10.97.249.132   <none>        80/TCP              8d
who-who                                               ClusterIP   10.97.1.245     <none>        80/TCP              27d
[20:January:2022:08:07:55]:(corp_us-ashburn-1_dataplane):~
○ $ cdop
Removed File: /tmp/01_primordial_git_branch
Removed File: /tmp/10_compartments_git_branch
Removed File: /tmp/opc_git_branch
Removed File: /tmp/osvc-infra_git_branch
Removed File: /tmp/tmp_git_branch
[20:January:2022:08:08:46]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ gup
Already on 'main'
git pull --ff-only
Use of the Oracle network and applications is intended solely for Oracle's authorized users. The use of these resources by Oracle employees and contractors is subject to company policies, including the Code of Conduct, Acceptable Use Policy and Information Protection Policy; access may be monitored and logged, to the extent permitted by law, in accordance with Oracle policies. Unauthorized use may result in termination of your access, disciplinary action and/or civil and criminal penalties.
remote: Enumerating objects: 64, done.
remote: Counting objects: 100% (64/64), done.
remote: Compressing objects: 100% (34/34), done.
remote: Total 64 (delta 28), reused 64 (delta 28), pack-reused 0
Unpacking objects: 100% (64/64), done.
From orahub.oci.oraclecorp.com:osvc-sre-dev/osvc-platform
   51193ad..0758d64  210518-000014_SourceLevelMetrics -> origin/210518-000014_SourceLevelMetrics
   5d4773a..0139083  220112-000023_Address_agora_app_review_comments_preprod -> origin/220112-000023_Address_agora_app_review_comments_preprod
   8c15578..e77a1fe  220112-000023_Address_agora_app_review_comments_prod -> origin/220112-000023_Address_agora_app_review_comments_prod
 * [new branch]      agora_kafka_2.8.0 -> origin/agora_kafka_2.8.0
   0a1d687..e8530cf  opaec_snapshot_main -> origin/opaec_snapshot_main
Already up-to-date.
/home/opc/galorndon/osvc-platform
[20:January:2022:08:10:05]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn as-jcs
NAME                                           READY   STATUS              RESTARTS   AGE     IP              NODE          NOMINATED NODE   READINESS GATES
echo-769c9d8c54-tjvkp                          0/1     ErrImageNeverPull   0          3h28m   10.245.11.112   10.11.8.46    <none>           <none>
lorax-analyticsmanager-c74f47d85-kp2f6         2/2     Running             0          3m24s   10.245.23.62    10.11.9.195   <none>           <none>
lorax-cachehost-54b4d8cf87-kv4xp               1/2     ImagePullBackOff    0          3m22s   10.245.31.36    10.11.9.6     <none>           <none>
lorax-cachehost-54b4d8cf87-nrr25               2/2     Running             0          3m24s   10.245.23.63    10.11.9.195   <none>           <none>
lorax-commandmanager-6874498b4f-rnjn2          2/2     Running             0          3m25s   10.245.23.61    10.11.9.195   <none>           <none>
lorax-contextmanager-55c6bbbfcd-2bdms          1/2     ImagePullBackOff    0          3m25s   10.245.31.33    10.11.9.6     <none>           <none>
lorax-customizationmanager-6946844dc7-zpdvw    1/2     ImagePullBackOff    0          3m22s   10.245.31.37    10.11.9.6     <none>           <none>
lorax-elementmanager-69f8686c4c-8mpmw          2/2     Running             0          3m22s   10.245.23.66    10.11.9.195   <none>           <none>
lorax-eventmanager-7bff46cb7-d2rj7             2/2     Running             0          3m23s   10.245.23.64    10.11.9.195   <none>           <none>
lorax-eventworker-79b4d4f5f4-8gtwj             1/2     ImagePullBackOff    0          3m25s   10.245.31.32    10.11.9.6     <none>           <none>
lorax-queuemanager-5d6f67b79f-4nnhn            2/2     Running             0          3m22s   10.245.23.67    10.11.9.195   <none>           <none>
lorax-realtimedatamanager-75c56cdbc5-sg9gv     1/2     ImagePullBackOff    0          3m23s   10.245.31.34    10.11.9.6     <none>           <none>
lorax-reportdesignmanager-78559cf46c-lvqgf     1/2     ImagePullBackOff    0          3m23s   10.245.31.35    10.11.9.6     <none>           <none>
lorax-reportpublishmanager-75b9d675f7-6h44x    1/2     ImagePullBackOff    0          3m25s   10.245.31.30    10.11.9.6     <none>           <none>
lorax-searchmanager-665bc9f64b-szhtw           2/2     Running             0          3m22s   10.245.23.65    10.11.9.195   <none>           <none>
lorax-securityservice-77d5fcbc47-ch46d         1/2     ImagePullBackOff    0          3m25s   10.245.31.31    10.11.9.6     <none>           <none>
who-internal-who-deployment-869d6dd88d-kt955   2/2     Running             317        5d9h    10.245.18.30    10.11.9.160   <none>           <none>
[20:January:2022:08:15:21]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ hln as-jcs
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
NAME  NAMESPACE REVISION  UPDATED                                 STATUS    CHART                                           APP VERSION
lorax as-jcs    1         2022-01-20 08:11:50.711153245 +0000 UTC deployed  lorax-dock-1.0.0-beta-20220119-17513105-nxhi    431ac7a500783c0421ee8721a3f7547f480becc9
[20:January:2022:08:15:39]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgn as-jcs events
LAST SEEN   TYPE     REASON   OBJECT                                MESSAGE
4m9s        Normal   Info     service/as-jcs-analyticsmanager-bui   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-analyticsmanager-bui
    namespace: as-jcs
    resourceVersion: 770181958
    uid: d19ef707-d763-4f48-a96b-14890d728f14
}: Received add service event
4m9s        Normal   Info     service/as-jcs-analyticsmanager-bui   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-analyticsmanager-bui
    namespace: as-jcs
    resourceVersion: 770181958
    uid: d19ef707-d763-4f48-a96b-14890d728f14
}: Created service monitor
4m9s        Normal   Info     service/as-jcs-cachehost-bui          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-cachehost-bui
    namespace: as-jcs
    resourceVersion: 770181924
    uid: 7dc3ce7e-d381-4d19-9f9a-d4d25f6f3c0d
}: Received add service event
4m9s        Normal   Info     service/as-jcs-cachehost-bui          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-cachehost-bui
    namespace: as-jcs
    resourceVersion: 770181924
    uid: 7dc3ce7e-d381-4d19-9f9a-d4d25f6f3c0d
}: Created service monitor
4m9s        Normal   Info     service/as-jcs-commandmanager-bui     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-commandmanager-bui
    namespace: as-jcs
    resourceVersion: 770181940
    uid: 16e6cf8e-d3de-4125-b86a-fe8d7f5d3c39
}: Received add service event
4m9s        Normal   Info     service/as-jcs-commandmanager-bui     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-commandmanager-bui
    namespace: as-jcs
    resourceVersion: 770181940
    uid: 16e6cf8e-d3de-4125-b86a-fe8d7f5d3c39
}: Created service monitor
4m8s        Normal   Info     service/as-jcs-contextmanager-bui     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-contextmanager-bui
    namespace: as-jcs
    resourceVersion: 770181992
    uid: c1bf611c-a6d3-4add-9fb2-635308ee2c8d
}: Received add service event
4m8s        Normal   Info     service/as-jcs-contextmanager-bui     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-contextmanager-bui
    namespace: as-jcs
    resourceVersion: 770181992
    uid: c1bf611c-a6d3-4add-9fb2-635308ee2c8d
}: Created service monitor
4m9s        Normal   Info     service/as-jcs-customizationmanager-bui   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-customizationmanager-bui
    namespace: as-jcs
    resourceVersion: 770181915
    uid: c5cfe350-8291-493e-af1f-e8ebf94ac5bf
}: Received add service event
4m9s        Normal   Info     service/as-jcs-customizationmanager-bui   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-customizationmanager-bui
    namespace: as-jcs
    resourceVersion: 770181915
    uid: c5cfe350-8291-493e-af1f-e8ebf94ac5bf
}: Created service monitor
4m9s        Normal   Info     service/as-jcs-elementmanager-bui         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-elementmanager-bui
    namespace: as-jcs
    resourceVersion: 770181929
    uid: 8f158e83-37d4-40c2-aa5c-64420a1fcbc9
}: Received add service event
4m9s        Normal   Info     service/as-jcs-elementmanager-bui         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-elementmanager-bui
    namespace: as-jcs
    resourceVersion: 770181929
    uid: 8f158e83-37d4-40c2-aa5c-64420a1fcbc9
}: Created service monitor
4m8s        Normal   Info     service/as-jcs-eventmanager-bui           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-eventmanager-bui
    namespace: as-jcs
    resourceVersion: 770181986
    uid: a83f69e9-19fd-4371-9cd9-3bd876e0fb5e
}: Received add service event
4m8s        Normal   Info     service/as-jcs-eventmanager-bui           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-eventmanager-bui
    namespace: as-jcs
    resourceVersion: 770181986
    uid: a83f69e9-19fd-4371-9cd9-3bd876e0fb5e
}: Created service monitor
4m9s        Normal   Info     service/as-jcs-eventworker-bui            Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-eventworker-bui
    namespace: as-jcs
    resourceVersion: 770181909
    uid: e928b1ad-09f1-4b8a-937f-974381b4437e
}: Received add service event
4m9s        Normal   Info     service/as-jcs-eventworker-bui            Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-eventworker-bui
    namespace: as-jcs
    resourceVersion: 770181909
    uid: e928b1ad-09f1-4b8a-937f-974381b4437e
}: Created service monitor
4m9s        Normal   Info     service/as-jcs-queuemanager-bui           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-queuemanager-bui
    namespace: as-jcs
    resourceVersion: 770181912
    uid: 659261b8-6e48-4517-8f77-1e7857e65b6a
}: Received add service event
4m9s        Normal   Info     service/as-jcs-queuemanager-bui           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-queuemanager-bui
    namespace: as-jcs
    resourceVersion: 770181912
    uid: 659261b8-6e48-4517-8f77-1e7857e65b6a
}: Created service monitor
4m8s        Normal   Info     service/as-jcs-realtimedatamanager-bui    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-realtimedatamanager-bui
    namespace: as-jcs
    resourceVersion: 770181981
    uid: 7669aeec-7bec-45fb-a708-9f0fcfe29dd0
}: Received add service event
4m8s        Normal   Info     service/as-jcs-realtimedatamanager-bui    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-realtimedatamanager-bui
    namespace: as-jcs
    resourceVersion: 770181981
    uid: 7669aeec-7bec-45fb-a708-9f0fcfe29dd0
}: Created service monitor
4m9s        Normal   Info     service/as-jcs-reportdesignmanager-bui    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-reportdesignmanager-bui
    namespace: as-jcs
    resourceVersion: 770181935
    uid: 261e2a88-16b9-43cb-b939-7117430609e4
}: Created service monitor
4m9s        Normal   Info     service/as-jcs-reportdesignmanager-bui    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-reportdesignmanager-bui
    namespace: as-jcs
    resourceVersion: 770181935
    uid: 261e2a88-16b9-43cb-b939-7117430609e4
}: Received add service event
4m9s        Normal   Info     service/as-jcs-reportpublishmanager-bui   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-reportpublishmanager-bui
    namespace: as-jcs
    resourceVersion: 770181918
    uid: bc1581c1-7359-405a-b412-4068173c0810
}: Created service monitor
4m9s        Normal   Info     service/as-jcs-reportpublishmanager-bui   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-reportpublishmanager-bui
    namespace: as-jcs
    resourceVersion: 770181918
    uid: bc1581c1-7359-405a-b412-4068173c0810
}: Received add service event
4m9s        Normal   Info     service/as-jcs-searchmanager-bui          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-searchmanager-bui
    namespace: as-jcs
    resourceVersion: 770181970
    uid: 00c2b3a1-cd40-4714-98ec-ddd763ec89ee
}: Created service monitor
4m9s        Normal   Info     service/as-jcs-searchmanager-bui          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-searchmanager-bui
    namespace: as-jcs
    resourceVersion: 770181970
    uid: 00c2b3a1-cd40-4714-98ec-ddd763ec89ee
}: Received add service event
4m9s        Normal   Info     service/as-jcs-securityservice-bui        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-securityservice-bui
    namespace: as-jcs
    resourceVersion: 770181942
    uid: 8c8d836e-516d-4288-92c5-d237abf1540f
}: Created service monitor
4m9s        Normal   Info     service/as-jcs-securityservice-bui        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-jcs-securityservice-bui
    namespace: as-jcs
    resourceVersion: 770181942
    uid: 8c8d836e-516d-4288-92c5-d237abf1540f
}: Received add service event
56m         Warning   FailedCreate                   replicaset/echo-6677859554                                      Error creating: pods "echo-6677859554-nzqnv" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
40m         Warning   FailedCreate                   replicaset/echo-6677859554                                      Error creating: pods "echo-6677859554-kwlj8" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
23m         Warning   FailedCreate                   replicaset/echo-6677859554                                      Error creating: pods "echo-6677859554-xgb5v" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
6m43s       Warning   FailedCreate                   replicaset/echo-6677859554                                      Error creating: pods "echo-6677859554-np6x7" is forbidden: failed quota: as-jcs-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
3m59s       Warning   ErrImageNeverPull              pod/echo-769c9d8c54-tjvkp                                       Container image "echo:latest" is not present with pull policy of Never
2s          Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-analyticsmanager-autoscaler       unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
119s        Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-analyticsmanager-autoscaler       did not receive metrics for any ready pods
4m5s        Normal    Scheduled                      pod/lorax-analyticsmanager-c74f47d85-kp2f6                      Successfully assigned as-jcs/lorax-analyticsmanager-c74f47d85-kp2f6 to 10.11.9.195
4m5s        Normal    Pulled                         pod/lorax-analyticsmanager-c74f47d85-kp2f6                      Container image "docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3" already present on machine
4m5s        Normal    Created                        pod/lorax-analyticsmanager-c74f47d85-kp2f6                      Created container copy-vault-env
4m5s        Normal    Started                        pod/lorax-analyticsmanager-c74f47d85-kp2f6                      Started container copy-vault-env
4m4s        Normal    Pulled                         pod/lorax-analyticsmanager-c74f47d85-kp2f6                      Container image "docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5" already present on machine
4m4s        Normal    Created                        pod/lorax-analyticsmanager-c74f47d85-kp2f6                      Created container vault-agent
4m4s        Normal    Started                        pod/lorax-analyticsmanager-c74f47d85-kp2f6                      Started container vault-agent
4m4s        Normal    Pulled                         pod/lorax-analyticsmanager-c74f47d85-kp2f6                      Container image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5" already present on machine
4m4s        Normal    Created                        pod/lorax-analyticsmanager-c74f47d85-kp2f6                      Created container server
4m3s        Normal    Started                        pod/lorax-analyticsmanager-c74f47d85-kp2f6                      Started container server
3m19s       Warning   Unhealthy                      pod/lorax-analyticsmanager-c74f47d85-kp2f6                      Liveness probe failed: Get "http://10.245.23.62:9887/health/ping": dial tcp 10.245.23.62:9887: connect: connection refused
3m25s       Warning   Unhealthy                      pod/lorax-analyticsmanager-c74f47d85-kp2f6                      Readiness probe failed: Get "http://10.245.23.62:9887/ready": dial tcp 10.245.23.62:9887: connect: connection refused
3m5s        Warning   Unhealthy                      pod/lorax-analyticsmanager-c74f47d85-kp2f6                      Readiness probe failed: HTTP probe failed with statuscode: 503
4m6s        Normal    SuccessfulCreate               replicaset/lorax-analyticsmanager-c74f47d85                     Created pod: lorax-analyticsmanager-c74f47d85-kp2f6
4m8s        Normal    ScalingReplicaSet              deployment/lorax-analyticsmanager                               Scaled up replica set lorax-analyticsmanager-c74f47d85 to 1
4m4s        Normal    Scheduled                      pod/lorax-cachehost-54b4d8cf87-kv4xp                            Successfully assigned as-jcs/lorax-cachehost-54b4d8cf87-kv4xp to 10.11.9.6
4m3s        Normal    Pulled                         pod/lorax-cachehost-54b4d8cf87-kv4xp                            Container image "docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3" already present on machine
4m3s        Normal    Created                        pod/lorax-cachehost-54b4d8cf87-kv4xp                            Created container copy-vault-env
4m3s        Normal    Started                        pod/lorax-cachehost-54b4d8cf87-kv4xp                            Started container copy-vault-env
4m2s        Normal    Pulled                         pod/lorax-cachehost-54b4d8cf87-kv4xp                            Container image "docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5" already present on machine
4m2s        Normal    Created                        pod/lorax-cachehost-54b4d8cf87-kv4xp                            Created container vault-agent
4m2s        Normal    Started                        pod/lorax-cachehost-54b4d8cf87-kv4xp                            Started container vault-agent
25s         Normal    Pulling                        pod/lorax-cachehost-54b4d8cf87-kv4xp                            Pulling image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5"
50s         Warning   Failed                         pod/lorax-cachehost-54b4d8cf87-kv4xp                            Failed to pull image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5": rpc error: code = Unknown desc = context canceled
50s         Warning   Failed                         pod/lorax-cachehost-54b4d8cf87-kv4xp                            Error: ErrImagePull
39s         Normal    BackOff                        pod/lorax-cachehost-54b4d8cf87-kv4xp                            Back-off pulling image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5"
39s         Warning   Failed                         pod/lorax-cachehost-54b4d8cf87-kv4xp                            Error: ImagePullBackOff
4m5s        Normal    Scheduled                      pod/lorax-cachehost-54b4d8cf87-nrr25                            Successfully assigned as-jcs/lorax-cachehost-54b4d8cf87-nrr25 to 10.11.9.195
4m5s        Normal    Pulled                         pod/lorax-cachehost-54b4d8cf87-nrr25                            Container image "docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3" already present on machine
4m5s        Normal    Created                        pod/lorax-cachehost-54b4d8cf87-nrr25                            Created container copy-vault-env
4m4s        Normal    Started                        pod/lorax-cachehost-54b4d8cf87-nrr25                            Started container copy-vault-env
4m4s        Normal    Pulled                         pod/lorax-cachehost-54b4d8cf87-nrr25                            Container image "docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5" already present on machine
4m4s        Normal    Created                        pod/lorax-cachehost-54b4d8cf87-nrr25                            Created container vault-agent
4m4s        Normal    Started                        pod/lorax-cachehost-54b4d8cf87-nrr25                            Started container vault-agent
4m4s        Normal    Pulled                         pod/lorax-cachehost-54b4d8cf87-nrr25                            Container image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5" already present on machine
4m3s        Normal    Created                        pod/lorax-cachehost-54b4d8cf87-nrr25                            Created container server
4m3s        Normal    Started                        pod/lorax-cachehost-54b4d8cf87-nrr25                            Started container server
3m17s       Warning   Unhealthy                      pod/lorax-cachehost-54b4d8cf87-nrr25                            Readiness probe failed: Get "http://10.245.23.63:9878/ready": dial tcp 10.245.23.63:9878: connect: connection refused
3m26s       Warning   Unhealthy                      pod/lorax-cachehost-54b4d8cf87-nrr25                            Liveness probe failed: Get "http://10.245.23.63:9878/health/ping": dial tcp 10.245.23.63:9878: connect: connection refused
3m15s       Warning   Unhealthy                      pod/lorax-cachehost-54b4d8cf87-nrr25                            Liveness probe failed: Get "http://10.245.23.63:9878/health/ping": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
4m6s        Normal    SuccessfulCreate               replicaset/lorax-cachehost-54b4d8cf87                           Created pod: lorax-cachehost-54b4d8cf87-nrr25
4m4s        Normal    SuccessfulCreate               replicaset/lorax-cachehost-54b4d8cf87                           Created pod: lorax-cachehost-54b4d8cf87-kv4xp
118s        Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-cachehost-autoscaler              unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
4m8s        Normal    ScalingReplicaSet              deployment/lorax-cachehost                                      Scaled up replica set lorax-cachehost-54b4d8cf87 to 2
4m6s        Normal    Scheduled                      pod/lorax-commandmanager-6874498b4f-rnjn2                       Successfully assigned as-jcs/lorax-commandmanager-6874498b4f-rnjn2 to 10.11.9.195
4m6s        Normal    Pulled                         pod/lorax-commandmanager-6874498b4f-rnjn2                       Container image "docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3" already present on machine
4m6s        Normal    Created                        pod/lorax-commandmanager-6874498b4f-rnjn2                       Created container copy-vault-env
4m6s        Normal    Started                        pod/lorax-commandmanager-6874498b4f-rnjn2                       Started container copy-vault-env
4m5s        Normal    Pulled                         pod/lorax-commandmanager-6874498b4f-rnjn2                       Container image "docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5" already present on machine
4m5s        Normal    Created                        pod/lorax-commandmanager-6874498b4f-rnjn2                       Created container vault-agent
4m4s        Normal    Started                        pod/lorax-commandmanager-6874498b4f-rnjn2                       Started container vault-agent
4m4s        Normal    Pulled                         pod/lorax-commandmanager-6874498b4f-rnjn2                       Container image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5" already present on machine
4m4s        Normal    Created                        pod/lorax-commandmanager-6874498b4f-rnjn2                       Created container server
4m4s        Normal    Started                        pod/lorax-commandmanager-6874498b4f-rnjn2                       Started container server
3m21s       Warning   Unhealthy                      pod/lorax-commandmanager-6874498b4f-rnjn2                       Readiness probe failed: Get "http://10.245.23.61:9880/ready": dial tcp 10.245.23.61:9880: connect: connection refused
3m26s       Warning   Unhealthy                      pod/lorax-commandmanager-6874498b4f-rnjn2                       Liveness probe failed: Get "http://10.245.23.61:9880/health/ping": dial tcp 10.245.23.61:9880: connect: connection refused
3m1s        Warning   Unhealthy                      pod/lorax-commandmanager-6874498b4f-rnjn2                       Readiness probe failed: HTTP probe failed with statuscode: 503
4m7s        Normal    SuccessfulCreate               replicaset/lorax-commandmanager-6874498b4f                      Created pod: lorax-commandmanager-6874498b4f-rnjn2
119s        Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-commandmanager-autoscaler         unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
119s        Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-commandmanager-autoscaler         did not receive metrics for any ready pods
4m9s        Normal    ScalingReplicaSet              deployment/lorax-commandmanager                                 Scaled up replica set lorax-commandmanager-6874498b4f to 1
4m6s        Normal    Scheduled                      pod/lorax-contextmanager-55c6bbbfcd-2bdms                       Successfully assigned as-jcs/lorax-contextmanager-55c6bbbfcd-2bdms to 10.11.9.6
4m6s        Normal    Pulled                         pod/lorax-contextmanager-55c6bbbfcd-2bdms                       Container image "docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3" already present on machine
4m6s        Normal    Created                        pod/lorax-contextmanager-55c6bbbfcd-2bdms                       Created container copy-vault-env
4m6s        Normal    Started                        pod/lorax-contextmanager-55c6bbbfcd-2bdms                       Started container copy-vault-env
4m5s        Normal    Pulled                         pod/lorax-contextmanager-55c6bbbfcd-2bdms                       Container image "docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5" already present on machine
4m4s        Normal    Created                        pod/lorax-contextmanager-55c6bbbfcd-2bdms                       Created container vault-agent
4m4s        Normal    Started                        pod/lorax-contextmanager-55c6bbbfcd-2bdms                       Started container vault-agent
20s         Normal    Pulling                        pod/lorax-contextmanager-55c6bbbfcd-2bdms                       Pulling image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5"
50s         Warning   Failed                         pod/lorax-contextmanager-55c6bbbfcd-2bdms                       Failed to pull image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5": rpc error: code = Unknown desc = context canceled
50s         Warning   Failed                         pod/lorax-contextmanager-55c6bbbfcd-2bdms                       Error: ErrImagePull
35s         Normal    BackOff                        pod/lorax-contextmanager-55c6bbbfcd-2bdms                       Back-off pulling image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5"
35s         Warning   Failed                         pod/lorax-contextmanager-55c6bbbfcd-2bdms                       Error: ImagePullBackOff
4m7s        Normal    SuccessfulCreate               replicaset/lorax-contextmanager-55c6bbbfcd                      Created pod: lorax-contextmanager-55c6bbbfcd-2bdms
119s        Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-contextmanager-autoscaler         unable to get metrics for resource memory: no metrics returned from resource metrics API
118s        Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-contextmanager-autoscaler         unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
118s        Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-contextmanager-autoscaler         unable to get metrics for resource cpu: no metrics returned from resource metrics API
118s        Warning   FailedComputeMetricsReplicas   horizontalpodautoscaler/lorax-contextmanager-autoscaler         invalid metrics (3 invalid out of 3), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API
4m9s        Normal    ScalingReplicaSet              deployment/lorax-contextmanager                                 Scaled up replica set lorax-contextmanager-55c6bbbfcd to 1
4m4s        Normal    Scheduled                      pod/lorax-customizationmanager-6946844dc7-zpdvw                 Successfully assigned as-jcs/lorax-customizationmanager-6946844dc7-zpdvw to 10.11.9.6
4m3s        Normal    Pulled                         pod/lorax-customizationmanager-6946844dc7-zpdvw                 Container image "docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3" already present on machine
4m3s        Normal    Created                        pod/lorax-customizationmanager-6946844dc7-zpdvw                 Created container copy-vault-env
4m3s        Normal    Started                        pod/lorax-customizationmanager-6946844dc7-zpdvw                 Started container copy-vault-env
4m2s        Normal    Pulled                         pod/lorax-customizationmanager-6946844dc7-zpdvw                 Container image "docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5" already present on machine
4m2s        Normal    Created                        pod/lorax-customizationmanager-6946844dc7-zpdvw                 Created container vault-agent
4m2s        Normal    Started                        pod/lorax-customizationmanager-6946844dc7-zpdvw                 Started container vault-agent
21s         Normal    Pulling                        pod/lorax-customizationmanager-6946844dc7-zpdvw                 Pulling image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5"
47s         Warning   Failed                         pod/lorax-customizationmanager-6946844dc7-zpdvw                 Failed to pull image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5": rpc error: code = Unknown desc = context canceled
47s         Warning   Failed                         pod/lorax-customizationmanager-6946844dc7-zpdvw                 Error: ErrImagePull
35s         Normal    BackOff                        pod/lorax-customizationmanager-6946844dc7-zpdvw                 Back-off pulling image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5"
35s         Warning   Failed                         pod/lorax-customizationmanager-6946844dc7-zpdvw                 Error: ImagePullBackOff
4m4s        Normal    SuccessfulCreate               replicaset/lorax-customizationmanager-6946844dc7                Created pod: lorax-customizationmanager-6946844dc7-zpdvw
118s        Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-customizationmanager-autoscaler   unable to get metrics for resource memory: no metrics returned from resource metrics API
118s        Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-customizationmanager-autoscaler   unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
118s        Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-customizationmanager-autoscaler   unable to get metrics for resource cpu: no metrics returned from resource metrics API
118s        Warning   FailedComputeMetricsReplicas   horizontalpodautoscaler/lorax-customizationmanager-autoscaler   invalid metrics (3 invalid out of 3), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API
4m8s        Normal    ScalingReplicaSet              deployment/lorax-customizationmanager                           Scaled up replica set lorax-customizationmanager-6946844dc7 to 1
4m3s        Normal    Scheduled                      pod/lorax-elementmanager-69f8686c4c-8mpmw                       Successfully assigned as-jcs/lorax-elementmanager-69f8686c4c-8mpmw to 10.11.9.195
4m3s        Normal    Pulled                         pod/lorax-elementmanager-69f8686c4c-8mpmw                       Container image "docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3" already present on machine
4m2s        Normal    Created                        pod/lorax-elementmanager-69f8686c4c-8mpmw                       Created container copy-vault-env
4m2s        Normal    Started                        pod/lorax-elementmanager-69f8686c4c-8mpmw                       Started container copy-vault-env
4m1s        Normal    Pulled                         pod/lorax-elementmanager-69f8686c4c-8mpmw                       Container image "docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5" already present on machine
4m1s        Normal    Created                        pod/lorax-elementmanager-69f8686c4c-8mpmw                       Created container vault-agent
4m1s        Normal    Started                        pod/lorax-elementmanager-69f8686c4c-8mpmw                       Started container vault-agent
4m1s        Normal    Pulled                         pod/lorax-elementmanager-69f8686c4c-8mpmw                       Container image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5" already present on machine
4m1s        Normal    Created                        pod/lorax-elementmanager-69f8686c4c-8mpmw                       Created container server
4m1s        Normal    Started                        pod/lorax-elementmanager-69f8686c4c-8mpmw                       Started container server
3m18s       Warning   Unhealthy                      pod/lorax-elementmanager-69f8686c4c-8mpmw                       Liveness probe failed: Get "http://10.245.23.66:9888/health/ping": dial tcp 10.245.23.66:9888: connect: connection refused
3m18s       Warning   Unhealthy                      pod/lorax-elementmanager-69f8686c4c-8mpmw                       Readiness probe failed: Get "http://10.245.23.66:9888/ready": dial tcp 10.245.23.66:9888: connect: connection refused
2m58s       Warning   Unhealthy                      pod/lorax-elementmanager-69f8686c4c-8mpmw                       Readiness probe failed: HTTP probe failed with statuscode: 503
4m4s        Normal    SuccessfulCreate               replicaset/lorax-elementmanager-69f8686c4c                      Created pod: lorax-elementmanager-69f8686c4c-8mpmw
4m8s        Normal    ScalingReplicaSet              deployment/lorax-elementmanager                                 Scaled up replica set lorax-elementmanager-69f8686c4c to 1
4m5s        Normal    Scheduled                      pod/lorax-eventmanager-7bff46cb7-d2rj7                          Successfully assigned as-jcs/lorax-eventmanager-7bff46cb7-d2rj7 to 10.11.9.195
4m4s        Normal    Pulled                         pod/lorax-eventmanager-7bff46cb7-d2rj7                          Container image "docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3" already present on machine
4m4s        Normal    Created                        pod/lorax-eventmanager-7bff46cb7-d2rj7                          Created container copy-vault-env
4m4s        Normal    Started                        pod/lorax-eventmanager-7bff46cb7-d2rj7                          Started container copy-vault-env
4m4s        Normal    Pulled                         pod/lorax-eventmanager-7bff46cb7-d2rj7                          Container image "docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5" already present on machine
4m4s        Normal    Created                        pod/lorax-eventmanager-7bff46cb7-d2rj7                          Created container vault-agent
4m3s        Normal    Started                        pod/lorax-eventmanager-7bff46cb7-d2rj7                          Started container vault-agent
4m3s        Normal    Pulled                         pod/lorax-eventmanager-7bff46cb7-d2rj7                          Container image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5" already present on machine
4m3s        Normal    Created                        pod/lorax-eventmanager-7bff46cb7-d2rj7                          Created container server
4m3s        Normal    Started                        pod/lorax-eventmanager-7bff46cb7-d2rj7                          Started container server
3m25s       Warning   Unhealthy                      pod/lorax-eventmanager-7bff46cb7-d2rj7                          Liveness probe failed: Get "http://10.245.23.64:9882/health/ping": dial tcp 10.245.23.64:9882: connect: connection refused
3m24s       Warning   Unhealthy                      pod/lorax-eventmanager-7bff46cb7-d2rj7                          Readiness probe failed: Get "http://10.245.23.64:9882/ready": dial tcp 10.245.23.64:9882: connect: connection refused
3m4s        Warning   Unhealthy                      pod/lorax-eventmanager-7bff46cb7-d2rj7                          Readiness probe failed: HTTP probe failed with statuscode: 503
4m5s        Normal    SuccessfulCreate               replicaset/lorax-eventmanager-7bff46cb7                         Created pod: lorax-eventmanager-7bff46cb7-d2rj7
119s        Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-eventmanager-autoscaler           unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
119s        Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-eventmanager-autoscaler           did not receive metrics for any ready pods
4m8s        Normal    ScalingReplicaSet              deployment/lorax-eventmanager                                   Scaled up replica set lorax-eventmanager-7bff46cb7 to 1
4m6s        Normal    Scheduled                      pod/lorax-eventworker-79b4d4f5f4-8gtwj                          Successfully assigned as-jcs/lorax-eventworker-79b4d4f5f4-8gtwj to 10.11.9.6
4m6s        Normal    Pulled                         pod/lorax-eventworker-79b4d4f5f4-8gtwj                          Container image "docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3" already present on machine
4m6s        Normal    Created                        pod/lorax-eventworker-79b4d4f5f4-8gtwj                          Created container copy-vault-env
4m6s        Normal    Started                        pod/lorax-eventworker-79b4d4f5f4-8gtwj                          Started container copy-vault-env
4m5s        Normal    Pulled                         pod/lorax-eventworker-79b4d4f5f4-8gtwj                          Container image "docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5" already present on machine
4m4s        Normal    Created                        pod/lorax-eventworker-79b4d4f5f4-8gtwj                          Created container vault-agent
4m4s        Normal    Started                        pod/lorax-eventworker-79b4d4f5f4-8gtwj                          Started container vault-agent
18s         Normal    Pulling                        pod/lorax-eventworker-79b4d4f5f4-8gtwj                          Pulling image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5"
48s         Warning   Failed                         pod/lorax-eventworker-79b4d4f5f4-8gtwj                          Failed to pull image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5": rpc error: code = Unknown desc = context canceled
48s         Warning   Failed                         pod/lorax-eventworker-79b4d4f5f4-8gtwj                          Error: ErrImagePull
33s         Normal    BackOff                        pod/lorax-eventworker-79b4d4f5f4-8gtwj                          Back-off pulling image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5"
33s         Warning   Failed                         pod/lorax-eventworker-79b4d4f5f4-8gtwj                          Error: ImagePullBackOff
4m7s        Normal    SuccessfulCreate               replicaset/lorax-eventworker-79b4d4f5f4                         Created pod: lorax-eventworker-79b4d4f5f4-8gtwj
4m9s        Normal    ScalingReplicaSet              deployment/lorax-eventworker                                    Scaled up replica set lorax-eventworker-79b4d4f5f4 to 1
4m9s        Normal    Info                           configmap/lorax-lorax-dashboards                                Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-lorax-dashboards
    namespace: as-jcs
    resourceVersion: 770181905
    uid: 5765475d-f1dd-4ceb-9b89-560ebb4a9e34
}: Received add dashboard event
4m9s        Normal    Info                           configmap/lorax-lorax-dashboards                                Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-lorax-dashboards
    namespace: as-jcs
    resourceVersion: 770181905
    uid: 5765475d-f1dd-4ceb-9b89-560ebb4a9e34
}: Pushed add dashboard to kafka
4m9s        Normal    Info                           configmap/lorax-prometheus-shared-rules                         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-prometheus-shared-rules
    namespace: as-jcs
    resourceVersion: 770181903
    uid: 0b1bf488-33ec-43df-995e-e3ef0ed7e38b
}: Received add rule event
4m9s        Normal    Info                           configmap/lorax-prometheus-shared-rules                         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-prometheus-shared-rules
    namespace: as-jcs
    resourceVersion: 770181903
    uid: 0b1bf488-33ec-43df-995e-e3ef0ed7e38b
}: Pushed rule to kafka
4m3s        Normal    Scheduled                      pod/lorax-queuemanager-5d6f67b79f-4nnhn                         Successfully assigned as-jcs/lorax-queuemanager-5d6f67b79f-4nnhn to 10.11.9.195
4m2s        Normal    Pulled                         pod/lorax-queuemanager-5d6f67b79f-4nnhn                         Container image "docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3" already present on machine
4m2s        Normal    Created                        pod/lorax-queuemanager-5d6f67b79f-4nnhn                         Created container copy-vault-env
4m2s        Normal    Started                        pod/lorax-queuemanager-5d6f67b79f-4nnhn                         Started container copy-vault-env
4m1s        Normal    Pulled                         pod/lorax-queuemanager-5d6f67b79f-4nnhn                         Container image "docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5" already present on machine
4m1s        Normal    Created                        pod/lorax-queuemanager-5d6f67b79f-4nnhn                         Created container vault-agent
4m1s        Normal    Started                        pod/lorax-queuemanager-5d6f67b79f-4nnhn                         Started container vault-agent
4m1s        Normal    Pulled                         pod/lorax-queuemanager-5d6f67b79f-4nnhn                         Container image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5" already present on machine
4m1s        Normal    Created                        pod/lorax-queuemanager-5d6f67b79f-4nnhn                         Created container server
4m1s        Normal    Started                        pod/lorax-queuemanager-5d6f67b79f-4nnhn                         Started container server
3m23s       Warning   Unhealthy                      pod/lorax-queuemanager-5d6f67b79f-4nnhn                         Readiness probe failed: Get "http://10.245.23.67:9885/ready": dial tcp 10.245.23.67:9885: connect: connection refused
3m22s       Warning   Unhealthy                      pod/lorax-queuemanager-5d6f67b79f-4nnhn                         Liveness probe failed: Get "http://10.245.23.67:9885/health/ping": dial tcp 10.245.23.67:9885: connect: connection refused
3m3s        Warning   Unhealthy                      pod/lorax-queuemanager-5d6f67b79f-4nnhn                         Readiness probe failed: HTTP probe failed with statuscode: 503
4m4s        Normal    SuccessfulCreate               replicaset/lorax-queuemanager-5d6f67b79f                        Created pod: lorax-queuemanager-5d6f67b79f-4nnhn
4m8s        Normal    ScalingReplicaSet              deployment/lorax-queuemanager                                   Scaled up replica set lorax-queuemanager-5d6f67b79f to 1
4m5s        Normal    Scheduled                      pod/lorax-realtimedatamanager-75c56cdbc5-sg9gv                  Successfully assigned as-jcs/lorax-realtimedatamanager-75c56cdbc5-sg9gv to 10.11.9.6
4m5s        Normal    Pulled                         pod/lorax-realtimedatamanager-75c56cdbc5-sg9gv                  Container image "docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3" already present on machine
4m5s        Normal    Created                        pod/lorax-realtimedatamanager-75c56cdbc5-sg9gv                  Created container copy-vault-env
4m4s        Normal    Started                        pod/lorax-realtimedatamanager-75c56cdbc5-sg9gv                  Started container copy-vault-env
4m3s        Normal    Pulled                         pod/lorax-realtimedatamanager-75c56cdbc5-sg9gv                  Container image "docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5" already present on machine
4m3s        Normal    Created                        pod/lorax-realtimedatamanager-75c56cdbc5-sg9gv                  Created container vault-agent
4m3s        Normal    Started                        pod/lorax-realtimedatamanager-75c56cdbc5-sg9gv                  Started container vault-agent
22s         Normal    Pulling                        pod/lorax-realtimedatamanager-75c56cdbc5-sg9gv                  Pulling image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5"
51s         Warning   Failed                         pod/lorax-realtimedatamanager-75c56cdbc5-sg9gv                  Failed to pull image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5": rpc error: code = Unknown desc = context canceled
51s         Warning   Failed                         pod/lorax-realtimedatamanager-75c56cdbc5-sg9gv                  Error: ErrImagePull
37s         Normal    BackOff                        pod/lorax-realtimedatamanager-75c56cdbc5-sg9gv                  Back-off pulling image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5"
37s         Warning   Failed                         pod/lorax-realtimedatamanager-75c56cdbc5-sg9gv                  Error: ImagePullBackOff
4m5s        Normal    SuccessfulCreate               replicaset/lorax-realtimedatamanager-75c56cdbc5                 Created pod: lorax-realtimedatamanager-75c56cdbc5-sg9gv
4m8s        Normal    ScalingReplicaSet              deployment/lorax-realtimedatamanager                            Scaled up replica set lorax-realtimedatamanager-75c56cdbc5 to 1
4m5s        Normal    Scheduled                      pod/lorax-reportdesignmanager-78559cf46c-lvqgf                  Successfully assigned as-jcs/lorax-reportdesignmanager-78559cf46c-lvqgf to 10.11.9.6
4m4s        Normal    Pulled                         pod/lorax-reportdesignmanager-78559cf46c-lvqgf                  Container image "docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3" already present on machine
4m4s        Normal    Created                        pod/lorax-reportdesignmanager-78559cf46c-lvqgf                  Created container copy-vault-env
4m4s        Normal    Started                        pod/lorax-reportdesignmanager-78559cf46c-lvqgf                  Started container copy-vault-env
4m3s        Normal    Pulled                         pod/lorax-reportdesignmanager-78559cf46c-lvqgf                  Container image "docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5" already present on machine
4m3s        Normal    Created                        pod/lorax-reportdesignmanager-78559cf46c-lvqgf                  Created container vault-agent
4m3s        Normal    Started                        pod/lorax-reportdesignmanager-78559cf46c-lvqgf                  Started container vault-agent
24s         Normal    Pulling                        pod/lorax-reportdesignmanager-78559cf46c-lvqgf                  Pulling image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5"
51s         Warning   Failed                         pod/lorax-reportdesignmanager-78559cf46c-lvqgf                  Failed to pull image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5": rpc error: code = Unknown desc = context canceled
51s         Warning   Failed                         pod/lorax-reportdesignmanager-78559cf46c-lvqgf                  Error: ErrImagePull
38s         Normal    BackOff                        pod/lorax-reportdesignmanager-78559cf46c-lvqgf                  Back-off pulling image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5"
38s         Warning   Failed                         pod/lorax-reportdesignmanager-78559cf46c-lvqgf                  Error: ImagePullBackOff
4m5s        Normal    SuccessfulCreate               replicaset/lorax-reportdesignmanager-78559cf46c                 Created pod: lorax-reportdesignmanager-78559cf46c-lvqgf
4m8s        Normal    ScalingReplicaSet              deployment/lorax-reportdesignmanager                            Scaled up replica set lorax-reportdesignmanager-78559cf46c to 1
4m7s        Normal    Scheduled                      pod/lorax-reportpublishmanager-75b9d675f7-6h44x                 Successfully assigned as-jcs/lorax-reportpublishmanager-75b9d675f7-6h44x to 10.11.9.6
4m6s        Normal    Pulled                         pod/lorax-reportpublishmanager-75b9d675f7-6h44x                 Container image "docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3" already present on machine
4m6s        Normal    Created                        pod/lorax-reportpublishmanager-75b9d675f7-6h44x                 Created container copy-vault-env
4m6s        Normal    Started                        pod/lorax-reportpublishmanager-75b9d675f7-6h44x                 Started container copy-vault-env
4m5s        Normal    Pulled                         pod/lorax-reportpublishmanager-75b9d675f7-6h44x                 Container image "docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5" already present on machine
4m4s        Normal    Created                        pod/lorax-reportpublishmanager-75b9d675f7-6h44x                 Created container vault-agent
4m4s        Normal    Started                        pod/lorax-reportpublishmanager-75b9d675f7-6h44x                 Started container vault-agent
21s         Normal    Pulling                        pod/lorax-reportpublishmanager-75b9d675f7-6h44x                 Pulling image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5"
50s         Warning   Failed                         pod/lorax-reportpublishmanager-75b9d675f7-6h44x                 Failed to pull image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5": rpc error: code = Unknown desc = context canceled
50s         Warning   Failed                         pod/lorax-reportpublishmanager-75b9d675f7-6h44x                 Error: ErrImagePull
36s         Normal    BackOff                        pod/lorax-reportpublishmanager-75b9d675f7-6h44x                 Back-off pulling image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5"
36s         Warning   Failed                         pod/lorax-reportpublishmanager-75b9d675f7-6h44x                 Error: ImagePullBackOff
4m7s        Normal    SuccessfulCreate               replicaset/lorax-reportpublishmanager-75b9d675f7                Created pod: lorax-reportpublishmanager-75b9d675f7-6h44x
4m9s        Normal    ScalingReplicaSet              deployment/lorax-reportpublishmanager                           Scaled up replica set lorax-reportpublishmanager-75b9d675f7 to 1
4m4s        Normal    Scheduled                      pod/lorax-searchmanager-665bc9f64b-szhtw                        Successfully assigned as-jcs/lorax-searchmanager-665bc9f64b-szhtw to 10.11.9.195
4m3s        Normal    Pulled                         pod/lorax-searchmanager-665bc9f64b-szhtw                        Container image "docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3" already present on machine
4m3s        Normal    Created                        pod/lorax-searchmanager-665bc9f64b-szhtw                        Created container copy-vault-env
4m3s        Normal    Started                        pod/lorax-searchmanager-665bc9f64b-szhtw                        Started container copy-vault-env
4m3s        Normal    Pulled                         pod/lorax-searchmanager-665bc9f64b-szhtw                        Container image "docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5" already present on machine
4m2s        Normal    Created                        pod/lorax-searchmanager-665bc9f64b-szhtw                        Created container vault-agent
4m2s        Normal    Started                        pod/lorax-searchmanager-665bc9f64b-szhtw                        Started container vault-agent
4m2s        Normal    Pulled                         pod/lorax-searchmanager-665bc9f64b-szhtw                        Container image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5" already present on machine
4m2s        Normal    Created                        pod/lorax-searchmanager-665bc9f64b-szhtw                        Created container server
4m2s        Normal    Started                        pod/lorax-searchmanager-665bc9f64b-szhtw                        Started container server
3m26s       Warning   Unhealthy                      pod/lorax-searchmanager-665bc9f64b-szhtw                        Liveness probe failed: Get "http://10.245.23.65:9883/health/ping": dial tcp 10.245.23.65:9883: connect: connection refused
3m22s       Warning   Unhealthy                      pod/lorax-searchmanager-665bc9f64b-szhtw                        Readiness probe failed: Get "http://10.245.23.65:9883/ready": dial tcp 10.245.23.65:9883: connect: connection refused
3m2s        Warning   Unhealthy                      pod/lorax-searchmanager-665bc9f64b-szhtw                        Readiness probe failed: HTTP probe failed with statuscode: 503
4m4s        Normal    SuccessfulCreate               replicaset/lorax-searchmanager-665bc9f64b                       Created pod: lorax-searchmanager-665bc9f64b-szhtw
3s          Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-searchmanager-autoscaler          unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
118s        Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-searchmanager-autoscaler          did not receive metrics for any ready pods
4m8s        Normal    ScalingReplicaSet              deployment/lorax-searchmanager                                  Scaled up replica set lorax-searchmanager-665bc9f64b to 1
4m7s        Normal    Scheduled                      pod/lorax-securityservice-77d5fcbc47-ch46d                      Successfully assigned as-jcs/lorax-securityservice-77d5fcbc47-ch46d to 10.11.9.6
4m6s        Normal    Pulled                         pod/lorax-securityservice-77d5fcbc47-ch46d                      Container image "docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3" already present on machine
4m6s        Normal    Created                        pod/lorax-securityservice-77d5fcbc47-ch46d                      Created container copy-vault-env
4m6s        Normal    Started                        pod/lorax-securityservice-77d5fcbc47-ch46d                      Started container copy-vault-env
4m5s        Normal    Pulled                         pod/lorax-securityservice-77d5fcbc47-ch46d                      Container image "docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5" already present on machine
4m4s        Normal    Created                        pod/lorax-securityservice-77d5fcbc47-ch46d                      Created container vault-agent
4m4s        Normal    Started                        pod/lorax-securityservice-77d5fcbc47-ch46d                      Started container vault-agent
18s         Normal    Pulling                        pod/lorax-securityservice-77d5fcbc47-ch46d                      Pulling image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5"
48s         Warning   Failed                         pod/lorax-securityservice-77d5fcbc47-ch46d                      Failed to pull image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5": rpc error: code = Unknown desc = context canceled
48s         Warning   Failed                         pod/lorax-securityservice-77d5fcbc47-ch46d                      Error: ErrImagePull
33s         Normal    BackOff                        pod/lorax-securityservice-77d5fcbc47-ch46d                      Back-off pulling image "iad.ocir.io/osvccorp/bui/lorax:dock-2022.01.19-17.32.57-431ac7a5"
33s         Warning   Failed                         pod/lorax-securityservice-77d5fcbc47-ch46d                      Error: ImagePullBackOff
4m7s        Normal    SuccessfulCreate               replicaset/lorax-securityservice-77d5fcbc47                     Created pod: lorax-securityservice-77d5fcbc47-ch46d
118s        Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-securityservice-autoscaler        unable to get metrics for resource memory: no metrics returned from resource metrics API
118s        Warning   FailedGetPodsMetric            horizontalpodautoscaler/lorax-securityservice-autoscaler        unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
118s        Warning   FailedGetResourceMetric        horizontalpodautoscaler/lorax-securityservice-autoscaler        unable to get metrics for resource cpu: no metrics returned from resource metrics API
118s        Warning   FailedComputeMetricsReplicas   horizontalpodautoscaler/lorax-securityservice-autoscaler        invalid metrics (3 invalid out of 3), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API
4m9s        Normal    ScalingReplicaSet              deployment/lorax-securityservice                                Scaled up replica set lorax-securityservice-77d5fcbc47 to 1
4m27s       Warning   FailedMount                    pod/who-internal-who-deployment-869d6dd88d-kt955                MountVolume.SetUp failed for volume "who-config" : configmap "who-internal-config" not found
[20:January:2022:08:16:02]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn as-jcs
NAME                                           READY   STATUS              RESTARTS   AGE     IP              NODE          NOMINATED NODE   READINESS GATES
echo-769c9d8c54-tjvkp                          0/1     ErrImageNeverPull   0          3h29m   10.245.11.112   10.11.8.46    <none>           <none>
lorax-analyticsmanager-c74f47d85-kp2f6         2/2     Running             0          4m52s   10.245.23.62    10.11.9.195   <none>           <none>
lorax-cachehost-54b4d8cf87-kv4xp               1/2     ImagePullBackOff    0          4m50s   10.245.31.36    10.11.9.6     <none>           <none>
lorax-cachehost-54b4d8cf87-nrr25               2/2     Running             0          4m52s   10.245.23.63    10.11.9.195   <none>           <none>
lorax-commandmanager-6874498b4f-rnjn2          2/2     Running             0          4m53s   10.245.23.61    10.11.9.195   <none>           <none>
lorax-contextmanager-55c6bbbfcd-2bdms          1/2     ImagePullBackOff    0          4m53s   10.245.31.33    10.11.9.6     <none>           <none>
lorax-customizationmanager-6946844dc7-zpdvw    1/2     ImagePullBackOff    0          4m50s   10.245.31.37    10.11.9.6     <none>           <none>
lorax-elementmanager-69f8686c4c-8mpmw          2/2     Running             0          4m50s   10.245.23.66    10.11.9.195   <none>           <none>
lorax-eventmanager-7bff46cb7-d2rj7             2/2     Running             0          4m51s   10.245.23.64    10.11.9.195   <none>           <none>
lorax-eventworker-79b4d4f5f4-8gtwj             1/2     ImagePullBackOff    0          4m53s   10.245.31.32    10.11.9.6     <none>           <none>
lorax-queuemanager-5d6f67b79f-4nnhn            2/2     Running             0          4m50s   10.245.23.67    10.11.9.195   <none>           <none>
lorax-realtimedatamanager-75c56cdbc5-sg9gv     1/2     ImagePullBackOff    0          4m51s   10.245.31.34    10.11.9.6     <none>           <none>
lorax-reportdesignmanager-78559cf46c-lvqgf     1/2     ImagePullBackOff    0          4m51s   10.245.31.35    10.11.9.6     <none>           <none>
lorax-reportpublishmanager-75b9d675f7-6h44x    1/2     ImagePullBackOff    0          4m53s   10.245.31.30    10.11.9.6     <none>           <none>
lorax-searchmanager-665bc9f64b-szhtw           2/2     Running             0          4m50s   10.245.23.65    10.11.9.195   <none>           <none>
lorax-securityservice-77d5fcbc47-ch46d         1/2     ImagePullBackOff    0          4m53s   10.245.31.31    10.11.9.6     <none>           <none>
who-internal-who-deployment-869d6dd88d-kt955   2/2     Running             317        5d9h    10.245.18.30    10.11.9.160   <none>           <none>
[20:January:2022:08:16:48]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn as-ciat
NAME                                           READY   STATUS    RESTARTS   AGE     IP              NODE          NOMINATED NODE   READINESS GATES
lorax-analyticsmanager-75d985dfdf-6jmjm        2/2     Running   0          8h      10.245.14.74    10.11.9.104   <none>           <none>
lorax-analyticsmanager-75d985dfdf-mmxrw        2/2     Running   0          8h      10.245.13.192   10.11.9.145   <none>           <none>
lorax-analyticsmanager-75d985dfdf-zfr7b        2/2     Running   0          8h      10.245.20.140   10.11.8.225   <none>           <none>
lorax-cachehost-647b6cf7b4-4v6n7               2/2     Running   0          6h48m   10.245.13.197   10.11.9.145   <none>           <none>
lorax-cachehost-647b6cf7b4-jsqlv               2/2     Running   0          6h48m   10.245.27.178   10.11.8.60    <none>           <none>
lorax-cachehost-6c8948bf68-8gmdf               2/2     Running   0          8h      10.245.19.21    10.11.9.238   <none>           <none>
lorax-cachehost-6c8948bf68-mzvx7               2/2     Running   0          8h      10.245.26.8     10.11.8.67    <none>           <none>
lorax-cachehost-6c8948bf68-vf26t               2/2     Running   0          8h      10.245.15.225   10.11.9.78    <none>           <none>
lorax-commandmanager-586d58947d-2h89f          2/2     Running   0          8h      10.245.28.203   10.11.9.56    <none>           <none>
lorax-commandmanager-586d58947d-2rcmp          2/2     Running   0          8h      10.245.11.175   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-c4482          2/2     Running   0          8h      10.245.27.176   10.11.8.60    <none>           <none>
lorax-contextmanager-67956c7584-c4xkw          2/2     Running   0          8h      10.245.11.177   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-h82q5          2/2     Running   0          8h      10.245.3.92     10.11.8.232   <none>           <none>
lorax-customizationmanager-657bbf9586-h242p    2/2     Running   0          8h      10.245.16.201   10.11.8.203   <none>           <none>
lorax-customizationmanager-657bbf9586-rsbqg    2/2     Running   0          8h      10.245.29.214   10.11.9.62    <none>           <none>
lorax-elementmanager-7f74646f4d-n4cqx          2/2     Running   0          8h      10.245.28.39    10.11.8.31    <none>           <none>
lorax-eventmanager-759c5b9484-ch28f            2/2     Running   0          8h      10.245.15.226   10.11.9.78    <none>           <none>
lorax-eventworker-7c5b779bc-z4bb5              2/2     Running   0          8h      10.245.22.102   10.11.8.250   <none>           <none>
lorax-queuemanager-769996b97b-z4kqc            2/2     Running   0          8h      10.245.16.24    10.11.8.222   <none>           <none>
lorax-realtimedatamanager-866cd5886b-qq54f     2/2     Running   0          8h      10.245.29.213   10.11.9.62    <none>           <none>
lorax-reportdesignmanager-5c88cb777d-sxlx9     2/2     Running   0          8h      10.245.20.94    10.11.8.160   <none>           <none>
lorax-reportpublishmanager-58445646c4-9ztc7    2/2     Running   0          8h      10.245.11.176   10.11.9.208   <none>           <none>
lorax-searchmanager-7d97d6dfcc-gn4zg           2/2     Running   0          8h      10.245.29.212   10.11.9.62    <none>           <none>
lorax-searchmanager-7d97d6dfcc-q2x5t           2/2     Running   0          8h      10.245.14.73    10.11.9.104   <none>           <none>
lorax-securityservice-b9bb8695d-t7bz5          2/2     Running   0          8h      10.245.12.189   10.11.8.194   <none>           <none>
who-internal-who-deployment-65d44d6f4c-fqhbs   2/2     Running   0          8h      10.245.25.112   10.11.8.142   <none>           <none>
who-internal-who-deployment-65d44d6f4c-w8lnq   2/2     Running   0          8h      10.245.11.179   10.11.9.208   <none>           <none>
who-internal-who-deployment-65d44d6f4c-xljbh   2/2     Running   0          8h      10.245.16.199   10.11.8.203   <none>           <none>
who-who-deployment-576cd44f87-4dr8c            2/2     Running   0          8h      10.245.9.42     10.11.9.136   <none>           <none>
who-who-deployment-576cd44f87-lhb49            2/2     Running   0          8h      10.245.16.200   10.11.8.203   <none>           <none>
who-who-deployment-576cd44f87-lkhrc            2/2     Running   0          8h      10.245.23.136   10.11.8.174   <none>           <none>
[20:January:2022:08:17:13]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ hln as-ciat
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
NAME          NAMESPACE REVISION  UPDATED                                 STATUS    CHART               APP VERSION
horton        as-ciat   1         2022-01-20 07:58:28.289381908 +0000 UTC deployed  horton-dock-22.4.10 565ff6986f4ec0511b1de2218ba3c3244ad23e75
lorax         as-ciat   1         2022-01-20 07:58:28.318276004 +0000 UTC deployed  lorax-dock-22.4.7   431ac7a500783c0421ee8721a3f7547f480becc9
who           as-ciat   475       2022-01-20 07:58:26.188549191 +0000 UTC deployed  who-master-22.4.1   49f19d6e243010f2154536e8a95e18c1c5f07b48
who-internal  as-ciat   125       2022-01-20 07:58:40.13088022 +0000 UTC  deployed  who-master-22.4.1   49f19d6e243010f2154536e8a95e18c1c5f07b48
[20:January:2022:08:18:26]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ date
Thu Jan 20 08:18:30 UTC 2022
[20:January:2022:08:18:30]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ helm uninstall lorax -n as-ciat
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
release "lorax" uninstalled
[20:January:2022:08:20:36]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn as-ciat
NAME                                           READY   STATUS    RESTARTS   AGE     IP              NODE          NOMINATED NODE   READINESS GATES
lorax-analyticsmanager-75d985dfdf-6jmjm        2/2     Running   0          8h      10.245.14.74    10.11.9.104   <none>           <none>
lorax-analyticsmanager-75d985dfdf-mmxrw        2/2     Running   0          8h      10.245.13.192   10.11.9.145   <none>           <none>
lorax-analyticsmanager-75d985dfdf-zfr7b        2/2     Running   0          8h      10.245.20.140   10.11.8.225   <none>           <none>
lorax-cachehost-647b6cf7b4-4v6n7               2/2     Running   0          6h52m   10.245.13.197   10.11.9.145   <none>           <none>
lorax-cachehost-647b6cf7b4-jsqlv               2/2     Running   0          6h52m   10.245.27.178   10.11.8.60    <none>           <none>
lorax-cachehost-6c8948bf68-8gmdf               2/2     Running   0          8h      10.245.19.21    10.11.9.238   <none>           <none>
lorax-cachehost-6c8948bf68-mzvx7               2/2     Running   0          8h      10.245.26.8     10.11.8.67    <none>           <none>
lorax-cachehost-6c8948bf68-vf26t               2/2     Running   0          8h      10.245.15.225   10.11.9.78    <none>           <none>
lorax-commandmanager-586d58947d-2h89f          2/2     Running   0          8h      10.245.28.203   10.11.9.56    <none>           <none>
lorax-commandmanager-586d58947d-2rcmp          2/2     Running   0          8h      10.245.11.175   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-c4482          2/2     Running   0          8h      10.245.27.176   10.11.8.60    <none>           <none>
lorax-contextmanager-67956c7584-c4xkw          2/2     Running   0          8h      10.245.11.177   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-h82q5          2/2     Running   0          8h      10.245.3.92     10.11.8.232   <none>           <none>
lorax-customizationmanager-657bbf9586-h242p    2/2     Running   0          8h      10.245.16.201   10.11.8.203   <none>           <none>
lorax-customizationmanager-657bbf9586-rsbqg    2/2     Running   0          8h      10.245.29.214   10.11.9.62    <none>           <none>
lorax-elementmanager-7f74646f4d-n4cqx          2/2     Running   0          8h      10.245.28.39    10.11.8.31    <none>           <none>
lorax-eventmanager-759c5b9484-ch28f            2/2     Running   0          8h      10.245.15.226   10.11.9.78    <none>           <none>
lorax-eventworker-7c5b779bc-z4bb5              2/2     Running   0          8h      10.245.22.102   10.11.8.250   <none>           <none>
lorax-queuemanager-769996b97b-z4kqc            2/2     Running   0          8h      10.245.16.24    10.11.8.222   <none>           <none>
lorax-realtimedatamanager-866cd5886b-qq54f     2/2     Running   0          8h      10.245.29.213   10.11.9.62    <none>           <none>
lorax-reportdesignmanager-5c88cb777d-sxlx9     2/2     Running   0          8h      10.245.20.94    10.11.8.160   <none>           <none>
lorax-reportpublishmanager-58445646c4-9ztc7    2/2     Running   0          8h      10.245.11.176   10.11.9.208   <none>           <none>
lorax-searchmanager-7d97d6dfcc-gn4zg           2/2     Running   0          8h      10.245.29.212   10.11.9.62    <none>           <none>
lorax-searchmanager-7d97d6dfcc-q2x5t           2/2     Running   0          8h      10.245.14.73    10.11.9.104   <none>           <none>
lorax-securityservice-b9bb8695d-t7bz5          2/2     Running   0          8h      10.245.12.189   10.11.8.194   <none>           <none>
who-internal-who-deployment-65d44d6f4c-fqhbs   2/2     Running   0          8h      10.245.25.112   10.11.8.142   <none>           <none>
who-internal-who-deployment-65d44d6f4c-w8lnq   2/2     Running   0          8h      10.245.11.179   10.11.9.208   <none>           <none>
who-internal-who-deployment-65d44d6f4c-xljbh   2/2     Running   0          8h      10.245.16.199   10.11.8.203   <none>           <none>
who-who-deployment-576cd44f87-4dr8c            2/2     Running   0          8h      10.245.9.42     10.11.9.136   <none>           <none>
who-who-deployment-576cd44f87-lhb49            2/2     Running   0          8h      10.245.16.200   10.11.8.203   <none>           <none>
who-who-deployment-576cd44f87-lkhrc            2/2     Running   0          8h      10.245.23.136   10.11.8.174   <none>           <none>
[20:January:2022:08:21:29]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ hln as-ciat
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
NAME          NAMESPACE REVISION  UPDATED                                 STATUS    CHART               APP VERSION
horton        as-ciat   1         2022-01-20 07:58:28.289381908 +0000 UTC deployed  horton-dock-22.4.10 565ff6986f4ec0511b1de2218ba3c3244ad23e75
who           as-ciat   475       2022-01-20 07:58:26.188549191 +0000 UTC deployed  who-master-22.4.1   49f19d6e243010f2154536e8a95e18c1c5f07b48
who-internal  as-ciat   125       2022-01-20 07:58:40.13088022 +0000 UTC  deployed  who-master-22.4.1   49f19d6e243010f2154536e8a95e18c1c5f07b48
[20:January:2022:08:21:42]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgn as-ciat services
NAME                                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
as-ciat-agentbrowserui-20220120-07563127-q8qa         ClusterIP   10.97.48.0      <none>        80/TCP              23m
as-ciat-analyticsmanager-20220120-07563127-q8qa       ClusterIP   10.97.91.223    <none>        80/TCP              23m
as-ciat-cachehost-20220120-07563127-q8qa              ClusterIP   10.97.80.66     <none>        80/TCP              23m
as-ciat-coherence-20220120-07563127-q8qa              ClusterIP   None            <none>        9000/TCP,9001/TCP   23m
as-ciat-commandmanager-20220120-07563127-q8qa         ClusterIP   10.97.20.47     <none>        80/TCP              23m
as-ciat-contextmanager-20220120-07563127-q8qa         ClusterIP   10.97.2.28      <none>        80/TCP              23m
as-ciat-customizationmanager-20220120-07563127-q8qa   ClusterIP   10.97.201.59    <none>        80/TCP              23m
as-ciat-elementmanager-20220120-07563127-q8qa         ClusterIP   10.97.103.234   <none>        80/TCP              23m
as-ciat-eventmanager-20220120-07563127-q8qa           ClusterIP   10.97.210.242   <none>        80/TCP              23m
as-ciat-eventworker-20220120-07563127-q8qa            ClusterIP   10.97.76.215    <none>        80/TCP              23m
as-ciat-queuemanager-20220120-07563127-q8qa           ClusterIP   10.97.139.22    <none>        80/TCP              23m
as-ciat-realtimedatamanager-20220120-07563127-q8qa    ClusterIP   10.97.16.155    <none>        80/TCP              23m
as-ciat-reportdesignmanager-20220120-07563127-q8qa    ClusterIP   10.97.143.248   <none>        80/TCP              23m
as-ciat-reportpublishmanager-20220120-07563127-q8qa   ClusterIP   10.97.157.183   <none>        80/TCP              23m
as-ciat-searchmanager-20220120-07563127-q8qa          ClusterIP   10.97.206.139   <none>        80/TCP              23m
as-ciat-securityservice-20220120-07563127-q8qa        ClusterIP   10.97.71.88     <none>        80/TCP              23m
who-internal-who                                      ClusterIP   10.97.249.132   <none>        80/TCP              8d
who-who                                               ClusterIP   10.97.1.245     <none>        80/TCP              27d
[20:January:2022:08:22:03]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ hln as-ciat
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
NAME          NAMESPACE REVISION  UPDATED                                 STATUS    CHART               APP VERSION
horton        as-ciat   1         2022-01-20 08:28:29.830313039 +0000 UTC deployed  horton-dock-22.4.10 565ff6986f4ec0511b1de2218ba3c3244ad23e75
lorax         as-ciat   1         2022-01-20 08:28:37.853617072 +0000 UTC deployed  lorax-dock-22.4.7   431ac7a500783c0421ee8721a3f7547f480becc9
who           as-ciat   476       2022-01-20 08:28:38.92562348 +0000 UTC  deployed  who-master-22.4.1   49f19d6e243010f2154536e8a95e18c1c5f07b48
who-internal  as-ciat   126       2022-01-20 08:28:29.842755703 +0000 UTC deployed  who-master-22.4.1   49f19d6e243010f2154536e8a95e18c1c5f07b48
[20:January:2022:11:18:34]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[20:January:2022:13:29:57]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ hln as-ciat
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
NAME          NAMESPACE REVISION  UPDATED                                 STATUS    CHART               APP VERSION
horton        as-ciat   1         2022-01-20 08:28:29.830313039 +0000 UTC deployed  horton-dock-22.4.10 565ff6986f4ec0511b1de2218ba3c3244ad23e75
lorax         as-ciat   1         2022-01-20 08:28:37.853617072 +0000 UTC deployed  lorax-dock-22.4.7   431ac7a500783c0421ee8721a3f7547f480becc9
who           as-ciat   476       2022-01-20 08:28:38.92562348 +0000 UTC  deployed  who-master-22.4.1   49f19d6e243010f2154536e8a95e18c1c5f07b48
who-internal  as-ciat   126       2022-01-20 08:28:29.842755703 +0000 UTC deployed  who-master-22.4.1   49f19d6e243010f2154536e8a95e18c1c5f07b48
[20:January:2022:13:30:06]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ #kgn as-jcs events
[20:January:2022:13:30:21]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn as-ciat
NAME                                           READY   STATUS    RESTARTS   AGE   IP              NODE          NOMINATED NODE   READINESS GATES
lorax-analyticsmanager-75d985dfdf-6jmjm        2/2     Running   0          13h   10.245.14.74    10.11.9.104   <none>           <none>
lorax-analyticsmanager-75d985dfdf-mmxrw        2/2     Running   0          13h   10.245.13.192   10.11.9.145   <none>           <none>
lorax-cachehost-647b6cf7b4-4v6n7               2/2     Running   0          12h   10.245.13.197   10.11.9.145   <none>           <none>
lorax-cachehost-647b6cf7b4-jsqlv               2/2     Running   2          12h   10.245.27.178   10.11.8.60    <none>           <none>
lorax-cachehost-6c8948bf68-8gmdf               2/2     Running   0          13h   10.245.19.21    10.11.9.238   <none>           <none>
lorax-cachehost-6c8948bf68-mzvx7               2/2     Running   0          13h   10.245.26.8     10.11.8.67    <none>           <none>
lorax-cachehost-6c8948bf68-vf26t               2/2     Running   3          13h   10.245.15.225   10.11.9.78    <none>           <none>
lorax-commandmanager-586d58947d-2h89f          2/2     Running   0          13h   10.245.28.203   10.11.9.56    <none>           <none>
lorax-commandmanager-586d58947d-2rcmp          2/2     Running   0          13h   10.245.11.175   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-c4482          2/2     Running   0          13h   10.245.27.176   10.11.8.60    <none>           <none>
lorax-contextmanager-67956c7584-c4xkw          2/2     Running   0          13h   10.245.11.177   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-h82q5          2/2     Running   0          13h   10.245.3.92     10.11.8.232   <none>           <none>
lorax-customizationmanager-657bbf9586-rsbqg    2/2     Running   0          13h   10.245.29.214   10.11.9.62    <none>           <none>
lorax-elementmanager-7f74646f4d-n4cqx          2/2     Running   0          13h   10.245.28.39    10.11.8.31    <none>           <none>
lorax-eventmanager-759c5b9484-ch28f            2/2     Running   0          13h   10.245.15.226   10.11.9.78    <none>           <none>
lorax-eventworker-7c5b779bc-z4bb5              2/2     Running   0          13h   10.245.22.102   10.11.8.250   <none>           <none>
lorax-realtimedatamanager-866cd5886b-qq54f     2/2     Running   0          13h   10.245.29.213   10.11.9.62    <none>           <none>
lorax-reportpublishmanager-58445646c4-9ztc7    2/2     Running   0          13h   10.245.11.176   10.11.9.208   <none>           <none>
lorax-searchmanager-7d97d6dfcc-gn4zg           2/2     Running   0          13h   10.245.29.212   10.11.9.62    <none>           <none>
lorax-searchmanager-7d97d6dfcc-q2x5t           2/2     Running   0          13h   10.245.14.73    10.11.9.104   <none>           <none>
who-internal-who-deployment-65d44d6f4c-w8lnq   2/2     Running   0          13h   10.245.11.179   10.11.9.208   <none>           <none>
who-who-deployment-576cd44f87-4dr8c            2/2     Running   0          13h   10.245.9.42     10.11.9.136   <none>           <none>
[20:January:2022:13:30:32]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ oong
{
  "data": "osvccorp"
}
[20:January:2022:13:30:47]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[20:January:2022:13:30:57]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgno
NAME          STATUS   ROLES   AGE     VERSION
10.11.8.227   Ready    node    3h16m   v1.19.12
10.11.8.229   Ready    node    64d     v1.19.12
10.11.8.232   Ready    node    64d     v1.19.12
10.11.8.24    Ready    node    3h52m   v1.19.12
10.11.8.250   Ready    node    63d     v1.19.12
10.11.8.27    Ready    node    5h15m   v1.19.12
10.11.8.31    Ready    node    7d19h   v1.19.12
10.11.8.39    Ready    node    64d     v1.19.12
10.11.8.46    Ready    node    64d     v1.19.12
10.11.8.48    Ready    node    7d22h   v1.19.12
10.11.8.60    Ready    node    7d19h   v1.19.12
10.11.8.67    Ready    node    63d     v1.19.12
10.11.8.68    Ready    node    64d     v1.19.12
10.11.8.69    Ready    node    4h58m   v1.19.12
10.11.8.71    Ready    node    64d     v1.19.12
10.11.8.72    Ready    node    64d     v1.19.12
10.11.8.76    Ready    node    64d     v1.19.12
10.11.8.80    Ready    node    64d     v1.19.12
10.11.8.82    Ready    node    64d     v1.19.12
10.11.8.94    Ready    node    64d     v1.19.12
10.11.9.1     Ready    node    63d     v1.19.12
10.11.9.103   Ready    node    3h35m   v1.19.12
10.11.9.104   Ready    node    64d     v1.19.12
10.11.9.113   Ready    node    5h55m   v1.19.12
10.11.9.124   Ready    node    63d     v1.19.12
10.11.9.127   Ready    node    63d     v1.19.12
10.11.9.13    Ready    node    63d     v1.19.12
10.11.9.135   Ready    node    64d     v1.19.12
10.11.9.136   Ready    node    64d     v1.19.12
10.11.9.138   Ready    node    5h41m   v1.19.12
10.11.9.14    Ready    node    63d     v1.19.12
10.11.9.141   Ready    node    64d     v1.19.12
10.11.9.145   Ready    node    64d     v1.19.12
10.11.9.147   Ready    node    4h9m    v1.19.12
10.11.9.157   Ready    node    64d     v1.19.12
10.11.9.16    Ready    node    64d     v1.19.12
10.11.9.160   Ready    node    63d     v1.19.12
10.11.9.165   Ready    node    165m    v1.19.12
10.11.9.177   Ready    node    3h3m    v1.19.12
10.11.9.182   Ready    node    63d     v1.19.12
10.11.9.195   Ready    node    6h11m   v1.19.12
10.11.9.198   Ready    node    4h26m   v1.19.12
10.11.9.208   Ready    node    64d     v1.19.12
10.11.9.23    Ready    node    64d     v1.19.12
10.11.9.238   Ready    node    63d     v1.19.12
10.11.9.37    Ready    node    64d     v1.19.12
10.11.9.49    Ready    node    64d     v1.19.12
10.11.9.56    Ready    node    7d19h   v1.19.12
10.11.9.58    Ready    node    7d19h   v1.19.12
10.11.9.6     Ready    node    5h27m   v1.19.12
10.11.9.62    Ready    node    7d19h   v1.19.12
10.11.9.7     Ready    node    4h42m   v1.19.12
10.11.9.78    Ready    node    63d     v1.19.12
10.11.9.84    Ready    node    63d     v1.19.12
10.11.9.86    Ready    node    149m    v1.19.12
10.11.9.9     Ready    node    63d     v1.19.12
10.11.9.91    Ready    node    64d     v1.19.12
10.11.9.92    Ready    node    64d     v1.19.12
10.11.9.93    Ready    node    63d     v1.19.12
10.11.9.95    Ready    node    64d     v1.19.12
[20:January:2022:13:31:10]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[20:January:2022:13:31:34]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ helm uninstall lorax -n as-ciat
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
release "lorax" uninstalled
[20:January:2022:13:31:53]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ hln as-ciat
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
NAME          NAMESPACE REVISION  UPDATED                                 STATUS    CHART               APP VERSION
horton        as-ciat   1         2022-01-20 08:28:29.830313039 +0000 UTC deployed  horton-dock-22.4.10 565ff6986f4ec0511b1de2218ba3c3244ad23e75
who           as-ciat   476       2022-01-20 08:28:38.92562348 +0000 UTC  deployed  who-master-22.4.1   49f19d6e243010f2154536e8a95e18c1c5f07b48
who-internal  as-ciat   126       2022-01-20 08:28:29.842755703 +0000 UTC deployed  who-master-22.4.1   49f19d6e243010f2154536e8a95e18c1c5f07b48
[20:January:2022:13:32:05]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn as-ciat
NAME                                           READY   STATUS    RESTARTS   AGE   IP              NODE          NOMINATED NODE   READINESS GATES
lorax-analyticsmanager-75d985dfdf-6jmjm        2/2     Running   0          13h   10.245.14.74    10.11.9.104   <none>           <none>
lorax-analyticsmanager-75d985dfdf-mmxrw        2/2     Running   0          13h   10.245.13.192   10.11.9.145   <none>           <none>
lorax-cachehost-647b6cf7b4-4v6n7               2/2     Running   0          12h   10.245.13.197   10.11.9.145   <none>           <none>
lorax-cachehost-647b6cf7b4-jsqlv               2/2     Running   2          12h   10.245.27.178   10.11.8.60    <none>           <none>
lorax-cachehost-6c8948bf68-8gmdf               2/2     Running   0          13h   10.245.19.21    10.11.9.238   <none>           <none>
lorax-cachehost-6c8948bf68-mzvx7               2/2     Running   0          13h   10.245.26.8     10.11.8.67    <none>           <none>
lorax-cachehost-6c8948bf68-vf26t               2/2     Running   3          13h   10.245.15.225   10.11.9.78    <none>           <none>
lorax-commandmanager-586d58947d-2h89f          2/2     Running   0          13h   10.245.28.203   10.11.9.56    <none>           <none>
lorax-commandmanager-586d58947d-2rcmp          2/2     Running   0          13h   10.245.11.175   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-c4482          2/2     Running   0          13h   10.245.27.176   10.11.8.60    <none>           <none>
lorax-contextmanager-67956c7584-c4xkw          2/2     Running   0          13h   10.245.11.177   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-h82q5          2/2     Running   0          13h   10.245.3.92     10.11.8.232   <none>           <none>
lorax-customizationmanager-657bbf9586-rsbqg    2/2     Running   0          13h   10.245.29.214   10.11.9.62    <none>           <none>
lorax-elementmanager-7f74646f4d-n4cqx          2/2     Running   0          13h   10.245.28.39    10.11.8.31    <none>           <none>
lorax-eventmanager-759c5b9484-ch28f            2/2     Running   0          13h   10.245.15.226   10.11.9.78    <none>           <none>
lorax-eventworker-7c5b779bc-z4bb5              2/2     Running   0          13h   10.245.22.102   10.11.8.250   <none>           <none>
lorax-realtimedatamanager-866cd5886b-qq54f     2/2     Running   0          13h   10.245.29.213   10.11.9.62    <none>           <none>
lorax-reportpublishmanager-58445646c4-9ztc7    2/2     Running   0          13h   10.245.11.176   10.11.9.208   <none>           <none>
lorax-searchmanager-7d97d6dfcc-gn4zg           2/2     Running   0          13h   10.245.29.212   10.11.9.62    <none>           <none>
lorax-searchmanager-7d97d6dfcc-q2x5t           2/2     Running   0          13h   10.245.14.73    10.11.9.104   <none>           <none>
who-internal-who-deployment-65d44d6f4c-w8lnq   2/2     Running   0          13h   10.245.11.179   10.11.9.208   <none>           <none>
who-who-deployment-576cd44f87-4dr8c            2/2     Running   0          13h   10.245.9.42     10.11.9.136   <none>           <none>
[20:January:2022:13:32:21]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn as-ciat
NAME                          READY   UP-TO-DATE   AVAILABLE   AGE
horton-horton-deployment      0/3     0            0           5h4m
who-internal-who-deployment   1/3     0            1           9d
who-who-deployment            1/3     0            1           27d
[20:January:2022:13:32:44]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgn as-ciat services
NAME                                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
as-ciat-agentbrowserui-20220120-08263133-m5tc         ClusterIP   10.97.211.119   <none>        80/TCP              5h4m
as-ciat-analyticsmanager-20220120-07563127-q8qa       ClusterIP   10.97.91.223    <none>        80/TCP              5h34m
as-ciat-cachehost-20220120-07563127-q8qa              ClusterIP   10.97.80.66     <none>        80/TCP              5h34m
as-ciat-coherence-20220120-07563127-q8qa              ClusterIP   None            <none>        9000/TCP,9001/TCP   5h34m
as-ciat-commandmanager-20220120-07563127-q8qa         ClusterIP   10.97.20.47     <none>        80/TCP              5h34m
as-ciat-contextmanager-20220120-07563127-q8qa         ClusterIP   10.97.2.28      <none>        80/TCP              5h34m
as-ciat-customizationmanager-20220120-07563127-q8qa   ClusterIP   10.97.201.59    <none>        80/TCP              5h34m
as-ciat-elementmanager-20220120-07563127-q8qa         ClusterIP   10.97.103.234   <none>        80/TCP              5h34m
as-ciat-eventmanager-20220120-07563127-q8qa           ClusterIP   10.97.210.242   <none>        80/TCP              5h34m
as-ciat-eventworker-20220120-07563127-q8qa            ClusterIP   10.97.76.215    <none>        80/TCP              5h34m
as-ciat-queuemanager-20220120-07563127-q8qa           ClusterIP   10.97.139.22    <none>        80/TCP              5h34m
as-ciat-realtimedatamanager-20220120-07563127-q8qa    ClusterIP   10.97.16.155    <none>        80/TCP              5h34m
as-ciat-reportdesignmanager-20220120-07563127-q8qa    ClusterIP   10.97.143.248   <none>        80/TCP              5h34m
as-ciat-reportpublishmanager-20220120-07563127-q8qa   ClusterIP   10.97.157.183   <none>        80/TCP              5h34m
as-ciat-searchmanager-20220120-07563127-q8qa          ClusterIP   10.97.206.139   <none>        80/TCP              5h34m
as-ciat-securityservice-20220120-07563127-q8qa        ClusterIP   10.97.71.88     <none>        80/TCP              5h34m
who-internal-who                                      ClusterIP   10.97.249.132   <none>        80/TCP              9d
who-who                                               ClusterIP   10.97.1.245     <none>        80/TCP              27d
[20:January:2022:13:33:17]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgn as-ciat secrets
NAME                                   TYPE                                  DATA   AGE
consul-tls                             Opaque                                1      142d
default-token-zrljs                    kubernetes.io/service-account-token   3      142d
ocirsecret                             kubernetes.io/dockerconfigjson        1      142d
osvc-token                             kubernetes.io/dockerconfigjson        1      139d
osvcstage-ocirsecret                   kubernetes.io/dockerconfigjson        1      142d
sh.helm.release.v1.horton.v1           helm.sh/release.v1                    1      5h4m
sh.helm.release.v1.who-internal.v117   helm.sh/release.v1                    1      9h
sh.helm.release.v1.who-internal.v118   helm.sh/release.v1                    1      9h
sh.helm.release.v1.who-internal.v119   helm.sh/release.v1                    1      8h
sh.helm.release.v1.who-internal.v120   helm.sh/release.v1                    1      8h
sh.helm.release.v1.who-internal.v121   helm.sh/release.v1                    1      7h34m
sh.helm.release.v1.who-internal.v122   helm.sh/release.v1                    1      7h4m
sh.helm.release.v1.who-internal.v123   helm.sh/release.v1                    1      6h34m
sh.helm.release.v1.who-internal.v124   helm.sh/release.v1                    1      6h4m
sh.helm.release.v1.who-internal.v125   helm.sh/release.v1                    1      5h34m
sh.helm.release.v1.who-internal.v126   helm.sh/release.v1                    1      5h4m
sh.helm.release.v1.who.v467            helm.sh/release.v1                    1      9h
sh.helm.release.v1.who.v468            helm.sh/release.v1                    1      9h
sh.helm.release.v1.who.v469            helm.sh/release.v1                    1      8h
sh.helm.release.v1.who.v470            helm.sh/release.v1                    1      8h
sh.helm.release.v1.who.v471            helm.sh/release.v1                    1      7h34m
sh.helm.release.v1.who.v472            helm.sh/release.v1                    1      7h4m
sh.helm.release.v1.who.v473            helm.sh/release.v1                    1      6h34m
sh.helm.release.v1.who.v474            helm.sh/release.v1                    1      6h4m
sh.helm.release.v1.who.v475            helm.sh/release.v1                    1      5h34m
sh.helm.release.v1.who.v476            helm.sh/release.v1                    1      5h4m
vault-tls                              Opaque                                1      142d
[20:January:2022:13:33:26]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[20:January:2022:13:33:37]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ hln as-ciat
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
NAME          NAMESPACE REVISION  UPDATED                                 STATUS    CHART               APP VERSION
horton        as-ciat   1         2022-01-20 08:28:29.830313039 +0000 UTC deployed  horton-dock-22.4.10 565ff6986f4ec0511b1de2218ba3c3244ad23e75
who           as-ciat   476       2022-01-20 08:28:38.92562348 +0000 UTC  deployed  who-master-22.4.1   49f19d6e243010f2154536e8a95e18c1c5f07b48
who-internal  as-ciat   126       2022-01-20 08:28:29.842755703 +0000 UTC deployed  who-master-22.4.1   49f19d6e243010f2154536e8a95e18c1c5f07b48
[20:January:2022:13:34:04]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ helm uninstall who-internal -n as-ciat
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
release "who-internal" uninstalled
[20:January:2022:13:34:53]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ ll /home/opc/.kube/pv2_corp.config
-rw-r--r-- 1 opc opc 7061 Jan 20 04:41 /home/opc/.kube/pv2_corp.config
[20:January:2022:13:35:00]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ chmod 400 /home/opc/.kube/pv2_corp.config
[20:January:2022:13:35:02]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ hln as-ciat
NAME    NAMESPACE REVISION  UPDATED                                 STATUS    CHART               APP VERSION
horton  as-ciat   1         2022-01-20 08:28:29.830313039 +0000 UTC deployed  horton-dock-22.4.10 565ff6986f4ec0511b1de2218ba3c3244ad23e75
who     as-ciat   476       2022-01-20 08:28:38.92562348 +0000 UTC  deployed  who-master-22.4.1   49f19d6e243010f2154536e8a95e18c1c5f07b48
[20:January:2022:13:35:10]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgn as-ciat secrets
NAME                           TYPE                                  DATA   AGE
consul-tls                     Opaque                                1      142d
default-token-zrljs            kubernetes.io/service-account-token   3      142d
ocirsecret                     kubernetes.io/dockerconfigjson        1      142d
osvc-token                     kubernetes.io/dockerconfigjson        1      139d
osvcstage-ocirsecret           kubernetes.io/dockerconfigjson        1      142d
sh.helm.release.v1.horton.v1   helm.sh/release.v1                    1      5h6m
sh.helm.release.v1.who.v467    helm.sh/release.v1                    1      9h
sh.helm.release.v1.who.v468    helm.sh/release.v1                    1      9h
sh.helm.release.v1.who.v469    helm.sh/release.v1                    1      8h
sh.helm.release.v1.who.v470    helm.sh/release.v1                    1      8h
sh.helm.release.v1.who.v471    helm.sh/release.v1                    1      7h36m
sh.helm.release.v1.who.v472    helm.sh/release.v1                    1      7h6m
sh.helm.release.v1.who.v473    helm.sh/release.v1                    1      6h36m
sh.helm.release.v1.who.v474    helm.sh/release.v1                    1      6h6m
sh.helm.release.v1.who.v475    helm.sh/release.v1                    1      5h36m
sh.helm.release.v1.who.v476    helm.sh/release.v1                    1      5h6m
vault-tls                      Opaque                                1      142d
[20:January:2022:13:35:20]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn as-ciat
NAME                                           READY   STATUS    RESTARTS   AGE   IP              NODE          NOMINATED NODE   READINESS GATES
lorax-analyticsmanager-75d985dfdf-6jmjm        2/2     Running   0          13h   10.245.14.74    10.11.9.104   <none>           <none>
lorax-analyticsmanager-75d985dfdf-mmxrw        2/2     Running   0          13h   10.245.13.192   10.11.9.145   <none>           <none>
lorax-cachehost-647b6cf7b4-4v6n7               2/2     Running   0          12h   10.245.13.197   10.11.9.145   <none>           <none>
lorax-cachehost-647b6cf7b4-jsqlv               2/2     Running   2          12h   10.245.27.178   10.11.8.60    <none>           <none>
lorax-cachehost-6c8948bf68-8gmdf               2/2     Running   0          13h   10.245.19.21    10.11.9.238   <none>           <none>
lorax-cachehost-6c8948bf68-mzvx7               2/2     Running   0          13h   10.245.26.8     10.11.8.67    <none>           <none>
lorax-cachehost-6c8948bf68-vf26t               2/2     Running   3          13h   10.245.15.225   10.11.9.78    <none>           <none>
lorax-commandmanager-586d58947d-2h89f          2/2     Running   0          13h   10.245.28.203   10.11.9.56    <none>           <none>
lorax-commandmanager-586d58947d-2rcmp          2/2     Running   0          13h   10.245.11.175   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-c4482          2/2     Running   0          13h   10.245.27.176   10.11.8.60    <none>           <none>
lorax-contextmanager-67956c7584-c4xkw          2/2     Running   0          13h   10.245.11.177   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-h82q5          2/2     Running   0          13h   10.245.3.92     10.11.8.232   <none>           <none>
lorax-customizationmanager-657bbf9586-rsbqg    2/2     Running   0          13h   10.245.29.214   10.11.9.62    <none>           <none>
lorax-elementmanager-7f74646f4d-n4cqx          2/2     Running   0          13h   10.245.28.39    10.11.8.31    <none>           <none>
lorax-eventmanager-759c5b9484-ch28f            2/2     Running   0          13h   10.245.15.226   10.11.9.78    <none>           <none>
lorax-eventworker-7c5b779bc-z4bb5              2/2     Running   0          13h   10.245.22.102   10.11.8.250   <none>           <none>
lorax-realtimedatamanager-866cd5886b-qq54f     2/2     Running   0          13h   10.245.29.213   10.11.9.62    <none>           <none>
lorax-reportpublishmanager-58445646c4-9ztc7    2/2     Running   0          13h   10.245.11.176   10.11.9.208   <none>           <none>
lorax-searchmanager-7d97d6dfcc-gn4zg           2/2     Running   0          13h   10.245.29.212   10.11.9.62    <none>           <none>
lorax-searchmanager-7d97d6dfcc-q2x5t           2/2     Running   0          13h   10.245.14.73    10.11.9.104   <none>           <none>
who-internal-who-deployment-65d44d6f4c-w8lnq   2/2     Running   0          13h   10.245.11.179   10.11.9.208   <none>           <none>
who-who-deployment-576cd44f87-4dr8c            2/2     Running   0          13h   10.245.9.42     10.11.9.136   <none>           <none>
[20:January:2022:13:35:31]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn as-ciat
NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
horton-horton-deployment   0/3     0            0           5h7m
who-who-deployment         1/3     0            1           27d
[20:January:2022:13:35:50]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kdpn as-ciat who-internal-who-deployment-65d44d6f4c-w8lnq
Name:         who-internal-who-deployment-65d44d6f4c-w8lnq
Namespace:    as-ciat
Priority:     0
Node:         10.11.9.208/10.11.9.208
Start Time:   Wed, 19 Jan 2022 23:59:42 +0000
Labels:       app=who
              cluster=as-ciat
              isInternalDeployment=true
              pod-template-hash=65d44d6f4c
Annotations:  as.osvc.oracle.com/app-version: 49f19d6e243010f2154536e8a95e18c1c5f07b48
              as.osvc.oracle.com/managed: true
              kubectl.kubernetes.io/default-container: server
              kubernetes.io/psp: psp-protect-docker
              vault.security.banzaicloud.io/log-level: warn
              vault.security.banzaicloud.io/vault-addr: https://vault.query.corp.consul:8200
              vault.security.banzaicloud.io/vault-agent-configmap: as-ciat-va-configmap
              vault.security.banzaicloud.io/vault-env-daemon: true
              vault.security.banzaicloud.io/vault-path: k8s-iad-dataplane
              vault.security.banzaicloud.io/vault-role: as-ciat_role
              vault.security.banzaicloud.io/vault-skip-verify: true
Status:       Running
IP:           10.245.11.179
IPs:
  IP:           10.245.11.179
Controlled By:  ReplicaSet/who-internal-who-deployment-65d44d6f4c
Init Containers:
  copy-vault-env:
    Container ID:  docker://6825d99219efe834fc0264e5016bdb2ab8fee64b54f900006df3b08edb113e60
    Image:         docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3
    Image ID:      docker-pullable://docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env@sha256:7435d1c641c0aa0fd4faaff32ab189b29d3d909b100d36d4d90df3b79cee8ce3
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -c
      cp /usr/local/bin/vault-env /vault/
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 19 Jan 2022 23:59:43 +0000
      Finished:     Wed, 19 Jan 2022 23:59:43 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     250m
      memory:  64Mi
    Requests:
      cpu:        50m
      memory:     64Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zrljs (ro)
      /vault/ from vault-env (rw)
Containers:
  vault-agent:
    Container ID:  docker://1815ac26e20e2de92b5d2ec8c7a88d0b079004ed64555b5738a29ad663e386b4
    Image:         docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5
    Image ID:      docker-pullable://docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault@sha256:57438e97b72c05368ca06f4d5f34667e965248635718001abdea8fa11f18adad
    Port:          <none>
    Host Port:     <none>
    Args:
      agent
      -config
      /vault/config/config.hcl
    State:          Running
      Started:      Wed, 19 Jan 2022 23:59:43 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  128Mi
    Requests:
      cpu:     100m
      memory:  128Mi
    Environment:
      VAULT_ADDR:         https://vault.query.corp.consul:8200
      VAULT_SKIP_VERIFY:  true
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zrljs (ro)
      /vault/ from vault-env (rw)
      /vault/config/config.hcl from agent-configmap (ro,path="config.hcl")
      /vault/secrets from agent-secrets (rw)
  server:
    Container ID:  docker://141f1a0149672fb04ca247c219e62b550bc74dd58ffef551599581354b022a87
    Image:         iad.ocir.io/osvccorp/bui/who:master-2022.01.19-17.33.25-49f19d6e
    Image ID:      docker-pullable://iad.ocir.io/osvccorp/bui/who@sha256:8458ebf460e2aa3364312c89ee9a7990f70750ff2c37a7e7c248a31872b4fbbb
    Port:          7001/TCP
    Host Port:     0/TCP
    Command:
      /vault/vault-env
    Args:
      /bin/sh
      -c
      java     -XX:+UnlockExperimentalVMOptions     -XX:+UseCGroupMemoryLimitForHeap     -XX:+HeapDumpOnOutOfMemoryError     -XX:HeapDumpPath=/app/logs/heap_pid%%p.hprof     -XX:+UseG1GC     -XX:MaxGCPauseMillis=250     -Dorg.oracle.fusion.service.kubernetes.namespace=${NAMESPACE}     -Dorg.oracle.fusion.service.ContextPath=/app/config     -Dorg.oracle.fusion.service.resourcePath=/app/config     -Dorg.oracle.fusion.service.hostName=0.0.0.0     -Dorg.oracle.fusion.service.portNumber=7001     -Dorg.oracle.fusion.service.consulHttpApiUrl=${CONSUL}     -Dconsul.base.address=${CONSUL}     -Dorg.oracle.fusion.service.path=${SERVICE_PATH}     -Dorg.oracle.fusion.service.devMode=${DEVMODE}     -Dorg.oracle.fusion.service.buiBasePath=${BUI_BASE_PATH}     -Dorg.oracle.fusion.service.clusterName=${CLUSTER}     -Djava.rmi.server.hostname=who     -Dlogback.configurationFile=/app/config/live/logback.xml     -Dorg.oracle.fusion.service.jolokiaRealmFile=/app/config/realm.properties     -Dorg.oracle.fusion.service.jolokiaRole=asJolokiaRole     -Duser.dir=/app     ${JAVA_OPTS}     -cp     /app/*:/app/lib/*     org.oracle.service.router.Host
    State:          Running
      Started:      Wed, 19 Jan 2022 23:59:43 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  4Gi
    Requests:
      cpu:      500m
      memory:   2Gi
    Liveness:   http-get http://:7001/AgentWeb/ping delay=0s timeout=1s period=10s #success=1 #failure=5
    Readiness:  http-get http://:7001/AgentWeb/ready delay=0s timeout=1s period=10s #success=1 #failure=10
    Environment:
      JAVA_OPTS:                     -Dorg.oracle.fusion.service.k8s=true -Dorg.oracle.fusion.service.kubernetes.domain=cluster.local -Dorg.oracle.fusion.service.kubernetes.singleVersion=true -Dorg.oracle.fusion.service.jaeger.enabled=true -Dorg.oracle.fusion.service.forwardedForHeader=X-Original-Forwarded-For -Dorg.oracle.fusion.service.defaultVersion=20220119-23563149-ashd -Dorg.oracle.fusion.service.consul.certPath=/consul/ca.crt -Dorg.oracle.fusion.service.vault.token-path=/vault/.vault-token -Dorg.oracle.fusion.service.vault.enabled=true -Dorg.oracle.fusion.service.vault.truststore=/vault/ca.crt -Dorg.oracle.fusion.service.vault.ssl.enabled=true
                                     -Dkafka.bootstrap.servers=bui-kafka-kafka-brokers.bui:9093 -Dkafka.security.protocol=SSL -Dkafka.auth.keystore.path=/vault/secrets/keystore.p12 -Dkafka.auth.truststore.path=/vault/secrets/truststore.p12 -Dkafka.auth.issuing.ca.path=/vault/secrets/issuing_ca.pem -Dkafka.auth.private.key.path=/vault/secrets/private_key.pem -Dkafka.auth.certificate.path=/vault/secrets/certificate.pem
                                     -Dlogging.api.kafka.enabled=false -Dschema.registry.url=https://schemaregistry-proxy-kafka.service.iad-dataplane.corp.consul -Dorg.oracle.fusion.service.who.internalOnly=true
      CLUSTER:                       as-ciat
      VERSION:                       as-ciat (v1:metadata.namespace)
      SERVICE_PATH:                  /AgentWeb
      BUI_BASE_PATH:                 as-ciat.corp.channels.ocs.oc-test.com
      DEVMODE:                       true
      NAMESPACE:                     as-ciat (v1:metadata.namespace)
      HOST_IP:                        (v1:status.hostIP)
      CONSUL:                        https://$(HOST_IP):8501
      CONSUL_TOKEN:                  vault:/cpe_consul/creds/as-ciat_consul_role#token
      JAEGER_AGENT_HOST:              (v1:status.hostIP)
      JAEGER_AGENT_PORT:             6831
      JAEGER_AGENT_PROTOCOL:         udp
      VAULT_ADDR:                    https://vault.query.corp.consul:8200
      VAULT_SKIP_VERIFY:             true
      VAULT_AUTH_METHOD:             jwt
      VAULT_PATH:                    k8s-iad-dataplane
      VAULT_ROLE:                    as-ciat_role
      VAULT_IGNORE_MISSING_SECRETS:  false
      VAULT_ENV_PASSTHROUGH:
      VAULT_JSON_LOG:                false
      VAULT_CLIENT_TIMEOUT:          10s
      VAULT_LOG_LEVEL:               warn
      VAULT_ENV_DAEMON:              true
    Mounts:
      /app/config/live from who-config (rw)
      /consul from consul-ca (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zrljs (ro)
      /vault/ from vault-env (rw)
      /vault/ca.crt from vault-ca (ro,path="ca.crt")
      /vault/secrets from agent-secrets (rw)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  who-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      who-internal-config
    Optional:  false
  consul-ca:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  consul-tls
    Optional:    false
  vault-ca:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  vault-tls
    Optional:    false
  default-token-zrljs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-zrljs
    Optional:    false
  vault-env:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  agent-secrets:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  agent-configmap:
    Type:        ConfigMap (a volume populated by a ConfigMap)
    Name:        as-ciat-va-configmap
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason       Age                From     Message
  ----     ------       ----               ----     -------
  Warning  FailedMount  31s (x8 over 94s)  kubelet  MountVolume.SetUp failed for volume "who-config" : configmap "who-internal-config" not found
[20:January:2022:13:36:29]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgn as-ciat services
NAME                                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
as-ciat-agentbrowserui-20220120-08263133-m5tc         ClusterIP   10.97.211.119   <none>        80/TCP              5h8m
as-ciat-analyticsmanager-20220120-07563127-q8qa       ClusterIP   10.97.91.223    <none>        80/TCP              5h38m
as-ciat-cachehost-20220120-07563127-q8qa              ClusterIP   10.97.80.66     <none>        80/TCP              5h38m
as-ciat-coherence-20220120-07563127-q8qa              ClusterIP   None            <none>        9000/TCP,9001/TCP   5h38m
as-ciat-commandmanager-20220120-07563127-q8qa         ClusterIP   10.97.20.47     <none>        80/TCP              5h38m
as-ciat-contextmanager-20220120-07563127-q8qa         ClusterIP   10.97.2.28      <none>        80/TCP              5h38m
as-ciat-customizationmanager-20220120-07563127-q8qa   ClusterIP   10.97.201.59    <none>        80/TCP              5h38m
as-ciat-elementmanager-20220120-07563127-q8qa         ClusterIP   10.97.103.234   <none>        80/TCP              5h38m
as-ciat-eventmanager-20220120-07563127-q8qa           ClusterIP   10.97.210.242   <none>        80/TCP              5h38m
as-ciat-eventworker-20220120-07563127-q8qa            ClusterIP   10.97.76.215    <none>        80/TCP              5h38m
as-ciat-queuemanager-20220120-07563127-q8qa           ClusterIP   10.97.139.22    <none>        80/TCP              5h38m
as-ciat-realtimedatamanager-20220120-07563127-q8qa    ClusterIP   10.97.16.155    <none>        80/TCP              5h38m
as-ciat-reportdesignmanager-20220120-07563127-q8qa    ClusterIP   10.97.143.248   <none>        80/TCP              5h38m
as-ciat-reportpublishmanager-20220120-07563127-q8qa   ClusterIP   10.97.157.183   <none>        80/TCP              5h38m
as-ciat-searchmanager-20220120-07563127-q8qa          ClusterIP   10.97.206.139   <none>        80/TCP              5h38m
as-ciat-securityservice-20220120-07563127-q8qa        ClusterIP   10.97.71.88     <none>        80/TCP              5h38m
who-who                                               ClusterIP   10.97.1.245     <none>        80/TCP              27d
[20:January:2022:13:37:04]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kdno 10.11.9.208
Name:               10.11.9.208
Roles:              node
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/instance-type=VM.Standard.E4.Flex
                    beta.kubernetes.io/os=linux
                    displayName=oke-c2tqztfmy3t-nhudgq7i5vq-sy5obopmzdq-10
                    failure-domain.beta.kubernetes.io/region=iad
                    failure-domain.beta.kubernetes.io/zone=US-ASHBURN-AD-3
                    hostname=oke-c2tqztfmy3t-nhudgq7i5vq-sy5obopmzdq-10
                    internal_addr=10.11.9.208
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=10.11.9.208
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/node=
                    node.info.ds_proxymux_client=true
                    node.info/compartment.id_prefix=ocid1.compartment.oc1
                    node.info/compartment.id_suffix=aaaaaaaaohtpypgcsueudftlryawztlk4i6rq5vh7w5aldgkb7pfolhftvwa
                    node.info/compartment.name=dataplane
                    node.info/kubeletVersion=v1.19
                    oci.oraclecloud.com/fault-domain=FAULT-DOMAIN-2
                    oke-apps-cluster=apps_np2-corp-iad
                    oke.oraclecloud.com/node.info.private_subnet=true
                    oke.oraclecloud.com/node.info.private_worker=true
                    oke.oraclecloud.com/tenant_agent.version=1.38.1-3a04b4c6c8-713
Annotations:        alpha.kubernetes.io/provided-node-ip: 10.11.9.208
                    csi.volume.kubernetes.io/nodeid: {"fss.csi.oraclecloud.com":"10.11.9.208"}
                    flannel.alpha.coreos.com/backend-data: {"VNI":1,"VtepMAC":"86:f3:60:80:3b:63"}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 10.11.9.208
                    node.alpha.kubernetes.io/ttl: 0
                    oci.oraclecloud.com/compartment-id: ocid1.compartment.oc1..aaaaaaaaohtpypgcsueudftlryawztlk4i6rq5vh7w5aldgkb7pfolhftvwa
                    oci.oraclecloud.com/node-pool-id: ocid1.nodepool.oc1.iad.aaaaaaaavlnttjhxf46h7a7jzfe5es3y5hji4nvluyb7yyz5vnhudgq7i5vq
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 17 Nov 2021 12:29:37 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  10.11.9.208
  AcquireTime:     <unset>
  RenewTime:       Thu, 20 Jan 2022 13:37:37 +0000
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Wed, 17 Nov 2021 12:30:56 +0000   Wed, 17 Nov 2021 12:30:56 +0000   FlannelIsUp                  Flannel is running on this node
  MemoryPressure       False   Thu, 20 Jan 2022 13:36:09 +0000   Wed, 17 Nov 2021 12:29:37 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Thu, 20 Jan 2022 13:36:09 +0000   Wed, 17 Nov 2021 12:29:37 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Thu, 20 Jan 2022 13:36:09 +0000   Wed, 17 Nov 2021 12:29:37 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Thu, 20 Jan 2022 13:36:09 +0000   Wed, 17 Nov 2021 12:30:57 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  10.11.9.208
  Hostname:    10.11.9.208
Capacity:
  cpu:                32
  ephemeral-storage:  410816300Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             131119672Ki
  pods:               110
Allocatable:
  cpu:                32
  ephemeral-storage:  378608301454
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             131017272Ki
  pods:               110
System Info:
  Machine ID:                 cca401755b404d559cc0e8629fdd7dee
  System UUID:                cca40175-5b40-4d55-9cc0-e8629fdd7dee
  Boot ID:                    d805ce16-43d1-406e-95e8-bce25f95418e
  Kernel Version:             5.4.17-2136.300.7.el7uek.x86_64
  OS Image:                   Oracle Linux Server 7.9
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://19.3.11
  Kubelet Version:            v1.19.12
  Kube-Proxy Version:         v1.19.12
PodCIDR:                      10.245.11.128/25
PodCIDRs:                     10.245.11.128/25
ProviderID:                   ocid1.instance.oc1.iad.anuwcljrxpuma2qczpaqc2iwbgqbg2kxawdmakfx4fkaldux5ipwu4k76l2q
Non-terminated Pods:          (67 in total)
  Namespace                   Name                                                            CPU Requests  CPU Limits   Memory Requests  Memory Limits  Age
  ---------                   ----                                                            ------------  ----------   ---------------  -------------  ---
  agora-psr                   agora-master-images-695ddb5cc9-wzlxm                            800m (2%)     2 (6%)       2Gi (1%)         3Gi (2%)       49d
  analytics-ci                vault-569655f78d-gq5w9                                          350m (1%)     500m (1%)    750Mi (0%)       1274Mi (0%)    6h33m
  analytics                   locust-worker-7d95fc6d5d-cht2t                                  100m (0%)     500m (1%)    128Mi (0%)       128Mi (0%)     14d
  as-accelxo2                 lorax-analyticsmanager-57db594b6f-b6xrn                         600m (1%)     600m (1%)    2176Mi (1%)      2176Mi (1%)    64d
  as-accelxo2                 lorax-cachehost-7759d5c6cc-fsp9b                                600m (1%)     600m (1%)    2176Mi (1%)      2176Mi (1%)    49d
  as-accelxo2                 lorax-customizationmanager-c9c778585-pzt76                      600m (1%)     600m (1%)    2176Mi (1%)      2176Mi (1%)    64d
  as-atv                      lorax-elementmanager-5b8d6bc85d-nsnt4                           600m (1%)     600m (1%)    2176Mi (1%)      2176Mi (1%)    7d3h
  as-atv2                     lorax-realtimedatamanager-75c47bf47c-6bkbx                      600m (1%)     600m (1%)    2176Mi (1%)      2176Mi (1%)    3d7h
  as-bcous                    lorax-searchmanager-74dd4459bb-jtkn2                            600m (1%)     600m (1%)    2176Mi (1%)      2176Mi (1%)    9d
  as-buikf1                   lorax-assetmanager-775d6c8dd-nsgxw                              600m (1%)     600m (1%)    2176Mi (1%)      2176Mi (1%)    3d2h
  as-ccptvm                   horton-horton-deployment-8478b7db94-6lpkh                       1100m (3%)    1100m (3%)   2176Mi (1%)      2176Mi (1%)    25h
  as-ccptvm                   lorax-cachehost-69f9b5b6d-c9vg5                                 600m (1%)     600m (1%)    4224Mi (3%)      4224Mi (3%)    25h
  as-ccptvm                   lorax-realtimedatamanager-694b9f968-9knm2                       600m (1%)     600m (1%)    2176Mi (1%)      2176Mi (1%)    25h
  as-chat                     lorax-reportpublishmanager-7cc8ff4ddb-9mr5h                     600m (1%)     600m (1%)    2176Mi (1%)      2176Mi (1%)    2d6h
  as-chat2                    lorax-elementmanager-56cdcbbcbb-7kzz8                           600m (1%)     600m (1%)    2176Mi (1%)      2176Mi (1%)    31h
  as-chat3                    lorax-realtimedatamanager-575977fc55-26gkf                      600m (1%)     600m (1%)    2176Mi (1%)      2176Mi (1%)    37d
  as-ciat                     lorax-commandmanager-586d58947d-2rcmp                           600m (1%)     1100m (3%)   2176Mi (1%)      4224Mi (3%)    13h
  as-ciat                     lorax-contextmanager-67956c7584-c4xkw                           600m (1%)     1100m (3%)   4224Mi (3%)      8320Mi (6%)    13h
  as-ciat                     lorax-reportpublishmanager-58445646c4-9ztc7                     600m (1%)     1100m (3%)   2176Mi (1%)      4224Mi (3%)    13h
  as-ciat                     who-internal-who-deployment-65d44d6f4c-w8lnq                    600m (1%)     1100m (3%)   2176Mi (1%)      4224Mi (3%)    13h
  as-coreserveridc            lorax-elementmanager-cf7cc9cf6-mxfg7                            600m (1%)     600m (1%)    2176Mi (1%)      2176Mi (1%)    64d
  as-dock                     lorax-searchmanager-55fdf6686c-9p9kn                            600m (1%)     600m (1%)    2176Mi (1%)      2176Mi (1%)    19h
  as-psr                      lorax-32112191001-customizationmanager-56bf98bd56-9ktv7         600m (1%)     600m (1%)    2176Mi (1%)      2176Mi (1%)    21h
  as-psr                      lorax-32112191001-eventmanager-84f656d794-gmrzc                 600m (1%)     600m (1%)    2176Mi (1%)      2176Mi (1%)    21h
  as-psr                      lorax-32112191001-searchmanager-6f75bbb9f7-9szs8                600m (1%)     600m (1%)    2176Mi (1%)      2176Mi (1%)    21h
  as-saas3                    lorax-queuemanager-68db659b5f-2l2b4                             600m (1%)     600m (1%)    2176Mi (1%)      2176Mi (1%)    3d8h
  as-seof                     lorax-reportdesignmanager-645968b7f8-7wzqf                      600m (1%)     600m (1%)    2176Mi (1%)      2176Mi (1%)    49d
  as-seof2                    lorax-eventworker-78748fdb4f-b82t8                              600m (1%)     600m (1%)    2176Mi (1%)      2176Mi (1%)    34d
  as-workspacestvm            lorax-customizationmanager-6759846ff5-gngw2                     600m (1%)     600m (1%)    2176Mi (1%)      2176Mi (1%)    23h
  casper                      cadp-deployment-7475c556cb-t4cnm                                1100m (3%)    1100m (3%)   1152Mi (0%)      1152Mi (0%)    64d
  consul                      cpeconsul-consul-4bb5k                                          100m (0%)     400m (1%)    256Mi (0%)       1Gi (0%)       13d
  data-pipeline-ci            schema-service-7694d5b7f-fz4vf                                  500m (1%)     1 (3%)       2Gi (1%)         4Gi (3%)       11h
  data-pipeline-dev1          schema-service-66984c5d4f-gh87f                                 500m (1%)     1 (3%)       2Gi (1%)         4Gi (3%)       7d4h
  data-pipeline-dev2          kafka-osvc-bdc668649-gk4r2                                      1 (3%)        1 (3%)       2Gi (1%)         3Gi (2%)       19h
  data-pipeline-dev2          vault-96bd5959f-7tgxw                                           225m (0%)     500m (1%)    625Mi (0%)       1274Mi (0%)    19h
  data-pipeline-psr           search-indexer-oci-5d9db5b66f-bbmk4                             600m (1%)     1100m (3%)   2176Mi (1%)      4224Mi (3%)    64d
  dcs-integration             redis-slave-1                                                   750m (2%)     1 (3%)       4Gi (3%)         4Gi (3%)       7d7h
  frontend                    ingress-internet-iad-da-controller-f9db769d4-w7xgh              700m (2%)     1200m (3%)   512Mi (0%)       1Gi (0%)       13d
  helios-sandbox3             adapter-746f4bbd86-5xqbm                                        200m (0%)     300m (0%)    1152Mi (0%)      2176Mi (1%)    46h
  helios-sandbox3             event-scheduler-68cccc55cf-nqs4s                                200m (0%)     300m (0%)    2176Mi (1%)      4224Mi (3%)    46h
  helios-sandbox3             sender-f579d87c9-c9d7r                                          200m (0%)     300m (0%)    1152Mi (0%)      2176Mi (1%)    46h
  helios-sandbox4             sender-565479bdbf-9tgxs                                         200m (0%)     300m (0%)    1152Mi (0%)      2176Mi (1%)    14d
  helios                      dead-letter-processor-55b54bc96b-n6hjd                          200m (0%)     300m (0%)    2176Mi (1%)      4224Mi (3%)    24h
  infra-monitoring            prometheus-operator-node-exporter-2r7rv                         0 (0%)        0 (0%)       0 (0%)           0 (0%)         64d
  kafka                       strimzi-cluster-operator-7fbc69c4f4-9l7pz                       1500m (4%)    3500m (10%)  3Gi (2%)         4Gi (3%)       3d2h
  kube-system                 coredns-6fb5644559-x7qfd                                        100m (0%)     0 (0%)       70Mi (0%)        200Mi (0%)     3h16m
  kube-system                 csi-oci-node-k7bdn                                              30m (0%)      500m (1%)    70Mi (0%)        300Mi (0%)     8d
  kube-system                 kube-flannel-ds-46cdh                                           100m (0%)     1 (3%)       50Mi (0%)        500Mi (0%)     64d
  kube-system                 kube-proxy-d7svr                                                0 (0%)        0 (0%)       0 (0%)           0 (0%)         64d
  kube-system                 nodelocaldns-llnrg                                              50m (0%)      400m (1%)    100Mi (0%)       200Mi (0%)     19h
  kube-system                 proxymux-client-f4gwh                                           50m (0%)      500m (1%)    64Mi (0%)        256Mi (0%)     36d
  kweet-develop               kweet-facebook-client-kweet-facebook-client-84c76d58b8-kps2m    300m (0%)     2100m (6%)   384Mi (0%)       1152Mi (0%)    64d
  mercury-cert                mercury-osvc-bridge-metrics-data-pipeline-7fcc6854b8-65gzz      200m (0%)     1100m (3%)   2176Mi (1%)      6272Mi (4%)    2d1h
  mercury-cert                mercury-social-bridge-6fcccf4d58-659vp                          350m (1%)     1100m (3%)   640Mi (0%)       4224Mi (3%)    5d7h
  mercury-chate2e             mercury-osvc-bridge-api-services-8468758667-xvzzk               200m (0%)     1100m (3%)   2176Mi (1%)      6272Mi (4%)    5d7h
  mercury-dev                 mercury-omnichannel-offer-processor-7f5c7c6d6b-q5fv8            200m (0%)     1100m (3%)   2176Mi (1%)      6272Mi (4%)    16d
  mercury-dev                 mercury-osvc-bridge-provisioning-processor-56d65bd969-88bkd     200m (0%)     1100m (3%)   2176Mi (1%)      6272Mi (4%)    16d
  mercury-dev                 mercury-routing-processor-agent-assignment-844d549756-wzzqw     200m (0%)     1100m (3%)   2176Mi (1%)      6272Mi (4%)    10d
  mercury                     mercury-realtime-channel-processor-7b9948fcd5-xtdxb             200m (0%)     1100m (3%)   2176Mi (1%)      6272Mi (4%)    13d
  mercury                     mercury-transcript-api-667754f85b-nwp42                         200m (0%)     1100m (3%)   2176Mi (1%)      6272Mi (4%)    13d
  mercury                     mercury-transcript-processor-bf8b9d6b5-ks2fz                    200m (0%)     1100m (3%)   2176Mi (1%)      6272Mi (4%)    13d
  mercury                     mercury-user-preference-service-7c5bbf8f8f-8ck8p                200m (0%)     1100m (3%)   2176Mi (1%)      6272Mi (4%)    13d
  mercury                     mercury-work-api-fd78d4fd9-8glmf                                200m (0%)     1100m (3%)   2176Mi (1%)      6272Mi (4%)    13d
  monitoring-agent            fluentd-2vczd                                                   400m (1%)     1200m (3%)   1000Mi (0%)      1800Mi (1%)    13d
  monitoring-agent            jaeger-agent-rkdm5                                              100m (0%)     200m (0%)    64Mi (0%)        128Mi (0%)     64d
  monitoring-agent            loki-stsrelay-mh459                                             200m (0%)     800m (2%)    128Mi (0%)       1Gi (0%)       9d
  visitorservice              visitorservice-kafka-zookeeper-1                                500m (1%)     1 (3%)       1Gi (0%)         2Gi (1%)       5d21h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests        Limits
  --------           --------        ------
  cpu                29505m (92%)    54200m (169%)
  memory             114793Mi (89%)  193068Mi (150%)
  ephemeral-storage  25596Mi (7%)    118436Mi (32%)
  hugepages-1Gi      0 (0%)          0 (0%)
  hugepages-2Mi      0 (0%)          0 (0%)
Events:              <none>
[20:January:2022:13:37:42]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgn as-ciat events
LAST SEEN   TYPE     REASON   OBJECT                                                  MESSAGE
31m         Normal   Info     service/as-ciat-agentbrowserui-20220120-08263133-m5tc   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-agentbrowserui-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205462
    uid: 459cfc0e-de96-466b-aad1-c84f836619ff
}: Created service monitor
31m         Normal   Info     service/as-ciat-agentbrowserui-20220120-08263133-m5tc   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-agentbrowserui-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205462
    uid: 459cfc0e-de96-466b-aad1-c84f836619ff
}: Received add service event
38m         Normal   Info     service/as-ciat-analyticsmanager-20220120-07563127-q8qa   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-analyticsmanager-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163214
    uid: 82b135e9-c55c-4e9e-a659-c48a8e54ac75
}: Received add service event
38m         Normal   Info     service/as-ciat-analyticsmanager-20220120-07563127-q8qa   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-analyticsmanager-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163214
    uid: 82b135e9-c55c-4e9e-a659-c48a8e54ac75
}: Created service monitor
29m         Normal   Info     service/as-ciat-analyticsmanager-20220120-08263133-m5tc   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-analyticsmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205681
    uid: 49cf53ce-bc1d-4a7d-a771-b444940d5ef8
}: Received add service event
10m         Normal   Info     service/as-ciat-analyticsmanager-20220120-08263133-m5tc   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-analyticsmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646704
    uid: 49cf53ce-bc1d-4a7d-a771-b444940d5ef8
}: Received delete service event
10m         Normal   Info     service/as-ciat-analyticsmanager-20220120-08263133-m5tc   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-analyticsmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646704
    uid: 49cf53ce-bc1d-4a7d-a771-b444940d5ef8
}: Deleted service monitor
29m         Normal   Info     service/as-ciat-analyticsmanager-20220120-08263133-m5tc   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-analyticsmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205681
    uid: 49cf53ce-bc1d-4a7d-a771-b444940d5ef8
}: Created service monitor
38m         Normal   Info     service/as-ciat-cachehost-20220120-07563127-q8qa          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-cachehost-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163252
    uid: b7131aaf-cc61-46ba-a3a0-245a47f181fa
}: Received add service event
38m         Normal   Info     service/as-ciat-cachehost-20220120-07563127-q8qa          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-cachehost-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163252
    uid: b7131aaf-cc61-46ba-a3a0-245a47f181fa
}: Created service monitor
35m         Normal   Info     service/as-ciat-cachehost-20220120-08263133-m5tc          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-cachehost-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205722
    uid: ab4b234b-3ac1-4e1d-b7b3-9baebae5ed67
}: Received add service event
10m         Normal   Info     service/as-ciat-cachehost-20220120-08263133-m5tc          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-cachehost-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646687
    uid: ab4b234b-3ac1-4e1d-b7b3-9baebae5ed67
}: Received delete service event
10m         Normal   Info     service/as-ciat-cachehost-20220120-08263133-m5tc          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-cachehost-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646687
    uid: ab4b234b-3ac1-4e1d-b7b3-9baebae5ed67
}: Deleted service monitor
35m         Normal   Info     service/as-ciat-cachehost-20220120-08263133-m5tc          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-cachehost-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205722
    uid: ab4b234b-3ac1-4e1d-b7b3-9baebae5ed67
}: Created service monitor
33m         Normal   Info     service/as-ciat-commandmanager-20220120-07563127-q8qa     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-commandmanager-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163267
    uid: 7703088f-1f97-405d-99a8-bab81c36a900
}: Created service monitor
33m         Normal   Info     service/as-ciat-commandmanager-20220120-07563127-q8qa     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-commandmanager-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163267
    uid: 7703088f-1f97-405d-99a8-bab81c36a900
}: Received add service event
10m         Normal   Info     service/as-ciat-commandmanager-20220120-08263133-m5tc     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-commandmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646745
    uid: 8ede3aa1-fb39-40ec-b046-38f51bba1e51
}: Deleted service monitor
45m         Normal   Info     service/as-ciat-commandmanager-20220120-08263133-m5tc     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-commandmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205708
    uid: 8ede3aa1-fb39-40ec-b046-38f51bba1e51
}: Received add service event
10m         Normal   Info     service/as-ciat-commandmanager-20220120-08263133-m5tc     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-commandmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646745
    uid: 8ede3aa1-fb39-40ec-b046-38f51bba1e51
}: Received delete service event
45m         Normal   Info     service/as-ciat-commandmanager-20220120-08263133-m5tc     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-commandmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205708
    uid: 8ede3aa1-fb39-40ec-b046-38f51bba1e51
}: Created service monitor
34m         Normal   Info     service/as-ciat-contextmanager-20220120-07563127-q8qa     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-contextmanager-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163223
    uid: 6eecac8d-5121-4277-a6b3-69fce0d44cdd
}: Received add service event
34m         Normal   Info     service/as-ciat-contextmanager-20220120-07563127-q8qa     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-contextmanager-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163223
    uid: 6eecac8d-5121-4277-a6b3-69fce0d44cdd
}: Created service monitor
46m         Normal   Info     service/as-ciat-contextmanager-20220120-08263133-m5tc     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-contextmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205734
    uid: c74034e8-2528-4426-91fc-76fbb45fd623
}: Created service monitor
10m         Normal   Info     service/as-ciat-contextmanager-20220120-08263133-m5tc     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-contextmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646706
    uid: c74034e8-2528-4426-91fc-76fbb45fd623
}: Received delete service event
10m         Normal   Info     service/as-ciat-contextmanager-20220120-08263133-m5tc     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-contextmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646706
    uid: c74034e8-2528-4426-91fc-76fbb45fd623
}: Deleted service monitor
46m         Normal   Info     service/as-ciat-contextmanager-20220120-08263133-m5tc     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-contextmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205734
    uid: c74034e8-2528-4426-91fc-76fbb45fd623
}: Received add service event
33m         Normal   Info     service/as-ciat-customizationmanager-20220120-07563127-q8qa   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-customizationmanager-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163243
    uid: fac4700b-ce15-4cc1-98bc-e098bcce9ec7
}: Received add service event
33m         Normal   Info     service/as-ciat-customizationmanager-20220120-07563127-q8qa   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-customizationmanager-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163243
    uid: fac4700b-ce15-4cc1-98bc-e098bcce9ec7
}: Created service monitor
10m         Normal   Info     service/as-ciat-customizationmanager-20220120-08263133-m5tc   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-customizationmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646747
    uid: c93d6313-d0c6-4201-9006-96dafbe9cdc7
}: Deleted service monitor
49m         Normal   Info     service/as-ciat-customizationmanager-20220120-08263133-m5tc   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-customizationmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205692
    uid: c93d6313-d0c6-4201-9006-96dafbe9cdc7
}: Created service monitor
49m         Normal   Info     service/as-ciat-customizationmanager-20220120-08263133-m5tc   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-customizationmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205692
    uid: c93d6313-d0c6-4201-9006-96dafbe9cdc7
}: Received add service event
10m         Normal   Info     service/as-ciat-customizationmanager-20220120-08263133-m5tc   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-customizationmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646747
    uid: c93d6313-d0c6-4201-9006-96dafbe9cdc7
}: Received delete service event
31m         Normal   Info     service/as-ciat-elementmanager-20220120-07563127-q8qa         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-elementmanager-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163206
    uid: 4ec79b01-90f4-4023-89ac-9ef9ceb6a407
}: Received add service event
31m         Normal   Info     service/as-ciat-elementmanager-20220120-07563127-q8qa         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-elementmanager-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163206
    uid: 4ec79b01-90f4-4023-89ac-9ef9ceb6a407
}: Created service monitor
10m         Normal   Info     service/as-ciat-elementmanager-20220120-08263133-m5tc         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-elementmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646742
    uid: 2a65a901-a301-401f-b3cc-21d1b373b3fd
}: Received delete service event
44m         Normal   Info     service/as-ciat-elementmanager-20220120-08263133-m5tc         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-elementmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205687
    uid: 2a65a901-a301-401f-b3cc-21d1b373b3fd
}: Created service monitor
44m         Normal   Info     service/as-ciat-elementmanager-20220120-08263133-m5tc         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-elementmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205687
    uid: 2a65a901-a301-401f-b3cc-21d1b373b3fd
}: Received add service event
10m         Normal   Info     service/as-ciat-elementmanager-20220120-08263133-m5tc         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-elementmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646742
    uid: 2a65a901-a301-401f-b3cc-21d1b373b3fd
}: Deleted service monitor
27m         Normal   Info     service/as-ciat-eventmanager-20220120-07563127-q8qa           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventmanager-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163204
    uid: d4b34a53-f146-4074-80c4-205c19704f3e
}: Received add service event
27m         Normal   Info     service/as-ciat-eventmanager-20220120-07563127-q8qa           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventmanager-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163204
    uid: d4b34a53-f146-4074-80c4-205c19704f3e
}: Created service monitor
31m         Normal   Info     service/as-ciat-eventmanager-20220120-08263133-m5tc           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205726
    uid: 05003780-ec0e-4587-b4fa-bd7a0aced437
}: Created service monitor
10m         Normal   Info     service/as-ciat-eventmanager-20220120-08263133-m5tc           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646689
    uid: 05003780-ec0e-4587-b4fa-bd7a0aced437
}: Received delete service event
31m         Normal   Info     service/as-ciat-eventmanager-20220120-08263133-m5tc           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205726
    uid: 05003780-ec0e-4587-b4fa-bd7a0aced437
}: Received add service event
10m         Normal   Info     service/as-ciat-eventmanager-20220120-08263133-m5tc           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646689
    uid: 05003780-ec0e-4587-b4fa-bd7a0aced437
}: Deleted service monitor
37m         Normal   Info     service/as-ciat-eventworker-20220120-07563127-q8qa            Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventworker-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163237
    uid: 066db710-b77e-4b45-b070-7e454911a8fe
}: Created service monitor
37m         Normal   Info     service/as-ciat-eventworker-20220120-07563127-q8qa            Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventworker-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163237
    uid: 066db710-b77e-4b45-b070-7e454911a8fe
}: Received add service event
10m         Normal   Info     service/as-ciat-eventworker-20220120-08263133-m5tc            Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventworker-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646688
    uid: a68f9f0c-8d28-4329-9fd4-a80434d6890c
}: Deleted service monitor
31m         Normal   Info     service/as-ciat-eventworker-20220120-08263133-m5tc            Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventworker-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205712
    uid: a68f9f0c-8d28-4329-9fd4-a80434d6890c
}: Received add service event
10m         Normal   Info     service/as-ciat-eventworker-20220120-08263133-m5tc            Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventworker-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646688
    uid: a68f9f0c-8d28-4329-9fd4-a80434d6890c
}: Received delete service event
31m         Normal   Info     service/as-ciat-eventworker-20220120-08263133-m5tc            Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-eventworker-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205712
    uid: a68f9f0c-8d28-4329-9fd4-a80434d6890c
}: Created service monitor
27m         Normal   Info     service/as-ciat-queuemanager-20220120-07563127-q8qa           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-queuemanager-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163227
    uid: 42cd16d2-1a1c-4eda-b9db-892a9a2eb1dd
}: Created service monitor
27m         Normal   Info     service/as-ciat-queuemanager-20220120-07563127-q8qa           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-queuemanager-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163227
    uid: 42cd16d2-1a1c-4eda-b9db-892a9a2eb1dd
}: Received add service event
10m         Normal   Info     service/as-ciat-queuemanager-20220120-08263133-m5tc           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-queuemanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646703
    uid: 03d16f90-5bd8-4de4-ba3a-0b135578493f
}: Received delete service event
31m         Normal   Info     service/as-ciat-queuemanager-20220120-08263133-m5tc           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-queuemanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205693
    uid: 03d16f90-5bd8-4de4-ba3a-0b135578493f
}: Created service monitor
10m         Normal   Info     service/as-ciat-queuemanager-20220120-08263133-m5tc           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-queuemanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646703
    uid: 03d16f90-5bd8-4de4-ba3a-0b135578493f
}: Deleted service monitor
31m         Normal   Info     service/as-ciat-queuemanager-20220120-08263133-m5tc           Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-queuemanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205693
    uid: 03d16f90-5bd8-4de4-ba3a-0b135578493f
}: Received add service event
39m         Normal   Info     service/as-ciat-realtimedatamanager-20220120-07563127-q8qa    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-realtimedatamanager-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163230
    uid: d89f7002-f714-4a0a-bdd5-cd182a810561
}: Received add service event
39m         Normal   Info     service/as-ciat-realtimedatamanager-20220120-07563127-q8qa    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-realtimedatamanager-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163230
    uid: d89f7002-f714-4a0a-bdd5-cd182a810561
}: Created service monitor
33m         Normal   Info     service/as-ciat-realtimedatamanager-20220120-08263133-m5tc    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-realtimedatamanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205717
    uid: fb2603ec-3e31-4289-b8bb-e4032df5b6b8
}: Received add service event
10m         Normal   Info     service/as-ciat-realtimedatamanager-20220120-08263133-m5tc    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-realtimedatamanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646698
    uid: fb2603ec-3e31-4289-b8bb-e4032df5b6b8
}: Received delete service event
33m         Normal   Info     service/as-ciat-realtimedatamanager-20220120-08263133-m5tc    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-realtimedatamanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205717
    uid: fb2603ec-3e31-4289-b8bb-e4032df5b6b8
}: Created service monitor
10m         Normal   Info     service/as-ciat-realtimedatamanager-20220120-08263133-m5tc    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-realtimedatamanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646698
    uid: fb2603ec-3e31-4289-b8bb-e4032df5b6b8
}: Deleted service monitor
45m         Normal   Info     service/as-ciat-reportdesignmanager-20220120-07563127-q8qa    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportdesignmanager-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163234
    uid: a3589184-9ee8-4a7e-beca-762e7dc5ff27
}: Received add service event
45m         Normal   Info     service/as-ciat-reportdesignmanager-20220120-07563127-q8qa    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportdesignmanager-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163234
    uid: a3589184-9ee8-4a7e-beca-762e7dc5ff27
}: Created service monitor
10m         Normal   Info     service/as-ciat-reportdesignmanager-20220120-08263133-m5tc    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportdesignmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646743
    uid: d9556b15-cc2f-4687-9ba8-e698c5763bb6
}: Received delete service event
10m         Normal   Info     service/as-ciat-reportdesignmanager-20220120-08263133-m5tc    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportdesignmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646743
    uid: d9556b15-cc2f-4687-9ba8-e698c5763bb6
}: Deleted service monitor
28m         Normal   Info     service/as-ciat-reportdesignmanager-20220120-08263133-m5tc    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportdesignmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205685
    uid: d9556b15-cc2f-4687-9ba8-e698c5763bb6
}: Created service monitor
28m         Normal   Info     service/as-ciat-reportdesignmanager-20220120-08263133-m5tc    Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportdesignmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205685
    uid: d9556b15-cc2f-4687-9ba8-e698c5763bb6
}: Received add service event
41m         Normal   Info     service/as-ciat-reportpublishmanager-20220120-07563127-q8qa   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportpublishmanager-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163258
    uid: f6d57d79-8fe6-422a-9e4c-cde2990097c7
}: Created service monitor
41m         Normal   Info     service/as-ciat-reportpublishmanager-20220120-07563127-q8qa   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportpublishmanager-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163258
    uid: f6d57d79-8fe6-422a-9e4c-cde2990097c7
}: Received add service event
10m         Normal   Info     service/as-ciat-reportpublishmanager-20220120-08263133-m5tc   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportpublishmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646746
    uid: e5ba9a46-d1cb-461f-a3c6-c456b09f02b5
}: Received delete service event
26m         Normal   Info     service/as-ciat-reportpublishmanager-20220120-08263133-m5tc   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportpublishmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205700
    uid: e5ba9a46-d1cb-461f-a3c6-c456b09f02b5
}: Received add service event
26m         Normal   Info     service/as-ciat-reportpublishmanager-20220120-08263133-m5tc   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportpublishmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205700
    uid: e5ba9a46-d1cb-461f-a3c6-c456b09f02b5
}: Created service monitor
10m         Normal   Info     service/as-ciat-reportpublishmanager-20220120-08263133-m5tc   Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-reportpublishmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646746
    uid: e5ba9a46-d1cb-461f-a3c6-c456b09f02b5
}: Deleted service monitor
35m         Normal   Info     service/as-ciat-searchmanager-20220120-07563127-q8qa          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-searchmanager-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163225
    uid: 3385c525-a887-45cb-a8b3-5f2bc6f28e9d
}: Created service monitor
35m         Normal   Info     service/as-ciat-searchmanager-20220120-07563127-q8qa          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-searchmanager-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163225
    uid: 3385c525-a887-45cb-a8b3-5f2bc6f28e9d
}: Received add service event
10m         Normal   Info     service/as-ciat-searchmanager-20220120-08263133-m5tc          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-searchmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646690
    uid: 5f0d6e71-4a04-46e7-a967-b18e5e492405
}: Deleted service monitor
10m         Normal   Info     service/as-ciat-searchmanager-20220120-08263133-m5tc          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-searchmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646690
    uid: 5f0d6e71-4a04-46e7-a967-b18e5e492405
}: Received delete service event
34m         Normal   Info     service/as-ciat-searchmanager-20220120-08263133-m5tc          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-searchmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205730
    uid: 5f0d6e71-4a04-46e7-a967-b18e5e492405
}: Received add service event
34m         Normal   Info     service/as-ciat-searchmanager-20220120-08263133-m5tc          Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-searchmanager-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205730
    uid: 5f0d6e71-4a04-46e7-a967-b18e5e492405
}: Created service monitor
30m         Normal   Info     service/as-ciat-securityservice-20220120-07563127-q8qa        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-securityservice-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163259
    uid: 7439981f-c4cb-4386-bc5e-df04615bd761
}: Created service monitor
30m         Normal   Info     service/as-ciat-securityservice-20220120-07563127-q8qa        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-securityservice-20220120-07563127-q8qa
    namespace: as-ciat
    resourceVersion: 770163259
    uid: 7439981f-c4cb-4386-bc5e-df04615bd761
}: Received add service event
37m         Normal   Info     service/as-ciat-securityservice-20220120-08263133-m5tc        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-securityservice-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205704
    uid: 7856a0c0-bd3e-4e47-84bb-b723de370116
}: Created service monitor
10m         Normal   Info     service/as-ciat-securityservice-20220120-08263133-m5tc        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-securityservice-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646702
    uid: 7856a0c0-bd3e-4e47-84bb-b723de370116
}: Deleted service monitor
10m         Normal   Info     service/as-ciat-securityservice-20220120-08263133-m5tc        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-securityservice-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770646702
    uid: 7856a0c0-bd3e-4e47-84bb-b723de370116
}: Received delete service event
37m         Normal   Info     service/as-ciat-securityservice-20220120-08263133-m5tc        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: as-ciat-securityservice-20220120-08263133-m5tc
    namespace: as-ciat
    resourceVersion: 770205704
    uid: 7856a0c0-bd3e-4e47-84bb-b723de370116
}: Received add service event
20s         Warning   FailedGetResourceMetric   horizontalpodautoscaler/horton-autoscaler                       unable to get metrics for resource memory: no metrics returned from resource metrics API
43m         Warning   FailedCreate              replicaset/horton-horton-deployment-575597484d                  Error creating: pods "horton-horton-deployment-575597484d-68qr8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/horton-horton-deployment-575597484d                  Error creating: pods "horton-horton-deployment-575597484d-wt859" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/horton-horton-deployment-575597484d                  Error creating: pods "horton-horton-deployment-575597484d-rkptv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/horton-horton-deployment-5cb5d5847b                  Error creating: pods "horton-horton-deployment-5cb5d5847b-xmqhr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/horton-horton-deployment-5cb5d5847b                  Error creating: pods "horton-horton-deployment-5cb5d5847b-xkrk5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/horton-horton-deployment-5cb5d5847b                  Error creating: pods "horton-horton-deployment-5cb5d5847b-mvbmf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/horton-horton-deployment-6c564fb8fd                  Error creating: pods "horton-horton-deployment-6c564fb8fd-tzlsg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/horton-horton-deployment-6c564fb8fd                  Error creating: pods "horton-horton-deployment-6c564fb8fd-ctrsw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/horton-horton-deployment-6c564fb8fd                  Error creating: pods "horton-horton-deployment-6c564fb8fd-26ssl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m56s       Warning   FailedCreate              replicaset/horton-horton-deployment-6c564fb8fd                  Error creating: pods "horton-horton-deployment-6c564fb8fd-5xnbk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate              replicaset/horton-horton-deployment-75bf7f55c4                  Error creating: pods "horton-horton-deployment-75bf7f55c4-vmfx9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/horton-horton-deployment-75bf7f55c4                  Error creating: pods "horton-horton-deployment-75bf7f55c4-blg8w" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/horton-horton-deployment-75bf7f55c4                  Error creating: pods "horton-horton-deployment-75bf7f55c4-wwszw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
79s         Warning   FailedCreate              replicaset/horton-horton-deployment-75bf7f55c4                  Error creating: pods "horton-horton-deployment-75bf7f55c4-7z7tz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate              replicaset/horton-horton-deployment-7666bdd65b                  Error creating: pods "horton-horton-deployment-7666bdd65b-vpnvh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/horton-horton-deployment-7666bdd65b                  Error creating: pods "horton-horton-deployment-7666bdd65b-4nlkq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/horton-horton-deployment-7666bdd65b                  Error creating: pods "horton-horton-deployment-7666bdd65b-qc6dt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m59s       Warning   FailedCreate              replicaset/horton-horton-deployment-7666bdd65b                  Error creating: pods "horton-horton-deployment-7666bdd65b-nl5v6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/horton-horton-deployment-77dc74fcf6                  Error creating: pods "horton-horton-deployment-77dc74fcf6-lz4lq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate              replicaset/horton-horton-deployment-77dc74fcf6                  Error creating: pods "horton-horton-deployment-77dc74fcf6-84shs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/horton-horton-deployment-77dc74fcf6                  Error creating: pods "horton-horton-deployment-77dc74fcf6-9wtjd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m47s       Warning   FailedCreate              replicaset/horton-horton-deployment-77dc74fcf6                  Error creating: pods "horton-horton-deployment-77dc74fcf6-6djh4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate              replicaset/horton-horton-deployment-7956dcf558                  Error creating: pods "horton-horton-deployment-7956dcf558-nm77h" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/horton-horton-deployment-7956dcf558                  Error creating: pods "horton-horton-deployment-7956dcf558-tnljz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/horton-horton-deployment-7956dcf558                  Error creating: pods "horton-horton-deployment-7956dcf558-ccfgc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m59s       Warning   FailedCreate              replicaset/horton-horton-deployment-7956dcf558                  Error creating: pods "horton-horton-deployment-7956dcf558-dtlwr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/horton-horton-deployment-7b44565c7d                  Error creating: pods "horton-horton-deployment-7b44565c7d-lnmxj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/horton-horton-deployment-7b44565c7d                  Error creating: pods "horton-horton-deployment-7b44565c7d-grjpn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/horton-horton-deployment-7b44565c7d                  Error creating: pods "horton-horton-deployment-7b44565c7d-bf62m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44s         Warning   FailedCreate              replicaset/horton-horton-deployment-7b44565c7d                  Error creating: pods "horton-horton-deployment-7b44565c7d-8gkjj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
43m         Warning   FailedCreate              replicaset/horton-horton-deployment-7fdd67cb74                  Error creating: pods "horton-horton-deployment-7fdd67cb74-58hd5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
26m         Warning   FailedCreate              replicaset/horton-horton-deployment-7fdd67cb74                  Error creating: pods "horton-horton-deployment-7fdd67cb74-zl2kf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/horton-horton-deployment-7fdd67cb74                  Error creating: pods "horton-horton-deployment-7fdd67cb74-hctgn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/horton-horton-deployment-848fdb8b76                  Error creating: pods "horton-horton-deployment-848fdb8b76-hvxg6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate              replicaset/horton-horton-deployment-848fdb8b76                  Error creating: pods "horton-horton-deployment-848fdb8b76-g4m5k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/horton-horton-deployment-848fdb8b76                  Error creating: pods "horton-horton-deployment-848fdb8b76-6dlq9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m54s       Warning   FailedCreate              replicaset/horton-horton-deployment-848fdb8b76                  Error creating: pods "horton-horton-deployment-848fdb8b76-6cw7d" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate              replicaset/horton-horton-deployment-8557865c96                  Error creating: pods "horton-horton-deployment-8557865c96-x4f6k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/horton-horton-deployment-8557865c96                  Error creating: pods "horton-horton-deployment-8557865c96-bzs6f" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/horton-horton-deployment-8557865c96                  Error creating: pods "horton-horton-deployment-8557865c96-847sd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
68s         Warning   FailedCreate              replicaset/horton-horton-deployment-8557865c96                  Error creating: pods "horton-horton-deployment-8557865c96-6bs46" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate              replicaset/horton-horton-deployment-859d67c6dd                  Error creating: pods "horton-horton-deployment-859d67c6dd-rtzr4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/horton-horton-deployment-859d67c6dd                  Error creating: pods "horton-horton-deployment-859d67c6dd-5ggr5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18m         Warning   FailedCreate              replicaset/horton-horton-deployment-859d67c6dd                  Error creating: pods "horton-horton-deployment-859d67c6dd-vpmgb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
80s         Warning   FailedCreate              replicaset/horton-horton-deployment-859d67c6dd                  Error creating: pods "horton-horton-deployment-859d67c6dd-zkdxt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/horton-horton-deployment-b4cc54648                   Error creating: pods "horton-horton-deployment-b4cc54648-m2dcs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/horton-horton-deployment-b4cc54648                   Error creating: pods "horton-horton-deployment-b4cc54648-j2w8s" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
23m         Warning   FailedCreate              replicaset/horton-horton-deployment-b4cc54648                   Error creating: pods "horton-horton-deployment-b4cc54648-gjzwn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m19s       Warning   FailedCreate              replicaset/horton-horton-deployment-b4cc54648                   Error creating: pods "horton-horton-deployment-b4cc54648-zs4qz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/horton-horton-deployment-c94fb56d4                   Error creating: pods "horton-horton-deployment-c94fb56d4-72rwd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate              replicaset/horton-horton-deployment-c94fb56d4                   Error creating: pods "horton-horton-deployment-c94fb56d4-ttdnv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/horton-horton-deployment-c94fb56d4                   Error creating: pods "horton-horton-deployment-c94fb56d4-5vfxp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/horton-horton-deployment-cf96b5f96                   Error creating: pods "horton-horton-deployment-cf96b5f96-6swsd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/horton-horton-deployment-cf96b5f96                   Error creating: pods "horton-horton-deployment-cf96b5f96-2twlt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
13m         Warning   FailedCreate              replicaset/horton-horton-deployment-cf96b5f96                   Error creating: pods "horton-horton-deployment-cf96b5f96-4zq6x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-54bc6d977                     Error creating: pods "lorax-analyticsmanager-54bc6d977-mnlwc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-54bc6d977                     Error creating: pods "lorax-analyticsmanager-54bc6d977-k5bj6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-54bc6d977                     Error creating: pods "lorax-analyticsmanager-54bc6d977-wkjtb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m40s       Warning   FailedCreate              replicaset/lorax-analyticsmanager-54bc6d977                     Error creating: pods "lorax-analyticsmanager-54bc6d977-sznz2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-54cddc4b7c                    Error creating: pods "lorax-analyticsmanager-54cddc4b7c-f8rnq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-54cddc4b7c                    Error creating: pods "lorax-analyticsmanager-54cddc4b7c-j4bkf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-54cddc4b7c                    Error creating: pods "lorax-analyticsmanager-54cddc4b7c-vl66c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m1s        Warning   FailedCreate              replicaset/lorax-analyticsmanager-54cddc4b7c                    Error creating: pods "lorax-analyticsmanager-54cddc4b7c-kcxmh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-574b54fdcc                    Error creating: pods "lorax-analyticsmanager-574b54fdcc-dtqd2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-574b54fdcc                    Error creating: pods "lorax-analyticsmanager-574b54fdcc-d6k9q" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-574b54fdcc                    Error creating: pods "lorax-analyticsmanager-574b54fdcc-8kbkv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
43m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-5845b594f7                    Error creating: pods "lorax-analyticsmanager-5845b594f7-pgmfz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-5845b594f7                    Error creating: pods "lorax-analyticsmanager-5845b594f7-clzk6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-5845b594f7                    Error creating: pods "lorax-analyticsmanager-5845b594f7-x5w44" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-6467b86457                    Error creating: pods "lorax-analyticsmanager-6467b86457-8mcfw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-6467b86457                    Error creating: pods "lorax-analyticsmanager-6467b86457-q9mn8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-6467b86457                    Error creating: pods "lorax-analyticsmanager-6467b86457-b82gm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
15s         Warning   FailedCreate              replicaset/lorax-analyticsmanager-6467b86457                    Error creating: pods "lorax-analyticsmanager-6467b86457-lpshn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
43m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-649b9ff5c                     Error creating: pods "lorax-analyticsmanager-649b9ff5c-xvzcj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-649b9ff5c                     Error creating: pods "lorax-analyticsmanager-649b9ff5c-nv9zt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-649b9ff5c                     Error creating: pods "lorax-analyticsmanager-649b9ff5c-6h4xm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-6b84676cfc                    Error creating: pods "lorax-analyticsmanager-6b84676cfc-79stm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-6b84676cfc                    Error creating: pods "lorax-analyticsmanager-6b84676cfc-vztwj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-6b84676cfc                    Error creating: pods "lorax-analyticsmanager-6b84676cfc-tl77z" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
75s         Warning   FailedCreate              replicaset/lorax-analyticsmanager-6b84676cfc                    Error creating: pods "lorax-analyticsmanager-6b84676cfc-ltv74" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-6bf55f567c                    Error creating: pods "lorax-analyticsmanager-6bf55f567c-zpr25" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-6bf55f567c                    Error creating: pods "lorax-analyticsmanager-6bf55f567c-pkfhp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
23m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-6bf55f567c                    Error creating: pods "lorax-analyticsmanager-6bf55f567c-vwx9j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m16s       Warning   FailedCreate              replicaset/lorax-analyticsmanager-6bf55f567c                    Error creating: pods "lorax-analyticsmanager-6bf55f567c-szlqq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-6c8f878f7                     Error creating: pods "lorax-analyticsmanager-6c8f878f7-9j4b6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-6c8f878f7                     Error creating: pods "lorax-analyticsmanager-6c8f878f7-l9x5x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-6c8f878f7                     Error creating: pods "lorax-analyticsmanager-6c8f878f7-ft7s5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m46s       Warning   FailedCreate              replicaset/lorax-analyticsmanager-6c8f878f7                     Error creating: pods "lorax-analyticsmanager-6c8f878f7-6f9cv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-7464749dd9                    Error creating: pods "lorax-analyticsmanager-7464749dd9-fnwnq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-7464749dd9                    Error creating: pods "lorax-analyticsmanager-7464749dd9-25zdq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-7464749dd9                    Error creating: pods "lorax-analyticsmanager-7464749dd9-kjczn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24s         Warning   FailedCreate              replicaset/lorax-analyticsmanager-7464749dd9                    Error creating: pods "lorax-analyticsmanager-7464749dd9-9228v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-75c549d849                    Error creating: pods "lorax-analyticsmanager-75c549d849-rfr7g" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-75c549d849                    Error creating: pods "lorax-analyticsmanager-75c549d849-v5fh6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-75c549d849                    Error creating: pods "lorax-analyticsmanager-75c549d849-svsvv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
43s         Warning   FailedCreate              replicaset/lorax-analyticsmanager-75c549d849                    Error creating: pods "lorax-analyticsmanager-75c549d849-h85cl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44s         Warning   FailedMount               pod/lorax-analyticsmanager-75d985dfdf-6jmjm                     MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
104s        Warning   FailedMount               pod/lorax-analyticsmanager-75d985dfdf-mmxrw                     MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
49m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-75d985dfdf                    Error creating: pods "lorax-analyticsmanager-75d985dfdf-r729q" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
32m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-75d985dfdf                    Error creating: pods "lorax-analyticsmanager-75d985dfdf-ddqn7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
15m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-75d985dfdf                    Error creating: pods "lorax-analyticsmanager-75d985dfdf-9bzrt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-7fb55ff7fc                    Error creating: pods "lorax-analyticsmanager-7fb55ff7fc-gnnjt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-7fb55ff7fc                    Error creating: pods "lorax-analyticsmanager-7fb55ff7fc-zcsld" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-7fb55ff7fc                    Error creating: pods "lorax-analyticsmanager-7fb55ff7fc-wz7nw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-85cfd6bfd5                    Error creating: pods "lorax-analyticsmanager-85cfd6bfd5-jj6cm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-85cfd6bfd5                    Error creating: pods "lorax-analyticsmanager-85cfd6bfd5-75qvm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-85cfd6bfd5                    Error creating: pods "lorax-analyticsmanager-85cfd6bfd5-xhk8s" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m53s       Warning   FailedCreate              replicaset/lorax-analyticsmanager-85cfd6bfd5                    Error creating: pods "lorax-analyticsmanager-85cfd6bfd5-6gn5n" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-9f465d7fc                     Error creating: pods "lorax-analyticsmanager-9f465d7fc-26cbm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-9f465d7fc                     Error creating: pods "lorax-analyticsmanager-9f465d7fc-l8xfq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-9f465d7fc                     Error creating: pods "lorax-analyticsmanager-9f465d7fc-8gxwz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m36s       Warning   FailedCreate              replicaset/lorax-analyticsmanager-9f465d7fc                     Error creating: pods "lorax-analyticsmanager-9f465d7fc-dqr5f" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedGetResourceMetric   horizontalpodautoscaler/lorax-analyticsmanager-autoscaler       unable to get metrics for resource memory: no metrics returned from resource metrics API
47m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-f6bbcd5fc                     Error creating: pods "lorax-analyticsmanager-f6bbcd5fc-cn752" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-f6bbcd5fc                     Error creating: pods "lorax-analyticsmanager-f6bbcd5fc-xjd6r" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
13m         Warning   FailedCreate              replicaset/lorax-analyticsmanager-f6bbcd5fc                     Error creating: pods "lorax-analyticsmanager-f6bbcd5fc-84s48" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate              replicaset/lorax-cachehost-5656bcff96                           Error creating: pods "lorax-cachehost-5656bcff96-d99j6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-cachehost-5656bcff96                           Error creating: pods "lorax-cachehost-5656bcff96-v89dt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-cachehost-5656bcff96                           Error creating: pods "lorax-cachehost-5656bcff96-lgc25" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-cachehost-5b87c7bbd4                           Error creating: pods "lorax-cachehost-5b87c7bbd4-w76pc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-cachehost-5b87c7bbd4                           Error creating: pods "lorax-cachehost-5b87c7bbd4-x4wdf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-cachehost-5b87c7bbd4                           Error creating: pods "lorax-cachehost-5b87c7bbd4-gzhd9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m31s       Warning   FailedCreate              replicaset/lorax-cachehost-5b87c7bbd4                           Error creating: pods "lorax-cachehost-5b87c7bbd4-64rhk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-cachehost-5c5455dcd4                           Error creating: pods "lorax-cachehost-5c5455dcd4-bpdnt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-cachehost-5c5455dcd4                           Error creating: pods "lorax-cachehost-5c5455dcd4-wmdtm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-cachehost-5c5455dcd4                           Error creating: pods "lorax-cachehost-5c5455dcd4-4m4bf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-cachehost-5cb9c44cc6                           Error creating: pods "lorax-cachehost-5cb9c44cc6-b4w6b" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-cachehost-5cb9c44cc6                           Error creating: pods "lorax-cachehost-5cb9c44cc6-766j2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-cachehost-5cb9c44cc6                           Error creating: pods "lorax-cachehost-5cb9c44cc6-8wmdc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m55s       Warning   FailedCreate              replicaset/lorax-cachehost-5cb9c44cc6                           Error creating: pods "lorax-cachehost-5cb9c44cc6-g2qx2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
87s         Warning   FailedMount               pod/lorax-cachehost-647b6cf7b4-4v6n7                            MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
43s         Warning   FailedMount               pod/lorax-cachehost-647b6cf7b4-jsqlv                            MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
47m         Warning   FailedCreate              replicaset/lorax-cachehost-647b6cf7b4                           Error creating: pods "lorax-cachehost-647b6cf7b4-s7544" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-cachehost-647b6cf7b4                           Error creating: pods "lorax-cachehost-647b6cf7b4-9r55k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-cachehost-647b6cf7b4                           Error creating: pods "lorax-cachehost-647b6cf7b4-kzkzf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate              replicaset/lorax-cachehost-6b99bd7cd4                           Error creating: pods "lorax-cachehost-6b99bd7cd4-7wvpb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-cachehost-6b99bd7cd4                           Error creating: pods "lorax-cachehost-6b99bd7cd4-5dvp4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-cachehost-6b99bd7cd4                           Error creating: pods "lorax-cachehost-6b99bd7cd4-pvhmz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m2s        Warning   FailedCreate              replicaset/lorax-cachehost-6b99bd7cd4                           Error creating: pods "lorax-cachehost-6b99bd7cd4-m8lsf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
34s         Warning   FailedMount               pod/lorax-cachehost-6c8948bf68-8gmdf                            MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
59s         Warning   FailedMount               pod/lorax-cachehost-6c8948bf68-mzvx7                            MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
82s         Warning   FailedMount               pod/lorax-cachehost-6c8948bf68-vf26t                            MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
50m         Warning   FailedCreate              replicaset/lorax-cachehost-6dd4cd6fc6                           Error creating: pods "lorax-cachehost-6dd4cd6fc6-rs45v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-cachehost-6dd4cd6fc6                           Error creating: pods "lorax-cachehost-6dd4cd6fc6-sgq9b" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-cachehost-6dd4cd6fc6                           Error creating: pods "lorax-cachehost-6dd4cd6fc6-fzthp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
26s         Warning   FailedCreate              replicaset/lorax-cachehost-6dd4cd6fc6                           Error creating: pods "lorax-cachehost-6dd4cd6fc6-t49wn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-cachehost-6fcc6d6bf6                           Error creating: pods "lorax-cachehost-6fcc6d6bf6-z2rm6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-cachehost-6fcc6d6bf6                           Error creating: pods "lorax-cachehost-6fcc6d6bf6-9982k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-cachehost-6fcc6d6bf6                           Error creating: pods "lorax-cachehost-6fcc6d6bf6-pmxl6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-cachehost-7456cd54dd                           Error creating: pods "lorax-cachehost-7456cd54dd-n4q2n" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-cachehost-7456cd54dd                           Error creating: pods "lorax-cachehost-7456cd54dd-wl7lk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-cachehost-7456cd54dd                           Error creating: pods "lorax-cachehost-7456cd54dd-xfm4z" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-cachehost-7566cbbcb                            Error creating: pods "lorax-cachehost-7566cbbcb-zdk4n" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate              replicaset/lorax-cachehost-7566cbbcb                            Error creating: pods "lorax-cachehost-7566cbbcb-mfbxj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-cachehost-7566cbbcb                            Error creating: pods "lorax-cachehost-7566cbbcb-nl49v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m29s       Warning   FailedCreate              replicaset/lorax-cachehost-7566cbbcb                            Error creating: pods "lorax-cachehost-7566cbbcb-7srsx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-cachehost-7cbc6dd864                           Error creating: pods "lorax-cachehost-7cbc6dd864-49vhr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-cachehost-7cbc6dd864                           Error creating: pods "lorax-cachehost-7cbc6dd864-h6s4d" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
23m         Warning   FailedCreate              replicaset/lorax-cachehost-7cbc6dd864                           Error creating: pods "lorax-cachehost-7cbc6dd864-rsgfj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m16s       Warning   FailedCreate              replicaset/lorax-cachehost-7cbc6dd864                           Error creating: pods "lorax-cachehost-7cbc6dd864-jlclm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-cachehost-7cd66b4dbd                           Error creating: pods "lorax-cachehost-7cd66b4dbd-v7z6j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-cachehost-7cd66b4dbd                           Error creating: pods "lorax-cachehost-7cd66b4dbd-s7l9c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate              replicaset/lorax-cachehost-7cd66b4dbd                           Error creating: pods "lorax-cachehost-7cd66b4dbd-xdff8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
5s          Warning   FailedCreate              replicaset/lorax-cachehost-7cd66b4dbd                           Error creating: pods "lorax-cachehost-7cd66b4dbd-7trf8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
43m         Warning   FailedCreate              replicaset/lorax-cachehost-7f5b5c8844                           Error creating: pods "lorax-cachehost-7f5b5c8844-xpfqm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-cachehost-7f5b5c8844                           Error creating: pods "lorax-cachehost-7f5b5c8844-8lnjj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-cachehost-7f5b5c8844                           Error creating: pods "lorax-cachehost-7f5b5c8844-t2hwl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-cachehost-85457b4f7b                           Error creating: pods "lorax-cachehost-85457b4f7b-n4c5m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate              replicaset/lorax-cachehost-85457b4f7b                           Error creating: pods "lorax-cachehost-85457b4f7b-26sqh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-cachehost-85457b4f7b                           Error creating: pods "lorax-cachehost-85457b4f7b-dbxbj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m43s       Warning   FailedCreate              replicaset/lorax-cachehost-85457b4f7b                           Error creating: pods "lorax-cachehost-85457b4f7b-2pzjh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-cachehost-99b986d84                            Error creating: pods "lorax-cachehost-99b986d84-ctgtk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-cachehost-99b986d84                            Error creating: pods "lorax-cachehost-99b986d84-5qpjd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-cachehost-99b986d84                            Error creating: pods "lorax-cachehost-99b986d84-7x9fp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
37s         Warning   FailedCreate              replicaset/lorax-cachehost-99b986d84                            Error creating: pods "lorax-cachehost-99b986d84-7vjj7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=3100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedGetResourceMetric   horizontalpodautoscaler/lorax-cachehost-autoscaler              unable to get metrics for resource memory: no metrics returned from resource metrics API
44m         Warning   FailedCreate              replicaset/lorax-commandmanager-5449fc7644                      Error creating: pods "lorax-commandmanager-5449fc7644-mdtjh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-commandmanager-5449fc7644                      Error creating: pods "lorax-commandmanager-5449fc7644-2r6gd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-commandmanager-5449fc7644                      Error creating: pods "lorax-commandmanager-5449fc7644-h8x6f" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-commandmanager-54784c4744                      Error creating: pods "lorax-commandmanager-54784c4744-djglx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-commandmanager-54784c4744                      Error creating: pods "lorax-commandmanager-54784c4744-w22xf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-commandmanager-54784c4744                      Error creating: pods "lorax-commandmanager-54784c4744-69hxw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
42s         Warning   FailedCreate              replicaset/lorax-commandmanager-54784c4744                      Error creating: pods "lorax-commandmanager-54784c4744-nf6vl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-commandmanager-55557f49f4                      Error creating: pods "lorax-commandmanager-55557f49f4-67sj8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate              replicaset/lorax-commandmanager-55557f49f4                      Error creating: pods "lorax-commandmanager-55557f49f4-lpgqg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-commandmanager-55557f49f4                      Error creating: pods "lorax-commandmanager-55557f49f4-cbvf7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m27s       Warning   FailedCreate              replicaset/lorax-commandmanager-55557f49f4                      Error creating: pods "lorax-commandmanager-55557f49f4-jcgmg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-commandmanager-56b5bb78c8                      Error creating: pods "lorax-commandmanager-56b5bb78c8-wr4p6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-commandmanager-56b5bb78c8                      Error creating: pods "lorax-commandmanager-56b5bb78c8-8cvqn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-commandmanager-56b5bb78c8                      Error creating: pods "lorax-commandmanager-56b5bb78c8-n2xvx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
85s         Warning   FailedMount               pod/lorax-commandmanager-586d58947d-2h89f                       MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
35s         Warning   FailedMount               pod/lorax-commandmanager-586d58947d-2rcmp                       MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
57m         Warning   FailedCreate              replicaset/lorax-commandmanager-5db8948f7d                      Error creating: pods "lorax-commandmanager-5db8948f7d-hlcqz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate              replicaset/lorax-commandmanager-5db8948f7d                      Error creating: pods "lorax-commandmanager-5db8948f7d-j6582" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-commandmanager-5db8948f7d                      Error creating: pods "lorax-commandmanager-5db8948f7d-njvj5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m43s       Warning   FailedCreate              replicaset/lorax-commandmanager-5db8948f7d                      Error creating: pods "lorax-commandmanager-5db8948f7d-rmzxf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-commandmanager-5ddf8f65c4                      Error creating: pods "lorax-commandmanager-5ddf8f65c4-dmrwj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-commandmanager-5ddf8f65c4                      Error creating: pods "lorax-commandmanager-5ddf8f65c4-p5p57" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-commandmanager-5ddf8f65c4                      Error creating: pods "lorax-commandmanager-5ddf8f65c4-9bdgk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate              replicaset/lorax-commandmanager-5f49868c58                      Error creating: pods "lorax-commandmanager-5f49868c58-g2v2v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-commandmanager-5f49868c58                      Error creating: pods "lorax-commandmanager-5f49868c58-xtmbl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-commandmanager-5f49868c58                      Error creating: pods "lorax-commandmanager-5f49868c58-225nc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
78s         Warning   FailedCreate              replicaset/lorax-commandmanager-5f49868c58                      Error creating: pods "lorax-commandmanager-5f49868c58-bxtx4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-commandmanager-6bc5f7cc66                      Error creating: pods "lorax-commandmanager-6bc5f7cc66-f9pm9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-commandmanager-6bc5f7cc66                      Error creating: pods "lorax-commandmanager-6bc5f7cc66-7jf2v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-commandmanager-6bc5f7cc66                      Error creating: pods "lorax-commandmanager-6bc5f7cc66-vwq7c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m32s       Warning   FailedCreate              replicaset/lorax-commandmanager-6bc5f7cc66                      Error creating: pods "lorax-commandmanager-6bc5f7cc66-w9fsf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate              replicaset/lorax-commandmanager-764fc878fb                      Error creating: pods "lorax-commandmanager-764fc878fb-ggd86" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-commandmanager-764fc878fb                      Error creating: pods "lorax-commandmanager-764fc878fb-p594k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-commandmanager-764fc878fb                      Error creating: pods "lorax-commandmanager-764fc878fb-tzc4m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m1s        Warning   FailedCreate              replicaset/lorax-commandmanager-764fc878fb                      Error creating: pods "lorax-commandmanager-764fc878fb-6rrxv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-commandmanager-79c8bb6b6                       Error creating: pods "lorax-commandmanager-79c8bb6b6-dg4p6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-commandmanager-79c8bb6b6                       Error creating: pods "lorax-commandmanager-79c8bb6b6-99jjl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate              replicaset/lorax-commandmanager-79c8bb6b6                       Error creating: pods "lorax-commandmanager-79c8bb6b6-5sgkd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
15s         Warning   FailedCreate              replicaset/lorax-commandmanager-79c8bb6b6                       Error creating: pods "lorax-commandmanager-79c8bb6b6-k92s5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
46m         Warning   FailedCreate              replicaset/lorax-commandmanager-7ff6dd5f66                      Error creating: pods "lorax-commandmanager-7ff6dd5f66-5b8wn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-commandmanager-7ff6dd5f66                      Error creating: pods "lorax-commandmanager-7ff6dd5f66-lfnt9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
13m         Warning   FailedCreate              replicaset/lorax-commandmanager-7ff6dd5f66                      Error creating: pods "lorax-commandmanager-7ff6dd5f66-gl4vj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-commandmanager-84c6d99f8                       Error creating: pods "lorax-commandmanager-84c6d99f8-vhvsp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-commandmanager-84c6d99f8                       Error creating: pods "lorax-commandmanager-84c6d99f8-dj9pf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
23m         Warning   FailedCreate              replicaset/lorax-commandmanager-84c6d99f8                       Error creating: pods "lorax-commandmanager-84c6d99f8-jspdm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m15s       Warning   FailedCreate              replicaset/lorax-commandmanager-84c6d99f8                       Error creating: pods "lorax-commandmanager-84c6d99f8-rqds5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
43m         Warning   FailedCreate              replicaset/lorax-commandmanager-8645779974                      Error creating: pods "lorax-commandmanager-8645779974-f6lx5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-commandmanager-8645779974                      Error creating: pods "lorax-commandmanager-8645779974-qwm8w" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-commandmanager-8645779974                      Error creating: pods "lorax-commandmanager-8645779974-st7gg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-commandmanager-8698fbbf64                      Error creating: pods "lorax-commandmanager-8698fbbf64-qdcjh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-commandmanager-8698fbbf64                      Error creating: pods "lorax-commandmanager-8698fbbf64-lw8xg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-commandmanager-8698fbbf64                      Error creating: pods "lorax-commandmanager-8698fbbf64-hjljn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m49s       Warning   FailedCreate              replicaset/lorax-commandmanager-8698fbbf64                      Error creating: pods "lorax-commandmanager-8698fbbf64-56r6z" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedGetResourceMetric   horizontalpodautoscaler/lorax-commandmanager-autoscaler         unable to get metrics for resource memory: no metrics returned from resource metrics API
50m         Warning   FailedCreate              replicaset/lorax-commandmanager-d87887f76                       Error creating: pods "lorax-commandmanager-d87887f76-4jc9l" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-commandmanager-d87887f76                       Error creating: pods "lorax-commandmanager-d87887f76-bqc5x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate              replicaset/lorax-commandmanager-d87887f76                       Error creating: pods "lorax-commandmanager-d87887f76-rlv25" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
5s          Warning   FailedCreate              replicaset/lorax-commandmanager-d87887f76                       Error creating: pods "lorax-commandmanager-d87887f76-xw2hg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-contextmanager-5988486744                      Error creating: pods "lorax-contextmanager-5988486744-x452d" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-contextmanager-5988486744                      Error creating: pods "lorax-contextmanager-5988486744-lg7z2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-contextmanager-5988486744                      Error creating: pods "lorax-contextmanager-5988486744-kprwt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m33s       Warning   FailedCreate              replicaset/lorax-contextmanager-5988486744                      Error creating: pods "lorax-contextmanager-5988486744-n8ptx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-contextmanager-5bb7cf754                       Error creating: pods "lorax-contextmanager-5bb7cf754-rjbpw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-contextmanager-5bb7cf754                       Error creating: pods "lorax-contextmanager-5bb7cf754-z9gbp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate              replicaset/lorax-contextmanager-5bb7cf754                       Error creating: pods "lorax-contextmanager-5bb7cf754-7vmm8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
8s          Warning   FailedCreate              replicaset/lorax-contextmanager-5bb7cf754                       Error creating: pods "lorax-contextmanager-5bb7cf754-rppfd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-contextmanager-5d6b544f46                      Error creating: pods "lorax-contextmanager-5d6b544f46-lk8bz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-contextmanager-5d6b544f46                      Error creating: pods "lorax-contextmanager-5d6b544f46-lt6jm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-contextmanager-5d6b544f46                      Error creating: pods "lorax-contextmanager-5d6b544f46-q8qj7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37s         Warning   FailedCreate              replicaset/lorax-contextmanager-5d6b544f46                      Error creating: pods "lorax-contextmanager-5d6b544f46-jqg5d" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate              replicaset/lorax-contextmanager-676d9f4b96                      Error creating: pods "lorax-contextmanager-676d9f4b96-wm2mx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-contextmanager-676d9f4b96                      Error creating: pods "lorax-contextmanager-676d9f4b96-p6dww" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-contextmanager-676d9f4b96                      Error creating: pods "lorax-contextmanager-676d9f4b96-7nvbv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
75s         Warning   FailedCreate              replicaset/lorax-contextmanager-676d9f4b96                      Error creating: pods "lorax-contextmanager-676d9f4b96-k6hbf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
90s         Warning   FailedMount               pod/lorax-contextmanager-67956c7584-c4482                       MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
61s         Warning   FailedMount               pod/lorax-contextmanager-67956c7584-c4xkw                       MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
63s         Warning   FailedMount               pod/lorax-contextmanager-67956c7584-h82q5                       MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
44m         Warning   FailedCreate              replicaset/lorax-contextmanager-6b847c88b                       Error creating: pods "lorax-contextmanager-6b847c88b-hvqzx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-contextmanager-6b847c88b                       Error creating: pods "lorax-contextmanager-6b847c88b-2b4sq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-contextmanager-6b847c88b                       Error creating: pods "lorax-contextmanager-6b847c88b-59mpf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-contextmanager-6c58b5fbcb                      Error creating: pods "lorax-contextmanager-6c58b5fbcb-mhnkx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-contextmanager-6c58b5fbcb                      Error creating: pods "lorax-contextmanager-6c58b5fbcb-h5lls" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-contextmanager-6c58b5fbcb                      Error creating: pods "lorax-contextmanager-6c58b5fbcb-skjdr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
43m         Warning   FailedCreate              replicaset/lorax-contextmanager-6d6d7594bd                      Error creating: pods "lorax-contextmanager-6d6d7594bd-r2q99" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-contextmanager-6d6d7594bd                      Error creating: pods "lorax-contextmanager-6d6d7594bd-nr2fv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-contextmanager-6d6d7594bd                      Error creating: pods "lorax-contextmanager-6d6d7594bd-xn5dm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-contextmanager-6dc9f7cf84                      Error creating: pods "lorax-contextmanager-6dc9f7cf84-2s27j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate              replicaset/lorax-contextmanager-6dc9f7cf84                      Error creating: pods "lorax-contextmanager-6dc9f7cf84-8tc5v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-contextmanager-6dc9f7cf84                      Error creating: pods "lorax-contextmanager-6dc9f7cf84-4rk4d" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m42s       Warning   FailedCreate              replicaset/lorax-contextmanager-6dc9f7cf84                      Error creating: pods "lorax-contextmanager-6dc9f7cf84-cp4gb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-contextmanager-6fb65d9f4d                      Error creating: pods "lorax-contextmanager-6fb65d9f4d-jrqb2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-contextmanager-6fb65d9f4d                      Error creating: pods "lorax-contextmanager-6fb65d9f4d-6vdtn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
13m         Warning   FailedCreate              replicaset/lorax-contextmanager-6fb65d9f4d                      Error creating: pods "lorax-contextmanager-6fb65d9f4d-fbj8h" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-contextmanager-754fc85d44                      Error creating: pods "lorax-contextmanager-754fc85d44-qrtwn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-contextmanager-754fc85d44                      Error creating: pods "lorax-contextmanager-754fc85d44-fhhj5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
23m         Warning   FailedCreate              replicaset/lorax-contextmanager-754fc85d44                      Error creating: pods "lorax-contextmanager-754fc85d44-4r5f5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m15s       Warning   FailedCreate              replicaset/lorax-contextmanager-754fc85d44                      Error creating: pods "lorax-contextmanager-754fc85d44-cfdd6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-contextmanager-7d6c885588                      Error creating: pods "lorax-contextmanager-7d6c885588-4r6b2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-contextmanager-7d6c885588                      Error creating: pods "lorax-contextmanager-7d6c885588-httf6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-contextmanager-7d6c885588                      Error creating: pods "lorax-contextmanager-7d6c885588-vc544" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
26s         Warning   FailedCreate              replicaset/lorax-contextmanager-7d6c885588                      Error creating: pods "lorax-contextmanager-7d6c885588-kl945" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-contextmanager-98857974                        Error creating: pods "lorax-contextmanager-98857974-qx9wr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-contextmanager-98857974                        Error creating: pods "lorax-contextmanager-98857974-ksrpx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-contextmanager-98857974                        Error creating: pods "lorax-contextmanager-98857974-dxv25" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m51s       Warning   FailedCreate              replicaset/lorax-contextmanager-98857974                        Error creating: pods "lorax-contextmanager-98857974-7xmpb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
15m         Warning   FailedGetResourceMetric   horizontalpodautoscaler/lorax-contextmanager-autoscaler         unable to get metrics for resource memory: no metrics returned from resource metrics API
54m         Warning   FailedCreate              replicaset/lorax-contextmanager-d5dd69566                       Error creating: pods "lorax-contextmanager-d5dd69566-2jcn4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-contextmanager-d5dd69566                       Error creating: pods "lorax-contextmanager-d5dd69566-859pn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-contextmanager-d5dd69566                       Error creating: pods "lorax-contextmanager-d5dd69566-54nl9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m59s       Warning   FailedCreate              replicaset/lorax-contextmanager-d5dd69566                       Error creating: pods "lorax-contextmanager-d5dd69566-jl5pk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-contextmanager-f747f598d                       Error creating: pods "lorax-contextmanager-f747f598d-vpng2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate              replicaset/lorax-contextmanager-f747f598d                       Error creating: pods "lorax-contextmanager-f747f598d-nr75q" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-contextmanager-f747f598d                       Error creating: pods "lorax-contextmanager-f747f598d-v2d6r" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m28s       Warning   FailedCreate              replicaset/lorax-contextmanager-f747f598d                       Error creating: pods "lorax-contextmanager-f747f598d-dlvqx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-contextmanager-f7dd6d88d                       Error creating: pods "lorax-contextmanager-f7dd6d88d-wh8cp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-contextmanager-f7dd6d88d                       Error creating: pods "lorax-contextmanager-f7dd6d88d-k57ml" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-contextmanager-f7dd6d88d                       Error creating: pods "lorax-contextmanager-f7dd6d88d-62tvw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-customizationmanager-54bd59d66                 Error creating: pods "lorax-customizationmanager-54bd59d66-xbv4t" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-customizationmanager-54bd59d66                 Error creating: pods "lorax-customizationmanager-54bd59d66-x7bgz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
13m         Warning   FailedCreate              replicaset/lorax-customizationmanager-54bd59d66                 Error creating: pods "lorax-customizationmanager-54bd59d66-mbxmq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-customizationmanager-5fc4d78fcb                Error creating: pods "lorax-customizationmanager-5fc4d78fcb-lhfnh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-customizationmanager-5fc4d78fcb                Error creating: pods "lorax-customizationmanager-5fc4d78fcb-dd4x7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-customizationmanager-5fc4d78fcb                Error creating: pods "lorax-customizationmanager-5fc4d78fcb-5x9qw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m54s       Warning   FailedCreate              replicaset/lorax-customizationmanager-5fc4d78fcb                Error creating: pods "lorax-customizationmanager-5fc4d78fcb-qv8nv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-customizationmanager-6486f894d4                Error creating: pods "lorax-customizationmanager-6486f894d4-w8cn5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-customizationmanager-6486f894d4                Error creating: pods "lorax-customizationmanager-6486f894d4-jdv4f" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate              replicaset/lorax-customizationmanager-6486f894d4                Error creating: pods "lorax-customizationmanager-6486f894d4-s9shl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
5s          Warning   FailedCreate              replicaset/lorax-customizationmanager-6486f894d4                Error creating: pods "lorax-customizationmanager-6486f894d4-22bhc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-customizationmanager-656cf7bd54                Error creating: pods "lorax-customizationmanager-656cf7bd54-dp8t9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-customizationmanager-656cf7bd54                Error creating: pods "lorax-customizationmanager-656cf7bd54-nhsfk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-customizationmanager-656cf7bd54                Error creating: pods "lorax-customizationmanager-656cf7bd54-pmgj4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m31s       Warning   FailedCreate              replicaset/lorax-customizationmanager-656cf7bd54                Error creating: pods "lorax-customizationmanager-656cf7bd54-wqj2r" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40s         Warning   FailedMount               pod/lorax-customizationmanager-657bbf9586-rsbqg                 MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
47m         Warning   FailedCreate              replicaset/lorax-customizationmanager-657bbf9586                Error creating: pods "lorax-customizationmanager-657bbf9586-7b8xs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-customizationmanager-657bbf9586                Error creating: pods "lorax-customizationmanager-657bbf9586-n4n77" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
13m         Warning   FailedCreate              replicaset/lorax-customizationmanager-657bbf9586                Error creating: pods "lorax-customizationmanager-657bbf9586-cbxrb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
59m         Warning   FailedCreate              replicaset/lorax-customizationmanager-6cb5947b6                 Error creating: pods "lorax-customizationmanager-6cb5947b6-khp9d" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
43m         Warning   FailedCreate              replicaset/lorax-customizationmanager-6cb5947b6                 Error creating: pods "lorax-customizationmanager-6cb5947b6-nzjkp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
26m         Warning   FailedCreate              replicaset/lorax-customizationmanager-6cb5947b6                 Error creating: pods "lorax-customizationmanager-6cb5947b6-bd57j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
9m39s       Warning   FailedCreate              replicaset/lorax-customizationmanager-6cb5947b6                 Error creating: pods "lorax-customizationmanager-6cb5947b6-5bc9h" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-customizationmanager-7566984c4                 Error creating: pods "lorax-customizationmanager-7566984c4-lvct8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-customizationmanager-7566984c4                 Error creating: pods "lorax-customizationmanager-7566984c4-2n78j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-customizationmanager-7566984c4                 Error creating: pods "lorax-customizationmanager-7566984c4-mfrwr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24s         Warning   FailedCreate              replicaset/lorax-customizationmanager-7566984c4                 Error creating: pods "lorax-customizationmanager-7566984c4-h6xtd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-customizationmanager-76c587676d                Error creating: pods "lorax-customizationmanager-76c587676d-dtjb5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-customizationmanager-76c587676d                Error creating: pods "lorax-customizationmanager-76c587676d-hpfgp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-customizationmanager-76c587676d                Error creating: pods "lorax-customizationmanager-76c587676d-gpwlg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
39s         Warning   FailedCreate              replicaset/lorax-customizationmanager-76c587676d                Error creating: pods "lorax-customizationmanager-76c587676d-wvm9z" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-customizationmanager-7b5c9fd99d                Error creating: pods "lorax-customizationmanager-7b5c9fd99d-5lczv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-customizationmanager-7b5c9fd99d                Error creating: pods "lorax-customizationmanager-7b5c9fd99d-8lgvt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
23m         Warning   FailedCreate              replicaset/lorax-customizationmanager-7b5c9fd99d                Error creating: pods "lorax-customizationmanager-7b5c9fd99d-2tmbj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m17s       Warning   FailedCreate              replicaset/lorax-customizationmanager-7b5c9fd99d                Error creating: pods "lorax-customizationmanager-7b5c9fd99d-qxd7l" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate              replicaset/lorax-customizationmanager-7ccf74fbd4                Error creating: pods "lorax-customizationmanager-7ccf74fbd4-bcmfn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-customizationmanager-7ccf74fbd4                Error creating: pods "lorax-customizationmanager-7ccf74fbd4-7d2vd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-customizationmanager-7ccf74fbd4                Error creating: pods "lorax-customizationmanager-7ccf74fbd4-p5fb9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m59s       Warning   FailedCreate              replicaset/lorax-customizationmanager-7ccf74fbd4                Error creating: pods "lorax-customizationmanager-7ccf74fbd4-ksr2t" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-customizationmanager-8474f464b6                Error creating: pods "lorax-customizationmanager-8474f464b6-nsmq4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-customizationmanager-8474f464b6                Error creating: pods "lorax-customizationmanager-8474f464b6-m4xwm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-customizationmanager-8474f464b6                Error creating: pods "lorax-customizationmanager-8474f464b6-szzz9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate              replicaset/lorax-customizationmanager-84f9b9f66d                Error creating: pods "lorax-customizationmanager-84f9b9f66d-s8r7x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-customizationmanager-84f9b9f66d                Error creating: pods "lorax-customizationmanager-84f9b9f66d-256cn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-customizationmanager-84f9b9f66d                Error creating: pods "lorax-customizationmanager-84f9b9f66d-t7ktd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-customizationmanager-86b9c84b5d                Error creating: pods "lorax-customizationmanager-86b9c84b5d-bptgc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-customizationmanager-86b9c84b5d                Error creating: pods "lorax-customizationmanager-86b9c84b5d-5tb7j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
13m         Warning   FailedCreate              replicaset/lorax-customizationmanager-86b9c84b5d                Error creating: pods "lorax-customizationmanager-86b9c84b5d-ssx85" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate              replicaset/lorax-customizationmanager-8788f7f66                 Error creating: pods "lorax-customizationmanager-8788f7f66-xqlpj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-customizationmanager-8788f7f66                 Error creating: pods "lorax-customizationmanager-8788f7f66-mfdtb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-customizationmanager-8788f7f66                 Error creating: pods "lorax-customizationmanager-8788f7f66-w5nkg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
78s         Warning   FailedCreate              replicaset/lorax-customizationmanager-8788f7f66                 Error creating: pods "lorax-customizationmanager-8788f7f66-pknrv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedGetResourceMetric   horizontalpodautoscaler/lorax-customizationmanager-autoscaler   unable to get metrics for resource memory: no metrics returned from resource metrics API
53m         Warning   FailedCreate              replicaset/lorax-customizationmanager-bb8d7f986                 Error creating: pods "lorax-customizationmanager-bb8d7f986-mmgt7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate              replicaset/lorax-customizationmanager-bb8d7f986                 Error creating: pods "lorax-customizationmanager-bb8d7f986-b6kwj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-customizationmanager-bb8d7f986                 Error creating: pods "lorax-customizationmanager-bb8d7f986-j4hwv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m28s       Warning   FailedCreate              replicaset/lorax-customizationmanager-bb8d7f986                 Error creating: pods "lorax-customizationmanager-bb8d7f986-4b2fq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-customizationmanager-cfbf8558                  Error creating: pods "lorax-customizationmanager-cfbf8558-8b4bd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate              replicaset/lorax-customizationmanager-cfbf8558                  Error creating: pods "lorax-customizationmanager-cfbf8558-fjfkp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-customizationmanager-cfbf8558                  Error creating: pods "lorax-customizationmanager-cfbf8558-qnv7l" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m41s       Warning   FailedCreate              replicaset/lorax-customizationmanager-cfbf8558                  Error creating: pods "lorax-customizationmanager-cfbf8558-95hzf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-elementmanager-5b6bfc5664                      Error creating: pods "lorax-elementmanager-5b6bfc5664-2sg9d" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate              replicaset/lorax-elementmanager-5b6bfc5664                      Error creating: pods "lorax-elementmanager-5b6bfc5664-sxbgg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-elementmanager-5b6bfc5664                      Error creating: pods "lorax-elementmanager-5b6bfc5664-z96bc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m42s       Warning   FailedCreate              replicaset/lorax-elementmanager-5b6bfc5664                      Error creating: pods "lorax-elementmanager-5b6bfc5664-fbtkk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-elementmanager-65f875c58                       Error creating: pods "lorax-elementmanager-65f875c58-phvzh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-elementmanager-65f875c58                       Error creating: pods "lorax-elementmanager-65f875c58-v88gs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-elementmanager-65f875c58                       Error creating: pods "lorax-elementmanager-65f875c58-c9nh7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m50s       Warning   FailedCreate              replicaset/lorax-elementmanager-65f875c58                       Error creating: pods "lorax-elementmanager-65f875c58-zpw9w" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-elementmanager-69fcf98f4d                      Error creating: pods "lorax-elementmanager-69fcf98f4d-l2f9k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-elementmanager-69fcf98f4d                      Error creating: pods "lorax-elementmanager-69fcf98f4d-bk7p4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-elementmanager-69fcf98f4d                      Error creating: pods "lorax-elementmanager-69fcf98f4d-dv484" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m31s       Warning   FailedCreate              replicaset/lorax-elementmanager-69fcf98f4d                      Error creating: pods "lorax-elementmanager-69fcf98f4d-f2bnb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate              replicaset/lorax-elementmanager-6dd86c7dcb                      Error creating: pods "lorax-elementmanager-6dd86c7dcb-kxn9w" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-elementmanager-6dd86c7dcb                      Error creating: pods "lorax-elementmanager-6dd86c7dcb-4s8f7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-elementmanager-6dd86c7dcb                      Error creating: pods "lorax-elementmanager-6dd86c7dcb-k88r6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
78s         Warning   FailedCreate              replicaset/lorax-elementmanager-6dd86c7dcb                      Error creating: pods "lorax-elementmanager-6dd86c7dcb-6l7jm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-elementmanager-765f8845c8                      Error creating: pods "lorax-elementmanager-765f8845c8-k6vpm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-elementmanager-765f8845c8                      Error creating: pods "lorax-elementmanager-765f8845c8-dhxk6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate              replicaset/lorax-elementmanager-765f8845c8                      Error creating: pods "lorax-elementmanager-765f8845c8-xfp7k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
9s          Warning   FailedCreate              replicaset/lorax-elementmanager-765f8845c8                      Error creating: pods "lorax-elementmanager-765f8845c8-lxhwf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-elementmanager-777b868f84                      Error creating: pods "lorax-elementmanager-777b868f84-9hlrz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-elementmanager-777b868f84                      Error creating: pods "lorax-elementmanager-777b868f84-gpw8s" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-elementmanager-777b868f84                      Error creating: pods "lorax-elementmanager-777b868f84-ddfrn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27s         Warning   FailedCreate              replicaset/lorax-elementmanager-777b868f84                      Error creating: pods "lorax-elementmanager-777b868f84-rg7j8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate              replicaset/lorax-elementmanager-79cfbcf654                      Error creating: pods "lorax-elementmanager-79cfbcf654-mscqm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-elementmanager-79cfbcf654                      Error creating: pods "lorax-elementmanager-79cfbcf654-tx6s6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-elementmanager-79cfbcf654                      Error creating: pods "lorax-elementmanager-79cfbcf654-gvs96" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m          Warning   FailedCreate              replicaset/lorax-elementmanager-79cfbcf654                      Error creating: pods "lorax-elementmanager-79cfbcf654-xz92q" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
43m         Warning   FailedCreate              replicaset/lorax-elementmanager-7c4fc5c74d                      Error creating: pods "lorax-elementmanager-7c4fc5c74d-7cqhj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-elementmanager-7c4fc5c74d                      Error creating: pods "lorax-elementmanager-7c4fc5c74d-sgp7n" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-elementmanager-7c4fc5c74d                      Error creating: pods "lorax-elementmanager-7c4fc5c74d-wkk7v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-elementmanager-7f44848cfb                      Error creating: pods "lorax-elementmanager-7f44848cfb-nmdj2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate              replicaset/lorax-elementmanager-7f44848cfb                      Error creating: pods "lorax-elementmanager-7f44848cfb-l66wr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-elementmanager-7f44848cfb                      Error creating: pods "lorax-elementmanager-7f44848cfb-xx6sd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m29s       Warning   FailedCreate              replicaset/lorax-elementmanager-7f44848cfb                      Error creating: pods "lorax-elementmanager-7f44848cfb-6wnb6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
100s        Warning   FailedMount               pod/lorax-elementmanager-7f74646f4d-n4cqx                       MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
47m         Warning   FailedCreate              replicaset/lorax-elementmanager-84bbf749bd                      Error creating: pods "lorax-elementmanager-84bbf749bd-gx5ld" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-elementmanager-84bbf749bd                      Error creating: pods "lorax-elementmanager-84bbf749bd-hh5j7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-elementmanager-84bbf749bd                      Error creating: pods "lorax-elementmanager-84bbf749bd-pzw7c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-elementmanager-85575658b8                      Error creating: pods "lorax-elementmanager-85575658b8-h9w78" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-elementmanager-85575658b8                      Error creating: pods "lorax-elementmanager-85575658b8-djw9n" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
23m         Warning   FailedCreate              replicaset/lorax-elementmanager-85575658b8                      Error creating: pods "lorax-elementmanager-85575658b8-6jzcr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m18s       Warning   FailedCreate              replicaset/lorax-elementmanager-85575658b8                      Error creating: pods "lorax-elementmanager-85575658b8-fvjlk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-elementmanager-868c7dbb56                      Error creating: pods "lorax-elementmanager-868c7dbb56-wn49s" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-elementmanager-868c7dbb56                      Error creating: pods "lorax-elementmanager-868c7dbb56-mvwxd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
13m         Warning   FailedCreate              replicaset/lorax-elementmanager-868c7dbb56                      Error creating: pods "lorax-elementmanager-868c7dbb56-7d4kk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate              replicaset/lorax-elementmanager-b65cb6dc4                       Error creating: pods "lorax-elementmanager-b65cb6dc4-kxcjd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-elementmanager-b65cb6dc4                       Error creating: pods "lorax-elementmanager-b65cb6dc4-t4qlp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-elementmanager-b65cb6dc4                       Error creating: pods "lorax-elementmanager-b65cb6dc4-vt5b8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-elementmanager-cd8895564                       Error creating: pods "lorax-elementmanager-cd8895564-5grlx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-elementmanager-cd8895564                       Error creating: pods "lorax-elementmanager-cd8895564-lhmw8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-elementmanager-cd8895564                       Error creating: pods "lorax-elementmanager-cd8895564-pjwlg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-elementmanager-df8dc55db                       Error creating: pods "lorax-elementmanager-df8dc55db-fcbph" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-elementmanager-df8dc55db                       Error creating: pods "lorax-elementmanager-df8dc55db-d6ldr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-elementmanager-df8dc55db                       Error creating: pods "lorax-elementmanager-df8dc55db-d84mh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
43s         Warning   FailedCreate              replicaset/lorax-elementmanager-df8dc55db                       Error creating: pods "lorax-elementmanager-df8dc55db-5ngdv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-eventmanager-5b59b586fb                        Error creating: pods "lorax-eventmanager-5b59b586fb-vdqj6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate              replicaset/lorax-eventmanager-5b59b586fb                        Error creating: pods "lorax-eventmanager-5b59b586fb-q674x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-eventmanager-5b59b586fb                        Error creating: pods "lorax-eventmanager-5b59b586fb-2cr4c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m44s       Warning   FailedCreate              replicaset/lorax-eventmanager-5b59b586fb                        Error creating: pods "lorax-eventmanager-5b59b586fb-p5w9l" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-eventmanager-656dc54b5b                        Error creating: pods "lorax-eventmanager-656dc54b5b-7ztgf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-eventmanager-656dc54b5b                        Error creating: pods "lorax-eventmanager-656dc54b5b-ttk7g" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-eventmanager-656dc54b5b                        Error creating: pods "lorax-eventmanager-656dc54b5b-tckpj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m37s       Warning   FailedCreate              replicaset/lorax-eventmanager-656dc54b5b                        Error creating: pods "lorax-eventmanager-656dc54b5b-qt6p7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate              replicaset/lorax-eventmanager-679fd7fdd6                        Error creating: pods "lorax-eventmanager-679fd7fdd6-5r48m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-eventmanager-679fd7fdd6                        Error creating: pods "lorax-eventmanager-679fd7fdd6-7bc5s" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-eventmanager-679fd7fdd6                        Error creating: pods "lorax-eventmanager-679fd7fdd6-x8cs5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m3s        Warning   FailedCreate              replicaset/lorax-eventmanager-679fd7fdd6                        Error creating: pods "lorax-eventmanager-679fd7fdd6-xrf2h" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24s         Warning   FailedMount               pod/lorax-eventmanager-759c5b9484-ch28f                         MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
55m         Warning   FailedCreate              replicaset/lorax-eventmanager-759c5b9484                        Error creating: pods "lorax-eventmanager-759c5b9484-n77gv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
38m         Warning   FailedCreate              replicaset/lorax-eventmanager-759c5b9484                        Error creating: pods "lorax-eventmanager-759c5b9484-wphld" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
21m         Warning   FailedCreate              replicaset/lorax-eventmanager-759c5b9484                        Error creating: pods "lorax-eventmanager-759c5b9484-6k2xd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
5m16s       Warning   FailedCreate              replicaset/lorax-eventmanager-759c5b9484                        Error creating: pods "lorax-eventmanager-759c5b9484-qgbfl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-eventmanager-7755588b84                        Error creating: pods "lorax-eventmanager-7755588b84-tdfjd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-eventmanager-7755588b84                        Error creating: pods "lorax-eventmanager-7755588b84-2khk2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
13m         Warning   FailedCreate              replicaset/lorax-eventmanager-7755588b84                        Error creating: pods "lorax-eventmanager-7755588b84-d6n4w" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-eventmanager-795bb876b8                        Error creating: pods "lorax-eventmanager-795bb876b8-vbbvt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-eventmanager-795bb876b8                        Error creating: pods "lorax-eventmanager-795bb876b8-4zrjn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-eventmanager-795bb876b8                        Error creating: pods "lorax-eventmanager-795bb876b8-ptxw6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-eventmanager-79849f6bb                         Error creating: pods "lorax-eventmanager-79849f6bb-qdht4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-eventmanager-79849f6bb                         Error creating: pods "lorax-eventmanager-79849f6bb-8hz89" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate              replicaset/lorax-eventmanager-79849f6bb                         Error creating: pods "lorax-eventmanager-79849f6bb-lhc49" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18s         Warning   FailedCreate              replicaset/lorax-eventmanager-79849f6bb                         Error creating: pods "lorax-eventmanager-79849f6bb-2rhzl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-eventmanager-7c8946c76d                        Error creating: pods "lorax-eventmanager-7c8946c76d-2szmr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate              replicaset/lorax-eventmanager-7c8946c76d                        Error creating: pods "lorax-eventmanager-7c8946c76d-g6qcb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
19m         Warning   FailedCreate              replicaset/lorax-eventmanager-7c8946c76d                        Error creating: pods "lorax-eventmanager-7c8946c76d-jtrjz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m18s       Warning   FailedCreate              replicaset/lorax-eventmanager-7c8946c76d                        Error creating: pods "lorax-eventmanager-7c8946c76d-tmwc6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-eventmanager-7fd6c57c6d                        Error creating: pods "lorax-eventmanager-7fd6c57c6d-x5w7l" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-eventmanager-7fd6c57c6d                        Error creating: pods "lorax-eventmanager-7fd6c57c6d-nxzcd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-eventmanager-7fd6c57c6d                        Error creating: pods "lorax-eventmanager-7fd6c57c6d-256lb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28s         Warning   FailedCreate              replicaset/lorax-eventmanager-7fd6c57c6d                        Error creating: pods "lorax-eventmanager-7fd6c57c6d-z89fd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-eventmanager-84cb95685d                        Error creating: pods "lorax-eventmanager-84cb95685d-nknvp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-eventmanager-84cb95685d                        Error creating: pods "lorax-eventmanager-84cb95685d-rt2p9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-eventmanager-84cb95685d                        Error creating: pods "lorax-eventmanager-84cb95685d-l525h" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m52s       Warning   FailedCreate              replicaset/lorax-eventmanager-84cb95685d                        Error creating: pods "lorax-eventmanager-84cb95685d-rs5d8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-eventmanager-968787676                         Error creating: pods "lorax-eventmanager-968787676-t8m5s" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-eventmanager-968787676                         Error creating: pods "lorax-eventmanager-968787676-mvc9g" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
23m         Warning   FailedCreate              replicaset/lorax-eventmanager-968787676                         Error creating: pods "lorax-eventmanager-968787676-cxpxz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m18s       Warning   FailedCreate              replicaset/lorax-eventmanager-968787676                         Error creating: pods "lorax-eventmanager-968787676-qn9m9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate              replicaset/lorax-eventmanager-9ff868d                           Error creating: pods "lorax-eventmanager-9ff868d-nhc82" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-eventmanager-9ff868d                           Error creating: pods "lorax-eventmanager-9ff868d-8kx9l" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-eventmanager-9ff868d                           Error creating: pods "lorax-eventmanager-9ff868d-c958x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
66s         Warning   FailedCreate              replicaset/lorax-eventmanager-9ff868d                           Error creating: pods "lorax-eventmanager-9ff868d-xtbdc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
15m         Warning   FailedGetResourceMetric   horizontalpodautoscaler/lorax-eventmanager-autoscaler           unable to get metrics for resource memory: no metrics returned from resource metrics API
47m         Warning   FailedCreate              replicaset/lorax-eventmanager-b895c6d44                         Error creating: pods "lorax-eventmanager-b895c6d44-rnrtf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-eventmanager-b895c6d44                         Error creating: pods "lorax-eventmanager-b895c6d44-xnr96" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-eventmanager-b895c6d44                         Error creating: pods "lorax-eventmanager-b895c6d44-7tq94" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate              replicaset/lorax-eventmanager-bcb667494                         Error creating: pods "lorax-eventmanager-bcb667494-bgkrf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-eventmanager-bcb667494                         Error creating: pods "lorax-eventmanager-bcb667494-tqjjd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-eventmanager-bcb667494                         Error creating: pods "lorax-eventmanager-bcb667494-976vc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
43m         Warning   FailedCreate              replicaset/lorax-eventmanager-cb6f8df96                         Error creating: pods "lorax-eventmanager-cb6f8df96-xww4w" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-eventmanager-cb6f8df96                         Error creating: pods "lorax-eventmanager-cb6f8df96-szh8g" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-eventmanager-cb6f8df96                         Error creating: pods "lorax-eventmanager-cb6f8df96-8kbkz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-eventmanager-d7c568f86                         Error creating: pods "lorax-eventmanager-d7c568f86-5k8r6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-eventmanager-d7c568f86                         Error creating: pods "lorax-eventmanager-d7c568f86-8shjs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-eventmanager-d7c568f86                         Error creating: pods "lorax-eventmanager-d7c568f86-bsm5m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40s         Warning   FailedCreate              replicaset/lorax-eventmanager-d7c568f86                         Error creating: pods "lorax-eventmanager-d7c568f86-s65mb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-eventworker-5b788d465f                         Error creating: pods "lorax-eventworker-5b788d465f-c99t6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-eventworker-5b788d465f                         Error creating: pods "lorax-eventworker-5b788d465f-6qlt8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-eventworker-5b788d465f                         Error creating: pods "lorax-eventworker-5b788d465f-t6v6c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
25s         Warning   FailedCreate              replicaset/lorax-eventworker-5b788d465f                         Error creating: pods "lorax-eventworker-5b788d465f-w22zc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-eventworker-5bb85fdf8f                         Error creating: pods "lorax-eventworker-5bb85fdf8f-zfrw2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-eventworker-5bb85fdf8f                         Error creating: pods "lorax-eventworker-5bb85fdf8f-fgjdp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-eventworker-5bb85fdf8f                         Error creating: pods "lorax-eventworker-5bb85fdf8f-6z4z4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m53s       Warning   FailedCreate              replicaset/lorax-eventworker-5bb85fdf8f                         Error creating: pods "lorax-eventworker-5bb85fdf8f-bdz69" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate              replicaset/lorax-eventworker-5dfcc9c6bf                         Error creating: pods "lorax-eventworker-5dfcc9c6bf-w2sqz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-eventworker-5dfcc9c6bf                         Error creating: pods "lorax-eventworker-5dfcc9c6bf-2v7bd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-eventworker-5dfcc9c6bf                         Error creating: pods "lorax-eventworker-5dfcc9c6bf-n9lp2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-eventworker-6876c65579                         Error creating: pods "lorax-eventworker-6876c65579-vcdqg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-eventworker-6876c65579                         Error creating: pods "lorax-eventworker-6876c65579-qk4db" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
23m         Warning   FailedCreate              replicaset/lorax-eventworker-6876c65579                         Error creating: pods "lorax-eventworker-6876c65579-wdls5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m15s       Warning   FailedCreate              replicaset/lorax-eventworker-6876c65579                         Error creating: pods "lorax-eventworker-6876c65579-rd5kg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-eventworker-68c5d5f699                         Error creating: pods "lorax-eventworker-68c5d5f699-58jlv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-eventworker-68c5d5f699                         Error creating: pods "lorax-eventworker-68c5d5f699-q7st8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-eventworker-68c5d5f699                         Error creating: pods "lorax-eventworker-68c5d5f699-9sxq2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-eventworker-697ff7b947                         Error creating: pods "lorax-eventworker-697ff7b947-9vwgt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-eventworker-697ff7b947                         Error creating: pods "lorax-eventworker-697ff7b947-gn64f" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-eventworker-697ff7b947                         Error creating: pods "lorax-eventworker-697ff7b947-gjpdx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m31s       Warning   FailedCreate              replicaset/lorax-eventworker-697ff7b947                         Error creating: pods "lorax-eventworker-697ff7b947-kh7wc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate              replicaset/lorax-eventworker-69db9b44b7                         Error creating: pods "lorax-eventworker-69db9b44b7-bd7dc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-eventworker-69db9b44b7                         Error creating: pods "lorax-eventworker-69db9b44b7-zbvnr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-eventworker-69db9b44b7                         Error creating: pods "lorax-eventworker-69db9b44b7-glk9m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
79s         Warning   FailedCreate              replicaset/lorax-eventworker-69db9b44b7                         Error creating: pods "lorax-eventworker-69db9b44b7-twgnn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-eventworker-766699cdf                          Error creating: pods "lorax-eventworker-766699cdf-6zsx7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-eventworker-766699cdf                          Error creating: pods "lorax-eventworker-766699cdf-9b987" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate              replicaset/lorax-eventworker-766699cdf                          Error creating: pods "lorax-eventworker-766699cdf-qwrxt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7s          Warning   FailedCreate              replicaset/lorax-eventworker-766699cdf                          Error creating: pods "lorax-eventworker-766699cdf-ghtxb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-eventworker-78bdc448bf                         Error creating: pods "lorax-eventworker-78bdc448bf-tgmc8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-eventworker-78bdc448bf                         Error creating: pods "lorax-eventworker-78bdc448bf-bkb4x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-eventworker-78bdc448bf                         Error creating: pods "lorax-eventworker-78bdc448bf-x6tkt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
42s         Warning   FailedCreate              replicaset/lorax-eventworker-78bdc448bf                         Error creating: pods "lorax-eventworker-78bdc448bf-fzktz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-eventworker-79fbcf4895                         Error creating: pods "lorax-eventworker-79fbcf4895-8bgb8" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-eventworker-79fbcf4895                         Error creating: pods "lorax-eventworker-79fbcf4895-mdf9d" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-eventworker-79fbcf4895                         Error creating: pods "lorax-eventworker-79fbcf4895-zch48" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
99s         Warning   FailedMount               pod/lorax-eventworker-7c5b779bc-z4bb5                           MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
57m         Warning   FailedCreate              replicaset/lorax-eventworker-84454db699                         Error creating: pods "lorax-eventworker-84454db699-j4pms" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate              replicaset/lorax-eventworker-84454db699                         Error creating: pods "lorax-eventworker-84454db699-9dtjm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-eventworker-84454db699                         Error creating: pods "lorax-eventworker-84454db699-ggd6p" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m42s       Warning   FailedCreate              replicaset/lorax-eventworker-84454db699                         Error creating: pods "lorax-eventworker-84454db699-228dw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
59m         Warning   FailedCreate              replicaset/lorax-eventworker-8488fb5499                         Error creating: pods "lorax-eventworker-8488fb5499-k2szc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
43m         Warning   FailedCreate              replicaset/lorax-eventworker-8488fb5499                         Error creating: pods "lorax-eventworker-8488fb5499-4gznl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
26m         Warning   FailedCreate              replicaset/lorax-eventworker-8488fb5499                         Error creating: pods "lorax-eventworker-8488fb5499-xvlrk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
9m39s       Warning   FailedCreate              replicaset/lorax-eventworker-8488fb5499                         Error creating: pods "lorax-eventworker-8488fb5499-jb67f" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate              replicaset/lorax-eventworker-86d9655577                         Error creating: pods "lorax-eventworker-86d9655577-99px2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-eventworker-86d9655577                         Error creating: pods "lorax-eventworker-86d9655577-rsp6c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-eventworker-86d9655577                         Error creating: pods "lorax-eventworker-86d9655577-lmhh4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m          Warning   FailedCreate              replicaset/lorax-eventworker-86d9655577                         Error creating: pods "lorax-eventworker-86d9655577-p7r94" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-eventworker-8cc65fd4c                          Error creating: pods "lorax-eventworker-8cc65fd4c-vsjdr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate              replicaset/lorax-eventworker-8cc65fd4c                          Error creating: pods "lorax-eventworker-8cc65fd4c-tt4nf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
19m         Warning   FailedCreate              replicaset/lorax-eventworker-8cc65fd4c                          Error creating: pods "lorax-eventworker-8cc65fd4c-7zdfk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m12s       Warning   FailedCreate              replicaset/lorax-eventworker-8cc65fd4c                          Error creating: pods "lorax-eventworker-8cc65fd4c-r8njt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-eventworker-d68455667                          Error creating: pods "lorax-eventworker-d68455667-fxx6l" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-eventworker-d68455667                          Error creating: pods "lorax-eventworker-d68455667-cwjlm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
13m         Warning   FailedCreate              replicaset/lorax-eventworker-d68455667                          Error creating: pods "lorax-eventworker-d68455667-xcsw7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Normal    Info                      configmap/lorax-lorax-dashboards                                Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-lorax-dashboards
    namespace: as-ciat
    resourceVersion: 770646840
    uid: 7add2faf-d2f9-4dac-afe1-0c9d40b7882e
}: Received remove dashboard event
49m         Normal    Info                      configmap/lorax-lorax-dashboards                                Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-lorax-dashboards
    namespace: as-ciat
    resourceVersion: 770205678
    uid: 7add2faf-d2f9-4dac-afe1-0c9d40b7882e
}: Pushed add dashboard to kafka
49m         Normal    Info                      configmap/lorax-lorax-dashboards                                Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-lorax-dashboards
    namespace: as-ciat
    resourceVersion: 770205678
    uid: 7add2faf-d2f9-4dac-afe1-0c9d40b7882e
}: Received add dashboard event
10m         Normal    Info                      configmap/lorax-lorax-dashboards                                Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-lorax-dashboards
    namespace: as-ciat
    resourceVersion: 770646840
    uid: 7add2faf-d2f9-4dac-afe1-0c9d40b7882e
}: Pushed delete dashboard to kafka
49m         Normal    Info                      configmap/lorax-prometheus-shared-rules                         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-prometheus-shared-rules
    namespace: as-ciat
    resourceVersion: 770205676
    uid: d4cbce11-3372-43d7-8906-ec863696198b
}: Pushed rule to kafka
10m         Normal    Info                      configmap/lorax-prometheus-shared-rules                         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-prometheus-shared-rules
    namespace: as-ciat
    resourceVersion: 770646838
    uid: d4cbce11-3372-43d7-8906-ec863696198b
}: Pushed delete rule to kafka
49m         Normal    Info                      configmap/lorax-prometheus-shared-rules                         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-prometheus-shared-rules
    namespace: as-ciat
    resourceVersion: 770205676
    uid: d4cbce11-3372-43d7-8906-ec863696198b
}: Received add rule event
10m         Normal    Info                      configmap/lorax-prometheus-shared-rules                         Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: lorax-prometheus-shared-rules
    namespace: as-ciat
    resourceVersion: 770646838
    uid: d4cbce11-3372-43d7-8906-ec863696198b
}: Received delete event
53m         Warning   FailedCreate              replicaset/lorax-queuemanager-5644b9fcb4                        Error creating: pods "lorax-queuemanager-5644b9fcb4-89bdv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate              replicaset/lorax-queuemanager-5644b9fcb4                        Error creating: pods "lorax-queuemanager-5644b9fcb4-2klhd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-queuemanager-5644b9fcb4                        Error creating: pods "lorax-queuemanager-5644b9fcb4-w847c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m29s       Warning   FailedCreate              replicaset/lorax-queuemanager-5644b9fcb4                        Error creating: pods "lorax-queuemanager-5644b9fcb4-vpfk9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-queuemanager-58bf767f6d                        Error creating: pods "lorax-queuemanager-58bf767f6d-ffltz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-queuemanager-58bf767f6d                        Error creating: pods "lorax-queuemanager-58bf767f6d-7vb9g" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-queuemanager-58bf767f6d                        Error creating: pods "lorax-queuemanager-58bf767f6d-hmv4l" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate              replicaset/lorax-queuemanager-59b6d8b6c8                        Error creating: pods "lorax-queuemanager-59b6d8b6c8-9q24k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-queuemanager-59b6d8b6c8                        Error creating: pods "lorax-queuemanager-59b6d8b6c8-wfzlj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-queuemanager-59b6d8b6c8                        Error creating: pods "lorax-queuemanager-59b6d8b6c8-kdfz5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate              replicaset/lorax-queuemanager-59d7d8fc4d                        Error creating: pods "lorax-queuemanager-59d7d8fc4d-drp4r" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-queuemanager-59d7d8fc4d                        Error creating: pods "lorax-queuemanager-59d7d8fc4d-vgpvl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-queuemanager-59d7d8fc4d                        Error creating: pods "lorax-queuemanager-59d7d8fc4d-wl24z" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
79s         Warning   FailedCreate              replicaset/lorax-queuemanager-59d7d8fc4d                        Error creating: pods "lorax-queuemanager-59d7d8fc4d-9k7dn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-queuemanager-5cd5c79d56                        Error creating: pods "lorax-queuemanager-5cd5c79d56-xbfl5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate              replicaset/lorax-queuemanager-5cd5c79d56                        Error creating: pods "lorax-queuemanager-5cd5c79d56-kk9gr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-queuemanager-5cd5c79d56                        Error creating: pods "lorax-queuemanager-5cd5c79d56-dm4kk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m44s       Warning   FailedCreate              replicaset/lorax-queuemanager-5cd5c79d56                        Error creating: pods "lorax-queuemanager-5cd5c79d56-wfrsn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
43m         Warning   FailedCreate              replicaset/lorax-queuemanager-5f4fdf898d                        Error creating: pods "lorax-queuemanager-5f4fdf898d-bzt2j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-queuemanager-5f4fdf898d                        Error creating: pods "lorax-queuemanager-5f4fdf898d-wtr79" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-queuemanager-5f4fdf898d                        Error creating: pods "lorax-queuemanager-5f4fdf898d-7hqpn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-queuemanager-66f97c9d58                        Error creating: pods "lorax-queuemanager-66f97c9d58-n2ttk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-queuemanager-66f97c9d58                        Error creating: pods "lorax-queuemanager-66f97c9d58-n6xtx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate              replicaset/lorax-queuemanager-66f97c9d58                        Error creating: pods "lorax-queuemanager-66f97c9d58-xjhsq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18s         Warning   FailedCreate              replicaset/lorax-queuemanager-66f97c9d58                        Error creating: pods "lorax-queuemanager-66f97c9d58-8ph64" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate              replicaset/lorax-queuemanager-6f45494dc4                        Error creating: pods "lorax-queuemanager-6f45494dc4-79kh5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-queuemanager-6f45494dc4                        Error creating: pods "lorax-queuemanager-6f45494dc4-x5klt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-queuemanager-6f45494dc4                        Error creating: pods "lorax-queuemanager-6f45494dc4-99cqx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m          Warning   FailedCreate              replicaset/lorax-queuemanager-6f45494dc4                        Error creating: pods "lorax-queuemanager-6f45494dc4-mmxvp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-queuemanager-769996b97b                        Error creating: pods "lorax-queuemanager-769996b97b-qlmt7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-queuemanager-769996b97b                        Error creating: pods "lorax-queuemanager-769996b97b-wpw95" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-queuemanager-769996b97b                        Error creating: pods "lorax-queuemanager-769996b97b-vchn9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28s         Warning   FailedCreate              replicaset/lorax-queuemanager-769996b97b                        Error creating: pods "lorax-queuemanager-769996b97b-hggdm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-queuemanager-77b4fb                            Error creating: pods "lorax-queuemanager-77b4fb-2w94p" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-queuemanager-77b4fb                            Error creating: pods "lorax-queuemanager-77b4fb-l5mlf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-queuemanager-77b4fb                            Error creating: pods "lorax-queuemanager-77b4fb-xm7tv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
29s         Warning   FailedCreate              replicaset/lorax-queuemanager-77b4fb                            Error creating: pods "lorax-queuemanager-77b4fb-5hf6n" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-queuemanager-796f9bf95b                        Error creating: pods "lorax-queuemanager-796f9bf95b-lnwkf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-queuemanager-796f9bf95b                        Error creating: pods "lorax-queuemanager-796f9bf95b-tdnhj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-queuemanager-796f9bf95b                        Error creating: pods "lorax-queuemanager-796f9bf95b-phpbf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m36s       Warning   FailedCreate              replicaset/lorax-queuemanager-796f9bf95b                        Error creating: pods "lorax-queuemanager-796f9bf95b-g9c8k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-queuemanager-79c777b444                        Error creating: pods "lorax-queuemanager-79c777b444-62vpg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-queuemanager-79c777b444                        Error creating: pods "lorax-queuemanager-79c777b444-nz772" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
23m         Warning   FailedCreate              replicaset/lorax-queuemanager-79c777b444                        Error creating: pods "lorax-queuemanager-79c777b444-h6zsh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m17s       Warning   FailedCreate              replicaset/lorax-queuemanager-79c777b444                        Error creating: pods "lorax-queuemanager-79c777b444-7mhpg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-queuemanager-84d8b5fbb6                        Error creating: pods "lorax-queuemanager-84d8b5fbb6-hnknr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate              replicaset/lorax-queuemanager-84d8b5fbb6                        Error creating: pods "lorax-queuemanager-84d8b5fbb6-r9w7f" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
19m         Warning   FailedCreate              replicaset/lorax-queuemanager-84d8b5fbb6                        Error creating: pods "lorax-queuemanager-84d8b5fbb6-tw66x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m19s       Warning   FailedCreate              replicaset/lorax-queuemanager-84d8b5fbb6                        Error creating: pods "lorax-queuemanager-84d8b5fbb6-prr2c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-queuemanager-bb49dc7c6                         Error creating: pods "lorax-queuemanager-bb49dc7c6-b95vv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-queuemanager-bb49dc7c6                         Error creating: pods "lorax-queuemanager-bb49dc7c6-rxvdw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
13m         Warning   FailedCreate              replicaset/lorax-queuemanager-bb49dc7c6                         Error creating: pods "lorax-queuemanager-bb49dc7c6-qqqdm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-queuemanager-cb775f484                         Error creating: pods "lorax-queuemanager-cb775f484-t8vzj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-queuemanager-cb775f484                         Error creating: pods "lorax-queuemanager-cb775f484-htwlt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-queuemanager-cb775f484                         Error creating: pods "lorax-queuemanager-cb775f484-lj985" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-queuemanager-fc6fbbc78                         Error creating: pods "lorax-queuemanager-fc6fbbc78-f92cc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-queuemanager-fc6fbbc78                         Error creating: pods "lorax-queuemanager-fc6fbbc78-5kgwb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-queuemanager-fc6fbbc78                         Error creating: pods "lorax-queuemanager-fc6fbbc78-tbgcs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
42s         Warning   FailedCreate              replicaset/lorax-queuemanager-fc6fbbc78                         Error creating: pods "lorax-queuemanager-fc6fbbc78-j48zf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-59b9cf5654                 Error creating: pods "lorax-realtimedatamanager-59b9cf5654-s47jd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-59b9cf5654                 Error creating: pods "lorax-realtimedatamanager-59b9cf5654-gfcf7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-59b9cf5654                 Error creating: pods "lorax-realtimedatamanager-59b9cf5654-86tph" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
39s         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-59b9cf5654                 Error creating: pods "lorax-realtimedatamanager-59b9cf5654-v9zcd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-5c76b4cc46                 Error creating: pods "lorax-realtimedatamanager-5c76b4cc46-dprpm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-5c76b4cc46                 Error creating: pods "lorax-realtimedatamanager-5c76b4cc46-lsrf9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
19m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-5c76b4cc46                 Error creating: pods "lorax-realtimedatamanager-5c76b4cc46-2dt5r" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m12s       Warning   FailedCreate              replicaset/lorax-realtimedatamanager-5c76b4cc46                 Error creating: pods "lorax-realtimedatamanager-5c76b4cc46-l6d86" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-5cf45d597d                 Error creating: pods "lorax-realtimedatamanager-5cf45d597d-2rfvz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-5cf45d597d                 Error creating: pods "lorax-realtimedatamanager-5cf45d597d-vt4cl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-5cf45d597d                 Error creating: pods "lorax-realtimedatamanager-5cf45d597d-9crgz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-5db6b5974                  Error creating: pods "lorax-realtimedatamanager-5db6b5974-r82pj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-5db6b5974                  Error creating: pods "lorax-realtimedatamanager-5db6b5974-b559q" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-5db6b5974                  Error creating: pods "lorax-realtimedatamanager-5db6b5974-f45pp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
26s         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-5db6b5974                  Error creating: pods "lorax-realtimedatamanager-5db6b5974-rcsb4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-5f6697d946                 Error creating: pods "lorax-realtimedatamanager-5f6697d946-pzbqk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-5f6697d946                 Error creating: pods "lorax-realtimedatamanager-5f6697d946-dt66g" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-5f6697d946                 Error creating: pods "lorax-realtimedatamanager-5f6697d946-tg9h6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m34s       Warning   FailedCreate              replicaset/lorax-realtimedatamanager-5f6697d946                 Error creating: pods "lorax-realtimedatamanager-5f6697d946-k5r99" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-658bd645f4                 Error creating: pods "lorax-realtimedatamanager-658bd645f4-kzbz6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-658bd645f4                 Error creating: pods "lorax-realtimedatamanager-658bd645f4-qwrm6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
13m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-658bd645f4                 Error creating: pods "lorax-realtimedatamanager-658bd645f4-4sckf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-65c955c86                  Error creating: pods "lorax-realtimedatamanager-65c955c86-9jmk7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-65c955c86                  Error creating: pods "lorax-realtimedatamanager-65c955c86-cfss2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-65c955c86                  Error creating: pods "lorax-realtimedatamanager-65c955c86-j8qzj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
18s         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-65c955c86                  Error creating: pods "lorax-realtimedatamanager-65c955c86-ms4vt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-697dccddc4                 Error creating: pods "lorax-realtimedatamanager-697dccddc4-g6bjp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-697dccddc4                 Error creating: pods "lorax-realtimedatamanager-697dccddc4-q8jqp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-697dccddc4                 Error creating: pods "lorax-realtimedatamanager-697dccddc4-mzlhk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m43s       Warning   FailedCreate              replicaset/lorax-realtimedatamanager-697dccddc4                 Error creating: pods "lorax-realtimedatamanager-697dccddc4-f4vps" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-6fd8c8599d                 Error creating: pods "lorax-realtimedatamanager-6fd8c8599d-8vddl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-6fd8c8599d                 Error creating: pods "lorax-realtimedatamanager-6fd8c8599d-8clq2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-6fd8c8599d                 Error creating: pods "lorax-realtimedatamanager-6fd8c8599d-cwbm4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m1s        Warning   FailedCreate              replicaset/lorax-realtimedatamanager-6fd8c8599d                 Error creating: pods "lorax-realtimedatamanager-6fd8c8599d-v4kd5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-745fcbf486                 Error creating: pods "lorax-realtimedatamanager-745fcbf486-nmq2m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-745fcbf486                 Error creating: pods "lorax-realtimedatamanager-745fcbf486-4rpt9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-745fcbf486                 Error creating: pods "lorax-realtimedatamanager-745fcbf486-6jwkx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m29s       Warning   FailedCreate              replicaset/lorax-realtimedatamanager-745fcbf486                 Error creating: pods "lorax-realtimedatamanager-745fcbf486-2vjln" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-7567b876b6                 Error creating: pods "lorax-realtimedatamanager-7567b876b6-lspjb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-7567b876b6                 Error creating: pods "lorax-realtimedatamanager-7567b876b6-b9z2q" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-7567b876b6                 Error creating: pods "lorax-realtimedatamanager-7567b876b6-d8jq6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
56s         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-7567b876b6                 Error creating: pods "lorax-realtimedatamanager-7567b876b6-qxpm5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-7c7c8775c4                 Error creating: pods "lorax-realtimedatamanager-7c7c8775c4-62w7w" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-7c7c8775c4                 Error creating: pods "lorax-realtimedatamanager-7c7c8775c4-zbf2p" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-7c7c8775c4                 Error creating: pods "lorax-realtimedatamanager-7c7c8775c4-h9qsq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-7cd4d9f8b                  Error creating: pods "lorax-realtimedatamanager-7cd4d9f8b-nzv2x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-7cd4d9f8b                  Error creating: pods "lorax-realtimedatamanager-7cd4d9f8b-x2wjl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-7cd4d9f8b                  Error creating: pods "lorax-realtimedatamanager-7cd4d9f8b-jtvx6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-7f67c866b                  Error creating: pods "lorax-realtimedatamanager-7f67c866b-x82cm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-7f67c866b                  Error creating: pods "lorax-realtimedatamanager-7f67c866b-lwvqh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-7f67c866b                  Error creating: pods "lorax-realtimedatamanager-7f67c866b-wdjwt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m20s       Warning   FailedCreate              replicaset/lorax-realtimedatamanager-7f67c866b                  Error creating: pods "lorax-realtimedatamanager-7f67c866b-ws8gz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
46s         Warning   FailedMount               pod/lorax-realtimedatamanager-866cd5886b-qq54f                  MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
43m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-86d957ccc8                 Error creating: pods "lorax-realtimedatamanager-86d957ccc8-4h467" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-86d957ccc8                 Error creating: pods "lorax-realtimedatamanager-86d957ccc8-j7bhb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-realtimedatamanager-86d957ccc8                 Error creating: pods "lorax-realtimedatamanager-86d957ccc8-867ql" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-558b94946b                 Error creating: pods "lorax-reportdesignmanager-558b94946b-ccb7x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-558b94946b                 Error creating: pods "lorax-reportdesignmanager-558b94946b-zj4gl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-558b94946b                 Error creating: pods "lorax-reportdesignmanager-558b94946b-djfq2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m30s       Warning   FailedCreate              replicaset/lorax-reportdesignmanager-558b94946b                 Error creating: pods "lorax-reportdesignmanager-558b94946b-6vkkg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-56465c769d                 Error creating: pods "lorax-reportdesignmanager-56465c769d-tm8vf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-56465c769d                 Error creating: pods "lorax-reportdesignmanager-56465c769d-zh7cs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-56465c769d                 Error creating: pods "lorax-reportdesignmanager-56465c769d-c6d4q" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
49m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-5c88cb777d                 Error creating: pods "lorax-reportdesignmanager-5c88cb777d-r4jtl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
32m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-5c88cb777d                 Error creating: pods "lorax-reportdesignmanager-5c88cb777d-2gssg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-5c88cb777d                 Error creating: pods "lorax-reportdesignmanager-5c88cb777d-phl24" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-5f7d7cd974                 Error creating: pods "lorax-reportdesignmanager-5f7d7cd974-gs2jr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-5f7d7cd974                 Error creating: pods "lorax-reportdesignmanager-5f7d7cd974-qtk8m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-5f7d7cd974                 Error creating: pods "lorax-reportdesignmanager-5f7d7cd974-t5k25" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
66s         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-5f7d7cd974                 Error creating: pods "lorax-reportdesignmanager-5f7d7cd974-9wjfb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-6567494f98                 Error creating: pods "lorax-reportdesignmanager-6567494f98-lngfm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-6567494f98                 Error creating: pods "lorax-reportdesignmanager-6567494f98-v59zw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-6567494f98                 Error creating: pods "lorax-reportdesignmanager-6567494f98-c7vk9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m45s       Warning   FailedCreate              replicaset/lorax-reportdesignmanager-6567494f98                 Error creating: pods "lorax-reportdesignmanager-6567494f98-jxrpx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-669965954b                 Error creating: pods "lorax-reportdesignmanager-669965954b-q9jmg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-669965954b                 Error creating: pods "lorax-reportdesignmanager-669965954b-zwr2s" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-669965954b                 Error creating: pods "lorax-reportdesignmanager-669965954b-lpqmw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
59m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-6744b65cc4                 Error creating: pods "lorax-reportdesignmanager-6744b65cc4-gx6th" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
43m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-6744b65cc4                 Error creating: pods "lorax-reportdesignmanager-6744b65cc4-ll9bm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
26m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-6744b65cc4                 Error creating: pods "lorax-reportdesignmanager-6744b65cc4-txffn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
9m40s       Warning   FailedCreate              replicaset/lorax-reportdesignmanager-6744b65cc4                 Error creating: pods "lorax-reportdesignmanager-6744b65cc4-jlnxb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-69b5f89b58                 Error creating: pods "lorax-reportdesignmanager-69b5f89b58-7gmrl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-69b5f89b58                 Error creating: pods "lorax-reportdesignmanager-69b5f89b58-7l9wz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-69b5f89b58                 Error creating: pods "lorax-reportdesignmanager-69b5f89b58-cjhh9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-6c874c8bd6                 Error creating: pods "lorax-reportdesignmanager-6c874c8bd6-b8r9v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-6c874c8bd6                 Error creating: pods "lorax-reportdesignmanager-6c874c8bd6-gj88t" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-6c874c8bd6                 Error creating: pods "lorax-reportdesignmanager-6c874c8bd6-srjkr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m37s       Warning   FailedCreate              replicaset/lorax-reportdesignmanager-6c874c8bd6                 Error creating: pods "lorax-reportdesignmanager-6c874c8bd6-2j4jp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-6cd9df9b54                 Error creating: pods "lorax-reportdesignmanager-6cd9df9b54-h5mqb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-6cd9df9b54                 Error creating: pods "lorax-reportdesignmanager-6cd9df9b54-n5mg6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-6cd9df9b54                 Error creating: pods "lorax-reportdesignmanager-6cd9df9b54-tfcwc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41s         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-6cd9df9b54                 Error creating: pods "lorax-reportdesignmanager-6cd9df9b54-k6tz7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-746cb6cc46                 Error creating: pods "lorax-reportdesignmanager-746cb6cc46-rrsqw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-746cb6cc46                 Error creating: pods "lorax-reportdesignmanager-746cb6cc46-qsjrd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-746cb6cc46                 Error creating: pods "lorax-reportdesignmanager-746cb6cc46-6mwlv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-7668f54b                   Error creating: pods "lorax-reportdesignmanager-7668f54b-tkr5m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-7668f54b                   Error creating: pods "lorax-reportdesignmanager-7668f54b-6bts4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-7668f54b                   Error creating: pods "lorax-reportdesignmanager-7668f54b-nvtk7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m2s        Warning   FailedCreate              replicaset/lorax-reportdesignmanager-7668f54b                   Error creating: pods "lorax-reportdesignmanager-7668f54b-s2928" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-7998f7595d                 Error creating: pods "lorax-reportdesignmanager-7998f7595d-q2pmx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-7998f7595d                 Error creating: pods "lorax-reportdesignmanager-7998f7595d-m4vbm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-7998f7595d                 Error creating: pods "lorax-reportdesignmanager-7998f7595d-hb8lf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24s         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-7998f7595d                 Error creating: pods "lorax-reportdesignmanager-7998f7595d-7kr62" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-7d67f9fb98                 Error creating: pods "lorax-reportdesignmanager-7d67f9fb98-5xf98" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-7d67f9fb98                 Error creating: pods "lorax-reportdesignmanager-7d67f9fb98-gndmj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
23m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-7d67f9fb98                 Error creating: pods "lorax-reportdesignmanager-7d67f9fb98-kl8bh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m19s       Warning   FailedCreate              replicaset/lorax-reportdesignmanager-7d67f9fb98                 Error creating: pods "lorax-reportdesignmanager-7d67f9fb98-gqk5w" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-848c78786                  Error creating: pods "lorax-reportdesignmanager-848c78786-mphjm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-848c78786                  Error creating: pods "lorax-reportdesignmanager-848c78786-vnh7n" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-848c78786                  Error creating: pods "lorax-reportdesignmanager-848c78786-wns6j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16s         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-848c78786                  Error creating: pods "lorax-reportdesignmanager-848c78786-j79f5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-8d4686c74                  Error creating: pods "lorax-reportdesignmanager-8d4686c74-cgsbf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-8d4686c74                  Error creating: pods "lorax-reportdesignmanager-8d4686c74-7zdnp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-reportdesignmanager-8d4686c74                  Error creating: pods "lorax-reportdesignmanager-8d4686c74-rgjlh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m31s       Warning   FailedCreate              replicaset/lorax-reportdesignmanager-8d4686c74                  Error creating: pods "lorax-reportdesignmanager-8d4686c74-d2ldl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-57d6d6f9db                Error creating: pods "lorax-reportpublishmanager-57d6d6f9db-qlzrv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-57d6d6f9db                Error creating: pods "lorax-reportpublishmanager-57d6d6f9db-rlr76" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-57d6d6f9db                Error creating: pods "lorax-reportpublishmanager-57d6d6f9db-p8r9s" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m31s       Warning   FailedCreate              replicaset/lorax-reportpublishmanager-57d6d6f9db                Error creating: pods "lorax-reportpublishmanager-57d6d6f9db-94djl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
58s         Warning   FailedMount               pod/lorax-reportpublishmanager-58445646c4-9ztc7                 MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
47m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-6779d8757d                Error creating: pods "lorax-reportpublishmanager-6779d8757d-7l2mn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-6779d8757d                Error creating: pods "lorax-reportpublishmanager-6779d8757d-g2jcd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-6779d8757d                Error creating: pods "lorax-reportpublishmanager-6779d8757d-ltkhz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-67f95f9b5b                Error creating: pods "lorax-reportpublishmanager-67f95f9b5b-xhjfl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-67f95f9b5b                Error creating: pods "lorax-reportpublishmanager-67f95f9b5b-7jm9c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-67f95f9b5b                Error creating: pods "lorax-reportpublishmanager-67f95f9b5b-vqrgw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m52s       Warning   FailedCreate              replicaset/lorax-reportpublishmanager-67f95f9b5b                Error creating: pods "lorax-reportpublishmanager-67f95f9b5b-29jhx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-6867c97d86                Error creating: pods "lorax-reportpublishmanager-6867c97d86-rdrkr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-6867c97d86                Error creating: pods "lorax-reportpublishmanager-6867c97d86-vw4h5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-6867c97d86                Error creating: pods "lorax-reportpublishmanager-6867c97d86-4sjw6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m19s       Warning   FailedCreate              replicaset/lorax-reportpublishmanager-6867c97d86                Error creating: pods "lorax-reportpublishmanager-6867c97d86-nq8b5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-68d67bc598                Error creating: pods "lorax-reportpublishmanager-68d67bc598-xcrmz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-68d67bc598                Error creating: pods "lorax-reportpublishmanager-68d67bc598-4vgz9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-68d67bc598                Error creating: pods "lorax-reportpublishmanager-68d67bc598-l2khn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m43s       Warning   FailedCreate              replicaset/lorax-reportpublishmanager-68d67bc598                Error creating: pods "lorax-reportpublishmanager-68d67bc598-s67jq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-6b5b9cc74                 Error creating: pods "lorax-reportpublishmanager-6b5b9cc74-5l4dq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-6b5b9cc74                 Error creating: pods "lorax-reportpublishmanager-6b5b9cc74-52gc7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-6b5b9cc74                 Error creating: pods "lorax-reportpublishmanager-6b5b9cc74-ddtb9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
43m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-6d88fc458b                Error creating: pods "lorax-reportpublishmanager-6d88fc458b-b5gbx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-6d88fc458b                Error creating: pods "lorax-reportpublishmanager-6d88fc458b-f7dzn" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-6d88fc458b                Error creating: pods "lorax-reportpublishmanager-6d88fc458b-xnh69" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-6f8dd75d64                Error creating: pods "lorax-reportpublishmanager-6f8dd75d64-btb4c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-6f8dd75d64                Error creating: pods "lorax-reportpublishmanager-6f8dd75d64-4vfxl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-6f8dd75d64                Error creating: pods "lorax-reportpublishmanager-6f8dd75d64-cst7l" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
42s         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-6f8dd75d64                Error creating: pods "lorax-reportpublishmanager-6f8dd75d64-l2z7m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-6ff7947fc4                Error creating: pods "lorax-reportpublishmanager-6ff7947fc4-kg4rp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-6ff7947fc4                Error creating: pods "lorax-reportpublishmanager-6ff7947fc4-z2gpm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-6ff7947fc4                Error creating: pods "lorax-reportpublishmanager-6ff7947fc4-7896z" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
78s         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-6ff7947fc4                Error creating: pods "lorax-reportpublishmanager-6ff7947fc4-hd6nl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-79bc79d75d                Error creating: pods "lorax-reportpublishmanager-79bc79d75d-4nb6q" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-79bc79d75d                Error creating: pods "lorax-reportpublishmanager-79bc79d75d-tllv9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-79bc79d75d                Error creating: pods "lorax-reportpublishmanager-79bc79d75d-pngpv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m30s       Warning   FailedCreate              replicaset/lorax-reportpublishmanager-79bc79d75d                Error creating: pods "lorax-reportpublishmanager-79bc79d75d-xlf4w" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-79c6c74b4d                Error creating: pods "lorax-reportpublishmanager-79c6c74b4d-vh25q" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-79c6c74b4d                Error creating: pods "lorax-reportpublishmanager-79c6c74b4d-rnhcs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-79c6c74b4d                Error creating: pods "lorax-reportpublishmanager-79c6c74b4d-xzf7v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m3s        Warning   FailedCreate              replicaset/lorax-reportpublishmanager-79c6c74b4d                Error creating: pods "lorax-reportpublishmanager-79c6c74b4d-h6z7l" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-7cb6b6f458                Error creating: pods "lorax-reportpublishmanager-7cb6b6f458-882x2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-7cb6b6f458                Error creating: pods "lorax-reportpublishmanager-7cb6b6f458-7kkwx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-7cb6b6f458                Error creating: pods "lorax-reportpublishmanager-7cb6b6f458-fhgxj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
26s         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-7cb6b6f458                Error creating: pods "lorax-reportpublishmanager-7cb6b6f458-dnsrc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-85fcb8558b                Error creating: pods "lorax-reportpublishmanager-85fcb8558b-lfmgv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-85fcb8558b                Error creating: pods "lorax-reportpublishmanager-85fcb8558b-6cwpp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-85fcb8558b                Error creating: pods "lorax-reportpublishmanager-85fcb8558b-hpmrt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-ddcc6b64                  Error creating: pods "lorax-reportpublishmanager-ddcc6b64-btbhg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-ddcc6b64                  Error creating: pods "lorax-reportpublishmanager-ddcc6b64-qxcv5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-ddcc6b64                  Error creating: pods "lorax-reportpublishmanager-ddcc6b64-gbbc5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
6s          Warning   FailedCreate              replicaset/lorax-reportpublishmanager-ddcc6b64                  Error creating: pods "lorax-reportpublishmanager-ddcc6b64-f69zj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-f75747b5b                 Error creating: pods "lorax-reportpublishmanager-f75747b5b-22tw7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-f75747b5b                 Error creating: pods "lorax-reportpublishmanager-f75747b5b-nb89c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-reportpublishmanager-f75747b5b                 Error creating: pods "lorax-reportpublishmanager-f75747b5b-k9q58" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-searchmanager-5588bf669c                       Error creating: pods "lorax-searchmanager-5588bf669c-rs6qs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate              replicaset/lorax-searchmanager-5588bf669c                       Error creating: pods "lorax-searchmanager-5588bf669c-w6m8k" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-searchmanager-5588bf669c                       Error creating: pods "lorax-searchmanager-5588bf669c-x2d5z" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m29s       Warning   FailedCreate              replicaset/lorax-searchmanager-5588bf669c                       Error creating: pods "lorax-searchmanager-5588bf669c-jjqdt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate              replicaset/lorax-searchmanager-57f9fc4cb5                       Error creating: pods "lorax-searchmanager-57f9fc4cb5-tpq4p" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-searchmanager-57f9fc4cb5                       Error creating: pods "lorax-searchmanager-57f9fc4cb5-hxl78" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-searchmanager-57f9fc4cb5                       Error creating: pods "lorax-searchmanager-57f9fc4cb5-l8jrl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-searchmanager-57fd4db8f                        Error creating: pods "lorax-searchmanager-57fd4db8f-dsgrq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-searchmanager-57fd4db8f                        Error creating: pods "lorax-searchmanager-57fd4db8f-q8qfl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-searchmanager-57fd4db8f                        Error creating: pods "lorax-searchmanager-57fd4db8f-dp87m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-searchmanager-595f549689                       Error creating: pods "lorax-searchmanager-595f549689-c9zvw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-searchmanager-595f549689                       Error creating: pods "lorax-searchmanager-595f549689-m8h5r" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate              replicaset/lorax-searchmanager-595f549689                       Error creating: pods "lorax-searchmanager-595f549689-c5nqz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
19s         Warning   FailedCreate              replicaset/lorax-searchmanager-595f549689                       Error creating: pods "lorax-searchmanager-595f549689-xjn84" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
51m         Warning   FailedCreate              replicaset/lorax-searchmanager-5c4d488449                       Error creating: pods "lorax-searchmanager-5c4d488449-vfb2z" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-searchmanager-5c4d488449                       Error creating: pods "lorax-searchmanager-5c4d488449-cvt4d" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-searchmanager-5c4d488449                       Error creating: pods "lorax-searchmanager-5c4d488449-zgsfd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
77s         Warning   FailedCreate              replicaset/lorax-searchmanager-5c4d488449                       Error creating: pods "lorax-searchmanager-5c4d488449-wsptp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-searchmanager-5cbd688685                       Error creating: pods "lorax-searchmanager-5cbd688685-vlx9m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-searchmanager-5cbd688685                       Error creating: pods "lorax-searchmanager-5cbd688685-jz6b4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-searchmanager-5cbd688685                       Error creating: pods "lorax-searchmanager-5cbd688685-gq9vr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
28s         Warning   FailedCreate              replicaset/lorax-searchmanager-5cbd688685                       Error creating: pods "lorax-searchmanager-5cbd688685-xf476" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-searchmanager-5d75f85465                       Error creating: pods "lorax-searchmanager-5d75f85465-ljspd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-searchmanager-5d75f85465                       Error creating: pods "lorax-searchmanager-5d75f85465-vmm47" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
13m         Warning   FailedCreate              replicaset/lorax-searchmanager-5d75f85465                       Error creating: pods "lorax-searchmanager-5d75f85465-rqtkm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-searchmanager-5f95bf5bf                        Error creating: pods "lorax-searchmanager-5f95bf5bf-9lw5f" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-searchmanager-5f95bf5bf                        Error creating: pods "lorax-searchmanager-5f95bf5bf-bnp7h" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-searchmanager-5f95bf5bf                        Error creating: pods "lorax-searchmanager-5f95bf5bf-nwbr9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37s         Warning   FailedCreate              replicaset/lorax-searchmanager-5f95bf5bf                        Error creating: pods "lorax-searchmanager-5f95bf5bf-pfcbf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-searchmanager-79c6b8578f                       Error creating: pods "lorax-searchmanager-79c6b8578f-nsqn2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate              replicaset/lorax-searchmanager-79c6b8578f                       Error creating: pods "lorax-searchmanager-79c6b8578f-5vdch" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-searchmanager-79c6b8578f                       Error creating: pods "lorax-searchmanager-79c6b8578f-gqvnr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m40s       Warning   FailedCreate              replicaset/lorax-searchmanager-79c6b8578f                       Error creating: pods "lorax-searchmanager-79c6b8578f-vn9pk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate              replicaset/lorax-searchmanager-7d88df64bf                       Error creating: pods "lorax-searchmanager-7d88df64bf-vmjzk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-searchmanager-7d88df64bf                       Error creating: pods "lorax-searchmanager-7d88df64bf-cbl47" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-searchmanager-7d88df64bf                       Error creating: pods "lorax-searchmanager-7d88df64bf-m4729" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m59s       Warning   FailedCreate              replicaset/lorax-searchmanager-7d88df64bf                       Error creating: pods "lorax-searchmanager-7d88df64bf-2vtpb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
102s        Warning   FailedMount               pod/lorax-searchmanager-7d97d6dfcc-gn4zg                        MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
80s         Warning   FailedMount               pod/lorax-searchmanager-7d97d6dfcc-q2x5t                        MountVolume.SetUp failed for volume "lorax-config" : configmap "lorax-config" not found
53m         Warning   FailedCreate              replicaset/lorax-searchmanager-7f65b69969                       Error creating: pods "lorax-searchmanager-7f65b69969-9pqkv" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-searchmanager-7f65b69969                       Error creating: pods "lorax-searchmanager-7f65b69969-72zkl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-searchmanager-7f65b69969                       Error creating: pods "lorax-searchmanager-7f65b69969-5tdx5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m55s       Warning   FailedCreate              replicaset/lorax-searchmanager-7f65b69969                       Error creating: pods "lorax-searchmanager-7f65b69969-qs6wm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
43m         Warning   FailedCreate              replicaset/lorax-searchmanager-8475dfbb49                       Error creating: pods "lorax-searchmanager-8475dfbb49-mgq8q" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-searchmanager-8475dfbb49                       Error creating: pods "lorax-searchmanager-8475dfbb49-hbrcq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-searchmanager-8475dfbb49                       Error creating: pods "lorax-searchmanager-8475dfbb49-5dqvc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedGetResourceMetric   horizontalpodautoscaler/lorax-searchmanager-autoscaler          unable to get metrics for resource memory: no metrics returned from resource metrics API
47m         Warning   FailedCreate              replicaset/lorax-searchmanager-cf6668b77                        Error creating: pods "lorax-searchmanager-cf6668b77-xl6mw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-searchmanager-cf6668b77                        Error creating: pods "lorax-searchmanager-cf6668b77-j9bgx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-searchmanager-cf6668b77                        Error creating: pods "lorax-searchmanager-cf6668b77-pmnb5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-searchmanager-f76cdcfcc                        Error creating: pods "lorax-searchmanager-f76cdcfcc-r6w9m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-searchmanager-f76cdcfcc                        Error creating: pods "lorax-searchmanager-f76cdcfcc-zvzzz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
23m         Warning   FailedCreate              replicaset/lorax-searchmanager-f76cdcfcc                        Error creating: pods "lorax-searchmanager-f76cdcfcc-rwsv4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m16s       Warning   FailedCreate              replicaset/lorax-searchmanager-f76cdcfcc                        Error creating: pods "lorax-searchmanager-f76cdcfcc-wbb9t" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-searchmanager-fb54dff77                        Error creating: pods "lorax-searchmanager-fb54dff77-v8wsq" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-searchmanager-fb54dff77                        Error creating: pods "lorax-searchmanager-fb54dff77-q8w5m" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-searchmanager-fb54dff77                        Error creating: pods "lorax-searchmanager-fb54dff77-t5pgs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m34s       Warning   FailedCreate              replicaset/lorax-searchmanager-fb54dff77                        Error creating: pods "lorax-searchmanager-fb54dff77-fdzcm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-securityservice-55c498b866                     Error creating: pods "lorax-securityservice-55c498b866-b7mgb" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-securityservice-55c498b866                     Error creating: pods "lorax-securityservice-55c498b866-wxc8v" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-securityservice-55c498b866                     Error creating: pods "lorax-securityservice-55c498b866-n48cz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57s         Warning   FailedCreate              replicaset/lorax-securityservice-55c498b866                     Error creating: pods "lorax-securityservice-55c498b866-lq5kz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-securityservice-5b9b48c4                       Error creating: pods "lorax-securityservice-5b9b48c4-2l56q" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
36m         Warning   FailedCreate              replicaset/lorax-securityservice-5b9b48c4                       Error creating: pods "lorax-securityservice-5b9b48c4-nb5mh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-securityservice-5b9b48c4                       Error creating: pods "lorax-securityservice-5b9b48c4-lnjfr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m29s       Warning   FailedCreate              replicaset/lorax-securityservice-5b9b48c4                       Error creating: pods "lorax-securityservice-5b9b48c4-qnbj7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-securityservice-5d76cfb66                      Error creating: pods "lorax-securityservice-5d76cfb66-fzdqz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-securityservice-5d76cfb66                      Error creating: pods "lorax-securityservice-5d76cfb66-xkb7b" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
13m         Warning   FailedCreate              replicaset/lorax-securityservice-5d76cfb66                      Error creating: pods "lorax-securityservice-5d76cfb66-66b8j" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
53m         Warning   FailedCreate              replicaset/lorax-securityservice-5fd6596cbb                     Error creating: pods "lorax-securityservice-5fd6596cbb-5522r" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-securityservice-5fd6596cbb                     Error creating: pods "lorax-securityservice-5fd6596cbb-mgh5l" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-securityservice-5fd6596cbb                     Error creating: pods "lorax-securityservice-5fd6596cbb-4ldjl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
3m52s       Warning   FailedCreate              replicaset/lorax-securityservice-5fd6596cbb                     Error creating: pods "lorax-securityservice-5fd6596cbb-gdfnd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-securityservice-64757d4f7d                     Error creating: pods "lorax-securityservice-64757d4f7d-wtlzt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-securityservice-64757d4f7d                     Error creating: pods "lorax-securityservice-64757d4f7d-dnd9h" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
23m         Warning   FailedCreate              replicaset/lorax-securityservice-64757d4f7d                     Error creating: pods "lorax-securityservice-64757d4f7d-qpq7h" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m18s       Warning   FailedCreate              replicaset/lorax-securityservice-64757d4f7d                     Error creating: pods "lorax-securityservice-64757d4f7d-mz26x" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-securityservice-66958cb778                     Error creating: pods "lorax-securityservice-66958cb778-dq6m2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-securityservice-66958cb778                     Error creating: pods "lorax-securityservice-66958cb778-kkp4b" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-securityservice-66958cb778                     Error creating: pods "lorax-securityservice-66958cb778-dpbvm" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27s         Warning   FailedCreate              replicaset/lorax-securityservice-66958cb778                     Error creating: pods "lorax-securityservice-66958cb778-zqrzl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-securityservice-66b78457cd                     Error creating: pods "lorax-securityservice-66b78457cd-2vxxw" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
34m         Warning   FailedCreate              replicaset/lorax-securityservice-66b78457cd                     Error creating: pods "lorax-securityservice-66b78457cd-2thh2" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
17m         Warning   FailedCreate              replicaset/lorax-securityservice-66b78457cd                     Error creating: pods "lorax-securityservice-66b78457cd-f55wp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
43s         Warning   FailedCreate              replicaset/lorax-securityservice-66b78457cd                     Error creating: pods "lorax-securityservice-66b78457cd-z446d" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-securityservice-67c885b654                     Error creating: pods "lorax-securityservice-67c885b654-vxf4l" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-securityservice-67c885b654                     Error creating: pods "lorax-securityservice-67c885b654-2b6fz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-securityservice-67c885b654                     Error creating: pods "lorax-securityservice-67c885b654-d8tjt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
50m         Warning   FailedCreate              replicaset/lorax-securityservice-695b757654                     Error creating: pods "lorax-securityservice-695b757654-27krl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
33m         Warning   FailedCreate              replicaset/lorax-securityservice-695b757654                     Error creating: pods "lorax-securityservice-695b757654-plsr9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16m         Warning   FailedCreate              replicaset/lorax-securityservice-695b757654                     Error creating: pods "lorax-securityservice-695b757654-d6mgg" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
16s         Warning   FailedCreate              replicaset/lorax-securityservice-695b757654                     Error creating: pods "lorax-securityservice-695b757654-2rfzz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
43m         Warning   FailedCreate              replicaset/lorax-securityservice-6b65d9847d                     Error creating: pods "lorax-securityservice-6b65d9847d-xm86n" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-securityservice-6b65d9847d                     Error creating: pods "lorax-securityservice-6b65d9847d-jdwkt" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-securityservice-6b65d9847d                     Error creating: pods "lorax-securityservice-6b65d9847d-2kl5p" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-securityservice-6d94c549fd                     Error creating: pods "lorax-securityservice-6d94c549fd-rbvgp" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/lorax-securityservice-6d94c549fd                     Error creating: pods "lorax-securityservice-6d94c549fd-mjc6z" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-securityservice-6d94c549fd                     Error creating: pods "lorax-securityservice-6d94c549fd-txhms" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m32s       Warning   FailedCreate              replicaset/lorax-securityservice-6d94c549fd                     Error creating: pods "lorax-securityservice-6d94c549fd-zsvzz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/lorax-securityservice-7798cf45fd                     Error creating: pods "lorax-securityservice-7798cf45fd-rr72t" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate              replicaset/lorax-securityservice-7798cf45fd                     Error creating: pods "lorax-securityservice-7798cf45fd-jl8mc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/lorax-securityservice-7798cf45fd                     Error creating: pods "lorax-securityservice-7798cf45fd-fjvn9" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m42s       Warning   FailedCreate              replicaset/lorax-securityservice-7798cf45fd                     Error creating: pods "lorax-securityservice-7798cf45fd-vzb8c" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
54m         Warning   FailedCreate              replicaset/lorax-securityservice-78487b8f94                     Error creating: pods "lorax-securityservice-78487b8f94-nj2n5" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-securityservice-78487b8f94                     Error creating: pods "lorax-securityservice-78487b8f94-rs4bj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-securityservice-78487b8f94                     Error creating: pods "lorax-securityservice-78487b8f94-glh7n" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m1s        Warning   FailedCreate              replicaset/lorax-securityservice-78487b8f94                     Error creating: pods "lorax-securityservice-78487b8f94-zgcjh" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
44m         Warning   FailedCreate              replicaset/lorax-securityservice-86b7485fb6                     Error creating: pods "lorax-securityservice-86b7485fb6-wgqd6" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Warning   FailedCreate              replicaset/lorax-securityservice-86b7485fb6                     Error creating: pods "lorax-securityservice-86b7485fb6-7rdln" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
10m         Warning   FailedCreate              replicaset/lorax-securityservice-86b7485fb6                     Error creating: pods "lorax-securityservice-86b7485fb6-vnn9b" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
11m         Warning   FailedGetResourceMetric   horizontalpodautoscaler/lorax-securityservice-autoscaler        unable to get metrics for resource memory: no metrics returned from resource metrics API
54m         Warning   FailedCreate              replicaset/lorax-securityservice-b9bb8695d                      Error creating: pods "lorax-securityservice-b9bb8695d-hpwnk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
37m         Warning   FailedCreate              replicaset/lorax-securityservice-b9bb8695d                      Error creating: pods "lorax-securityservice-b9bb8695d-w2fqf" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
20m         Warning   FailedCreate              replicaset/lorax-securityservice-b9bb8695d                      Error creating: pods "lorax-securityservice-b9bb8695d-q22vj" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
4m15s       Warning   FailedCreate              replicaset/lorax-securityservice-b9bb8695d                      Error creating: pods "lorax-securityservice-b9bb8695d-f78wl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
47m         Warning   FailedCreate              replicaset/lorax-securityservice-c87b5d4c8                      Error creating: pods "lorax-securityservice-c87b5d4c8-6p2xc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/lorax-securityservice-c87b5d4c8                      Error creating: pods "lorax-securityservice-c87b5d4c8-fk285" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/lorax-securityservice-c87b5d4c8                      Error creating: pods "lorax-securityservice-c87b5d4c8-pqcfs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
2m          Warning   FailedGetPodsMetric       horizontalpodautoscaler/who-autoscaler                          unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
50m         Normal    Info                      configmap/who-dashboards                                        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: who-dashboards
    namespace: as-ciat
    resourceVersion: 751636959
    uid: 05f498ae-3014-4426-9475-927e3e8b0278
}: Received add dashboard event
50m         Normal    Info                      configmap/who-dashboards                                        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: who-dashboards
    namespace: as-ciat
    resourceVersion: 751636959
    uid: 05f498ae-3014-4426-9475-927e3e8b0278
}: Pushed add dashboard to kafka
12m         Warning   FailedGetPodsMetric       horizontalpodautoscaler/who-internal-autoscaler                 unable to get metric endpoint_all_meter: unable to fetch metrics from custom metrics API: the server could not find the metric endpoint_all_meter for pods
7m8s        Normal    Info                      configmap/who-internal-dashboards                               Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: who-internal-dashboards
    namespace: as-ciat
    resourceVersion: 770650188
    uid: 90a8fe2f-54a9-472f-8d4f-2f648548c9cf
}: Received remove dashboard event
7m8s        Normal    Info                      configmap/who-internal-dashboards                               Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: who-internal-dashboards
    namespace: as-ciat
    resourceVersion: 770650188
    uid: 90a8fe2f-54a9-472f-8d4f-2f648548c9cf
}: Pushed delete dashboard to kafka
50m         Normal    Info                      configmap/who-internal-dashboards                               Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: who-internal-dashboards
    namespace: as-ciat
    resourceVersion: 751637090
    uid: 90a8fe2f-54a9-472f-8d4f-2f648548c9cf
}: Pushed add dashboard to kafka
50m         Normal    Info                      configmap/who-internal-dashboards                               Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: who-internal-dashboards
    namespace: as-ciat
    resourceVersion: 751637090
    uid: 90a8fe2f-54a9-472f-8d4f-2f648548c9cf
}: Received add dashboard event
7m8s        Normal    Info                      configmap/who-internal-prometheus-k8s-rules                     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: who-internal-prometheus-k8s-rules
    namespace: as-ciat
    resourceVersion: 770650187
    uid: ad97d5ef-3d27-4d80-ac11-93eed7165c61
}: Pushed delete rule to kafka
49m         Normal    Info                      configmap/who-internal-prometheus-k8s-rules                     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: who-internal-prometheus-k8s-rules
    namespace: as-ciat
    resourceVersion: 751367784
    uid: ad97d5ef-3d27-4d80-ac11-93eed7165c61
}: Pushed rule to kafka
7m8s        Normal    Info                      configmap/who-internal-prometheus-k8s-rules                     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: who-internal-prometheus-k8s-rules
    namespace: as-ciat
    resourceVersion: 770650187
    uid: ad97d5ef-3d27-4d80-ac11-93eed7165c61
}: Received delete event
49m         Normal    Info                      configmap/who-internal-prometheus-k8s-rules                     Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: who-internal-prometheus-k8s-rules
    namespace: as-ciat
    resourceVersion: 751367784
    uid: ad97d5ef-3d27-4d80-ac11-93eed7165c61
}: Received add rule event
47s         Warning   FailedMount               pod/who-internal-who-deployment-65d44d6f4c-w8lnq                MountVolume.SetUp failed for volume "who-config" : configmap "who-internal-config" not found
46m         Warning   FailedCreate              replicaset/who-internal-who-deployment-65d44d6f4c               Error creating: pods "who-internal-who-deployment-65d44d6f4c-7p2ct" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
30m         Warning   FailedCreate              replicaset/who-internal-who-deployment-65d44d6f4c               Error creating: pods "who-internal-who-deployment-65d44d6f4c-h4xmk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
13m         Warning   FailedCreate              replicaset/who-internal-who-deployment-65d44d6f4c               Error creating: pods "who-internal-who-deployment-65d44d6f4c-z547w" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/who-internal-who-deployment-784cccc67f               Error creating: pods "who-internal-who-deployment-784cccc67f-c6k85" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
41m         Warning   FailedCreate              replicaset/who-internal-who-deployment-784cccc67f               Error creating: pods "who-internal-who-deployment-784cccc67f-z5pzs" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/who-internal-who-deployment-784cccc67f               Error creating: pods "who-internal-who-deployment-784cccc67f-z95rr" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m54s       Warning   FailedCreate              replicaset/who-internal-who-deployment-784cccc67f               Error creating: pods "who-internal-who-deployment-784cccc67f-mbngk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m13s       Normal    Info                      service/who-internal-who                                        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: who-internal-who
    namespace: as-ciat
    resourceVersion: 770650102
    uid: 6812769a-0c6a-4615-8002-f8cc8feff0f9
}: Deleted service monitor
43m         Normal    Info                      service/who-internal-who                                        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: who-internal-who
    namespace: as-ciat
    resourceVersion: 751637097
    uid: 6812769a-0c6a-4615-8002-f8cc8feff0f9
}: Received add service event
43m         Normal    Info                      service/who-internal-who                                        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: who-internal-who
    namespace: as-ciat
    resourceVersion: 751637097
    uid: 6812769a-0c6a-4615-8002-f8cc8feff0f9
}: Created service monitor
7m13s       Normal    Info                      service/who-internal-who                                        Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: who-internal-who
    namespace: as-ciat
    resourceVersion: 770650102
    uid: 6812769a-0c6a-4615-8002-f8cc8feff0f9
}: Received delete service event
49m         Normal    Info                      configmap/who-prometheus-k8s-rules                              Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: who-prometheus-k8s-rules
    namespace: as-ciat
    resourceVersion: 714897381
    uid: 286b8003-8359-4e3e-9d95-b6ac8f5f09fa
}: Received add rule event
49m         Normal    Info                      configmap/who-prometheus-k8s-rules                              Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: ConfigMap
    name: who-prometheus-k8s-rules
    namespace: as-ciat
    resourceVersion: 714897381
    uid: 286b8003-8359-4e3e-9d95-b6ac8f5f09fa
}: Pushed rule to kafka
47m         Warning   FailedCreate              replicaset/who-who-deployment-576cd44f87                        Error creating: pods "who-who-deployment-576cd44f87-8pl2f" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
31m         Warning   FailedCreate              replicaset/who-who-deployment-576cd44f87                        Error creating: pods "who-who-deployment-576cd44f87-wmknl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
14m         Warning   FailedCreate              replicaset/who-who-deployment-576cd44f87                        Error creating: pods "who-who-deployment-576cd44f87-p5zpc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
57m         Warning   FailedCreate              replicaset/who-who-deployment-7bbd669859                        Error creating: pods "who-who-deployment-7bbd669859-7hct7" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
40m         Warning   FailedCreate              replicaset/who-who-deployment-7bbd669859                        Error creating: pods "who-who-deployment-7bbd669859-gdznz" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
24m         Warning   FailedCreate              replicaset/who-who-deployment-7bbd669859                        Error creating: pods "who-who-deployment-7bbd669859-b42jd" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
7m33s       Warning   FailedCreate              replicaset/who-who-deployment-7bbd669859                        Error creating: pods "who-who-deployment-7bbd669859-ts64h" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
27m         Normal    Info                      service/who-who                                                 Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: who-who
    namespace: as-ciat
    resourceVersion: 751636962
    uid: e491e275-d6d1-4e6f-aff2-01c4972b1318
}: Created service monitor
27m         Normal    Info                      service/who-who                                                 Processed by AppCluster: dataplane Namespace: monitoring-agent Obj: class V1ObjectReference {
    apiVersion: v1
    fieldPath: null
    kind: Service
    name: who-who
    namespace: as-ciat
    resourceVersion: 751636962
    uid: e491e275-d6d1-4e6f-aff2-01c4972b1318
}: Received add service event
[20:January:2022:13:41:55]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[20:January:2022:13:44:45]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[20:January:2022:13:45:05]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn as-ciat who-internal-who-deployment-65d44d6f4c-w8lnq -oyaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    as.osvc.oracle.com/app-version: 49f19d6e243010f2154536e8a95e18c1c5f07b48
    as.osvc.oracle.com/managed: "true"
    kubectl.kubernetes.io/default-container: server
    kubernetes.io/psp: psp-protect-docker
    vault.security.banzaicloud.io/log-level: warn
    vault.security.banzaicloud.io/vault-addr: https://vault.query.corp.consul:8200
    vault.security.banzaicloud.io/vault-agent-configmap: as-ciat-va-configmap
    vault.security.banzaicloud.io/vault-env-daemon: "true"
    vault.security.banzaicloud.io/vault-path: k8s-iad-dataplane
    vault.security.banzaicloud.io/vault-role: as-ciat_role
    vault.security.banzaicloud.io/vault-skip-verify: "true"
  creationTimestamp: "2022-01-19T23:59:42Z"
  generateName: who-internal-who-deployment-65d44d6f4c-
  labels:
    app: who
    cluster: as-ciat
    isInternalDeployment: "true"
    pod-template-hash: 65d44d6f4c
  name: who-internal-who-deployment-65d44d6f4c-w8lnq
  namespace: as-ciat
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: who-internal-who-deployment-65d44d6f4c
    uid: 19ef980f-c82a-4a75-af04-fa286e5426c0
  resourceVersion: "769488722"
  selfLink: /api/v1/namespaces/as-ciat/pods/who-internal-who-deployment-65d44d6f4c-w8lnq
  uid: 0d7de340-9240-4e0c-bfc3-8f1fe27af77c
spec:
  containers:
  - args:
    - agent
    - -config
    - /vault/config/config.hcl
    env:
    - name: VAULT_ADDR
      value: https://vault.query.corp.consul:8200
    - name: VAULT_SKIP_VERIFY
      value: "true"
    image: docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5
    imagePullPolicy: IfNotPresent
    name: vault-agent
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 100m
        memory: 128Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        add:
        - IPC_LOCK
        - SYS_PTRACE
        drop:
        - NET_RAW
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /vault/
      name: vault-env
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-zrljs
      readOnly: true
    - mountPath: /vault/secrets
      name: agent-secrets
    - mountPath: /vault/config/config.hcl
      name: agent-configmap
      readOnly: true
      subPath: config.hcl
  - args:
    - /bin/sh
    - -c
    - java     -XX:+UnlockExperimentalVMOptions     -XX:+UseCGroupMemoryLimitForHeap     -XX:+HeapDumpOnOutOfMemoryError     -XX:HeapDumpPath=/app/logs/heap_pid%%p.hprof     -XX:+UseG1GC     -XX:MaxGCPauseMillis=250     -Dorg.oracle.fusion.service.kubernetes.namespace=${NAMESPACE}     -Dorg.oracle.fusion.service.ContextPath=/app/config     -Dorg.oracle.fusion.service.resourcePath=/app/config     -Dorg.oracle.fusion.service.hostName=0.0.0.0     -Dorg.oracle.fusion.service.portNumber=7001     -Dorg.oracle.fusion.service.consulHttpApiUrl=${CONSUL}     -Dconsul.base.address=${CONSUL}     -Dorg.oracle.fusion.service.path=${SERVICE_PATH}     -Dorg.oracle.fusion.service.devMode=${DEVMODE}     -Dorg.oracle.fusion.service.buiBasePath=${BUI_BASE_PATH}     -Dorg.oracle.fusion.service.clusterName=${CLUSTER}     -Djava.rmi.server.hostname=who     -Dlogback.configurationFile=/app/config/live/logback.xml     -Dorg.oracle.fusion.service.jolokiaRealmFile=/app/config/realm.properties     -Dorg.oracle.fusion.service.jolokiaRole=asJolokiaRole     -Duser.dir=/app     ${JAVA_OPTS}     -cp     /app/*:/app/lib/*     org.oracle.service.router.Host
    command:
    - /vault/vault-env
    env:
    - name: JAVA_OPTS
      value: |-
        -Dorg.oracle.fusion.service.k8s=true -Dorg.oracle.fusion.service.kubernetes.domain=cluster.local -Dorg.oracle.fusion.service.kubernetes.singleVersion=true -Dorg.oracle.fusion.service.jaeger.enabled=true -Dorg.oracle.fusion.service.forwardedForHeader=X-Original-Forwarded-For -Dorg.oracle.fusion.service.defaultVersion=20220119-23563149-ashd -Dorg.oracle.fusion.service.consul.certPath=/consul/ca.crt -Dorg.oracle.fusion.service.vault.token-path=/vault/.vault-token -Dorg.oracle.fusion.service.vault.enabled=true -Dorg.oracle.fusion.service.vault.truststore=/vault/ca.crt -Dorg.oracle.fusion.service.vault.ssl.enabled=true
        -Dkafka.bootstrap.servers=bui-kafka-kafka-brokers.bui:9093 -Dkafka.security.protocol=SSL -Dkafka.auth.keystore.path=/vault/secrets/keystore.p12 -Dkafka.auth.truststore.path=/vault/secrets/truststore.p12 -Dkafka.auth.issuing.ca.path=/vault/secrets/issuing_ca.pem -Dkafka.auth.private.key.path=/vault/secrets/private_key.pem -Dkafka.auth.certificate.path=/vault/secrets/certificate.pem
        -Dlogging.api.kafka.enabled=false -Dschema.registry.url=https://schemaregistry-proxy-kafka.service.iad-dataplane.corp.consul -Dorg.oracle.fusion.service.who.internalOnly=true
    - name: CLUSTER
      value: as-ciat
    - name: VERSION
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.namespace
    - name: SERVICE_PATH
      value: /AgentWeb
    - name: BUI_BASE_PATH
      value: as-ciat.corp.channels.ocs.oc-test.com
    - name: DEVMODE
      value: "true"
    - name: NAMESPACE
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.namespace
    - name: HOST_IP
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.hostIP
    - name: CONSUL
      value: https://$(HOST_IP):8501
    - name: CONSUL_TOKEN
      value: vault:/cpe_consul/creds/as-ciat_consul_role#token
    - name: JAEGER_AGENT_HOST
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.hostIP
    - name: JAEGER_AGENT_PORT
      value: "6831"
    - name: JAEGER_AGENT_PROTOCOL
      value: udp
    - name: VAULT_ADDR
      value: https://vault.query.corp.consul:8200
    - name: VAULT_SKIP_VERIFY
      value: "true"
    - name: VAULT_AUTH_METHOD
      value: jwt
    - name: VAULT_PATH
      value: k8s-iad-dataplane
    - name: VAULT_ROLE
      value: as-ciat_role
    - name: VAULT_IGNORE_MISSING_SECRETS
      value: "false"
    - name: VAULT_ENV_PASSTHROUGH
    - name: VAULT_JSON_LOG
      value: "false"
    - name: VAULT_CLIENT_TIMEOUT
      value: 10s
    - name: VAULT_LOG_LEVEL
      value: warn
    - name: VAULT_ENV_DAEMON
      value: "true"
    image: iad.ocir.io/osvccorp/bui/who:master-2022.01.19-17.33.25-49f19d6e
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 5
      httpGet:
        path: /AgentWeb/ping
        port: 7001
        scheme: HTTP
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    name: server
    ports:
    - containerPort: 7001
      name: http
      protocol: TCP
    readinessProbe:
      failureThreshold: 10
      httpGet:
        path: /AgentWeb/ready
        port: 7001
        scheme: HTTP
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    resources:
      limits:
        cpu: "1"
        memory: 4Gi
      requests:
        cpu: 500m
        memory: 2Gi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - NET_RAW
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /app/config/live
      name: who-config
    - mountPath: /consul
      name: consul-ca
      readOnly: true
    - mountPath: /vault/ca.crt
      name: vault-ca
      readOnly: true
      subPath: ca.crt
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-zrljs
      readOnly: true
    - mountPath: /vault/
      name: vault-env
    - mountPath: /vault/secrets
      name: agent-secrets
  dnsConfig:
    options:
    - name: ndots
      value: "2"
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  imagePullSecrets:
  - name: osvc-token
  initContainers:
  - command:
    - sh
    - -c
    - cp /usr/local/bin/vault-env /vault/
    image: docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3
    imagePullPolicy: IfNotPresent
    name: copy-vault-env
    resources:
      limits:
        cpu: 250m
        memory: 64Mi
      requests:
        cpu: 50m
        memory: 64Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - NET_RAW
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /vault/
      name: vault-env
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-zrljs
      readOnly: true
  nodeName: 10.11.9.208
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  shareProcessNamespace: true
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - configMap:
      defaultMode: 420
      items:
      - key: logback.xml
        path: logback.xml
      name: who-internal-config
    name: who-config
  - name: consul-ca
    secret:
      defaultMode: 420
      items:
      - key: ca.crt
        path: ca.crt
      secretName: consul-tls
  - name: vault-ca
    secret:
      defaultMode: 420
      items:
      - key: ca.crt
        path: ca.crt
      secretName: vault-tls
  - name: default-token-zrljs
    secret:
      defaultMode: 420
      secretName: default-token-zrljs
  - emptyDir:
      medium: Memory
    name: vault-env
  - emptyDir:
      medium: Memory
    name: agent-secrets
  - configMap:
      defaultMode: 420
      items:
      - key: config.hcl
        path: config.hcl
      name: as-ciat-va-configmap
    name: agent-configmap
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2022-01-19T23:59:43Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2022-01-19T23:59:53Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2022-01-19T23:59:53Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2022-01-19T23:59:42Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: docker://141f1a0149672fb04ca247c219e62b550bc74dd58ffef551599581354b022a87
    image: iad.ocir.io/osvccorp/bui/who:master-2022.01.19-17.33.25-49f19d6e
    imageID: docker-pullable://iad.ocir.io/osvccorp/bui/who@sha256:8458ebf460e2aa3364312c89ee9a7990f70750ff2c37a7e7c248a31872b4fbbb
    lastState: {}
    name: server
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2022-01-19T23:59:43Z"
  - containerID: docker://1815ac26e20e2de92b5d2ec8c7a88d0b079004ed64555b5738a29ad663e386b4
    image: docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5
    imageID: docker-pullable://docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault@sha256:57438e97b72c05368ca06f4d5f34667e965248635718001abdea8fa11f18adad
    lastState: {}
    name: vault-agent
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2022-01-19T23:59:43Z"
  hostIP: 10.11.9.208
  initContainerStatuses:
  - containerID: docker://6825d99219efe834fc0264e5016bdb2ab8fee64b54f900006df3b08edb113e60
    image: docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3
    imageID: docker-pullable://docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env@sha256:7435d1c641c0aa0fd4faaff32ab189b29d3d909b100d36d4d90df3b79cee8ce3
    lastState: {}
    name: copy-vault-env
    ready: true
    restartCount: 0
    state:
      terminated:
        containerID: docker://6825d99219efe834fc0264e5016bdb2ab8fee64b54f900006df3b08edb113e60
        exitCode: 0
        finishedAt: "2022-01-19T23:59:43Z"
        reason: Completed
        startedAt: "2022-01-19T23:59:43Z"
  phase: Running
  podIP: 10.245.11.179
  podIPs:
  - ip: 10.245.11.179
  qosClass: Burstable
  startTime: "2022-01-19T23:59:42Z"
[20:January:2022:13:45:22]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[20:January:2022:13:45:52]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ #kubectl patch ingress rasa-x-1586547433 -n rasa -p '{"metadata":{"finalizers":null}}'
[20:January:2022:13:50:52]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ #kgpn as-ciat who-internal-who-deployment-65d44d6f4c-w8lnq -oyaml
[20:January:2022:13:50:57]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn as-ciat
NAME                                           READY   STATUS    RESTARTS   AGE   IP              NODE          NOMINATED NODE   READINESS GATES
lorax-analyticsmanager-75d985dfdf-6jmjm        2/2     Running   0          13h   10.245.14.74    10.11.9.104   <none>           <none>
lorax-analyticsmanager-75d985dfdf-mmxrw        2/2     Running   0          13h   10.245.13.192   10.11.9.145   <none>           <none>
lorax-cachehost-647b6cf7b4-4v6n7               2/2     Running   0          12h   10.245.13.197   10.11.9.145   <none>           <none>
lorax-cachehost-647b6cf7b4-jsqlv               2/2     Running   2          12h   10.245.27.178   10.11.8.60    <none>           <none>
lorax-cachehost-6c8948bf68-8gmdf               2/2     Running   0          13h   10.245.19.21    10.11.9.238   <none>           <none>
lorax-cachehost-6c8948bf68-mzvx7               2/2     Running   0          13h   10.245.26.8     10.11.8.67    <none>           <none>
lorax-cachehost-6c8948bf68-vf26t               2/2     Running   3          13h   10.245.15.225   10.11.9.78    <none>           <none>
lorax-commandmanager-586d58947d-2h89f          2/2     Running   0          13h   10.245.28.203   10.11.9.56    <none>           <none>
lorax-commandmanager-586d58947d-2rcmp          2/2     Running   0          13h   10.245.11.175   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-c4482          2/2     Running   0          13h   10.245.27.176   10.11.8.60    <none>           <none>
lorax-contextmanager-67956c7584-c4xkw          2/2     Running   0          13h   10.245.11.177   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-h82q5          2/2     Running   0          13h   10.245.3.92     10.11.8.232   <none>           <none>
lorax-customizationmanager-657bbf9586-rsbqg    2/2     Running   0          13h   10.245.29.214   10.11.9.62    <none>           <none>
lorax-elementmanager-7f74646f4d-n4cqx          2/2     Running   0          13h   10.245.28.39    10.11.8.31    <none>           <none>
lorax-eventmanager-759c5b9484-ch28f            2/2     Running   0          13h   10.245.15.226   10.11.9.78    <none>           <none>
lorax-eventworker-7c5b779bc-z4bb5              2/2     Running   0          13h   10.245.22.102   10.11.8.250   <none>           <none>
lorax-realtimedatamanager-866cd5886b-qq54f     2/2     Running   0          13h   10.245.29.213   10.11.9.62    <none>           <none>
lorax-reportpublishmanager-58445646c4-9ztc7    2/2     Running   0          13h   10.245.11.176   10.11.9.208   <none>           <none>
lorax-searchmanager-7d97d6dfcc-gn4zg           2/2     Running   0          13h   10.245.29.212   10.11.9.62    <none>           <none>
lorax-searchmanager-7d97d6dfcc-q2x5t           2/2     Running   0          13h   10.245.14.73    10.11.9.104   <none>           <none>
who-internal-who-deployment-65d44d6f4c-w8lnq   2/2     Running   0          13h   10.245.11.179   10.11.9.208   <none>           <none>
who-who-deployment-576cd44f87-4dr8c            2/2     Running   0          13h   10.245.9.42     10.11.9.136   <none>           <none>
[20:January:2022:13:51:13]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ k patch pod -n as-ciat who-internal-who-deployment-65d44d6f4c-w8lnq -p '{"metadata":{"finalizers":null}}'
pod/who-internal-who-deployment-65d44d6f4c-w8lnq patched (no change)
[20:January:2022:13:52:28]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn as-ciat
NAME                                           READY   STATUS    RESTARTS   AGE   IP              NODE          NOMINATED NODE   READINESS GATES
lorax-analyticsmanager-75d985dfdf-6jmjm        2/2     Running   0          13h   10.245.14.74    10.11.9.104   <none>           <none>
lorax-analyticsmanager-75d985dfdf-mmxrw        2/2     Running   0          13h   10.245.13.192   10.11.9.145   <none>           <none>
lorax-cachehost-647b6cf7b4-4v6n7               2/2     Running   0          12h   10.245.13.197   10.11.9.145   <none>           <none>
lorax-cachehost-647b6cf7b4-jsqlv               2/2     Running   2          12h   10.245.27.178   10.11.8.60    <none>           <none>
lorax-cachehost-6c8948bf68-8gmdf               2/2     Running   0          13h   10.245.19.21    10.11.9.238   <none>           <none>
lorax-cachehost-6c8948bf68-mzvx7               2/2     Running   0          13h   10.245.26.8     10.11.8.67    <none>           <none>
lorax-cachehost-6c8948bf68-vf26t               2/2     Running   3          13h   10.245.15.225   10.11.9.78    <none>           <none>
lorax-commandmanager-586d58947d-2h89f          2/2     Running   0          13h   10.245.28.203   10.11.9.56    <none>           <none>
lorax-commandmanager-586d58947d-2rcmp          2/2     Running   0          13h   10.245.11.175   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-c4482          2/2     Running   0          13h   10.245.27.176   10.11.8.60    <none>           <none>
lorax-contextmanager-67956c7584-c4xkw          2/2     Running   0          13h   10.245.11.177   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-h82q5          2/2     Running   0          13h   10.245.3.92     10.11.8.232   <none>           <none>
lorax-customizationmanager-657bbf9586-rsbqg    2/2     Running   0          13h   10.245.29.214   10.11.9.62    <none>           <none>
lorax-elementmanager-7f74646f4d-n4cqx          2/2     Running   0          13h   10.245.28.39    10.11.8.31    <none>           <none>
lorax-eventmanager-759c5b9484-ch28f            2/2     Running   0          13h   10.245.15.226   10.11.9.78    <none>           <none>
lorax-eventworker-7c5b779bc-z4bb5              2/2     Running   0          13h   10.245.22.102   10.11.8.250   <none>           <none>
lorax-realtimedatamanager-866cd5886b-qq54f     2/2     Running   0          13h   10.245.29.213   10.11.9.62    <none>           <none>
lorax-reportpublishmanager-58445646c4-9ztc7    2/2     Running   0          13h   10.245.11.176   10.11.9.208   <none>           <none>
lorax-searchmanager-7d97d6dfcc-gn4zg           2/2     Running   0          13h   10.245.29.212   10.11.9.62    <none>           <none>
lorax-searchmanager-7d97d6dfcc-q2x5t           2/2     Running   0          13h   10.245.14.73    10.11.9.104   <none>           <none>
who-internal-who-deployment-65d44d6f4c-w8lnq   2/2     Running   0          13h   10.245.11.179   10.11.9.208   <none>           <none>
who-who-deployment-576cd44f87-4dr8c            2/2     Running   0          13h   10.245.9.42     10.11.9.136   <none>           <none>
[20:January:2022:13:52:38]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn as-ciat who-internal-who-deployment-65d44d6f4c-w8lnq -oyaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    as.osvc.oracle.com/app-version: 49f19d6e243010f2154536e8a95e18c1c5f07b48
    as.osvc.oracle.com/managed: "true"
    kubectl.kubernetes.io/default-container: server
    kubernetes.io/psp: psp-protect-docker
    vault.security.banzaicloud.io/log-level: warn
    vault.security.banzaicloud.io/vault-addr: https://vault.query.corp.consul:8200
    vault.security.banzaicloud.io/vault-agent-configmap: as-ciat-va-configmap
    vault.security.banzaicloud.io/vault-env-daemon: "true"
    vault.security.banzaicloud.io/vault-path: k8s-iad-dataplane
    vault.security.banzaicloud.io/vault-role: as-ciat_role
    vault.security.banzaicloud.io/vault-skip-verify: "true"
  creationTimestamp: "2022-01-19T23:59:42Z"
  generateName: who-internal-who-deployment-65d44d6f4c-
  labels:
    app: who
    cluster: as-ciat
    isInternalDeployment: "true"
    pod-template-hash: 65d44d6f4c
  name: who-internal-who-deployment-65d44d6f4c-w8lnq
  namespace: as-ciat
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: who-internal-who-deployment-65d44d6f4c
    uid: 19ef980f-c82a-4a75-af04-fa286e5426c0
  resourceVersion: "769488722"
  selfLink: /api/v1/namespaces/as-ciat/pods/who-internal-who-deployment-65d44d6f4c-w8lnq
  uid: 0d7de340-9240-4e0c-bfc3-8f1fe27af77c
spec:
  containers:
  - args:
    - agent
    - -config
    - /vault/config/config.hcl
    env:
    - name: VAULT_ADDR
      value: https://vault.query.corp.consul:8200
    - name: VAULT_SKIP_VERIFY
      value: "true"
    image: docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5
    imagePullPolicy: IfNotPresent
    name: vault-agent
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 100m
        memory: 128Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        add:
        - IPC_LOCK
        - SYS_PTRACE
        drop:
        - NET_RAW
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /vault/
      name: vault-env
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-zrljs
      readOnly: true
    - mountPath: /vault/secrets
      name: agent-secrets
    - mountPath: /vault/config/config.hcl
      name: agent-configmap
      readOnly: true
      subPath: config.hcl
  - args:
    - /bin/sh
    - -c
    - java     -XX:+UnlockExperimentalVMOptions     -XX:+UseCGroupMemoryLimitForHeap     -XX:+HeapDumpOnOutOfMemoryError     -XX:HeapDumpPath=/app/logs/heap_pid%%p.hprof     -XX:+UseG1GC     -XX:MaxGCPauseMillis=250     -Dorg.oracle.fusion.service.kubernetes.namespace=${NAMESPACE}     -Dorg.oracle.fusion.service.ContextPath=/app/config     -Dorg.oracle.fusion.service.resourcePath=/app/config     -Dorg.oracle.fusion.service.hostName=0.0.0.0     -Dorg.oracle.fusion.service.portNumber=7001     -Dorg.oracle.fusion.service.consulHttpApiUrl=${CONSUL}     -Dconsul.base.address=${CONSUL}     -Dorg.oracle.fusion.service.path=${SERVICE_PATH}     -Dorg.oracle.fusion.service.devMode=${DEVMODE}     -Dorg.oracle.fusion.service.buiBasePath=${BUI_BASE_PATH}     -Dorg.oracle.fusion.service.clusterName=${CLUSTER}     -Djava.rmi.server.hostname=who     -Dlogback.configurationFile=/app/config/live/logback.xml     -Dorg.oracle.fusion.service.jolokiaRealmFile=/app/config/realm.properties     -Dorg.oracle.fusion.service.jolokiaRole=asJolokiaRole     -Duser.dir=/app     ${JAVA_OPTS}     -cp     /app/*:/app/lib/*     org.oracle.service.router.Host
    command:
    - /vault/vault-env
    env:
    - name: JAVA_OPTS
      value: |-
        -Dorg.oracle.fusion.service.k8s=true -Dorg.oracle.fusion.service.kubernetes.domain=cluster.local -Dorg.oracle.fusion.service.kubernetes.singleVersion=true -Dorg.oracle.fusion.service.jaeger.enabled=true -Dorg.oracle.fusion.service.forwardedForHeader=X-Original-Forwarded-For -Dorg.oracle.fusion.service.defaultVersion=20220119-23563149-ashd -Dorg.oracle.fusion.service.consul.certPath=/consul/ca.crt -Dorg.oracle.fusion.service.vault.token-path=/vault/.vault-token -Dorg.oracle.fusion.service.vault.enabled=true -Dorg.oracle.fusion.service.vault.truststore=/vault/ca.crt -Dorg.oracle.fusion.service.vault.ssl.enabled=true
        -Dkafka.bootstrap.servers=bui-kafka-kafka-brokers.bui:9093 -Dkafka.security.protocol=SSL -Dkafka.auth.keystore.path=/vault/secrets/keystore.p12 -Dkafka.auth.truststore.path=/vault/secrets/truststore.p12 -Dkafka.auth.issuing.ca.path=/vault/secrets/issuing_ca.pem -Dkafka.auth.private.key.path=/vault/secrets/private_key.pem -Dkafka.auth.certificate.path=/vault/secrets/certificate.pem
        -Dlogging.api.kafka.enabled=false -Dschema.registry.url=https://schemaregistry-proxy-kafka.service.iad-dataplane.corp.consul -Dorg.oracle.fusion.service.who.internalOnly=true
    - name: CLUSTER
      value: as-ciat
    - name: VERSION
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.namespace
    - name: SERVICE_PATH
      value: /AgentWeb
    - name: BUI_BASE_PATH
      value: as-ciat.corp.channels.ocs.oc-test.com
    - name: DEVMODE
      value: "true"
    - name: NAMESPACE
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.namespace
    - name: HOST_IP
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.hostIP
    - name: CONSUL
      value: https://$(HOST_IP):8501
    - name: CONSUL_TOKEN
      value: vault:/cpe_consul/creds/as-ciat_consul_role#token
    - name: JAEGER_AGENT_HOST
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.hostIP
    - name: JAEGER_AGENT_PORT
      value: "6831"
    - name: JAEGER_AGENT_PROTOCOL
      value: udp
    - name: VAULT_ADDR
      value: https://vault.query.corp.consul:8200
    - name: VAULT_SKIP_VERIFY
      value: "true"
    - name: VAULT_AUTH_METHOD
      value: jwt
    - name: VAULT_PATH
      value: k8s-iad-dataplane
    - name: VAULT_ROLE
      value: as-ciat_role
    - name: VAULT_IGNORE_MISSING_SECRETS
      value: "false"
    - name: VAULT_ENV_PASSTHROUGH
    - name: VAULT_JSON_LOG
      value: "false"
    - name: VAULT_CLIENT_TIMEOUT
      value: 10s
    - name: VAULT_LOG_LEVEL
      value: warn
    - name: VAULT_ENV_DAEMON
      value: "true"
    image: iad.ocir.io/osvccorp/bui/who:master-2022.01.19-17.33.25-49f19d6e
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 5
      httpGet:
        path: /AgentWeb/ping
        port: 7001
        scheme: HTTP
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    name: server
    ports:
    - containerPort: 7001
      name: http
      protocol: TCP
    readinessProbe:
      failureThreshold: 10
      httpGet:
        path: /AgentWeb/ready
        port: 7001
        scheme: HTTP
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    resources:
      limits:
        cpu: "1"
        memory: 4Gi
      requests:
        cpu: 500m
        memory: 2Gi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - NET_RAW
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /app/config/live
      name: who-config
    - mountPath: /consul
      name: consul-ca
      readOnly: true
    - mountPath: /vault/ca.crt
      name: vault-ca
      readOnly: true
      subPath: ca.crt
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-zrljs
      readOnly: true
    - mountPath: /vault/
      name: vault-env
    - mountPath: /vault/secrets
      name: agent-secrets
  dnsConfig:
    options:
    - name: ndots
      value: "2"
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  imagePullSecrets:
  - name: osvc-token
  initContainers:
  - command:
    - sh
    - -c
    - cp /usr/local/bin/vault-env /vault/
    image: docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3
    imagePullPolicy: IfNotPresent
    name: copy-vault-env
    resources:
      limits:
        cpu: 250m
        memory: 64Mi
      requests:
        cpu: 50m
        memory: 64Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - NET_RAW
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /vault/
      name: vault-env
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-zrljs
      readOnly: true
  nodeName: 10.11.9.208
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  shareProcessNamespace: true
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - configMap:
      defaultMode: 420
      items:
      - key: logback.xml
        path: logback.xml
      name: who-internal-config
    name: who-config
  - name: consul-ca
    secret:
      defaultMode: 420
      items:
      - key: ca.crt
        path: ca.crt
      secretName: consul-tls
  - name: vault-ca
    secret:
      defaultMode: 420
      items:
      - key: ca.crt
        path: ca.crt
      secretName: vault-tls
  - name: default-token-zrljs
    secret:
      defaultMode: 420
      secretName: default-token-zrljs
  - emptyDir:
      medium: Memory
    name: vault-env
  - emptyDir:
      medium: Memory
    name: agent-secrets
  - configMap:
      defaultMode: 420
      items:
      - key: config.hcl
        path: config.hcl
      name: as-ciat-va-configmap
    name: agent-configmap
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2022-01-19T23:59:43Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2022-01-19T23:59:53Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2022-01-19T23:59:53Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2022-01-19T23:59:42Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: docker://141f1a0149672fb04ca247c219e62b550bc74dd58ffef551599581354b022a87
    image: iad.ocir.io/osvccorp/bui/who:master-2022.01.19-17.33.25-49f19d6e
    imageID: docker-pullable://iad.ocir.io/osvccorp/bui/who@sha256:8458ebf460e2aa3364312c89ee9a7990f70750ff2c37a7e7c248a31872b4fbbb
    lastState: {}
    name: server
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2022-01-19T23:59:43Z"
  - containerID: docker://1815ac26e20e2de92b5d2ec8c7a88d0b079004ed64555b5738a29ad663e386b4
    image: docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5
    imageID: docker-pullable://docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault@sha256:57438e97b72c05368ca06f4d5f34667e965248635718001abdea8fa11f18adad
    lastState: {}
    name: vault-agent
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2022-01-19T23:59:43Z"
  hostIP: 10.11.9.208
  initContainerStatuses:
  - containerID: docker://6825d99219efe834fc0264e5016bdb2ab8fee64b54f900006df3b08edb113e60
    image: docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3
    imageID: docker-pullable://docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env@sha256:7435d1c641c0aa0fd4faaff32ab189b29d3d909b100d36d4d90df3b79cee8ce3
    lastState: {}
    name: copy-vault-env
    ready: true
    restartCount: 0
    state:
      terminated:
        containerID: docker://6825d99219efe834fc0264e5016bdb2ab8fee64b54f900006df3b08edb113e60
        exitCode: 0
        finishedAt: "2022-01-19T23:59:43Z"
        reason: Completed
        startedAt: "2022-01-19T23:59:43Z"
  phase: Running
  podIP: 10.245.11.179
  podIPs:
  - ip: 10.245.11.179
  qosClass: Burstable
  startTime: "2022-01-19T23:59:42Z"
[20:January:2022:13:52:54]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kdpn as-ciat who-internal-who-deployment-65d44d6f4c-w8lnq
Name:         who-internal-who-deployment-65d44d6f4c-w8lnq
Namespace:    as-ciat
Priority:     0
Node:         10.11.9.208/10.11.9.208
Start Time:   Wed, 19 Jan 2022 23:59:42 +0000
Labels:       app=who
              cluster=as-ciat
              isInternalDeployment=true
              pod-template-hash=65d44d6f4c
Annotations:  as.osvc.oracle.com/app-version: 49f19d6e243010f2154536e8a95e18c1c5f07b48
              as.osvc.oracle.com/managed: true
              kubectl.kubernetes.io/default-container: server
              kubernetes.io/psp: psp-protect-docker
              vault.security.banzaicloud.io/log-level: warn
              vault.security.banzaicloud.io/vault-addr: https://vault.query.corp.consul:8200
              vault.security.banzaicloud.io/vault-agent-configmap: as-ciat-va-configmap
              vault.security.banzaicloud.io/vault-env-daemon: true
              vault.security.banzaicloud.io/vault-path: k8s-iad-dataplane
              vault.security.banzaicloud.io/vault-role: as-ciat_role
              vault.security.banzaicloud.io/vault-skip-verify: true
Status:       Running
IP:           10.245.11.179
IPs:
  IP:           10.245.11.179
Controlled By:  ReplicaSet/who-internal-who-deployment-65d44d6f4c
Init Containers:
  copy-vault-env:
    Container ID:  docker://6825d99219efe834fc0264e5016bdb2ab8fee64b54f900006df3b08edb113e60
    Image:         docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3
    Image ID:      docker-pullable://docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env@sha256:7435d1c641c0aa0fd4faaff32ab189b29d3d909b100d36d4d90df3b79cee8ce3
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -c
      cp /usr/local/bin/vault-env /vault/
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 19 Jan 2022 23:59:43 +0000
      Finished:     Wed, 19 Jan 2022 23:59:43 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     250m
      memory:  64Mi
    Requests:
      cpu:        50m
      memory:     64Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zrljs (ro)
      /vault/ from vault-env (rw)
Containers:
  vault-agent:
    Container ID:  docker://1815ac26e20e2de92b5d2ec8c7a88d0b079004ed64555b5738a29ad663e386b4
    Image:         docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5
    Image ID:      docker-pullable://docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault@sha256:57438e97b72c05368ca06f4d5f34667e965248635718001abdea8fa11f18adad
    Port:          <none>
    Host Port:     <none>
    Args:
      agent
      -config
      /vault/config/config.hcl
    State:          Running
      Started:      Wed, 19 Jan 2022 23:59:43 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  128Mi
    Requests:
      cpu:     100m
      memory:  128Mi
    Environment:
      VAULT_ADDR:         https://vault.query.corp.consul:8200
      VAULT_SKIP_VERIFY:  true
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zrljs (ro)
      /vault/ from vault-env (rw)
      /vault/config/config.hcl from agent-configmap (ro,path="config.hcl")
      /vault/secrets from agent-secrets (rw)
  server:
    Container ID:  docker://141f1a0149672fb04ca247c219e62b550bc74dd58ffef551599581354b022a87
    Image:         iad.ocir.io/osvccorp/bui/who:master-2022.01.19-17.33.25-49f19d6e
    Image ID:      docker-pullable://iad.ocir.io/osvccorp/bui/who@sha256:8458ebf460e2aa3364312c89ee9a7990f70750ff2c37a7e7c248a31872b4fbbb
    Port:          7001/TCP
    Host Port:     0/TCP
    Command:
      /vault/vault-env
    Args:
      /bin/sh
      -c
      java     -XX:+UnlockExperimentalVMOptions     -XX:+UseCGroupMemoryLimitForHeap     -XX:+HeapDumpOnOutOfMemoryError     -XX:HeapDumpPath=/app/logs/heap_pid%%p.hprof     -XX:+UseG1GC     -XX:MaxGCPauseMillis=250     -Dorg.oracle.fusion.service.kubernetes.namespace=${NAMESPACE}     -Dorg.oracle.fusion.service.ContextPath=/app/config     -Dorg.oracle.fusion.service.resourcePath=/app/config     -Dorg.oracle.fusion.service.hostName=0.0.0.0     -Dorg.oracle.fusion.service.portNumber=7001     -Dorg.oracle.fusion.service.consulHttpApiUrl=${CONSUL}     -Dconsul.base.address=${CONSUL}     -Dorg.oracle.fusion.service.path=${SERVICE_PATH}     -Dorg.oracle.fusion.service.devMode=${DEVMODE}     -Dorg.oracle.fusion.service.buiBasePath=${BUI_BASE_PATH}     -Dorg.oracle.fusion.service.clusterName=${CLUSTER}     -Djava.rmi.server.hostname=who     -Dlogback.configurationFile=/app/config/live/logback.xml     -Dorg.oracle.fusion.service.jolokiaRealmFile=/app/config/realm.properties     -Dorg.oracle.fusion.service.jolokiaRole=asJolokiaRole     -Duser.dir=/app     ${JAVA_OPTS}     -cp     /app/*:/app/lib/*     org.oracle.service.router.Host
    State:          Running
      Started:      Wed, 19 Jan 2022 23:59:43 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  4Gi
    Requests:
      cpu:      500m
      memory:   2Gi
    Liveness:   http-get http://:7001/AgentWeb/ping delay=0s timeout=1s period=10s #success=1 #failure=5
    Readiness:  http-get http://:7001/AgentWeb/ready delay=0s timeout=1s period=10s #success=1 #failure=10
    Environment:
      JAVA_OPTS:                     -Dorg.oracle.fusion.service.k8s=true -Dorg.oracle.fusion.service.kubernetes.domain=cluster.local -Dorg.oracle.fusion.service.kubernetes.singleVersion=true -Dorg.oracle.fusion.service.jaeger.enabled=true -Dorg.oracle.fusion.service.forwardedForHeader=X-Original-Forwarded-For -Dorg.oracle.fusion.service.defaultVersion=20220119-23563149-ashd -Dorg.oracle.fusion.service.consul.certPath=/consul/ca.crt -Dorg.oracle.fusion.service.vault.token-path=/vault/.vault-token -Dorg.oracle.fusion.service.vault.enabled=true -Dorg.oracle.fusion.service.vault.truststore=/vault/ca.crt -Dorg.oracle.fusion.service.vault.ssl.enabled=true
                                     -Dkafka.bootstrap.servers=bui-kafka-kafka-brokers.bui:9093 -Dkafka.security.protocol=SSL -Dkafka.auth.keystore.path=/vault/secrets/keystore.p12 -Dkafka.auth.truststore.path=/vault/secrets/truststore.p12 -Dkafka.auth.issuing.ca.path=/vault/secrets/issuing_ca.pem -Dkafka.auth.private.key.path=/vault/secrets/private_key.pem -Dkafka.auth.certificate.path=/vault/secrets/certificate.pem
                                     -Dlogging.api.kafka.enabled=false -Dschema.registry.url=https://schemaregistry-proxy-kafka.service.iad-dataplane.corp.consul -Dorg.oracle.fusion.service.who.internalOnly=true
      CLUSTER:                       as-ciat
      VERSION:                       as-ciat (v1:metadata.namespace)
      SERVICE_PATH:                  /AgentWeb
      BUI_BASE_PATH:                 as-ciat.corp.channels.ocs.oc-test.com
      DEVMODE:                       true
      NAMESPACE:                     as-ciat (v1:metadata.namespace)
      HOST_IP:                        (v1:status.hostIP)
      CONSUL:                        https://$(HOST_IP):8501
      CONSUL_TOKEN:                  vault:/cpe_consul/creds/as-ciat_consul_role#token
      JAEGER_AGENT_HOST:              (v1:status.hostIP)
      JAEGER_AGENT_PORT:             6831
      JAEGER_AGENT_PROTOCOL:         udp
      VAULT_ADDR:                    https://vault.query.corp.consul:8200
      VAULT_SKIP_VERIFY:             true
      VAULT_AUTH_METHOD:             jwt
      VAULT_PATH:                    k8s-iad-dataplane
      VAULT_ROLE:                    as-ciat_role
      VAULT_IGNORE_MISSING_SECRETS:  false
      VAULT_ENV_PASSTHROUGH:
      VAULT_JSON_LOG:                false
      VAULT_CLIENT_TIMEOUT:          10s
      VAULT_LOG_LEVEL:               warn
      VAULT_ENV_DAEMON:              true
    Mounts:
      /app/config/live from who-config (rw)
      /consul from consul-ca (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zrljs (ro)
      /vault/ from vault-env (rw)
      /vault/ca.crt from vault-ca (ro,path="ca.crt")
      /vault/secrets from agent-secrets (rw)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  who-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      who-internal-config
    Optional:  false
  consul-ca:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  consul-tls
    Optional:    false
  vault-ca:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  vault-tls
    Optional:    false
  default-token-zrljs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-zrljs
    Optional:    false
  vault-env:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  agent-secrets:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  agent-configmap:
    Type:        ConfigMap (a volume populated by a ConfigMap)
    Name:        as-ciat-va-configmap
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason       Age                From     Message
  ----     ------       ----               ----     -------
  Warning  FailedMount  5s (x17 over 18m)  kubelet  MountVolume.SetUp failed for volume "who-config" : configmap "who-internal-config" not found
[20:January:2022:13:53:24]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[20:January:2022:13:53:52]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn as-ciat
NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
horton-horton-deployment   0/3     0            0           5h25m
who-who-deployment         1/3     0            1           27d
[20:January:2022:13:54:02]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn as-ciat who-who-deployment -oyaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "476"
    meta.helm.sh/release-name: who
    meta.helm.sh/release-namespace: as-ciat
  creationTimestamp: "2021-12-24T02:06:09Z"
  generation: 1069
  labels:
    app.kubernetes.io/managed-by: Helm
    com.oracle.osvc.api/service-cluster: as-ciat
  name: who-who-deployment
  namespace: as-ciat
  resourceVersion: "770398834"
  selfLink: /apis/apps/v1/namespaces/as-ciat/deployments/who-who-deployment
  uid: 5ef2452a-2787-49fe-90cd-5cb17fc10519
spec:
  progressDeadlineSeconds: 600
  replicas: 3
  revisionHistoryLimit: 3
  selector:
    matchLabels:
      app: who
      cluster: as-ciat
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      annotations:
        as.osvc.oracle.com/app-version: 49f19d6e243010f2154536e8a95e18c1c5f07b48
        as.osvc.oracle.com/managed: "true"
        kubectl.kubernetes.io/default-container: server
        vault.security.banzaicloud.io/log-level: warn
        vault.security.banzaicloud.io/vault-addr: https://vault.query.corp.consul:8200
        vault.security.banzaicloud.io/vault-agent-configmap: as-ciat-va-configmap
        vault.security.banzaicloud.io/vault-env-daemon: "true"
        vault.security.banzaicloud.io/vault-path: k8s-iad-dataplane
        vault.security.banzaicloud.io/vault-role: as-ciat_role
        vault.security.banzaicloud.io/vault-skip-verify: "true"
      creationTimestamp: null
      labels:
        app: who
        cluster: as-ciat
        isInternalDeployment: "false"
    spec:
      containers:
      - env:
        - name: JAVA_OPTS
          value: |-
            -Dorg.oracle.fusion.service.k8s=true -Dorg.oracle.fusion.service.kubernetes.domain=cluster.local -Dorg.oracle.fusion.service.kubernetes.singleVersion=true -Dorg.oracle.fusion.service.jaeger.enabled=true -Dorg.oracle.fusion.service.forwardedForHeader=X-Original-Forwarded-For -Dorg.oracle.fusion.service.defaultVersion=20220120-08263133-m5tc -Dorg.oracle.fusion.service.consul.certPath=/consul/ca.crt -Dorg.oracle.fusion.service.vault.token-path=/vault/.vault-token -Dorg.oracle.fusion.service.vault.enabled=true -Dorg.oracle.fusion.service.vault.truststore=/vault/ca.crt -Dorg.oracle.fusion.service.vault.ssl.enabled=true
            -Dkafka.bootstrap.servers=bui-kafka-kafka-brokers.bui:9093 -Dkafka.security.protocol=SSL -Dkafka.auth.keystore.path=/vault/secrets/keystore.p12 -Dkafka.auth.truststore.path=/vault/secrets/truststore.p12 -Dkafka.auth.issuing.ca.path=/vault/secrets/issuing_ca.pem -Dkafka.auth.private.key.path=/vault/secrets/private_key.pem -Dkafka.auth.certificate.path=/vault/secrets/certificate.pem
            -Dlogging.api.kafka.enabled=false -Dschema.registry.url=https://schemaregistry-proxy-kafka.service.iad-dataplane.corp.consul -Dorg.oracle.fusion.service.who.internalOnly=false
        - name: CLUSTER
          value: as-ciat
        - name: VERSION
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: SERVICE_PATH
          value: /AgentWeb
        - name: BUI_BASE_PATH
          value: as-ciat.corp.channels.ocs.oc-test.com
        - name: DEVMODE
          value: "true"
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: HOST_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.hostIP
        - name: CONSUL
          value: https://$(HOST_IP):8501
        - name: CONSUL_TOKEN
          value: vault:/cpe_consul/creds/as-ciat_consul_role#token
        - name: JAEGER_AGENT_HOST
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.hostIP
        - name: JAEGER_AGENT_PORT
          value: "6831"
        - name: JAEGER_AGENT_PROTOCOL
          value: udp
        image: iad.ocir.io/osvccorp/bui/who:master-2022.01.19-17.33.25-49f19d6e
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /AgentWeb/ping
            port: 7001
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: server
        ports:
        - containerPort: 7001
          name: http
          protocol: TCP
        readinessProbe:
          failureThreshold: 10
          httpGet:
            path: /AgentWeb/ready
            port: 7001
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          limits:
            cpu: "1"
            memory: 4Gi
          requests:
            cpu: 500m
            memory: 2Gi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /app/config/live
          name: who-config
        - mountPath: /consul
          name: consul-ca
          readOnly: true
        - mountPath: /vault/ca.crt
          name: vault-ca
          readOnly: true
          subPath: ca.crt
      dnsConfig:
        options:
        - name: ndots
          value: "2"
      dnsPolicy: ClusterFirst
      imagePullSecrets:
      - name: osvc-token
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: default
      serviceAccountName: default
      terminationGracePeriodSeconds: 30
      volumes:
      - configMap:
          defaultMode: 420
          items:
          - key: logback.xml
            path: logback.xml
          name: who-config
        name: who-config
      - name: consul-ca
        secret:
          defaultMode: 420
          items:
          - key: ca.crt
            path: ca.crt
          secretName: consul-tls
      - name: vault-ca
        secret:
          defaultMode: 420
          items:
          - key: ca.crt
            path: ca.crt
          secretName: vault-tls
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: "2022-01-20T07:58:30Z"
    lastUpdateTime: "2022-01-20T07:58:30Z"
    message: 'pods "who-who-deployment-7595845b58-s7jf5" is forbidden: exceeded quota:
      as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m,
      limited: limits.cpu=48'
    reason: FailedCreate
    status: "True"
    type: ReplicaFailure
  - lastTransitionTime: "2022-01-20T09:45:06Z"
    lastUpdateTime: "2022-01-20T09:45:06Z"
    message: Deployment does not have minimum availability.
    reason: MinimumReplicasUnavailable
    status: "False"
    type: Available
  - lastTransitionTime: "2022-01-20T10:29:47Z"
    lastUpdateTime: "2022-01-20T10:29:47Z"
    message: ReplicaSet "who-who-deployment-7bbd669859" has timed out progressing.
    reason: ProgressDeadlineExceeded
    status: "False"
    type: Progressing
  observedGeneration: 1069
  readyReplicas: 1
  replicas: 1
  unavailableReplicas: 3
[20:January:2022:13:54:10]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[20:January:2022:13:54:33]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[20:January:2022:13:55:11]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn as-ciat
NAME                                           READY   STATUS    RESTARTS   AGE   IP              NODE          NOMINATED NODE   READINESS GATES
lorax-analyticsmanager-75d985dfdf-6jmjm        2/2     Running   0          13h   10.245.14.74    10.11.9.104   <none>           <none>
lorax-analyticsmanager-75d985dfdf-mmxrw        2/2     Running   0          13h   10.245.13.192   10.11.9.145   <none>           <none>
lorax-cachehost-647b6cf7b4-4v6n7               2/2     Running   0          12h   10.245.13.197   10.11.9.145   <none>           <none>
lorax-cachehost-647b6cf7b4-jsqlv               2/2     Running   2          12h   10.245.27.178   10.11.8.60    <none>           <none>
lorax-cachehost-6c8948bf68-8gmdf               2/2     Running   0          13h   10.245.19.21    10.11.9.238   <none>           <none>
lorax-cachehost-6c8948bf68-mzvx7               2/2     Running   0          13h   10.245.26.8     10.11.8.67    <none>           <none>
lorax-cachehost-6c8948bf68-vf26t               2/2     Running   3          13h   10.245.15.225   10.11.9.78    <none>           <none>
lorax-commandmanager-586d58947d-2h89f          2/2     Running   0          13h   10.245.28.203   10.11.9.56    <none>           <none>
lorax-commandmanager-586d58947d-2rcmp          2/2     Running   0          13h   10.245.11.175   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-c4482          2/2     Running   0          13h   10.245.27.176   10.11.8.60    <none>           <none>
lorax-contextmanager-67956c7584-c4xkw          2/2     Running   0          13h   10.245.11.177   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-h82q5          2/2     Running   0          13h   10.245.3.92     10.11.8.232   <none>           <none>
lorax-customizationmanager-657bbf9586-rsbqg    2/2     Running   0          13h   10.245.29.214   10.11.9.62    <none>           <none>
lorax-elementmanager-7f74646f4d-n4cqx          2/2     Running   0          13h   10.245.28.39    10.11.8.31    <none>           <none>
lorax-eventmanager-759c5b9484-ch28f            2/2     Running   0          13h   10.245.15.226   10.11.9.78    <none>           <none>
lorax-eventworker-7c5b779bc-z4bb5              2/2     Running   0          13h   10.245.22.102   10.11.8.250   <none>           <none>
lorax-realtimedatamanager-866cd5886b-qq54f     2/2     Running   0          13h   10.245.29.213   10.11.9.62    <none>           <none>
lorax-reportpublishmanager-58445646c4-9ztc7    2/2     Running   0          13h   10.245.11.176   10.11.9.208   <none>           <none>
lorax-searchmanager-7d97d6dfcc-gn4zg           2/2     Running   0          13h   10.245.29.212   10.11.9.62    <none>           <none>
lorax-searchmanager-7d97d6dfcc-q2x5t           2/2     Running   0          13h   10.245.14.73    10.11.9.104   <none>           <none>
who-internal-who-deployment-65d44d6f4c-w8lnq   2/2     Running   0          13h   10.245.11.179   10.11.9.208   <none>           <none>
who-who-deployment-576cd44f87-4dr8c            2/2     Running   0          13h   10.245.9.42     10.11.9.136   <none>           <none>
[20:January:2022:13:55:25]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn as-ciat who-who-deployment-576cd44f87-4dr8c -oyaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    as.osvc.oracle.com/app-version: 49f19d6e243010f2154536e8a95e18c1c5f07b48
    as.osvc.oracle.com/managed: "true"
    kubectl.kubernetes.io/default-container: server
    kubernetes.io/psp: psp-protect-docker
    vault.security.banzaicloud.io/log-level: warn
    vault.security.banzaicloud.io/vault-addr: https://vault.query.corp.consul:8200
    vault.security.banzaicloud.io/vault-agent-configmap: as-ciat-va-configmap
    vault.security.banzaicloud.io/vault-env-daemon: "true"
    vault.security.banzaicloud.io/vault-path: k8s-iad-dataplane
    vault.security.banzaicloud.io/vault-role: as-ciat_role
    vault.security.banzaicloud.io/vault-skip-verify: "true"
  creationTimestamp: "2022-01-19T23:59:39Z"
  generateName: who-who-deployment-576cd44f87-
  labels:
    app: who
    cluster: as-ciat
    isInternalDeployment: "false"
    pod-template-hash: 576cd44f87
  name: who-who-deployment-576cd44f87-4dr8c
  namespace: as-ciat
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: who-who-deployment-576cd44f87
    uid: 628ae0a1-246c-4531-abed-605650ff3f97
  resourceVersion: "769489861"
  selfLink: /api/v1/namespaces/as-ciat/pods/who-who-deployment-576cd44f87-4dr8c
  uid: 3aa5e6b4-0d68-4318-855b-e382383cfd2e
spec:
  containers:
  - args:
    - agent
    - -config
    - /vault/config/config.hcl
    env:
    - name: VAULT_ADDR
      value: https://vault.query.corp.consul:8200
    - name: VAULT_SKIP_VERIFY
      value: "true"
    image: docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5
    imagePullPolicy: IfNotPresent
    name: vault-agent
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 100m
        memory: 128Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        add:
        - IPC_LOCK
        - SYS_PTRACE
        drop:
        - NET_RAW
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /vault/
      name: vault-env
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-zrljs
      readOnly: true
    - mountPath: /vault/secrets
      name: agent-secrets
    - mountPath: /vault/config/config.hcl
      name: agent-configmap
      readOnly: true
      subPath: config.hcl
  - args:
    - /bin/sh
    - -c
    - java     -XX:+UnlockExperimentalVMOptions     -XX:+UseCGroupMemoryLimitForHeap     -XX:+HeapDumpOnOutOfMemoryError     -XX:HeapDumpPath=/app/logs/heap_pid%%p.hprof     -XX:+UseG1GC     -XX:MaxGCPauseMillis=250     -Dorg.oracle.fusion.service.kubernetes.namespace=${NAMESPACE}     -Dorg.oracle.fusion.service.ContextPath=/app/config     -Dorg.oracle.fusion.service.resourcePath=/app/config     -Dorg.oracle.fusion.service.hostName=0.0.0.0     -Dorg.oracle.fusion.service.portNumber=7001     -Dorg.oracle.fusion.service.consulHttpApiUrl=${CONSUL}     -Dconsul.base.address=${CONSUL}     -Dorg.oracle.fusion.service.path=${SERVICE_PATH}     -Dorg.oracle.fusion.service.devMode=${DEVMODE}     -Dorg.oracle.fusion.service.buiBasePath=${BUI_BASE_PATH}     -Dorg.oracle.fusion.service.clusterName=${CLUSTER}     -Djava.rmi.server.hostname=who     -Dlogback.configurationFile=/app/config/live/logback.xml     -Dorg.oracle.fusion.service.jolokiaRealmFile=/app/config/realm.properties     -Dorg.oracle.fusion.service.jolokiaRole=asJolokiaRole     -Duser.dir=/app     ${JAVA_OPTS}     -cp     /app/*:/app/lib/*     org.oracle.service.router.Host
    command:
    - /vault/vault-env
    env:
    - name: JAVA_OPTS
      value: |-
        -Dorg.oracle.fusion.service.k8s=true -Dorg.oracle.fusion.service.kubernetes.domain=cluster.local -Dorg.oracle.fusion.service.kubernetes.singleVersion=true -Dorg.oracle.fusion.service.jaeger.enabled=true -Dorg.oracle.fusion.service.forwardedForHeader=X-Original-Forwarded-For -Dorg.oracle.fusion.service.defaultVersion=20220119-23563149-ashd -Dorg.oracle.fusion.service.consul.certPath=/consul/ca.crt -Dorg.oracle.fusion.service.vault.token-path=/vault/.vault-token -Dorg.oracle.fusion.service.vault.enabled=true -Dorg.oracle.fusion.service.vault.truststore=/vault/ca.crt -Dorg.oracle.fusion.service.vault.ssl.enabled=true
        -Dkafka.bootstrap.servers=bui-kafka-kafka-brokers.bui:9093 -Dkafka.security.protocol=SSL -Dkafka.auth.keystore.path=/vault/secrets/keystore.p12 -Dkafka.auth.truststore.path=/vault/secrets/truststore.p12 -Dkafka.auth.issuing.ca.path=/vault/secrets/issuing_ca.pem -Dkafka.auth.private.key.path=/vault/secrets/private_key.pem -Dkafka.auth.certificate.path=/vault/secrets/certificate.pem
        -Dlogging.api.kafka.enabled=false -Dschema.registry.url=https://schemaregistry-proxy-kafka.service.iad-dataplane.corp.consul -Dorg.oracle.fusion.service.who.internalOnly=false
    - name: CLUSTER
      value: as-ciat
    - name: VERSION
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.namespace
    - name: SERVICE_PATH
      value: /AgentWeb
    - name: BUI_BASE_PATH
      value: as-ciat.corp.channels.ocs.oc-test.com
    - name: DEVMODE
      value: "true"
    - name: NAMESPACE
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.namespace
    - name: HOST_IP
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.hostIP
    - name: CONSUL
      value: https://$(HOST_IP):8501
    - name: CONSUL_TOKEN
      value: vault:/cpe_consul/creds/as-ciat_consul_role#token
    - name: JAEGER_AGENT_HOST
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.hostIP
    - name: JAEGER_AGENT_PORT
      value: "6831"
    - name: JAEGER_AGENT_PROTOCOL
      value: udp
    - name: VAULT_ADDR
      value: https://vault.query.corp.consul:8200
    - name: VAULT_SKIP_VERIFY
      value: "true"
    - name: VAULT_AUTH_METHOD
      value: jwt
    - name: VAULT_PATH
      value: k8s-iad-dataplane
    - name: VAULT_ROLE
      value: as-ciat_role
    - name: VAULT_IGNORE_MISSING_SECRETS
      value: "false"
    - name: VAULT_ENV_PASSTHROUGH
    - name: VAULT_JSON_LOG
      value: "false"
    - name: VAULT_CLIENT_TIMEOUT
      value: 10s
    - name: VAULT_LOG_LEVEL
      value: warn
    - name: VAULT_ENV_DAEMON
      value: "true"
    image: iad.ocir.io/osvccorp/bui/who:master-2022.01.19-17.33.25-49f19d6e
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 5
      httpGet:
        path: /AgentWeb/ping
        port: 7001
        scheme: HTTP
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    name: server
    ports:
    - containerPort: 7001
      name: http
      protocol: TCP
    readinessProbe:
      failureThreshold: 10
      httpGet:
        path: /AgentWeb/ready
        port: 7001
        scheme: HTTP
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    resources:
      limits:
        cpu: "1"
        memory: 4Gi
      requests:
        cpu: 500m
        memory: 2Gi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - NET_RAW
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /app/config/live
      name: who-config
    - mountPath: /consul
      name: consul-ca
      readOnly: true
    - mountPath: /vault/ca.crt
      name: vault-ca
      readOnly: true
      subPath: ca.crt
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-zrljs
      readOnly: true
    - mountPath: /vault/
      name: vault-env
    - mountPath: /vault/secrets
      name: agent-secrets
  dnsConfig:
    options:
    - name: ndots
      value: "2"
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  imagePullSecrets:
  - name: osvc-token
  initContainers:
  - command:
    - sh
    - -c
    - cp /usr/local/bin/vault-env /vault/
    image: docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3
    imagePullPolicy: IfNotPresent
    name: copy-vault-env
    resources:
      limits:
        cpu: 250m
        memory: 64Mi
      requests:
        cpu: 50m
        memory: 64Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - NET_RAW
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /vault/
      name: vault-env
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-zrljs
      readOnly: true
  nodeName: 10.11.9.136
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  shareProcessNamespace: true
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - configMap:
      defaultMode: 420
      items:
      - key: logback.xml
        path: logback.xml
      name: who-config
    name: who-config
  - name: consul-ca
    secret:
      defaultMode: 420
      items:
      - key: ca.crt
        path: ca.crt
      secretName: consul-tls
  - name: vault-ca
    secret:
      defaultMode: 420
      items:
      - key: ca.crt
        path: ca.crt
      secretName: vault-tls
  - name: default-token-zrljs
    secret:
      defaultMode: 420
      secretName: default-token-zrljs
  - emptyDir:
      medium: Memory
    name: vault-env
  - emptyDir:
      medium: Memory
    name: agent-secrets
  - configMap:
      defaultMode: 420
      items:
      - key: config.hcl
        path: config.hcl
      name: as-ciat-va-configmap
    name: agent-configmap
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2022-01-19T23:59:40Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2022-01-20T00:00:39Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2022-01-20T00:00:39Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2022-01-19T23:59:39Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: docker://f57052660b8f29e336d2d2c3e838a439f3243e1c28dec6dfa27abb6e0be276f7
    image: iad.ocir.io/osvccorp/bui/who:master-2022.01.19-17.33.25-49f19d6e
    imageID: docker-pullable://iad.ocir.io/osvccorp/bui/who@sha256:8458ebf460e2aa3364312c89ee9a7990f70750ff2c37a7e7c248a31872b4fbbb
    lastState: {}
    name: server
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2022-01-20T00:00:27Z"
  - containerID: docker://a6e53c7dba1f67d99caf0e8f636480f031b9f26cfb44092fe95a56d8582572fa
    image: docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5
    imageID: docker-pullable://docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault@sha256:57438e97b72c05368ca06f4d5f34667e965248635718001abdea8fa11f18adad
    lastState: {}
    name: vault-agent
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2022-01-19T23:59:40Z"
  hostIP: 10.11.9.136
  initContainerStatuses:
  - containerID: docker://ad3f3be384cd771cf8e83164dc502f9212e00533ae5590d93f99bccb1b6fe578
    image: docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3
    imageID: docker-pullable://docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env@sha256:7435d1c641c0aa0fd4faaff32ab189b29d3d909b100d36d4d90df3b79cee8ce3
    lastState: {}
    name: copy-vault-env
    ready: true
    restartCount: 0
    state:
      terminated:
        containerID: docker://ad3f3be384cd771cf8e83164dc502f9212e00533ae5590d93f99bccb1b6fe578
        exitCode: 0
        finishedAt: "2022-01-19T23:59:40Z"
        reason: Completed
        startedAt: "2022-01-19T23:59:40Z"
  phase: Running
  podIP: 10.245.9.42
  podIPs:
  - ip: 10.245.9.42
  qosClass: Burstable
  startTime: "2022-01-19T23:59:39Z"
[20:January:2022:13:55:35]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[20:January:2022:14:00:52]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ #kgpn as-ciat who-who-deployment-576cd44f87-4dr8c -oyaml
[20:January:2022:14:00:55]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kubectl describe rs/who-who-deployment-576cd44f87-4dr8c -n as-ciat
Error from server (NotFound): replicasets.apps "who-who-deployment-576cd44f87-4dr8c" not found
[20:January:2022:14:01:08]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kubectl describe rs/who-who-deployment -n as-ciat
Error from server (NotFound): replicasets.apps "who-who-deployment" not found
[20:January:2022:14:01:17]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kubectl describe rs/who-who-deployment-576cd44f87 -n as-ciat
Name:           who-who-deployment-576cd44f87
Namespace:      as-ciat
Selector:       app=who,cluster=as-ciat,pod-template-hash=576cd44f87
Labels:         app=who
                cluster=as-ciat
                isInternalDeployment=false
                pod-template-hash=576cd44f87
Annotations:    deployment.kubernetes.io/desired-replicas: 3
                deployment.kubernetes.io/max-replicas: 4
                deployment.kubernetes.io/revision: 461
                meta.helm.sh/release-name: who
                meta.helm.sh/release-namespace: as-ciat
Controlled By:  Deployment/who-who-deployment
Replicas:       1 current / 3 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=who
                    cluster=as-ciat
                    isInternalDeployment=false
                    pod-template-hash=576cd44f87
  Annotations:      as.osvc.oracle.com/app-version: 49f19d6e243010f2154536e8a95e18c1c5f07b48
                    as.osvc.oracle.com/managed: true
                    kubectl.kubernetes.io/default-container: server
                    vault.security.banzaicloud.io/log-level: warn
                    vault.security.banzaicloud.io/vault-addr: https://vault.query.corp.consul:8200
                    vault.security.banzaicloud.io/vault-agent-configmap: as-ciat-va-configmap
                    vault.security.banzaicloud.io/vault-env-daemon: true
                    vault.security.banzaicloud.io/vault-path: k8s-iad-dataplane
                    vault.security.banzaicloud.io/vault-role: as-ciat_role
                    vault.security.banzaicloud.io/vault-skip-verify: true
  Service Account:  default
  Containers:
   server:
    Image:      iad.ocir.io/osvccorp/bui/who:master-2022.01.19-17.33.25-49f19d6e
    Port:       7001/TCP
    Host Port:  0/TCP
    Limits:
      cpu:     1
      memory:  4Gi
    Requests:
      cpu:      500m
      memory:   2Gi
    Liveness:   http-get http://:7001/AgentWeb/ping delay=0s timeout=1s period=10s #success=1 #failure=5
    Readiness:  http-get http://:7001/AgentWeb/ready delay=0s timeout=1s period=10s #success=1 #failure=10
    Environment:
      JAVA_OPTS:              -Dorg.oracle.fusion.service.k8s=true -Dorg.oracle.fusion.service.kubernetes.domain=cluster.local -Dorg.oracle.fusion.service.kubernetes.singleVersion=true -Dorg.oracle.fusion.service.jaeger.enabled=true -Dorg.oracle.fusion.service.forwardedForHeader=X-Original-Forwarded-For -Dorg.oracle.fusion.service.defaultVersion=20220119-23563149-ashd -Dorg.oracle.fusion.service.consul.certPath=/consul/ca.crt -Dorg.oracle.fusion.service.vault.token-path=/vault/.vault-token -Dorg.oracle.fusion.service.vault.enabled=true -Dorg.oracle.fusion.service.vault.truststore=/vault/ca.crt -Dorg.oracle.fusion.service.vault.ssl.enabled=true
                              -Dkafka.bootstrap.servers=bui-kafka-kafka-brokers.bui:9093 -Dkafka.security.protocol=SSL -Dkafka.auth.keystore.path=/vault/secrets/keystore.p12 -Dkafka.auth.truststore.path=/vault/secrets/truststore.p12 -Dkafka.auth.issuing.ca.path=/vault/secrets/issuing_ca.pem -Dkafka.auth.private.key.path=/vault/secrets/private_key.pem -Dkafka.auth.certificate.path=/vault/secrets/certificate.pem
                              -Dlogging.api.kafka.enabled=false -Dschema.registry.url=https://schemaregistry-proxy-kafka.service.iad-dataplane.corp.consul -Dorg.oracle.fusion.service.who.internalOnly=false
      CLUSTER:                as-ciat
      VERSION:                 (v1:metadata.namespace)
      SERVICE_PATH:           /AgentWeb
      BUI_BASE_PATH:          as-ciat.corp.channels.ocs.oc-test.com
      DEVMODE:                true
      NAMESPACE:               (v1:metadata.namespace)
      HOST_IP:                 (v1:status.hostIP)
      CONSUL:                 https://$(HOST_IP):8501
      CONSUL_TOKEN:           vault:/cpe_consul/creds/as-ciat_consul_role#token
      JAEGER_AGENT_HOST:       (v1:status.hostIP)
      JAEGER_AGENT_PORT:      6831
      JAEGER_AGENT_PROTOCOL:  udp
    Mounts:
      /app/config/live from who-config (rw)
      /consul from consul-ca (ro)
      /vault/ca.crt from vault-ca (ro,path="ca.crt")
  Volumes:
   who-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      who-config
    Optional:  false
   consul-ca:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  consul-tls
    Optional:    false
   vault-ca:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  vault-tls
    Optional:    false
Conditions:
  Type             Status  Reason
  ----             ------  ------
  ReplicaFailure   True    FailedCreate
Events:
  Type     Reason        Age   From                   Message
  ----     ------        ----  ----                   -------
  Warning  FailedCreate  51m   replicaset-controller  Error creating: pods "who-who-deployment-576cd44f87-wmknl" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
  Warning  FailedCreate  34m   replicaset-controller  Error creating: pods "who-who-deployment-576cd44f87-p5zpc" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
  Warning  FailedCreate  18m   replicaset-controller  Error creating: pods "who-who-deployment-576cd44f87-j2d79" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
  Warning  FailedCreate  83s   replicaset-controller  Error creating: pods "who-who-deployment-576cd44f87-sckpx" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
[20:January:2022:14:02:05]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[20:January:2022:14:02:34]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[20:January:2022:14:03:06]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kubectl describe rs/who-internal-who-deployment-65d44d6f4c -n as-ciat
Name:           who-internal-who-deployment-65d44d6f4c
Namespace:      as-ciat
Selector:       app=who,cluster=as-ciat,pod-template-hash=65d44d6f4c
Labels:         app=who
                cluster=as-ciat
                isInternalDeployment=true
                pod-template-hash=65d44d6f4c
Annotations:    deployment.kubernetes.io/desired-replicas: 3
                deployment.kubernetes.io/max-replicas: 4
                deployment.kubernetes.io/revision: 111
                meta.helm.sh/release-name: who-internal
                meta.helm.sh/release-namespace: as-ciat
Controlled By:  Deployment/who-internal-who-deployment
Replicas:       1 current / 3 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=who
                    cluster=as-ciat
                    isInternalDeployment=true
                    pod-template-hash=65d44d6f4c
  Annotations:      as.osvc.oracle.com/app-version: 49f19d6e243010f2154536e8a95e18c1c5f07b48
                    as.osvc.oracle.com/managed: true
                    kubectl.kubernetes.io/default-container: server
                    vault.security.banzaicloud.io/log-level: warn
                    vault.security.banzaicloud.io/vault-addr: https://vault.query.corp.consul:8200
                    vault.security.banzaicloud.io/vault-agent-configmap: as-ciat-va-configmap
                    vault.security.banzaicloud.io/vault-env-daemon: true
                    vault.security.banzaicloud.io/vault-path: k8s-iad-dataplane
                    vault.security.banzaicloud.io/vault-role: as-ciat_role
                    vault.security.banzaicloud.io/vault-skip-verify: true
  Service Account:  default
  Containers:
   server:
    Image:      iad.ocir.io/osvccorp/bui/who:master-2022.01.19-17.33.25-49f19d6e
    Port:       7001/TCP
    Host Port:  0/TCP
    Limits:
      cpu:     1
      memory:  4Gi
    Requests:
      cpu:      500m
      memory:   2Gi
    Liveness:   http-get http://:7001/AgentWeb/ping delay=0s timeout=1s period=10s #success=1 #failure=5
    Readiness:  http-get http://:7001/AgentWeb/ready delay=0s timeout=1s period=10s #success=1 #failure=10
    Environment:
      JAVA_OPTS:              -Dorg.oracle.fusion.service.k8s=true -Dorg.oracle.fusion.service.kubernetes.domain=cluster.local -Dorg.oracle.fusion.service.kubernetes.singleVersion=true -Dorg.oracle.fusion.service.jaeger.enabled=true -Dorg.oracle.fusion.service.forwardedForHeader=X-Original-Forwarded-For -Dorg.oracle.fusion.service.defaultVersion=20220119-23563149-ashd -Dorg.oracle.fusion.service.consul.certPath=/consul/ca.crt -Dorg.oracle.fusion.service.vault.token-path=/vault/.vault-token -Dorg.oracle.fusion.service.vault.enabled=true -Dorg.oracle.fusion.service.vault.truststore=/vault/ca.crt -Dorg.oracle.fusion.service.vault.ssl.enabled=true
                              -Dkafka.bootstrap.servers=bui-kafka-kafka-brokers.bui:9093 -Dkafka.security.protocol=SSL -Dkafka.auth.keystore.path=/vault/secrets/keystore.p12 -Dkafka.auth.truststore.path=/vault/secrets/truststore.p12 -Dkafka.auth.issuing.ca.path=/vault/secrets/issuing_ca.pem -Dkafka.auth.private.key.path=/vault/secrets/private_key.pem -Dkafka.auth.certificate.path=/vault/secrets/certificate.pem
                              -Dlogging.api.kafka.enabled=false -Dschema.registry.url=https://schemaregistry-proxy-kafka.service.iad-dataplane.corp.consul -Dorg.oracle.fusion.service.who.internalOnly=true
      CLUSTER:                as-ciat
      VERSION:                 (v1:metadata.namespace)
      SERVICE_PATH:           /AgentWeb
      BUI_BASE_PATH:          as-ciat.corp.channels.ocs.oc-test.com
      DEVMODE:                true
      NAMESPACE:               (v1:metadata.namespace)
      HOST_IP:                 (v1:status.hostIP)
      CONSUL:                 https://$(HOST_IP):8501
      CONSUL_TOKEN:           vault:/cpe_consul/creds/as-ciat_consul_role#token
      JAEGER_AGENT_HOST:       (v1:status.hostIP)
      JAEGER_AGENT_PORT:      6831
      JAEGER_AGENT_PROTOCOL:  udp
    Mounts:
      /app/config/live from who-config (rw)
      /consul from consul-ca (ro)
      /vault/ca.crt from vault-ca (ro,path="ca.crt")
  Volumes:
   who-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      who-internal-config
    Optional:  false
   consul-ca:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  consul-tls
    Optional:    false
   vault-ca:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  vault-tls
    Optional:    false
Conditions:
  Type             Status  Reason
  ----             ------  ------
  ReplicaFailure   True    FailedCreate
Events:
  Type     Reason        Age   From                   Message
  ----     ------        ----  ----                   -------
  Warning  FailedCreate  51m   replicaset-controller  Error creating: pods "who-internal-who-deployment-65d44d6f4c-h4xmk" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
  Warning  FailedCreate  35m   replicaset-controller  Error creating: pods "who-internal-who-deployment-65d44d6f4c-z547w" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
  Warning  FailedCreate  18m   replicaset-controller  Error creating: pods "who-internal-who-deployment-65d44d6f4c-c7v8w" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
  Warning  FailedCreate  103s  replicaset-controller  Error creating: pods "who-internal-who-deployment-65d44d6f4c-qdjr4" is forbidden: exceeded quota: as-ciat-resource-quota, requested: limits.cpu=1100m, used: limits.cpu=47400m, limited: limits.cpu=48
[20:January:2022:14:03:32]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ #kubectl delete deployment nginx-deployment --cascade=orphan
[20:January:2022:14:06:29]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ #kubectl describe rs/who-internal-who-deployment-65d44d6f4c -n as-ciat
[20:January:2022:14:06:37]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kubectl delete deployment who-internal-who-deployment --cascade=orphan -n as-ciat
Error from server (NotFound): deployments.apps "who-internal-who-deployment" not found
[20:January:2022:14:07:02]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn as-ciat
NAME                                           READY   STATUS    RESTARTS   AGE   IP              NODE          NOMINATED NODE   READINESS GATES
lorax-analyticsmanager-75d985dfdf-6jmjm        2/2     Running   0          14h   10.245.14.74    10.11.9.104   <none>           <none>
lorax-analyticsmanager-75d985dfdf-mmxrw        2/2     Running   0          14h   10.245.13.192   10.11.9.145   <none>           <none>
lorax-cachehost-647b6cf7b4-4v6n7               2/2     Running   0          12h   10.245.13.197   10.11.9.145   <none>           <none>
lorax-cachehost-647b6cf7b4-jsqlv               2/2     Running   2          12h   10.245.27.178   10.11.8.60    <none>           <none>
lorax-cachehost-6c8948bf68-8gmdf               2/2     Running   0          14h   10.245.19.21    10.11.9.238   <none>           <none>
lorax-cachehost-6c8948bf68-mzvx7               2/2     Running   0          14h   10.245.26.8     10.11.8.67    <none>           <none>
lorax-cachehost-6c8948bf68-vf26t               2/2     Running   3          14h   10.245.15.225   10.11.9.78    <none>           <none>
lorax-commandmanager-586d58947d-2h89f          2/2     Running   0          14h   10.245.28.203   10.11.9.56    <none>           <none>
lorax-commandmanager-586d58947d-2rcmp          2/2     Running   0          14h   10.245.11.175   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-c4482          2/2     Running   0          14h   10.245.27.176   10.11.8.60    <none>           <none>
lorax-contextmanager-67956c7584-c4xkw          2/2     Running   0          14h   10.245.11.177   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-h82q5          2/2     Running   0          14h   10.245.3.92     10.11.8.232   <none>           <none>
lorax-customizationmanager-657bbf9586-rsbqg    2/2     Running   0          14h   10.245.29.214   10.11.9.62    <none>           <none>
lorax-elementmanager-7f74646f4d-n4cqx          2/2     Running   0          14h   10.245.28.39    10.11.8.31    <none>           <none>
lorax-eventmanager-759c5b9484-ch28f            2/2     Running   0          14h   10.245.15.226   10.11.9.78    <none>           <none>
lorax-eventworker-7c5b779bc-z4bb5              2/2     Running   0          14h   10.245.22.102   10.11.8.250   <none>           <none>
lorax-realtimedatamanager-866cd5886b-qq54f     2/2     Running   0          14h   10.245.29.213   10.11.9.62    <none>           <none>
lorax-reportpublishmanager-58445646c4-9ztc7    2/2     Running   0          14h   10.245.11.176   10.11.9.208   <none>           <none>
lorax-searchmanager-7d97d6dfcc-gn4zg           2/2     Running   0          14h   10.245.29.212   10.11.9.62    <none>           <none>
lorax-searchmanager-7d97d6dfcc-q2x5t           2/2     Running   0          14h   10.245.14.73    10.11.9.104   <none>           <none>
who-internal-who-deployment-65d44d6f4c-w8lnq   2/2     Running   0          14h   10.245.11.179   10.11.9.208   <none>           <none>
who-who-deployment-576cd44f87-4dr8c            2/2     Running   0          14h   10.245.9.42     10.11.9.136   <none>           <none>
[20:January:2022:14:07:13]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgn as-ciat all | grep internal
pod/who-internal-who-deployment-65d44d6f4c-w8lnq   2/2     Running   0          14h
replicaset.apps/who-internal-who-deployment-54b47b487b   0         0         0       6h8m
replicaset.apps/who-internal-who-deployment-5844c7df47   0         0         0       8h
replicaset.apps/who-internal-who-deployment-65d44d6f4c   3         1         1       14h
replicaset.apps/who-internal-who-deployment-66d59bc96f   0         0         0       18h
replicaset.apps/who-internal-who-deployment-679bbc7c47   0         0         0       11h
replicaset.apps/who-internal-who-deployment-67d6d8c44b   0         0         0       7h38m
replicaset.apps/who-internal-who-deployment-69f7fb4dfb   0         0         0       8h
replicaset.apps/who-internal-who-deployment-777f4b78f4   0         0         0       17h
replicaset.apps/who-internal-who-deployment-784cccc67f   1         0         0       5h39m
replicaset.apps/who-internal-who-deployment-788ffcc766   0         0         0       9h
replicaset.apps/who-internal-who-deployment-7bcbd746c    0         0         0       12h
replicaset.apps/who-internal-who-deployment-7bd5c447d9   0         0         0       7h8m
replicaset.apps/who-internal-who-deployment-7cb74474d7   0         0         0       6h38m
replicaset.apps/who-internal-who-deployment-7cdfb8c485   0         0         0       9h
replicaset.apps/who-internal-who-deployment-7fffbcf677   0         0         0       10h
replicaset.apps/who-internal-who-deployment-85586746c8   0         0         0       11h
replicaset.apps/who-internal-who-deployment-8575746b4c   0         0         0       12h
replicaset.apps/who-internal-who-deployment-8b79bbcb7    0         0         0       15h
replicaset.apps/who-internal-who-deployment-c79b6986c    0         0         0       10h
[20:January:2022:14:07:46]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[20:January:2022:14:10:12]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ k delete replicaset.apps/who-internal-who-deployment-679bbc7c47 -n as-ciat
replicaset.apps "who-internal-who-deployment-679bbc7c47" deleted
[20:January:2022:14:10:39]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgn as-ciat all | grep internal
pod/who-internal-who-deployment-65d44d6f4c-w8lnq   2/2     Running   0          14h
replicaset.apps/who-internal-who-deployment-54b47b487b   0         0         0       6h12m
replicaset.apps/who-internal-who-deployment-5844c7df47   0         0         0       8h
replicaset.apps/who-internal-who-deployment-65d44d6f4c   3         1         1       14h
replicaset.apps/who-internal-who-deployment-66d59bc96f   0         0         0       18h
replicaset.apps/who-internal-who-deployment-67d6d8c44b   0         0         0       7h42m
replicaset.apps/who-internal-who-deployment-69f7fb4dfb   0         0         0       8h
replicaset.apps/who-internal-who-deployment-777f4b78f4   0         0         0       17h
replicaset.apps/who-internal-who-deployment-784cccc67f   1         0         0       5h42m
replicaset.apps/who-internal-who-deployment-788ffcc766   0         0         0       9h
replicaset.apps/who-internal-who-deployment-7bcbd746c    0         0         0       12h
replicaset.apps/who-internal-who-deployment-7bd5c447d9   0         0         0       7h12m
replicaset.apps/who-internal-who-deployment-7cb74474d7   0         0         0       6h42m
replicaset.apps/who-internal-who-deployment-7cdfb8c485   0         0         0       9h
replicaset.apps/who-internal-who-deployment-7fffbcf677   0         0         0       10h
replicaset.apps/who-internal-who-deployment-85586746c8   0         0         0       11h
replicaset.apps/who-internal-who-deployment-8575746b4c   0         0         0       12h
replicaset.apps/who-internal-who-deployment-8b79bbcb7    0         0         0       15h
replicaset.apps/who-internal-who-deployment-c79b6986c    0         0         0       10h
[20:January:2022:14:11:04]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[20:January:2022:14:14:55]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn as-ciat
NAME                                           READY   STATUS    RESTARTS   AGE   IP              NODE          NOMINATED NODE   READINESS GATES
lorax-analyticsmanager-75d985dfdf-6jmjm        2/2     Running   0          14h   10.245.14.74    10.11.9.104   <none>           <none>
lorax-analyticsmanager-75d985dfdf-mmxrw        2/2     Running   0          14h   10.245.13.192   10.11.9.145   <none>           <none>
lorax-cachehost-647b6cf7b4-4v6n7               2/2     Running   0          12h   10.245.13.197   10.11.9.145   <none>           <none>
lorax-cachehost-647b6cf7b4-jsqlv               2/2     Running   2          12h   10.245.27.178   10.11.8.60    <none>           <none>
lorax-cachehost-6c8948bf68-8gmdf               2/2     Running   0          14h   10.245.19.21    10.11.9.238   <none>           <none>
lorax-cachehost-6c8948bf68-mzvx7               2/2     Running   0          14h   10.245.26.8     10.11.8.67    <none>           <none>
lorax-cachehost-6c8948bf68-vf26t               2/2     Running   3          14h   10.245.15.225   10.11.9.78    <none>           <none>
lorax-commandmanager-586d58947d-2h89f          2/2     Running   0          14h   10.245.28.203   10.11.9.56    <none>           <none>
lorax-commandmanager-586d58947d-2rcmp          2/2     Running   0          14h   10.245.11.175   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-c4482          2/2     Running   0          14h   10.245.27.176   10.11.8.60    <none>           <none>
lorax-contextmanager-67956c7584-c4xkw          2/2     Running   0          14h   10.245.11.177   10.11.9.208   <none>           <none>
lorax-contextmanager-67956c7584-h82q5          2/2     Running   0          14h   10.245.3.92     10.11.8.232   <none>           <none>
lorax-customizationmanager-657bbf9586-rsbqg    2/2     Running   0          14h   10.245.29.214   10.11.9.62    <none>           <none>
lorax-elementmanager-7f74646f4d-n4cqx          2/2     Running   0          14h   10.245.28.39    10.11.8.31    <none>           <none>
lorax-eventmanager-759c5b9484-ch28f            2/2     Running   0          14h   10.245.15.226   10.11.9.78    <none>           <none>
lorax-eventworker-7c5b779bc-z4bb5              2/2     Running   0          14h   10.245.22.102   10.11.8.250   <none>           <none>
lorax-realtimedatamanager-866cd5886b-qq54f     2/2     Running   0          14h   10.245.29.213   10.11.9.62    <none>           <none>
lorax-reportpublishmanager-58445646c4-9ztc7    2/2     Running   0          14h   10.245.11.176   10.11.9.208   <none>           <none>
lorax-searchmanager-7d97d6dfcc-gn4zg           2/2     Running   0          14h   10.245.29.212   10.11.9.62    <none>           <none>
lorax-searchmanager-7d97d6dfcc-q2x5t           2/2     Running   0          14h   10.245.14.73    10.11.9.104   <none>           <none>
who-internal-who-deployment-65d44d6f4c-w8lnq   2/2     Running   0          14h   10.245.11.179   10.11.9.208   <none>           <none>
who-who-deployment-576cd44f87-4dr8c            2/2     Running   0          14h   10.245.9.42     10.11.9.136   <none>           <none>
[20:January:2022:14:16:34]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[20:January:2022:14:18:29]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ sressh opc@10.11.9.208
Ncat: Proxy connection failed: Connection refused.
[20:January:2022:14:18:48]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[20:January:2022:14:28:03]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[20:January:2022:16:49:09]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[21:January:2022:04:40:12]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ v2prod
[21:January:2022:04:40:17]:(prod_sa-santiago-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgc
CURRENT   NAME                               CLUSTER               AUTHINFO           NAMESPACE
          prod_ap-hyderabad-1_controlplane   cluster-cbjr52qcfoa   user-cbjr52qcfoa
          prod_ap-hyderabad-1_dataplane      cluster-cpj2zxktnda   user-cpj2zxktnda
          prod_ap-melbourne-1_controlplane   cluster-czgcn3bmjtg   user-czgcn3bmjtg
          prod_ap-melbourne-1_dataplane      cluster-c4dazdggfst   user-c4dazdggfst
          prod_ap-mumbai-1_controlplane      cluster-cjiru3mifia   user-cjiru3mifia
          prod_ap-mumbai-1_dataplane         cluster-ci3ypr6tmga   user-ci3ypr6tmga
          prod_ap-osaka-1_controlplane       cluster-cp5327xtmfa   user-cp5327xtmfa
          prod_ap-osaka-1_dataplane          cluster-cqldck6xatq   user-cqldck6xatq
          prod_ap-sydney-1_controlplane      cluster-c4wembugqyt   user-c4wembugqyt
          prod_ap-sydney-1_dataplane         cluster-cswezbzgyyg   user-cswezbzgyyg
          prod_ap-tokyo-1_controlplane       cluster-cz54pc3z4wa   user-cz54pc3z4wa
          prod_ap-tokyo-1_dataplane          cluster-cyjtbtbbjfq   user-cyjtbtbbjfq
          prod_ca-montreal-1_controlplane    cluster-czdoolemnqw   user-czdoolemnqw
          prod_ca-montreal-1_dataplane       cluster-cqtkytbgq3d   user-cqtkytbgq3d
          prod_ca-toronto-1_controlplane     cluster-cygcmjxhfsd   user-cygcmjxhfsd
          prod_ca-toronto-1_dataplane        cluster-cywmodfmm3d   user-cywmodfmm3d
          prod_eu-amsterdam-1_controlplane   cluster-c4tkmjwgbrt   user-c4tkmjwgbrt
          prod_eu-amsterdam-1_dataplane      cluster-cstsy3bmjrd   user-cstsy3bmjrd
          prod_eu-frankfurt-1_controlplane   cluster-cydcytcg43d   user-cydcytcg43d
          prod_eu-frankfurt-1_dataplane      cluster-crdgzbrmjsd   user-crdgzbrmjsd
          prod_eu-zurich-1_controlplane      cluster-cwmoxn3bfwq   user-cwmoxn3bfwq
          prod_eu-zurich-1_dataplane         cluster-chtoe5bnsza   user-chtoe5bnsza
          prod_me-dubai-1_controlplane       cluster-ce5puidc4ya   user-ce5puidc4ya
          prod_me-dubai-1_dataplane          cluster-c77hcd3vizq   user-c77hcd3vizq
          prod_me-jeddah-1_controlplane      cluster-cpzagsoukeq   user-cpzagsoukeq
          prod_me-jeddah-1_dataplane         cluster-cf4f222mrfa   user-cf4f222mrfa
          prod_sa-santiago-1_controlplane    cluster-czxpimgibeq   user-czxpimgibeq
*         prod_sa-santiago-1_dataplane       cluster-cuutxfvj2ea   user-cuutxfvj2ea
          prod_sa-saopaulo-1_controlplane    cluster-cc75n7hujpq   user-cc75n7hujpq
          prod_sa-saopaulo-1_dataplane       cluster-cpjqrjtbotq   user-cpjqrjtbotq
          prod_sa-vinhedo-1_controlplane     cluster-cihx6aprjua   user-cihx6aprjua
          prod_sa-vinhedo-1_dataplane        cluster-ciqmbmn6wta   user-ciqmbmn6wta
          prod_uk-cardiff-1_controlplane     cluster-ctggmtbmfrg   user-ctggmtbmfrg
          prod_uk-cardiff-1_dataplane        cluster-cqwmnrqmztd   user-cqwmnrqmztd
          prod_uk-london-1_controlplane      cluster-cywgnlcmqzd   user-cywgnlcmqzd
          prod_uk-london-1_dataplane         cluster-c3tazjsmjsd   user-c3tazjsmjsd
          prod_us-ashburn-1_controlplane     cluster-czwczjzgrsw   user-czwczjzgrsw
          prod_us-ashburn-1_dataplane        cluster-c3wkyjyguzt   user-c3wkyjyguzt
          prod_us-phoenix-1_controlplane     cluster-c3tkobugrst   user-c3tkobugrst
          prod_us-phoenix-1_dataplane        cluster-csdqylfmi2g   user-csdqylfmi2g
          prod_us-phoenix-1_deployment       cluster-c4tgolgmjsd   user-c4tgolgmjsd
[21:January:2022:04:40:19]:(prod_sa-santiago-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kuc prod_us-phoenix-1_dataplane
Switched to context "prod_us-phoenix-1_dataplane".
[21:January:2022:04:40:28]:(prod_sa-santiago-1_dataplane):~/galorndon/osvc-platform
○ (main) $ rp
[21:January:2022:04:40:29]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn mercury-psr
NAME                                                              READY   STATUS    RESTARTS   AGE   IP             NODE          NOMINATED NODE   READINESS GATES
mercury-agent-command-service-59dcc858fc-gtl4m                    2/2     Running   0          29h   10.245.1.100   10.20.1.38    <none>           <none>
mercury-agent-command-service-59dcc858fc-ldzzz                    2/2     Running   0          29h   10.245.3.49    10.20.0.7     <none>           <none>
mercury-agent-command-service-59dcc858fc-zmkng                    2/2     Running   3          29h   10.245.8.145   10.20.1.158   <none>           <none>
mercury-channel-api-6f9c9c9dc5-f89lq                              2/2     Running   3          29h   10.245.8.143   10.20.1.158   <none>           <none>
mercury-consumer-command-service-76d45c9fcc-v89kt                 2/2     Running   0          29h   10.245.4.142   10.20.1.227   <none>           <none>
mercury-consumer-command-service-76d45c9fcc-x2dv2                 2/2     Running   0          29h   10.245.6.11    10.20.1.44    <none>           <none>
mercury-consumer-command-service-76d45c9fcc-xt68z                 2/2     Running   0          29h   10.245.2.40    10.20.1.35    <none>           <none>
mercury-custom-availability-service-569d4cf687-7dx62              2/2     Running   0          29h   10.245.5.197   10.20.0.143   <none>           <none>
mercury-custom-availability-service-569d4cf687-gsnqf              2/2     Running   3          29h   10.245.6.12    10.20.1.44    <none>           <none>
mercury-custom-availability-service-569d4cf687-w4qwz              2/2     Running   2          29h   10.245.4.139   10.20.1.227   <none>           <none>
mercury-data-mask-api-59584bc855-jhs6j                            2/2     Running   0          29h   10.245.4.25    10.20.1.27    <none>           <none>
mercury-data-mask-api-59584bc855-sl229                            2/2     Running   0          29h   10.245.4.147   10.20.1.227   <none>           <none>
mercury-data-mask-api-59584bc855-zq6bv                            2/2     Running   0          29h   10.245.3.240   10.20.0.250   <none>           <none>
mercury-engagement-queue-api-68fd4c96df-8slrm                     2/2     Running   2          29h   10.245.3.55    10.20.0.7     <none>           <none>
mercury-engagement-queue-api-68fd4c96df-z46rc                     2/2     Running   2          29h   10.245.2.140   10.20.1.191   <none>           <none>
mercury-engagement-queue-api-68fd4c96df-zcmfj                     2/2     Running   2          29h   10.245.2.42    10.20.1.35    <none>           <none>
mercury-enrichment-service-84c964b545-clw7g                       2/2     Running   0          29h   10.245.3.243   10.20.0.250   <none>           <none>
mercury-enrichment-service-84c964b545-gqvh2                       2/2     Running   0          29h   10.245.1.224   10.20.0.29    <none>           <none>
mercury-enrichment-service-84c964b545-zhkmp                       2/2     Running   0          29h   10.245.0.240   10.20.0.244   <none>           <none>
mercury-event-sync-service-57d9bd5765-96x4k                       2/2     Running   0          29h   10.245.4.29    10.20.1.27    <none>           <none>
mercury-event-sync-service-57d9bd5765-cbfqt                       2/2     Running   0          29h   10.245.7.181   10.20.1.224   <none>           <none>
mercury-event-sync-service-57d9bd5765-wpzkk                       2/2     Running   0          29h   10.245.0.50    10.20.1.247   <none>           <none>
mercury-integration-in-processor-849dff9756-4gcgz                 2/2     Running   3          29h   10.245.7.180   10.20.1.224   <none>           <none>
mercury-integration-in-processor-849dff9756-dl2p8                 2/2     Running   3          29h   10.245.3.241   10.20.0.250   <none>           <none>
mercury-integration-in-processor-849dff9756-pm6md                 2/2     Running   0          29h   10.245.4.41    10.20.1.27    <none>           <none>
mercury-integration-out-processor-689bdb99c4-66xvr                2/2     Running   3          29h   10.245.3.242   10.20.0.250   <none>           <none>
mercury-integration-out-processor-689bdb99c4-bldbg                2/2     Running   2          29h   10.245.4.30    10.20.1.27    <none>           <none>
mercury-integration-out-processor-689bdb99c4-vl4cv                2/2     Running   2          29h   10.245.7.182   10.20.1.224   <none>           <none>
mercury-kweet-facebook-client-7fddbc78f9-242wn                    2/2     Running   0          29h   10.245.8.141   10.20.1.158   <none>           <none>
mercury-kweet-facebook-client-7fddbc78f9-ckf75                    2/2     Running   0          29h   10.245.4.11    10.20.1.27    <none>           <none>
mercury-kweet-facebook-client-7fddbc78f9-nprbf                    2/2     Running   2          29h   10.245.1.94    10.20.1.38    <none>           <none>
mercury-kweet-facebook-webhook-5bc7986869-2npxp                   2/2     Running   0          29h   10.245.4.21    10.20.1.27    <none>           <none>
mercury-kweet-facebook-webhook-5bc7986869-bdtjb                   2/2     Running   3          29h   10.245.4.138   10.20.1.227   <none>           <none>
mercury-kweet-facebook-webhook-5bc7986869-d2r8q                   2/2     Running   3          29h   10.245.2.39    10.20.1.35    <none>           <none>
mercury-kweet-twiliosms-client-6b8f6ffc78-46p9m                   2/2     Running   0          29h   10.245.8.142   10.20.1.158   <none>           <none>
mercury-kweet-twiliosms-client-6b8f6ffc78-5zv2v                   2/2     Running   0          29h   10.245.6.7     10.20.1.44    <none>           <none>
mercury-kweet-twiliosms-client-6b8f6ffc78-tfqd8                   2/2     Running   0          29h   10.245.1.225   10.20.0.29    <none>           <none>
mercury-kweet-userprofiles-d9c5cdc6b-gqdhb                        2/2     Running   0          29h   10.245.8.144   10.20.1.158   <none>           <none>
mercury-kweet-userprofiles-d9c5cdc6b-jmwwb                        2/2     Running   0          29h   10.245.1.96    10.20.1.38    <none>           <none>
mercury-kweet-userprofiles-d9c5cdc6b-vzb7b                        2/2     Running   3          29h   10.245.4.14    10.20.1.27    <none>           <none>
mercury-mercury-ui-6f58d49b55-92tl8                               2/2     Running   0          29h   10.245.5.182   10.20.0.143   <none>           <none>
mercury-mercury-ui-6f58d49b55-jdvnq                               2/2     Running   0          29h   10.245.4.23    10.20.1.27    <none>           <none>
mercury-mercury-ui-6f58d49b55-zc7m2                               2/2     Running   0          29h   10.245.4.143   10.20.1.227   <none>           <none>
mercury-metric-aggregation-processor-68ff54fdb9-bkf8j             2/2     Running   0          29h   10.245.4.22    10.20.1.27    <none>           <none>
mercury-metric-aggregation-processor-68ff54fdb9-pbf9c             2/2     Running   0          29h   10.245.4.137   10.20.1.227   <none>           <none>
mercury-metric-aggregation-processor-68ff54fdb9-rldmm             2/2     Running   0          29h   10.245.5.180   10.20.0.143   <none>           <none>
mercury-metric-fusion-bridge-7f49b75ffb-gpj4g                     2/2     Running   0          29h   10.245.3.64    10.20.0.7     <none>           <none>
mercury-metric-fusion-bridge-7f49b75ffb-s7x7t                     2/2     Running   2          29h   10.245.4.140   10.20.1.227   <none>           <none>
mercury-metric-fusion-bridge-7f49b75ffb-wqf2r                     2/2     Running   0          29h   10.245.8.148   10.20.1.158   <none>           <none>
mercury-metric-generation-processor-54447b6657-gjjqd              2/2     Running   0          29h   10.245.2.36    10.20.1.35    <none>           <none>
mercury-metric-generation-processor-54447b6657-mbz9f              2/2     Running   0          29h   10.245.6.9     10.20.1.44    <none>           <none>
mercury-metric-generation-processor-54447b6657-qpcqc              2/2     Running   0          29h   10.245.1.98    10.20.1.38    <none>           <none>
mercury-metric-internal-translation-processor-57cf478c6-l2sb4     2/2     Running   2          29h   10.245.7.185   10.20.1.224   <none>           <none>
mercury-metric-internal-translation-processor-57cf478c6-lc8df     2/2     Running   2          29h   10.245.6.169   10.20.1.239   <none>           <none>
mercury-metric-internal-translation-processor-57cf478c6-zwlmn     2/2     Running   0          29h   10.245.0.53    10.20.1.247   <none>           <none>
mercury-metric-proxy-service-576df9c79f-6wgr6                     2/2     Running   0          29h   10.245.4.20    10.20.1.27    <none>           <none>
mercury-metric-proxy-service-576df9c79f-hsvfd                     2/2     Running   3          29h   10.245.1.101   10.20.1.38    <none>           <none>
mercury-metric-proxy-service-576df9c79f-vsfzd                     2/2     Running   0          29h   10.245.2.38    10.20.1.35    <none>           <none>
mercury-omnichannel-assignment-processor-587b8c9b6b-2675k         2/2     Running   2          29h   10.245.1.103   10.20.1.38    <none>           <none>
mercury-omnichannel-assignment-processor-587b8c9b6b-5nlch         2/2     Running   2          29h   10.245.8.147   10.20.1.158   <none>           <none>
mercury-omnichannel-assignment-processor-587b8c9b6b-fh9pr         2/2     Running   3          29h   10.245.4.26    10.20.1.27    <none>           <none>
mercury-omnichannel-offer-processor-76f947c586-9qfh6              2/2     Running   3          29h   10.245.3.52    10.20.0.7     <none>           <none>
mercury-omnichannel-offer-processor-76f947c586-l2c5q              2/2     Running   2          29h   10.245.2.133   10.20.1.191   <none>           <none>
mercury-omnichannel-offer-processor-76f947c586-x9sgc              2/2     Running   3          29h   10.245.8.146   10.20.1.158   <none>           <none>
mercury-osvc-bridge-api-services-756ff69dbd-b89zr                 2/2     Running   3          29h   10.245.5.187   10.20.0.143   <none>           <none>
mercury-osvc-bridge-api-services-756ff69dbd-jhlqf                 2/2     Running   2          29h   10.245.3.50    10.20.0.7     <none>           <none>
mercury-osvc-bridge-api-services-756ff69dbd-t2prz                 2/2     Running   3          29h   10.245.1.104   10.20.1.38    <none>           <none>
mercury-osvc-bridge-metrics-data-pipeline-59bfff645f-dbwpm        2/2     Running   0          29h   10.245.3.65    10.20.0.7     <none>           <none>
mercury-osvc-bridge-metrics-data-pipeline-59bfff645f-p6v69        2/2     Running   0          29h   10.245.2.137   10.20.1.191   <none>           <none>
mercury-osvc-bridge-metrics-data-pipeline-59bfff645f-vc5gq        2/2     Running   0          29h   10.245.5.192   10.20.0.143   <none>           <none>
mercury-osvc-bridge-osvc-data-extractor-855f9bd5bc-28rkm          2/2     Running   0          29h   10.245.2.136   10.20.1.191   <none>           <none>
mercury-osvc-bridge-osvc-data-extractor-855f9bd5bc-g9p5r          2/2     Running   1          29h   10.245.6.16    10.20.1.44    <none>           <none>
mercury-osvc-bridge-osvc-data-extractor-855f9bd5bc-grdg9          2/2     Running   3          29h   10.245.5.188   10.20.0.143   <none>           <none>
mercury-osvc-bridge-provisioning-processor-67cb69489-9llvc        2/2     Running   0          29h   10.245.0.51    10.20.1.247   <none>           <none>
mercury-osvc-bridge-state-processor-b655b7468-8k4xw               2/2     Running   0          29h   10.245.4.40    10.20.1.27    <none>           <none>
mercury-osvc-bridge-state-processor-b655b7468-ldjqw               2/2     Running   0          29h   10.245.2.143   10.20.1.191   <none>           <none>
mercury-osvc-bridge-state-processor-b655b7468-qtnjl               2/2     Running   0          29h   10.245.8.36    10.20.0.134   <none>           <none>
mercury-osvc-bridge-state-query-service-64dd4bbdc8-6lxwl          2/2     Running   3          29h   10.245.5.183   10.20.0.143   <none>           <none>
mercury-osvc-bridge-state-query-service-64dd4bbdc8-gx2x4          2/2     Running   2          29h   10.245.1.102   10.20.1.38    <none>           <none>
mercury-osvc-bridge-state-query-service-64dd4bbdc8-t7lxk          2/2     Running   3          29h   10.245.6.14    10.20.1.44    <none>           <none>
mercury-osvc-bridge-task-controller-569d4f4ff-67vcr               2/2     Running   0          29h   10.245.5.189   10.20.0.143   <none>           <none>
mercury-osvc-bridge-task-controller-569d4f4ff-69gtw               2/2     Running   1          29h   10.245.1.106   10.20.1.38    <none>           <none>
mercury-osvc-bridge-task-controller-569d4f4ff-8g589               2/2     Running   1          29h   10.245.6.17    10.20.1.44    <none>           <none>
mercury-provisioning-monitor-676848bb67-dqx88                     2/2     Running   0          29h   10.245.0.49    10.20.1.247   <none>           <none>
mercury-provisioning-monitor-676848bb67-k7sxg                     2/2     Running   0          29h   10.245.1.107   10.20.1.38    <none>           <none>
mercury-provisioning-monitor-676848bb67-kfwjp                     2/2     Running   0          29h   10.245.0.239   10.20.0.244   <none>           <none>
mercury-provisioning-processor-769945d9d8-6ddck                   2/2     Running   3          29h   10.245.4.17    10.20.1.27    <none>           <none>
mercury-provisioning-processor-769945d9d8-csqcn                   2/2     Running   3          29h   10.245.2.37    10.20.1.35    <none>           <none>
mercury-provisioning-processor-769945d9d8-dd7wr                   2/2     Running   0          29h   10.245.1.112   10.20.1.38    <none>           <none>
mercury-psr-kafka-entity-operator-858f557bb9-xzhlh                3/3     Running   0          29h   10.245.5.14    10.20.0.4     <none>           <none>
mercury-psr-kafka-kafka-0                                         2/2     Running   0          29h   10.245.5.11    10.20.0.4     <none>           <none>
mercury-psr-kafka-kafka-1                                         2/2     Running   0          29h   10.245.8.138   10.20.1.158   <none>           <none>
mercury-psr-kafka-kafka-2                                         2/2     Running   0          29h   10.245.1.223   10.20.0.29    <none>           <none>
mercury-psr-kafka-kafka-3                                         2/2     Running   0          29h   10.245.5.12    10.20.0.4     <none>           <none>
mercury-psr-kafka-kafka-4                                         2/2     Running   0          29h   10.245.8.139   10.20.1.158   <none>           <none>
mercury-psr-kafka-kafka-5                                         2/2     Running   0          29h   10.245.7.179   10.20.1.224   <none>           <none>
mercury-psr-kafka-kafka-6                                         2/2     Running   0          29h   10.245.5.13    10.20.0.4     <none>           <none>
mercury-psr-kafka-kafka-7                                         2/2     Running   0          29h   10.245.8.140   10.20.1.158   <none>           <none>
mercury-psr-kafka-kafka-8                                         2/2     Running   0          29h   10.245.2.131   10.20.1.191   <none>           <none>
mercury-psr-kafka-kafka-exporter-75f8b86dc4-czj8f                 1/1     Running   0          29h   10.245.5.15    10.20.0.4     <none>           <none>
mercury-psr-kafka-zookeeper-0                                     1/1     Running   0          29h   10.245.5.10    10.20.0.4     <none>           <none>
mercury-psr-kafka-zookeeper-1                                     1/1     Running   0          29h   10.245.8.137   10.20.1.158   <none>           <none>
mercury-psr-kafka-zookeeper-2                                     1/1     Running   0          29h   10.245.7.17    10.20.1.52    <none>           <none>
mercury-queue-agent-info-processor-5b4fcbb58c-pfm42               2/2     Running   3          29h   10.245.6.18    10.20.1.44    <none>           <none>
mercury-queue-agent-info-processor-5b4fcbb58c-v4dm6               2/2     Running   3          29h   10.245.4.149   10.20.1.227   <none>           <none>
mercury-queue-agent-info-processor-5b4fcbb58c-x5jch               2/2     Running   3          29h   10.245.5.191   10.20.0.143   <none>           <none>
mercury-realtime-channel-processor-64755f8cc9-4k5kj               2/2     Running   0          29h   10.245.4.144   10.20.1.227   <none>           <none>
mercury-realtime-channel-processor-64755f8cc9-jdzdd               2/2     Running   0          29h   10.245.6.15    10.20.1.44    <none>           <none>
mercury-realtime-channel-processor-64755f8cc9-tjzrw               2/2     Running   0          29h   10.245.5.184   10.20.0.143   <none>           <none>
mercury-resource-channel-processor-7d96c47886-4lnnz               2/2     Running   3          29h   10.245.2.142   10.20.1.191   <none>           <none>
mercury-resource-channel-processor-7d96c47886-hpvkk               2/2     Running   3          29h   10.245.2.44    10.20.1.35    <none>           <none>
mercury-resource-channel-processor-7d96c47886-khkqw               2/2     Running   2          29h   10.245.0.241   10.20.0.244   <none>           <none>
mercury-resource-state-processor-6c47fbbcb5-4kvhf                 2/2     Running   0          29h   10.245.3.53    10.20.0.7     <none>           <none>
mercury-resource-state-processor-6c47fbbcb5-mz22q                 2/2     Running   0          29h   10.245.2.41    10.20.1.35    <none>           <none>
mercury-resource-state-processor-6c47fbbcb5-x2w2s                 2/2     Running   0          29h   10.245.4.150   10.20.1.227   <none>           <none>
mercury-resource-work-processor-d447bfd4c-6lhkn                   2/2     Running   0          29h   10.245.5.179   10.20.0.143   <none>           <none>
mercury-resource-work-processor-d447bfd4c-ch5nj                   2/2     Running   0          29h   10.245.6.10    10.20.1.44    <none>           <none>
mercury-resource-work-processor-d447bfd4c-ctd2n                   2/2     Running   0          29h   10.245.4.136   10.20.1.227   <none>           <none>
mercury-routing-processor-agent-assignment-774885f97c-64tct       2/2     Running   2          29h   10.245.4.33    10.20.1.27    <none>           <none>
mercury-routing-processor-agent-assignment-774885f97c-8jbsz       2/2     Running   0          29h   10.245.2.144   10.20.1.191   <none>           <none>
mercury-routing-processor-agent-assignment-774885f97c-mfjdq       2/2     Running   2          29h   10.245.0.52    10.20.1.247   <none>           <none>
mercury-routing-processor-agent-events-processor-cd4c7689c7xct7   2/2     Running   0          29h   10.245.6.20    10.20.1.44    <none>           <none>
mercury-routing-processor-agent-events-processor-cd4c7689ccjmqv   2/2     Running   0          29h   10.245.3.245   10.20.0.250   <none>           <none>
mercury-routing-processor-agent-events-processor-cd4c7689cq6db2   2/2     Running   0          29h   10.245.4.152   10.20.1.227   <none>           <none>
mercury-routing-processor-queue-assignment-5464cd88bf-8j8pj       2/2     Running   2          29h   10.245.3.244   10.20.0.250   <none>           <none>
mercury-routing-processor-queue-assignment-5464cd88bf-bq6j9       2/2     Running   2          29h   10.245.6.19    10.20.1.44    <none>           <none>
mercury-routing-processor-queue-assignment-5464cd88bf-jnzl6       2/2     Running   3          29h   10.245.4.151   10.20.1.227   <none>           <none>
mercury-routing-processor-work-events-processor-7b6df456d826f79   2/2     Running   3          29h   10.245.2.132   10.20.1.191   <none>           <none>
mercury-routing-processor-work-events-processor-7b6df456d8h5n8x   2/2     Running   0          29h   10.245.5.198   10.20.0.143   <none>           <none>
mercury-routing-processor-work-events-processor-7b6df456d8hl7vv   2/2     Running   2          29h   10.245.0.238   10.20.0.244   <none>           <none>
mercury-session-housekeeping-processor-788fdfd5bb-2ncbw           2/2     Running   3          29h   10.245.5.196   10.20.0.143   <none>           <none>
mercury-session-housekeeping-processor-788fdfd5bb-l9z25           2/2     Running   2          29h   10.245.7.186   10.20.1.224   <none>           <none>
mercury-session-housekeeping-processor-788fdfd5bb-zhv58           2/2     Running   2          29h   10.245.4.35    10.20.1.27    <none>           <none>
mercury-session-processor-66d5445868-chdph                        2/2     Running   2          29h   10.245.5.195   10.20.0.143   <none>           <none>
mercury-session-processor-66d5445868-k67fv                        2/2     Running   2          29h   10.245.7.184   10.20.1.224   <none>           <none>
mercury-session-processor-66d5445868-pmv5w                        2/2     Running   2          29h   10.245.3.58    10.20.0.7     <none>           <none>
mercury-single-sign-on-service-5cd6974f66-7k458                   2/2     Running   2          29h   10.245.2.138   10.20.1.191   <none>           <none>
mercury-single-sign-on-service-5cd6974f66-dd78x                   2/2     Running   3          29h   10.245.5.193   10.20.0.143   <none>           <none>
mercury-single-sign-on-service-5cd6974f66-wmcb7                   2/2     Running   3          29h   10.245.4.32    10.20.1.27    <none>           <none>
mercury-social-bridge-5947d4fbb-6q7vt                             2/2     Running   3          29h   10.245.4.31    10.20.1.27    <none>           <none>
mercury-social-bridge-5947d4fbb-89x22                             2/2     Running   0          29h   10.245.8.37    10.20.0.134   <none>           <none>
mercury-social-bridge-5947d4fbb-v485v                             2/2     Running   2          29h   10.245.1.108   10.20.1.38    <none>           <none>
mercury-social-config-6b4cd9cb4d-dsmf5                            2/2     Running   3          29h   10.245.4.34    10.20.1.27    <none>           <none>
mercury-social-config-6b4cd9cb4d-m6txv                            2/2     Running   3          29h   10.245.1.109   10.20.1.38    <none>           <none>
mercury-social-config-6b4cd9cb4d-nlzkl                            2/2     Running   2          29h   10.245.5.194   10.20.0.143   <none>           <none>
mercury-static-assets-service-5b54b4c7d7-778vj                    2/2     Running   0          29h   10.245.8.35    10.20.0.134   <none>           <none>
mercury-static-assets-service-5b54b4c7d7-pbb5x                    2/2     Running   0          29h   10.245.3.63    10.20.0.7     <none>           <none>
mercury-static-assets-service-5b54b4c7d7-qd4hr                    2/2     Running   0          29h   10.245.1.110   10.20.1.38    <none>           <none>
mercury-tenant-downtime-monitor-78fdf4c974-hdg4h                  2/2     Running   0          29h   10.245.0.48    10.20.1.247   <none>           <none>
mercury-tenant-downtime-monitor-78fdf4c974-tp6gz                  2/2     Running   1          29h   10.245.1.105   10.20.1.38    <none>           <none>
mercury-tenant-downtime-monitor-78fdf4c974-w9bsj                  2/2     Running   1          29h   10.245.4.28    10.20.1.27    <none>           <none>
mercury-transcript-api-66568cb7cd-lj6n6                           2/2     Running   3          29h   10.245.4.148   10.20.1.227   <none>           <none>
mercury-transcript-api-66568cb7cd-n96zs                           2/2     Running   0          29h   10.245.5.185   10.20.0.143   <none>           <none>
mercury-transcript-api-66568cb7cd-qkngj                           2/2     Running   3          29h   10.245.0.237   10.20.0.244   <none>           <none>
mercury-transcript-processor-64779cbcd8-c4mx4                     2/2     Running   2          29h   10.245.1.111   10.20.1.38    <none>           <none>
mercury-transcript-processor-64779cbcd8-kkfbl                     2/2     Running   2          29h   10.245.8.149   10.20.1.158   <none>           <none>
mercury-transcript-processor-64779cbcd8-qq7k4                     2/2     Running   2          29h   10.245.4.39    10.20.1.27    <none>           <none>
mercury-user-preference-service-54c6587fc8-2h87d                  2/2     Running   0          29h   10.245.1.97    10.20.1.38    <none>           <none>
mercury-user-preference-service-54c6587fc8-ddlrk                  2/2     Running   0          29h   10.245.2.35    10.20.1.35    <none>           <none>
mercury-user-preference-service-54c6587fc8-k72kq                  2/2     Running   0          29h   10.245.6.8     10.20.1.44    <none>           <none>
mercury-work-api-b468f7db7-fcf62                                  2/2     Running   3          29h   10.245.2.43    10.20.1.35    <none>           <none>
mercury-work-api-b468f7db7-qsm7h                                  2/2     Running   3          29h   10.245.3.56    10.20.0.7     <none>           <none>
mercury-work-api-b468f7db7-zccn4                                  2/2     Running   3          29h   10.245.7.183   10.20.1.224   <none>           <none>
mercury-work-processor-6b888f5bbf-2twtx                           2/2     Running   0          29h   10.245.4.13    10.20.1.27    <none>           <none>
mercury-work-processor-6b888f5bbf-fwhks                           2/2     Running   0          29h   10.245.5.178   10.20.0.143   <none>           <none>
mercury-work-processor-6b888f5bbf-qnxpp                           2/2     Running   0          29h   10.245.4.135   10.20.1.227   <none>           <none>
[21:January:2022:04:40:45]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ hln mercury-psr
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_prod.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_prod.config

chmoNAME      NAMESPACE   REVISION  UPDATED                                 STATUS    CHART                 APP VERSION
app-kafka mercury-psr 1         2022-01-19 22:48:59.456337456 +0000 UTC deployed  cpe-kafka-0.1.0       0.1.0
appdata   mercury-psr 1         2022-01-19 22:49:17.75707579 +0000 UTC  deployed  kafka-data-0.1.0      null
mercury   mercury-psr 1         2022-01-19 22:50:04.789572242 +0000 UTC deployed  mercury-2201.18.1305  22.01.18-REL21.11-1305
[21:January:2022:04:41:10]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[21:January:2022:04:41:10]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ chmod 400 /home/opc/.kube/pv2_prod.config
[21:January:2022:04:41:13]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ date
Fri Jan 21 04:41:14 UTC 2022
[21:January:2022:04:41:14]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn mercury-psr mercury-work-processor-6b888f5bbf-2twtx -oyaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubernetes.io/psp: psp-protect-docker
    vault.security.banzaicloud.io/log-level: warn
    vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
    vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
    vault.security.banzaicloud.io/vault-env-daemon: "true"
    vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
    vault.security.banzaicloud.io/vault-role: mercury-psr
    vault.security.banzaicloud.io/vault-skip-verify: "true"
  creationTimestamp: "2022-01-19T22:53:30Z"
  generateName: mercury-work-processor-6b888f5bbf-
  labels:
    app.kubernetes.io/instance: mercury
    app.kubernetes.io/name: work-processor
    chronos.enabled: "True"
    chronos.maxPodLifetime: "1440"
    chronos.minAvailable: "0"
    pod-template-hash: 6b888f5bbf
  name: mercury-work-processor-6b888f5bbf-2twtx
  namespace: mercury-psr
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: mercury-work-processor-6b888f5bbf
    uid: ba335149-3910-48d8-9011-f12e166b5065
  resourceVersion: "292924915"
  selfLink: /api/v1/namespaces/mercury-psr/pods/mercury-work-processor-6b888f5bbf-2twtx
  uid: 37ba6a96-6144-4b28-8e9c-96c924c69509
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - work-processor
            - key: release
              operator: In
              values:
              - mercury
          topologyKey: failure-domain.beta.kubernetes.io/zone
        weight: 100
  containers:
  - args:
    - agent
    - -config
    - /vault/config/config.hcl
    env:
    - name: VAULT_ADDR
      value: https://vault.query.prod.consul:8200
    - name: VAULT_SKIP_VERIFY
      value: "true"
    image: docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5
    imagePullPolicy: IfNotPresent
    name: vault-agent
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 100m
        memory: 128Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        add:
        - IPC_LOCK
        - SYS_PTRACE
        drop:
        - NET_RAW
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /vault/
      name: vault-env
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: mercurygeneric-token-b6n4q
      readOnly: true
    - mountPath: /vault/secrets
      name: agent-secrets
    - mountPath: /vault/config/config.hcl
      name: agent-configmap
      readOnly: true
      subPath: config.hcl
  - args:
    - /bin/sh
    - -c
    - /mercury/runService.sh /work-processor/bin/work-processor
    command:
    - /vault/vault-env
    env:
    - name: AUTO_REFRESH_CONSUL_TOKEN
      value: vault:cpe_consul/creds/mercury-psr#token
    - name: KAFKA_TEMPLATED_PKI
      value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
        .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
        "ttl": "720h"}'
    - name: KEYSTORE_LOCATION
      value: /keystores
    - name: TRUSTSTORE_NAME
      value: truststore.jks
    - name: KEYSTORE_SECRETPATH
      value: /tmp/keystore-pass
    - name: KEYSTORE_TMPPASSFILENAME
      value: props/kafka.properties
    - name: KEYSTORE_NAME
      value: keystore.jks
    - name: SERVICEACCOUNT_SECRETPATH
      value: /var/run/secrets/kubernetes.io/serviceaccount
    - name: VAULT_ADDR
      value: https://vault.query.prod.consul:8200
    - name: VAULT_SECRETPATH
      value: /tmp/vaultca
    - name: PKI_VAULT_ENGINE_PATH
      value: v1/infra_pki/issue/kafka_client
    - name: CERT_CN
      value: mercury-psr-mercurygeneric-rw
    - name: KAFKA_SECURITY_PROTOCOL
      value: SSL
    - name: SA_ROLE
      value: mercury-psr_mercurygeneric
    - name: VAULT_SERVICEACCOUNT
      value: mercurygeneric
    - name: profile
      value: oci-k8s
    - name: DATACENTER
      value: OCI
    - name: RELEASE_NAME
      value: mercury
    - name: POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.namespace
    - name: POD_IP
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.podIP
    - name: POD_SERVICE_ACCOUNT
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: spec.serviceAccountName
    - name: MERCURY_URL_PATTERN
      value: https://engagement-psr.us1.channels.ocs.oraclecloud.com/mercury-psr
    - name: JAEGER_REMOTE_REPORTER
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.hostIP
    - name: VAULT_TOKEN
      value: vault:login
    - name: HOST_IP
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.hostIP
    - name: CONSUL_HTTP_ADDR
      value: https://$(HOST_IP):8501
    - name: CONSUL_TOKEN
      value: vault:/cpe_consul/creds/mercury-psr#token
    - name: JAVA_OPTS
      value: '-Dvault.enabled=true -Dvault.namespace=mercury-psr -Dvault.transit.kafkaMessageEncryption.path=transit_mercury
        -Dvault.base.address=https://vault.query.prod.consul:8200 -Dvault.shared.kv.engine=/kv
        -Dthidwick.vault.uri=https://vault.query.prod.consul:8200 -Dthidwick.vault.transit.cache-expiration=300
        -Dthidwick.vault.kv.cache-expiration=300 -Dthidwick.vault.serviceaccount=mercurygeneric
        -Dlog4j2.formatMsgNoLookups=true -Dmercury.cpeVersion=v2 -Dfile.token.enabled=true
        -Dvault.authentication=TOKEN -Dvault.role=mercury-psr -Dvault.path=k8s-phx-dataplane
        -Dkafka.log_level=INFO -Dthidwick-server.logback.level=INFO -Dthidwick-server.logback.service.log_level=INFO
        -Dthidwick.kafka.message.encryption.cipher=AES/GCM -Djwt.tenant.validation.enabled=false
        -Dthidwick.kafka.schema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul
        -Dthidwick.kafka.bootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443
        -Dthidwick.tms.base.tmsStateStream.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443
        -Djavax.net.ssl.trustStore=/keystores/truststore.jks -Djavax.net.ssl.keyStore=/keystores/keystore.jks
        -Dhttp.security.headers.enabled=true -Dhttp.rest.security.headers.enabled=true
        -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Xms1536m   -Xmx4096m   -Dtenant.realm=engagement-psr.us1.channels.ocs.oraclecloud.com '
    - name: VAULT_ADDR
      value: https://vault.query.prod.consul:8200
    - name: VAULT_SKIP_VERIFY
      value: "true"
    - name: VAULT_AUTH_METHOD
      value: jwt
    - name: VAULT_PATH
      value: k8s-phx-dataplane
    - name: VAULT_ROLE
      value: mercury-psr
    - name: VAULT_IGNORE_MISSING_SECRETS
      value: "false"
    - name: VAULT_ENV_PASSTHROUGH
    - name: VAULT_JSON_LOG
      value: "false"
    - name: VAULT_CLIENT_TIMEOUT
      value: 10s
    - name: VAULT_LOG_LEVEL
      value: warn
    - name: VAULT_ENV_DAEMON
      value: "true"
    image: iad.ocir.io/osvcstage/mercury/work-processor:21.10.29-TRUNK-943
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 1
      httpGet:
        path: /health/checks
        port: 8080
        scheme: HTTP
      initialDelaySeconds: 5
      periodSeconds: 30
      successThreshold: 1
      timeoutSeconds: 5
    name: work-processor
    ports:
    - containerPort: 8080
      name: http
      protocol: TCP
    readinessProbe:
      failureThreshold: 1
      httpGet:
        path: /health/ping
        port: 8080
        scheme: HTTP
      initialDelaySeconds: 5
      periodSeconds: 30
      successThreshold: 1
      timeoutSeconds: 5
    resources:
      limits:
        cpu: "3"
        ephemeral-storage: 10Gi
        memory: 8Gi
      requests:
        cpu: "2"
        ephemeral-storage: 5Gi
        memory: 3Gi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - NET_RAW
      runAsUser: 0
    startupProbe:
      failureThreshold: 120
      httpGet:
        path: /health/checks
        port: 8080
        scheme: HTTP
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 5
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /keystores
      name: keystore-volume
    - mountPath: /tmp/keystore-pass
      name: keystore-pass
    - mountPath: /tmp/vaultca
      name: vaultca
      readOnly: true
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: mercurygeneric-token-b6n4q
      readOnly: true
    - mountPath: /vault/
      name: vault-env
    - mountPath: /vault/secrets
      name: agent-secrets
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  imagePullSecrets:
  - name: osvcstage-ocirsecret
  initContainers:
  - command:
    - sh
    - -c
    - cp /usr/local/bin/vault-env /vault/
    image: docker-master.cdaas.oraclecloud.com/docker-osvc-local/banzaicloud/vault-env:1.14.3
    imagePullPolicy: IfNotPresent
    name: copy-vault-env
    resources:
      limits:
        cpu: 250m
        memory: 64Mi
      requests:
        cpu: 50m
        memory: 64Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - NET_RAW
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /vault/
      name: vault-env
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: mercurygeneric-token-b6n4q
      readOnly: true
  nodeName: 10.20.1.27
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: mercurygeneric
  serviceAccountName: mercurygeneric
  shareProcessNamespace: true
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: vaultca
    secret:
      defaultMode: 420
      secretName: vault-tls
  - name: keystore-pass
    secret:
      defaultMode: 420
      secretName: mercury-work-processor-keystore
  - emptyDir: {}
    name: keystore-volume
  - name: mercurygeneric-token-b6n4q
    secret:
      defaultMode: 420
      secretName: mercurygeneric-token-b6n4q
  - emptyDir:
      medium: Memory
    name: vault-env
  - emptyDir:
      medium: Memory
    name: agent-secrets
  - configMap:
      defaultMode: 420
      items:
      - key: config.hcl
        path: config.hcl
      name: mercury-psr-va-configmap
    name: agent-configmap
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2022-01-19T22:53:32Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2022-01-19T22:54:27Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2022-01-19T22:54:27Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2022-01-19T22:53:30Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: docker://2beebe89f70597e31733d3e7354a4c7bf164700413185801653e98cf3cbbbd49
    image: docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5
    imageID: docker-pullable://docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault@sha256:57438e97b72c05368ca06f4d5f34667e965248635718001abdea8fa11f18adad
    lastState: {}
    name: vault-agent
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2022-01-19T22:53:33Z"
  - containerID: docker://343e152859c52bd3e06b50295d6c7d3692337a695271303fd5a0fda3651cce4f
    image: iad.ocir.io/osvcstage/mercury/work-processor:21.10.29-TRUNK-943
    imageID: docker-pullable://iad.ocir.io/osvcstage/mercury/work-processor@sha256:ce681f5575ddaeb51932d27f2f2110d0161b0d860b0eadc5f8327f8e14634c61
    lastState: {}
    name: work-processor
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2022-01-19T22:53:33Z"
  hostIP: 10.20.1.27
  initContainerStatuses:
  - containerID: docker://d94b4bf6d8f3d5976a4bc4253416f1422a9302e406b960876c5d69667a642786
    image: docker-master.cdaas.oraclecloud.com/docker-osvc-local/banzaicloud/vault-env:1.14.3
    imageID: docker-pullable://docker-master.cdaas.oraclecloud.com/docker-osvc-local/banzaicloud/vault-env@sha256:7435d1c641c0aa0fd4faaff32ab189b29d3d909b100d36d4d90df3b79cee8ce3
    lastState: {}
    name: copy-vault-env
    ready: true
    restartCount: 0
    state:
      terminated:
        containerID: docker://d94b4bf6d8f3d5976a4bc4253416f1422a9302e406b960876c5d69667a642786
        exitCode: 0
        finishedAt: "2022-01-19T22:53:31Z"
        reason: Completed
        startedAt: "2022-01-19T22:53:31Z"
  phase: Running
  podIP: 10.245.4.13
  podIPs:
  - ip: 10.245.4.13
  qosClass: Burstable
  startTime: "2022-01-19T22:53:30Z"
[21:January:2022:04:43:43]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[21:January:2022:04:44:13]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgn mercury-psr kafka
NAME                DESIRED KAFKA REPLICAS   DESIRED ZK REPLICAS
mercury-psr-kafka   9                        3
[21:January:2022:04:44:24]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgn mercury-psr kafka mercury-psr-kafka -oyaml
apiVersion: kafka.strimzi.io/v1beta1
kind: Kafka
metadata:
  annotations:
    meta.helm.sh/release-name: app-kafka
    meta.helm.sh/release-namespace: mercury-psr
  creationTimestamp: "2022-01-19T22:49:08Z"
  generation: 1
  labels:
    app: cpekafka
    app.kubernetes.io/managed-by: Helm
    chart: cpe-kafka-0.1.0
    cluster_name: mercury-psr-kafka-phx
    heritage: Helm
    release: app-kafka
  name: mercury-psr-kafka
  namespace: mercury-psr
  resourceVersion: "292930294"
  selfLink: /apis/kafka.strimzi.io/v1beta1/namespaces/mercury-psr/kafkas/mercury-psr-kafka
  uid: 6c1b8b47-fa1c-4305-b04b-bb540933198b
spec:
  clientsCa:
    generateCertificateAuthority: false
  clusterCa:
    generateCertificateAuthority: false
  entityOperator:
    template:
      pod:
        metadata:
          labels:
            stage: dataplane
    tlsSidecar:
      livenessProbe: {}
      readinessProbe: {}
      resources:
        limits:
          cpu: 500m
          memory: 1500M
        requests:
          cpu: 100m
          memory: 750M
    topicOperator:
      livenessProbe: {}
      readinessProbe: {}
      resources:
        limits:
          cpu: 1000m
          memory: 1500M
        requests:
          cpu: 500m
          memory: 750M
    userOperator:
      livenessProbe: {}
      readinessProbe: {}
      resources:
        limits:
          cpu: 1000m
          memory: 1500M
        requests:
          cpu: 500m
          memory: 750M
  kafka:
    authorization:
      superUsers:
      - CN=mirrormaker
      type: simple
    config:
      auto.create.topics.enable: false
      auto.leader.rebalance.enable: true
      background.threads: 20
      default.replication.factor: 3
      group.initial.rebalance.delay.ms: 3
      group.max.session.timeout.ms: 300000
      leader.imbalance.per.broker.percentage: 10
      log.message.format.version: 2.5
      max.incremental.fetch.session.cache.slots: 4000
      max.request.size: 8388608
      message.max.bytes: 4194304
      num.io.threads: 8
      num.network.threads: 3
      num.partitions: 18
      offsets.topic.replication.factor: 3
      queued.max.requests: 500
      replica.fetch.max.bytes: 4194304
      replica.lag.time.max.ms: 10000
      socket.receive.buffer.bytes: 102400
      socket.request.max.bytes: 104857600
      socket.send.buffer.bytes: 102400
      transaction.abort.timed.out.transaction.cleanup.interval.ms: 60000
      transaction.state.log.min.isr: 2
      transaction.state.log.replication.factor: 3
      zookeeper.connection.timeout.ms: 6000
      zookeeper.max.in.flight.requests: 10
      zookeeper.session.timeout.ms: 6000
      zookeeper.sync.time.ms: 2000
    jvmOptions:
      -Xms: 8192m
      -Xmx: 24576m
    listeners:
      external:
        authentication:
          type: tls
        class: kafka
        configuration:
          bootstrap:
            dnsAnnotations:
              ingressWatcher/port: "443"
              ingressWatcher/service: mercury-psr-kafka-bootstrap-mercury-psr
            host: mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul
          brokers:
          - broker: 0
            dnsAnnotations:
              ingressWatcher/port: "443"
              ingressWatcher/service: mercury-psr-kafka-broker-0-mercury-psr
            host: mercury-psr-kafka-broker-0-mercury-psr.service.phx-dataplane.prod.consul
          - broker: 1
            dnsAnnotations:
              ingressWatcher/port: "443"
              ingressWatcher/service: mercury-psr-kafka-broker-1-mercury-psr
            host: mercury-psr-kafka-broker-1-mercury-psr.service.phx-dataplane.prod.consul
          - broker: 2
            dnsAnnotations:
              ingressWatcher/port: "443"
              ingressWatcher/service: mercury-psr-kafka-broker-2-mercury-psr
            host: mercury-psr-kafka-broker-2-mercury-psr.service.phx-dataplane.prod.consul
          - broker: 3
            dnsAnnotations:
              ingressWatcher/port: "443"
              ingressWatcher/service: mercury-psr-kafka-broker-3-mercury-psr
            host: mercury-psr-kafka-broker-3-mercury-psr.service.phx-dataplane.prod.consul
          - broker: 4
            dnsAnnotations:
              ingressWatcher/port: "443"
              ingressWatcher/service: mercury-psr-kafka-broker-4-mercury-psr
            host: mercury-psr-kafka-broker-4-mercury-psr.service.phx-dataplane.prod.consul
          - broker: 5
            dnsAnnotations:
              ingressWatcher/port: "443"
              ingressWatcher/service: mercury-psr-kafka-broker-5-mercury-psr
            host: mercury-psr-kafka-broker-5-mercury-psr.service.phx-dataplane.prod.consul
          - broker: 6
            dnsAnnotations:
              ingressWatcher/port: "443"
              ingressWatcher/service: mercury-psr-kafka-broker-6-mercury-psr
            host: mercury-psr-kafka-broker-6-mercury-psr.service.phx-dataplane.prod.consul
          - broker: 7
            dnsAnnotations:
              ingressWatcher/port: "443"
              ingressWatcher/service: mercury-psr-kafka-broker-7-mercury-psr
            host: mercury-psr-kafka-broker-7-mercury-psr.service.phx-dataplane.prod.consul
          - broker: 8
            dnsAnnotations:
              ingressWatcher/port: "443"
              ingressWatcher/service: mercury-psr-kafka-broker-8-mercury-psr
            host: mercury-psr-kafka-broker-8-mercury-psr.service.phx-dataplane.prod.consul
        type: ingress
      plain: {}
      tls:
        authentication:
          type: tls
    livenessProbe: {}
    metrics:
      lowercaseOutputName: true
      rules:
      - labels:
          clientId: $3
          partition: $5
          topic: $4
        name: kafka_server_$1_$2
        pattern: kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value
        type: GAUGE
      - labels:
          broker: $4:$5
          clientId: $3
        name: kafka_server_$1_$2
        pattern: kafka.server<type=(.+), name=(.+), clientId=(.+), brokerHost=(.+),
          brokerPort=(.+)><>Value
        type: GAUGE
      - name: kafka_$1_$2_$3_percent
        pattern: kafka.(\w+)<type=(.+), name=(.+)Percent\w*><>MeanRate
        type: GAUGE
      - name: kafka_$1_$2_$3_percent
        pattern: kafka.(\w+)<type=(.+), name=(.+)Percent\w*><>Value
        type: GAUGE
      - labels:
          $4: $5
        name: kafka_$1_$2_$3_percent
        pattern: kafka.(\w+)<type=(.+), name=(.+)Percent\w*, (.+)=(.+)><>Value
        type: GAUGE
      - labels:
          $4: $5
          $6: $7
        name: kafka_$1_$2_$3_total
        pattern: kafka.(\w+)<type=(.+), name=(.+)PerSec\w*, (.+)=(.+), (.+)=(.+)><>Count
        type: COUNTER
      - labels:
          $4: $5
        name: kafka_$1_$2_$3_total
        pattern: kafka.(\w+)<type=(.+), name=(.+)PerSec\w*, (.+)=(.+)><>Count
        type: COUNTER
      - name: kafka_$1_$2_$3_total
        pattern: kafka.(\w+)<type=(.+), name=(.+)PerSec\w*><>Count
        type: COUNTER
      - labels:
          $4: $5
          $6: $7
        name: kafka_$1_$2_$3
        pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.+), (.+)=(.+)><>Value
        type: GAUGE
      - labels:
          $4: $5
        name: kafka_$1_$2_$3
        pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.+)><>Value
        type: GAUGE
      - name: kafka_$1_$2_$3
        pattern: kafka.(\w+)<type=(.+), name=(.+)><>Value
        type: GAUGE
      - labels:
          $4: $5
          $6: $7
        name: kafka_$1_$2_$3_count
        pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.+), (.+)=(.+)><>Count
        type: COUNTER
      - labels:
          $4: $5
          $6: $7
          quantile: 0.$8
        name: kafka_$1_$2_$3
        pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.*), (.+)=(.+)><>(\d+)thPercentile
        type: GAUGE
      - labels:
          $4: $5
        name: kafka_$1_$2_$3_count
        pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.+)><>Count
        type: COUNTER
      - labels:
          $4: $5
          quantile: 0.$6
        name: kafka_$1_$2_$3
        pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.*)><>(\d+)thPercentile
        type: GAUGE
      - name: kafka_$1_$2_$3_count
        pattern: kafka.(\w+)<type=(.+), name=(.+)><>Count
        type: COUNTER
      - labels:
          quantile: 0.$4
        name: kafka_$1_$2_$3
        pattern: kafka.(\w+)<type=(.+), name=(.+)><>(\d+)thPercentile
        type: GAUGE
    rack:
      topologyKey: failure-domain.beta.kubernetes.io/zone
    readinessProbe: {}
    replicas: 9
    resources:
      limits:
        cpu: 12
        memory: 50Gi
      requests:
        cpu: 6
        memory: 25Gi
    storage:
      type: jbod
      volumes:
      - class: oci-bv
        deleteClaim: false
        id: 0
        size: 100Gi
        type: persistent-claim
    template:
      pod:
        metadata:
          labels:
            stage: dataplane
    tlsSidecar:
      livenessProbe: {}
      readinessProbe: {}
      resources:
        limits:
          cpu: 1000m
          memory: 1500M
        requests:
          cpu: 500m
          memory: 750M
    version: 2.5.0
  kafkaExporter:
    groupRegex: .*
    livenessProbe:
      initialDelaySeconds: 300
      periodSeconds: 60
      timeoutSeconds: 600
    readinessProbe:
      initialDelaySeconds: 300
      periodSeconds: 60
      timeoutSeconds: 600
    resources:
      limits:
        cpu: 2
        memory: 2G
      requests:
        cpu: 1
        memory: 512m
    template:
      pod:
        metadata:
          labels:
            stage: dataplane
    topicRegex: .*
  zookeeper:
    livenessProbe: {}
    metrics:
      lowercaseOutputName: true
      rules:
      - name: zookeeper_$2
        pattern: org.apache.ZooKeeperService<name0=ReplicatedServer_id(\d+)><>(\w+)
        type: GAUGE
      - labels:
          replicaId: $2
        name: zookeeper_$3
        pattern: org.apache.ZooKeeperService<name0=ReplicatedServer_id(\d+), name1=replica.(\d+)><>(\w+)
        type: GAUGE
      - labels:
          memberType: $3
          replicaId: $2
        name: zookeeper_$4
        pattern: org.apache.ZooKeeperService<name0=ReplicatedServer_id(\d+), name1=replica.(\d+),
          name2=(\w+)><>(Packets.*)
        type: COUNTER
      - labels:
          memberType: $3
          replicaId: $2
        name: zookeeper_$4
        pattern: org.apache.ZooKeeperService<name0=ReplicatedServer_id(\d+), name1=replica.(\d+),
          name2=(\w+)><>(\w+)
        type: GAUGE
      - labels:
          memberType: $3
          replicaId: $2
        name: zookeeper_$4_$5
        pattern: org.apache.ZooKeeperService<name0=ReplicatedServer_id(\d+), name1=replica.(\d+),
          name2=(\w+), name3=(\w+)><>(\w+)
        type: GAUGE
      - name: zookeeper_$2
        pattern: org.apache.ZooKeeperService<name0=StandaloneServer_port(\d+)><>(\w+)
        type: GAUGE
      - name: zookeeper_$2
        pattern: org.apache.ZooKeeperService<name0=StandaloneServer_port(\d+), name1=InMemoryDataTree><>(\w+)
        type: GAUGE
    readinessProbe: {}
    replicas: 3
    resources:
      limits:
        cpu: 4
        memory: 4Gi
      requests:
        cpu: 250m
        memory: 1Gi
    storage:
      class: oci-bv
      deleteClaim: false
      size: 50Gi
      type: persistent-claim
    template:
      pod:
        metadata:
          labels:
            stage: dataplane
status:
  conditions:
  - lastTransitionTime: 2022-01-19T22:58:01+0000
    status: "True"
    type: Ready
  listeners:
  - addresses:
    - host: mercury-psr-kafka-kafka-bootstrap.mercury-psr.svc
      port: 9092
    bootstrapServers: mercury-psr-kafka-kafka-bootstrap.mercury-psr.svc:9092
    type: plain
  - addresses:
    - host: mercury-psr-kafka-kafka-bootstrap.mercury-psr.svc
      port: 9093
    bootstrapServers: mercury-psr-kafka-kafka-bootstrap.mercury-psr.svc:9093
    certificates:
    - |
      -----BEGIN CERTIFICATE-----
      MIIDDTCCArOgAwIBAgIJAMgtM2WBsKH1MAoGCCqGSM49BAMCMIG5MQswCQYDVQQG
      EwJVUzELMAkGA1UECBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xGjAYBgNV
      BAkTETEwMSBTZWNvbmQgU3RyZWV0MQ4wDAYDVQQREwU5NDEwNTEXMBUGA1UEChMO
      SGFzaGlDb3JwIEluYy4xQDA+BgNVBAMTN0NvbnN1bCBBZ2VudCBDQSAyMTQyMDQ5
      MjU3OTY3NTM1NDg1MDU5Nzk0NTU4MDY3NDgxNzk5OTcwHhcNMjAxMTE1MDQwMjQ5
      WhcNMjUxMTE0MDQwMjQ5WjB0MQswCQYDVQQGEwJVUzELMAkGA1UECAwCQ0ExFTAT
      BgNVBAcMDFJlZHdvb2QgQ2l0eTEPMA0GA1UECgwGT3JhY2xlMQ0wCwYDVQQLDARP
      U3ZDMSEwHwYDVQQDDBhpbnRlcm1lZGlhdGUucHJvZC5jb25zdWwwggEiMA0GCSqG
      SIb3DQEBAQUAA4IBDwAwggEKAoIBAQDuRbU5YAglL9uXoJQxQFhTxnVjbYrJzQmn
      Z96AnWiOQCxNzn21q+lVzOHA/1OCBTsTJkwANK2fSj+KKJLJZYl9I0XeeDOmtegt
      TjSQDTx7SviZN4aXmGwe7JOeFhVMsQWDefWskmPMrdG6cFEXhxGlcKgPu+lTaH6F
      KwQUCM78Gxv50LqeNY6yzUIBYhAbuezf7UGZtXQpDhAZGTi+BnzC1NUr9U27wyHw
      6tWjJHTNEMWXpxeSQVoUy4jNPOODu+Cd1dV/J+5tZ+vNdx1eHhrx1cLNxZboys6W
      KS1H4kQiGhVstMq/U/zfKo3abzUVvjmJxWSLU+ceJk5g7qxjb9pfAgMBAAGjHTAb
      MAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEGMAoGCCqGSM49BAMCA0gAMEUCIQCR
      FFhxwM04VucNC1Qv7s7GQRVUBL2RslvwugdzuhQWGwIgARBXWxFeXm0Z3V7pKy7n
      zG/xgztCPqknPYn64n2ixfI=
      -----END CERTIFICATE-----
      -----BEGIN CERTIFICATE-----
      MIIC7jCCApSgAwIBAgIRAKEmXjqrUnCe+PrM7N4vPh0wCgYIKoZIzj0EAwIwgbkx
      CzAJBgNVBAYTAlVTMQswCQYDVQQIEwJDQTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNj
      bzEaMBgGA1UECRMRMTAxIFNlY29uZCBTdHJlZXQxDjAMBgNVBBETBTk0MTA1MRcw
      FQYDVQQKEw5IYXNoaUNvcnAgSW5jLjFAMD4GA1UEAxM3Q29uc3VsIEFnZW50IENB
      IDIxNDIwNDkyNTc5Njc1MzU0ODUwNTk3OTQ1NTgwNjc0ODE3OTk5NzAeFw0yMDEx
      MTMxODU2MjhaFw0yNTExMTIxODU2MjhaMIG5MQswCQYDVQQGEwJVUzELMAkGA1UE
      CBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xGjAYBgNVBAkTETEwMSBTZWNv
      bmQgU3RyZWV0MQ4wDAYDVQQREwU5NDEwNTEXMBUGA1UEChMOSGFzaGlDb3JwIElu
      Yy4xQDA+BgNVBAMTN0NvbnN1bCBBZ2VudCBDQSAyMTQyMDQ5MjU3OTY3NTM1NDg1
      MDU5Nzk0NTU4MDY3NDgxNzk5OTcwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAARK
      h/7kcfF7vBPn9A31JOwrsb2PNw9bNvm4x4IHqyBwXNQ5YVRezE4bcxiTyqf77Ttp
      W+R5oTsqT2JlGs5mHmf/o3sweTAOBgNVHQ8BAf8EBAMCAYYwDwYDVR0TAQH/BAUw
      AwEB/zApBgNVHQ4EIgQgq3SPPO6C8UnSfeGCgpsxbosB129T1ibzV6/RJXtXaB8w
      KwYDVR0jBCQwIoAgq3SPPO6C8UnSfeGCgpsxbosB129T1ibzV6/RJXtXaB8wCgYI
      KoZIzj0EAwIDSAAwRQIgETUzXkCddGqK/ZTxiXDyH6Z0sLEsHIPa36qon+Qk7DwC
      IQCA94LgTaKHvd/0NjHzyY8GgLoDCfmFyii6ZZJfh/oK6Q==
      -----END CERTIFICATE-----
    type: tls
  observedGeneration: 1
[21:January:2022:04:44:35]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[21:January:2022:04:45:07]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgn mercury-psr -oyaml
You must specify the type of resource to get. Use "kubectl api-resources" for a complete list of supported resources.

error: Required resource not specified.
Use "kubectl explain <resource>" for a detailed description of that resource (e.g. kubectl explain pods).
See 'kubectl get -h' for help and examples
[21:January:2022:04:45:14]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kg namespace mercury-psr -oyaml
apiVersion: v1
kind: Namespace
metadata:
  annotations:
    meta.helm.sh/release-name: mercury-psr
    meta.helm.sh/release-namespace: default
    ociGroup: osvcprod-OCI_Mercury
    ociRegions: us-ashburn-1,us-phoenix-1,eu-amsterdam-1,eu-frankfurt-1,uk-cardiff-1,uk-london-1,ap-melbourne-1,ap-sydney-1,ca-toronto-1,ca-montreal-1,ap-hyderabad-1,ap-mumbai-1,ap-osaka-1,ap-tokyo-1,sa-saopaulo-1,sa-vinhedo-1,me-jeddah-1,me-dubai-1,eu-zurich-1,sa-santiago-1
  creationTimestamp: "2021-10-14T05:50:27Z"
  labels:
    app.kubernetes.io/managed-by: Helm
  name: mercury-psr
  resourceVersion: "268514029"
  selfLink: /api/v1/namespaces/mercury-psr
  uid: aa58a547-5205-4738-a95b-245af24363af
spec:
  finalizers:
  - kubernetes
status:
  phase: Active
[21:January:2022:04:45:29]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kg resourcequota -n mercury-psr -oyaml
apiVersion: v1
items:
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    annotations:
      meta.helm.sh/release-name: mercury-psr
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2021-10-14T05:50:27Z"
    labels:
      app: oracle-namespace-0.0.9
      app.kubernetes.io/managed-by: Helm
      chart: oracle-namespace-0.0.9
      heritage: Helm
      release: mercury-psr
    name: mercury-psr-resource-quota
    namespace: mercury-psr
    resourceVersion: "292928637"
    selfLink: /api/v1/namespaces/mercury-psr/resourcequotas/mercury-psr-resource-quota
    uid: 6160126f-c345-401d-b109-e71c366f6a1c
  spec:
    hard:
      limits.cpu: "480"
      limits.memory: 2000Gi
      pods: "240"
      requests.cpu: "180"
      requests.memory: 1100Gi
  status:
    hard:
      limits.cpu: "480"
      limits.memory: 2000Gi
      pods: "240"
      requests.cpu: "180"
      requests.memory: 1100Gi
    used:
      limits.cpu: 368800m
      limits.memory: 1404700146Ki
      pods: "166"
      requests.cpu: 145450m
      requests.memory: 619556444672512m
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
[21:January:2022:04:46:35]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn mercury-psr
NAME                                               READY   UP-TO-DATE   AVAILABLE   AGE
mercury-agent-command-service                      3/3     3            3           29h
mercury-channel-api                                1/1     1            1           29h
mercury-consumer-command-service                   3/3     3            3           29h
mercury-custom-availability-service                3/3     3            3           29h
mercury-data-mask-api                              3/3     3            3           29h
mercury-engagement-queue-api                       3/3     3            3           29h
mercury-enrichment-service                         3/3     3            3           29h
mercury-event-sync-service                         3/3     3            3           29h
mercury-integration-in-processor                   3/3     3            3           29h
mercury-integration-out-processor                  3/3     3            3           29h
mercury-kweet-facebook-client                      3/3     3            3           29h
mercury-kweet-facebook-webhook                     3/3     3            3           29h
mercury-kweet-twiliosms-client                     3/3     3            3           29h
mercury-kweet-userprofiles                         3/3     3            3           29h
mercury-kweet-wechat-client                        0/0     0            0           29h
mercury-kweet-wechat-webhook                       0/0     0            0           29h
mercury-mercury-ui                                 3/3     3            3           29h
mercury-metric-aggregation-processor               3/3     3            3           29h
mercury-metric-fusion-bridge                       3/3     3            3           29h
mercury-metric-generation-processor                3/3     3            3           29h
mercury-metric-internal-translation-processor      3/3     3            3           29h
mercury-metric-proxy-service                       3/3     3            3           29h
mercury-omnichannel-assignment-processor           3/3     3            3           29h
mercury-omnichannel-offer-processor                3/3     3            3           29h
mercury-osvc-bridge-api-services                   3/3     3            3           29h
mercury-osvc-bridge-metrics-data-pipeline          3/3     3            3           29h
mercury-osvc-bridge-osvc-data-extractor            3/3     3            3           29h
mercury-osvc-bridge-provisioning-processor         1/1     1            1           29h
mercury-osvc-bridge-state-processor                3/3     3            3           29h
mercury-osvc-bridge-state-query-service            3/3     3            3           29h
mercury-osvc-bridge-task-controller                3/3     3            3           29h
mercury-provisioning-monitor                       3/3     3            3           29h
mercury-provisioning-processor                     3/3     3            3           29h
mercury-psr-kafka-entity-operator                  1/1     1            1           29h
mercury-psr-kafka-kafka-exporter                   1/1     1            1           29h
mercury-queue-agent-info-processor                 3/3     3            3           29h
mercury-realtime-channel-processor                 3/3     3            3           29h
mercury-resource-channel-processor                 3/3     3            3           29h
mercury-resource-state-processor                   3/3     3            3           29h
mercury-resource-work-processor                    3/3     3            3           29h
mercury-routing-processor-agent-assignment         3/3     3            3           29h
mercury-routing-processor-agent-events-processor   3/3     3            3           29h
mercury-routing-processor-queue-assignment         3/3     3            3           29h
mercury-routing-processor-work-events-processor    3/3     3            3           29h
mercury-session-housekeeping-processor             3/3     3            3           29h
mercury-session-processor                          3/3     3            3           29h
mercury-single-sign-on-service                     3/3     3            3           29h
mercury-social-bridge                              3/3     3            3           29h
mercury-social-config                              3/3     3            3           29h
mercury-static-assets-service                      3/3     3            3           29h
mercury-tenant-downtime-monitor                    3/3     3            3           29h
mercury-transcript-api                             3/3     3            3           29h
mercury-transcript-processor                       3/3     3            3           29h
mercury-user-preference-service                    3/3     3            3           29h
mercury-work-api                                   3/3     3            3           29h
mercury-work-processor                             3/3     3            3           29h
[21:January:2022:04:47:10]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn mercury-psr mercury-agent-command-service -oyaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
    meta.helm.sh/release-name: mercury
    meta.helm.sh/release-namespace: mercury-psr
  creationTimestamp: "2022-01-19T22:53:28Z"
  generation: 1
  labels:
    app.kubernetes.io/instance: mercury
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: agent-command-service
    helm.sh/chart: agent-command-service-22.01.18-REL21.11-4877
    monitoring: "true"
    project: mercury
    version: "0"
  name: mercury-agent-command-service
  namespace: mercury-psr
  resourceVersion: "292928780"
  selfLink: /apis/apps/v1/namespaces/mercury-psr/deployments/mercury-agent-command-service
  uid: 71b67bc2-55fb-4fdf-a24b-17d51e39338d
spec:
  progressDeadlineSeconds: 600
  replicas: 3
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/name: agent-command-service
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      annotations:
        vault.security.banzaicloud.io/log-level: warn
        vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
        vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
        vault.security.banzaicloud.io/vault-env-daemon: "true"
        vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
        vault.security.banzaicloud.io/vault-role: mercury-psr
        vault.security.banzaicloud.io/vault-skip-verify: "true"
      creationTimestamp: null
      labels:
        app.kubernetes.io/instance: mercury
        app.kubernetes.io/name: agent-command-service
        chronos.enabled: "True"
        chronos.maxPodLifetime: "1440"
        chronos.minAvailable: "0"
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - agent-command-service
                - key: release
                  operator: In
                  values:
                  - mercury
              topologyKey: failure-domain.beta.kubernetes.io/zone
            weight: 100
      containers:
      - env:
        - name: AUTO_REFRESH_CONSUL_TOKEN
          value: vault:cpe_consul/creds/mercury-psr#token
        - name: KAFKA_TEMPLATED_PKI
          value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
            .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
            "ttl": "720h"}'
        - name: KEYSTORE_LOCATION
          value: /keystores
        - name: TRUSTSTORE_NAME
          value: truststore.jks
        - name: KEYSTORE_SECRETPATH
          value: /tmp/keystore-pass
        - name: KEYSTORE_TMPPASSFILENAME
          value: props/kafka.properties
        - name: KEYSTORE_NAME
          value: keystore.jks
        - name: SERVICEACCOUNT_SECRETPATH
          value: /var/run/secrets/kubernetes.io/serviceaccount
        - name: VAULT_ADDR
          value: https://vault.query.prod.consul:8200
        - name: VAULT_SECRETPATH
          value: /tmp/vaultca
        - name: PKI_VAULT_ENGINE_PATH
          value: v1/infra_pki/issue/kafka_client
        - name: CERT_CN
          value: mercury-psr-mercurygeneric-rw
        - name: KAFKA_SECURITY_PROTOCOL
          value: SSL
        - name: SA_ROLE
          value: mercury-psr_mercurygeneric
        - name: VAULT_SERVICEACCOUNT
          value: mercurygeneric
        - name: profile
          value: oci-k8s
        - name: DATACENTER
          value: OCI
        - name: RELEASE_NAME
          value: mercury
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: POD_SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.serviceAccountName
        - name: MERCURY_URL_PATTERN
          value: https://engagement-psr.us1.channels.ocs.oraclecloud.com/mercury-psr
        - name: JAEGER_REMOTE_REPORTER
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.hostIP
        - name: VAULT_TOKEN
          value: vault:login
        - name: HOST_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.hostIP
        - name: CONSUL_HTTP_ADDR
          value: https://$(HOST_IP):8501
        - name: CONSUL_TOKEN
          value: vault:/cpe_consul/creds/mercury-psr#token
        - name: JAVA_OPTS
          value: '-Dvault.enabled=true -Dvault.namespace=mercury-psr -Dvault.transit.kafkaMessageEncryption.path=transit_mercury
            -Dvault.base.address=https://vault.query.prod.consul:8200 -Dvault.shared.kv.engine=/kv
            -Dthidwick.vault.uri=https://vault.query.prod.consul:8200 -Dthidwick.vault.transit.cache-expiration=300
            -Dthidwick.vault.kv.cache-expiration=300 -Dthidwick.vault.serviceaccount=mercurygeneric
            -Dlog4j2.formatMsgNoLookups=true -Dmercury.cpeVersion=v2 -Dfile.token.enabled=true
            -Dvault.authentication=TOKEN -Dvault.role=mercury-psr -Dvault.path=k8s-phx-dataplane
            -Dkafka.log_level=WARN -Dthidwick-server.logback.level=INFO -Dthidwick-server.logback.service.log_level=INFO
            -Dthidwick.kafka.message.encryption.cipher=AES/GCM -Djwt.tenant.validation.enabled=false
            -Dthidwick.kafka.schema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul
            -Dthidwick.kafka.bootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443
            -Dthidwick.tms.base.tmsStateStream.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443
            -Djavax.net.ssl.trustStore=/keystores/truststore.jks -Djavax.net.ssl.keyStore=/keystores/keystore.jks
            -Dhttp.security.headers.enabled=true -Dhttp.rest.security.headers.enabled=true
            -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Xms1536m   -Xmx4096m   -Dpreferences.service.url=http://mercury-user-preference-service:8080        -Dosvc.bridge.api.services.url=http://mercury-osvc-bridge-api-services:8080
            -Dcustom.availability.feature.enabled=false -Dcustom.availability.service.url=http://custom-availability-service:8080    -Dtenant.realm=engagement-psr.us1.channels.ocs.oraclecloud.com '
        image: iad.ocir.io/osvcstage/mercury/agent-command-service:22.01.18-REL21.11-4877
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 1
          httpGet:
            path: /health/checks
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 5
        name: agent-command-service
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        readinessProbe:
          failureThreshold: 1
          httpGet:
            path: /health/ping
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          limits:
            cpu: "6"
            ephemeral-storage: 50Gi
            memory: 10Gi
          requests:
            cpu: "4"
            ephemeral-storage: 2Gi
            memory: 4Gi
        securityContext:
          runAsUser: 0
        startupProbe:
          failureThreshold: 120
          httpGet:
            path: /health/checks
            port: 8080
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /keystores
          name: keystore-volume
        - mountPath: /tmp/keystore-pass
          name: keystore-pass
        - mountPath: /tmp/vaultca
          name: vaultca
          readOnly: true
      dnsPolicy: ClusterFirst
      imagePullSecrets:
      - name: osvcstage-ocirsecret
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: mercurygeneric
      serviceAccountName: mercurygeneric
      terminationGracePeriodSeconds: 30
      volumes:
      - name: vaultca
        secret:
          defaultMode: 420
          secretName: vault-tls
      - name: keystore-pass
        secret:
          defaultMode: 420
          secretName: mercury-agent-command-service-keystore
      - emptyDir: {}
        name: keystore-volume
status:
  availableReplicas: 3
  conditions:
  - lastTransitionTime: "2022-01-19T22:56:30Z"
    lastUpdateTime: "2022-01-19T22:56:30Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2022-01-19T22:53:28Z"
    lastUpdateTime: "2022-01-19T22:56:30Z"
    message: ReplicaSet "mercury-agent-command-service-59dcc858fc" has successfully
      progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 1
  readyReplicas: 3
  replicas: 3
  updatedReplicas: 3
[21:January:2022:04:47:26]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgc
CURRENT   NAME                               CLUSTER               AUTHINFO           NAMESPACE
          prod_ap-hyderabad-1_controlplane   cluster-cbjr52qcfoa   user-cbjr52qcfoa
          prod_ap-hyderabad-1_dataplane      cluster-cpj2zxktnda   user-cpj2zxktnda
          prod_ap-melbourne-1_controlplane   cluster-czgcn3bmjtg   user-czgcn3bmjtg
          prod_ap-melbourne-1_dataplane      cluster-c4dazdggfst   user-c4dazdggfst
          prod_ap-mumbai-1_controlplane      cluster-cjiru3mifia   user-cjiru3mifia
          prod_ap-mumbai-1_dataplane         cluster-ci3ypr6tmga   user-ci3ypr6tmga
          prod_ap-osaka-1_controlplane       cluster-cp5327xtmfa   user-cp5327xtmfa
          prod_ap-osaka-1_dataplane          cluster-cqldck6xatq   user-cqldck6xatq
          prod_ap-sydney-1_controlplane      cluster-c4wembugqyt   user-c4wembugqyt
          prod_ap-sydney-1_dataplane         cluster-cswezbzgyyg   user-cswezbzgyyg
          prod_ap-tokyo-1_controlplane       cluster-cz54pc3z4wa   user-cz54pc3z4wa
          prod_ap-tokyo-1_dataplane          cluster-cyjtbtbbjfq   user-cyjtbtbbjfq
          prod_ca-montreal-1_controlplane    cluster-czdoolemnqw   user-czdoolemnqw
          prod_ca-montreal-1_dataplane       cluster-cqtkytbgq3d   user-cqtkytbgq3d
          prod_ca-toronto-1_controlplane     cluster-cygcmjxhfsd   user-cygcmjxhfsd
          prod_ca-toronto-1_dataplane        cluster-cywmodfmm3d   user-cywmodfmm3d
          prod_eu-amsterdam-1_controlplane   cluster-c4tkmjwgbrt   user-c4tkmjwgbrt
          prod_eu-amsterdam-1_dataplane      cluster-cstsy3bmjrd   user-cstsy3bmjrd
          prod_eu-frankfurt-1_controlplane   cluster-cydcytcg43d   user-cydcytcg43d
          prod_eu-frankfurt-1_dataplane      cluster-crdgzbrmjsd   user-crdgzbrmjsd
          prod_eu-zurich-1_controlplane      cluster-cwmoxn3bfwq   user-cwmoxn3bfwq
          prod_eu-zurich-1_dataplane         cluster-chtoe5bnsza   user-chtoe5bnsza
          prod_me-dubai-1_controlplane       cluster-ce5puidc4ya   user-ce5puidc4ya
          prod_me-dubai-1_dataplane          cluster-c77hcd3vizq   user-c77hcd3vizq
          prod_me-jeddah-1_controlplane      cluster-cpzagsoukeq   user-cpzagsoukeq
          prod_me-jeddah-1_dataplane         cluster-cf4f222mrfa   user-cf4f222mrfa
          prod_sa-santiago-1_controlplane    cluster-czxpimgibeq   user-czxpimgibeq
          prod_sa-santiago-1_dataplane       cluster-cuutxfvj2ea   user-cuutxfvj2ea
          prod_sa-saopaulo-1_controlplane    cluster-cc75n7hujpq   user-cc75n7hujpq
          prod_sa-saopaulo-1_dataplane       cluster-cpjqrjtbotq   user-cpjqrjtbotq
          prod_sa-vinhedo-1_controlplane     cluster-cihx6aprjua   user-cihx6aprjua
          prod_sa-vinhedo-1_dataplane        cluster-ciqmbmn6wta   user-ciqmbmn6wta
          prod_uk-cardiff-1_controlplane     cluster-ctggmtbmfrg   user-ctggmtbmfrg
          prod_uk-cardiff-1_dataplane        cluster-cqwmnrqmztd   user-cqwmnrqmztd
          prod_uk-london-1_controlplane      cluster-cywgnlcmqzd   user-cywgnlcmqzd
          prod_uk-london-1_dataplane         cluster-c3tazjsmjsd   user-c3tazjsmjsd
          prod_us-ashburn-1_controlplane     cluster-czwczjzgrsw   user-czwczjzgrsw
          prod_us-ashburn-1_dataplane        cluster-c3wkyjyguzt   user-c3wkyjyguzt
          prod_us-phoenix-1_controlplane     cluster-c3tkobugrst   user-c3tkobugrst
*         prod_us-phoenix-1_dataplane        cluster-csdqylfmi2g   user-csdqylfmi2g
          prod_us-phoenix-1_deployment       cluster-c4tgolgmjsd   user-c4tgolgmjsd
[21:January:2022:04:52:56]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kuc prod_ap-mumbai-1_controlplane
error: open /home/opc/.kube/pv2_prod.config: permission denied
[21:January:2022:04:53:01]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ rp
[21:January:2022:04:53:02]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ ll /home/opc/.kube/pv2_prod.config
-r-------- 1 opc opc 95278 Jan 21 04:40 /home/opc/.kube/pv2_prod.config
[21:January:2022:04:53:12]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ chmod 600
chmod: missing operand after ‘600’
Try 'chmod --help' for more information.
[21:January:2022:04:53:18]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ chmod 600 /home/opc/.kube/pv2_prod.config
[21:January:2022:04:53:20]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kuc prod_ap-mumbai-1_controlplane
Switched to context "prod_ap-mumbai-1_controlplane".
[21:January:2022:04:53:23]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgc
CURRENT   NAME                               CLUSTER               AUTHINFO           NAMESPACE
          prod_ap-hyderabad-1_controlplane   cluster-cbjr52qcfoa   user-cbjr52qcfoa
          prod_ap-hyderabad-1_dataplane      cluster-cpj2zxktnda   user-cpj2zxktnda
          prod_ap-melbourne-1_controlplane   cluster-czgcn3bmjtg   user-czgcn3bmjtg
          prod_ap-melbourne-1_dataplane      cluster-c4dazdggfst   user-c4dazdggfst
*         prod_ap-mumbai-1_controlplane      cluster-cjiru3mifia   user-cjiru3mifia
          prod_ap-mumbai-1_dataplane         cluster-ci3ypr6tmga   user-ci3ypr6tmga
          prod_ap-osaka-1_controlplane       cluster-cp5327xtmfa   user-cp5327xtmfa
          prod_ap-osaka-1_dataplane          cluster-cqldck6xatq   user-cqldck6xatq
          prod_ap-sydney-1_controlplane      cluster-c4wembugqyt   user-c4wembugqyt
          prod_ap-sydney-1_dataplane         cluster-cswezbzgyyg   user-cswezbzgyyg
          prod_ap-tokyo-1_controlplane       cluster-cz54pc3z4wa   user-cz54pc3z4wa
          prod_ap-tokyo-1_dataplane          cluster-cyjtbtbbjfq   user-cyjtbtbbjfq
          prod_ca-montreal-1_controlplane    cluster-czdoolemnqw   user-czdoolemnqw
          prod_ca-montreal-1_dataplane       cluster-cqtkytbgq3d   user-cqtkytbgq3d
          prod_ca-toronto-1_controlplane     cluster-cygcmjxhfsd   user-cygcmjxhfsd
          prod_ca-toronto-1_dataplane        cluster-cywmodfmm3d   user-cywmodfmm3d
          prod_eu-amsterdam-1_controlplane   cluster-c4tkmjwgbrt   user-c4tkmjwgbrt
          prod_eu-amsterdam-1_dataplane      cluster-cstsy3bmjrd   user-cstsy3bmjrd
          prod_eu-frankfurt-1_controlplane   cluster-cydcytcg43d   user-cydcytcg43d
          prod_eu-frankfurt-1_dataplane      cluster-crdgzbrmjsd   user-crdgzbrmjsd
          prod_eu-zurich-1_controlplane      cluster-cwmoxn3bfwq   user-cwmoxn3bfwq
          prod_eu-zurich-1_dataplane         cluster-chtoe5bnsza   user-chtoe5bnsza
          prod_me-dubai-1_controlplane       cluster-ce5puidc4ya   user-ce5puidc4ya
          prod_me-dubai-1_dataplane          cluster-c77hcd3vizq   user-c77hcd3vizq
          prod_me-jeddah-1_controlplane      cluster-cpzagsoukeq   user-cpzagsoukeq
          prod_me-jeddah-1_dataplane         cluster-cf4f222mrfa   user-cf4f222mrfa
          prod_sa-santiago-1_controlplane    cluster-czxpimgibeq   user-czxpimgibeq
          prod_sa-santiago-1_dataplane       cluster-cuutxfvj2ea   user-cuutxfvj2ea
          prod_sa-saopaulo-1_controlplane    cluster-cc75n7hujpq   user-cc75n7hujpq
          prod_sa-saopaulo-1_dataplane       cluster-cpjqrjtbotq   user-cpjqrjtbotq
          prod_sa-vinhedo-1_controlplane     cluster-cihx6aprjua   user-cihx6aprjua
          prod_sa-vinhedo-1_dataplane        cluster-ciqmbmn6wta   user-ciqmbmn6wta
          prod_uk-cardiff-1_controlplane     cluster-ctggmtbmfrg   user-ctggmtbmfrg
          prod_uk-cardiff-1_dataplane        cluster-cqwmnrqmztd   user-cqwmnrqmztd
          prod_uk-london-1_controlplane      cluster-cywgnlcmqzd   user-cywgnlcmqzd
          prod_uk-london-1_dataplane         cluster-c3tazjsmjsd   user-c3tazjsmjsd
          prod_us-ashburn-1_controlplane     cluster-czwczjzgrsw   user-czwczjzgrsw
          prod_us-ashburn-1_dataplane        cluster-c3wkyjyguzt   user-c3wkyjyguzt
          prod_us-phoenix-1_controlplane     cluster-c3tkobugrst   user-c3tkobugrst
          prod_us-phoenix-1_dataplane        cluster-csdqylfmi2g   user-csdqylfmi2g
          prod_us-phoenix-1_deployment       cluster-c4tgolgmjsd   user-c4tgolgmjsd
[21:January:2022:04:53:27]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn tms
NAME                                                              READY   STATUS    RESTARTS   AGE   IP             NODE           NOMINATED NODE   READINESS GATES
tms-rest-proxy-tms-rest-proxy-66f4c94565-jqxbl                    1/1     Running   0          49d   10.245.0.34    10.20.98.79    <none>           <none>
tms-rest-proxy-tms-rest-proxy-66f4c94565-jtrj5                    1/1     Running   0          49d   10.245.4.186   10.20.98.223   <none>           <none>
tms-rest-proxy-tms-rest-proxy-66f4c94565-tglzt                    1/1     Running   0          49d   10.245.4.185   10.20.98.223   <none>           <none>
tms-template-service-tms-idcs-manager-7f748cb55b-26b22            1/1     Running   0          49d   10.245.6.54    10.20.98.66    <none>           <none>
tms-template-service-tms-idcs-manager-7f748cb55b-gcldz            1/1     Running   0          49d   10.245.6.55    10.20.98.66    <none>           <none>
tms-tenant-processor-tms-tenant-processor-6c557c77dd-8khlw        1/1     Running   67         49d   10.245.0.35    10.20.98.79    <none>           <none>
tms-tenant-processor-tms-tenant-processor-6c557c77dd-jtc2b        1/1     Running   67         49d   10.245.6.56    10.20.98.66    <none>           <none>
tms-tenant-processor-tms-tenant-processor-6c557c77dd-s2gqb        1/1     Running   1          49d   10.245.1.153   10.20.98.127   <none>           <none>
tms-test-meluha-service-tms-test-meluha-service-6dc6d8f795l649w   1/1     Running   0          49d   10.245.0.36    10.20.98.79    <none>           <none>
[21:January:2022:04:53:46]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgn tms events
No resources found in tms namespace.
[21:January:2022:04:53:55]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[21:January:2022:04:54:00]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kg events -n tms
No resources found in tms namespace.
[21:January:2022:04:54:11]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgktn tms
No resources found in tms namespace.
[21:January:2022:04:54:20]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgktn tms
No resources found in tms namespace.
[21:January:2022:04:54:37]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ hln tms
NAME                    NAMESPACE REVISION  UPDATED                                 STATUS    CHART                                         APP VERSION
tms-rest-proxy          tms       1         2021-10-13 22:44:06.723718777 +0000 UTC deployed  tms-rest-proxy-1.0.0-21-09-16-b0001           1.0.0-21-09-16-b0001
tms-template-service    tms       1         2021-10-13 22:44:05.785537551 +0000 UTC deployed  tms-template-service-1.0.0-21-05-10-b0001     1.0.0-21-05-10-b0001
tms-tenant-processor    tms       1         2021-10-13 22:44:06.115351842 +0000 UTC deployed  tms-tenant-processor-1.0.0-21-05-10-b0001     1.0.0-21-05-10-b0001
tms-test-meluha-service tms       1         2021-10-13 22:44:05.649034766 +0000 UTC deployed  tms-test-meluha-service-1.0.0-21-05-10-b0008  1.0.0-21-05-10-b0008
[21:January:2022:04:54:51]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kuc prod_ap-mumbai-1_dataplane
Switched to context "prod_ap-mumbai-1_dataplane".
[21:January:2022:04:55:18]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ rp
[21:January:2022:04:55:20]:(prod_ap-mumbai-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn tms
No resources found in tms namespace.
[21:January:2022:04:55:30]:(prod_ap-mumbai-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kuc prod_ap-mumbai-1_controlplane
Switched to context "prod_ap-mumbai-1_controlplane".
[21:January:2022:04:55:38]:(prod_ap-mumbai-1_dataplane):~/galorndon/osvc-platform
○ (main) $ rp
[21:January:2022:04:55:40]:(prod_ap-mumbai-1_controlplane):~/galorndon/osvc-platform
○ (main) $ kgpn tms
NAME                                                              READY   STATUS    RESTARTS   AGE   IP             NODE           NOMINATED NODE   READINESS GATES
tms-rest-proxy-tms-rest-proxy-66f4c94565-jqxbl                    1/1     Running   0          49d   10.245.0.34    10.20.98.79    <none>           <none>
tms-rest-proxy-tms-rest-proxy-66f4c94565-jtrj5                    1/1     Running   0          49d   10.245.4.186   10.20.98.223   <none>           <none>
tms-rest-proxy-tms-rest-proxy-66f4c94565-tglzt                    1/1     Running   0          49d   10.245.4.185   10.20.98.223   <none>           <none>
tms-template-service-tms-idcs-manager-7f748cb55b-26b22            1/1     Running   0          49d   10.245.6.54    10.20.98.66    <none>           <none>
tms-template-service-tms-idcs-manager-7f748cb55b-gcldz            1/1     Running   0          49d   10.245.6.55    10.20.98.66    <none>           <none>
tms-tenant-processor-tms-tenant-processor-6c557c77dd-8khlw        1/1     Running   67         49d   10.245.0.35    10.20.98.79    <none>           <none>
tms-tenant-processor-tms-tenant-processor-6c557c77dd-jtc2b        1/1     Running   67         49d   10.245.6.56    10.20.98.66    <none>           <none>
tms-tenant-processor-tms-tenant-processor-6c557c77dd-s2gqb        1/1     Running   1          49d   10.245.1.153   10.20.98.127   <none>           <none>
tms-test-meluha-service-tms-test-meluha-service-6dc6d8f795l649w   1/1     Running   0          49d   10.245.0.36    10.20.98.79    <none>           <none>
[21:January:2022:04:55:48]:(prod_ap-mumbai-1_controlplane):~/galorndon/osvc-platform
○ (main) $ kuc prod_ap-mumbai-1_dataplane
Switched to context "prod_ap-mumbai-1_dataplane".
[21:January:2022:04:56:02]:(prod_ap-mumbai-1_controlplane):~/galorndon/osvc-platform
○ (main) $ rp
[21:January:2022:04:56:03]:(prod_ap-mumbai-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpnk

kgnNAME                                                            READY   STATUS    RESTARTS   AGE    IP             NODE           NOMINATED NODE   READINESS GATES
schemaregistry-proxy-8b5879994-l4ngc                            2/2     Running   0          118d   10.245.5.155   10.20.96.73    <none>           <none>
shared-kafka-dr-kafka-0                                         2/2     Running   0          118d   10.245.3.141   10.20.97.210   <none>           <none>
shared-kafka-dr-kafka-1                                         2/2     Running   0          107d   10.245.6.146   10.20.96.199   <none>           <none>
shared-kafka-dr-kafka-2                                         2/2     Running   0          118d   10.245.5.5     10.20.96.97    <none>           <none>
shared-kafka-dr-kafka-exporter-cd8bb568c-tpn7v                  1/1     Running   0          118d   10.245.1.139   10.20.97.125   <none>           <none>
shared-kafka-dr-zookeeper-0                                     1/1     Running   0          118d   10.245.3.139   10.20.97.210   <none>           <none>
shared-kafka-dr-zookeeper-1                                     1/1     Running   0          118d   10.245.3.136   10.20.97.210   <none>           <none>
shared-kafka-dr-zookeeper-2                                     1/1     Running   0          118d   10.245.1.141   10.20.97.125   <none>           <none>
shared-kafka-entity-operator-b8fffd85d-fk2w9                    3/3     Running   4          118d   10.245.1.133   10.20.97.125   <none>           <none>
shared-kafka-kafka-0                                            2/2     Running   0          118d   10.245.1.138   10.20.97.125   <none>           <none>
shared-kafka-kafka-1                                            2/2     Running   0          118d   10.245.3.140   10.20.97.210   <none>           <none>
shared-kafka-kafka-2                                            2/2     Running   0          118d   10.245.3.137   10.20.97.210   <none>           <none>
shared-kafka-kafka-exporter-78cb89cf4d-9k9dx                    1/1     Running   0          118d   10.245.3.132   10.20.97.210   <none>           <none>
shared-kafka-mm2-ap-mumbai-1-mirrormaker2-6b45b89764-bmwjz      1/1     Running   0          118d   10.245.3.135   10.20.97.210   <none>           <none>
shared-kafka-mm2-ap-mumbai-1-mirrormaker2-6b45b89764-gxcnc      1/1     Running   0          118d   10.245.1.140   10.20.97.125   <none>           <none>
shared-kafka-mm2-dr-ap-mumbai-1-mirrormaker2-599754f974-qj5k7   1/1     Running   0          118d   10.245.3.133   10.20.97.210   <none>           <none>
shared-kafka-mm2-dr-ap-mumbai-1-mirrormaker2-599754f974-rzdvx   1/1     Running   0          118d   10.245.3.138   10.20.97.210   <none>           <none>
shared-kafka-zookeeper-0                                        1/1     Running   0          118d   10.245.1.144   10.20.97.125   <none>           <none>
shared-kafka-zookeeper-1                                        1/1     Running   0          118d   10.245.1.142   10.20.97.125   <none>           <none>
shared-kafka-zookeeper-2                                        1/1     Running   0          118d   10.245.5.6     10.20.96.97    <none>           <none>
shared-schemaregistry-54d5f95b64-9zj6m                          2/2     Running   0          118d   10.245.4.143   10.20.97.252   <none>           <none>
shared-schemaregistry-54d5f95b64-w62k9                          2/2     Running   0          118d   10.245.5.143   10.20.96.73    <none>           <none>
strimzi-cluster-operator-856b5f5768-drfnd                       1/1     Running   3          32d    10.245.2.11    10.20.96.79    <none>           <none>
[21:January:2022:04:56:08]:(prod_ap-mumbai-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[21:January:2022:04:56:08]:(prod_ap-mumbai-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgktnk > /tmp/sharedkafka.mumbai.list
[21:January:2022:04:56:22]:(prod_ap-mumbai-1_dataplane):~/galorndon/osvc-platform
○ (main) $ grep -ir tms /tmp/sharedkafka.mumbai.list
tms.application.map                                                                                                                                   3            3
tms.operation.request                                                                                                                                 3            3
tms.operation.request.tracker                                                                                                                         3            3
tms.operation.response                                                                                                                                3            3
tms.processor-tenant-tms.internal.state-changelog                                                                                                     3            3
tms.processor-tenantoperation-tms.tenant.operation-store-changelog---be45d405440773fd5316f5d1b79499df34c983bc                                         3            3
tms.processor-tenantoperation-tms.tenant.operation-store-repartition---76f4794fcaa8d49cf4fb293cd37a6dbc8bd8a929                                       3            3
tms.tenant.operation                                                                                                                                  3            3
tms.tenant.state                                                                                                                                      3            3
[21:January:2022:04:56:33]:(prod_ap-mumbai-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgktnk tms.application.map -oyaml
apiVersion: kafka.strimzi.io/v1beta1
kind: KafkaTopic
metadata:
  annotations:
    meta.helm.sh/release-name: kafka-data-tms
    meta.helm.sh/release-namespace: kafka
  creationTimestamp: "2021-09-29T05:22:11Z"
  generation: 1
  labels:
    app: kafka-data
    app.kubernetes.io/managed-by: Helm
    chart: kafka-data-0.1.0
    heritage: Helm
    release: kafka-data-tms
    strimzi.io/cluster: shared-kafka
  name: tms.application.map
  namespace: kafka
  resourceVersion: "25608624"
  uid: b699d28d-0750-486c-bc21-d0fe7163cc70
spec:
  config:
    cleanup.policy: compact
  partitions: 3
  replicas: 3
status:
  conditions:
  - lastTransitionTime: "2021-09-29T05:23:36.814200Z"
    status: "True"
    type: Ready
  observedGeneration: 1
[21:January:2022:04:56:50]:(prod_ap-mumbai-1_dataplane):~/galorndon/osvc-platform
○ (main) $ hln kafka
NAME                        NAMESPACE REVISION  UPDATED                                   STATUS    CHART                         APP VERSION
kafka-data-acs2             kafka     1         2021-09-28 23:18:13.363719636 -0600 -0600 deployed  kafka-data-0.1.0
kafka-data-agora            kafka     1         2021-11-16 13:07:06.877863207 +0000 UTC   deployed  kafka-data-0.1.0
kafka-data-analytics        kafka     1         2021-09-28 23:18:33.161297939 -0600 -0600 deployed  kafka-data-0.1.0
kafka-data-bui              kafka     1         2021-09-28 23:19:15.105516169 -0600 -0600 deployed  kafka-data-0.1.0
kafka-data-data-pipeline    kafka     2         2021-11-16 13:07:06.927234072 +0000 UTC   deployed  kafka-data-0.1.0
kafka-data-helios           kafka     1         2021-09-28 23:20:16.579651784 -0600 -0600 deployed  kafka-data-0.1.0
kafka-data-mercury          kafka     1         2021-09-28 23:20:56.371657345 -0600 -0600 deployed  kafka-data-0.1.0
kafka-data-opaec            kafka     1         2021-11-16 13:07:07.874646005 +0000 UTC   deployed  kafka-data-0.1.0
__consumer_offsets
ap-hyderabad-1.analytics.audit
ap-hyderabad-1.analytics.connect.configs
ap-hyderabad-1.analytics.connect.offsets
ap-hyderabad-1.analytics.connect.status
ap-hyderabad-1.analytics.connector.template
ap-hyderabad-1.analytics.data
ap-hyderabad-1.analytics.definition
ap-hyderabad-1.analytics.message.publisher.run.queue
ap-hyderabad-1.analytics.message.publisher.runs
ap-hyderabad-1.analytics.message.publisher.specs
ap-hyderabad-1.analytics.queue
ap-hyderabad-1.analytics.queue.state
ap-hyderabad-1.analytics.report.classification
ap-hyderabad-1.analytics.schema
ap-hyderabad-1.analytics.schema.mapping
ap-hyderabad-1.analytics.schema.ops
ap-hyderabad-1.analytics.tenant
ap-hyderabad-1.analytics.tenant.provision.info
ap-hyderabad-1.analytics.tenant.schema
ap-hyderabad-1.analytics.tenant.schema.deploy
ap-hyderabad-1.analytics.tenant.schema.deploy.response
ap-hyderabad-1.analytics.tenant.schema.mapping
ap-hyderabad-1.checkpoints.internal
ap-hyderabad-1.com.oracle.osvc.insights.metadata
ap-hyderabad-1.data-pipeline.binlog.raw.event
ap-hyderabad-1.data-pipeline.com.object.event
ap-hyderabad-1.data-pipeline.dbph02a-history
ap-hyderabad-1.data-pipeline.dbph02b-history
ap-hyderabad-1.data-pipeline.dbph95a-history
ap-hyderabad-1.data-pipeline.dbph95b-history
ap-hyderabad-1.data-pipeline.dbph96a-history
ap-hyderabad-1.data-pipeline.dbph96b-history
ap-hyderabad-1.data-pipeline.dbph97a-history
ap-hyderabad-1.data-pipeline.dbph97b-history
ap-hyderabad-1.data-pipeline.dbph98a-history
ap-hyderabad-1.data-pipeline.dbph98b-history
ap-hyderabad-1.data-pipeline.dbph99a-history
ap-hyderabad-1.data-pipeline.dbph99b-history
ap-hyderabad-1.data-pipeline.kafka-connect-configs
ap-hyderabad-1.data-pipeline.kafka-connect-offsets
ap-hyderabad-1.data-pipeline.kafka-connect-status
ap-hyderabad-1.data-pipeline.tenant.product.version
ap-hyderabad-1.dms.analytics.tenant.request
ap-hyderabad-1.dms.processor-MetadataStateStore-changelog
ap-hyderabad-1.dms.tenant.request
ap-hyderabad-1.dms.tenant.response
ap-hyderabad-1.functional-test
ap-hyderabad-1.heartbeats
ap-hyderabad-1.helios.analytics.event
"/tmp/kafkatopics" 122L, 7046C
kafka-data-pdb-provisioning kafka     1         2021-09-28 23:21:16.538455467 -0600 -0600 deployed  kafka-data-0.1.0
kafka-data-sitedashboard    kafka     1         2021-09-28 23:21:40.354540398 -0600 -0600 deployed  kafka-data-0.1.0
kafka-data-tms              kafka     2         2021-11-16 13:07:07.275368756 +0000 UTC   deployed  kafka-data-0.1.0
kafka-data-visitorservice   kafka     1         2021-09-28 23:22:28.293897481 -0600 -0600 deployed  kafka-data-0.1.0
kafka-data-xo               kafka     1         2021-09-28 23:22:46.700828713 -0600 -0600 deployed  kafka-data-0.1.0
kafka-schema-registry       kafka     2         2021-09-28 23:08:43.50710299 -0600 -0600  deployed  osvc-schema-registry-0.3.0    1.0
kafka-shared                kafka     3         2021-09-28 23:06:06.978535893 -0600 -0600 deployed  cpe-kafka-0.1.0               0.1.0
kafka-shared-mirrormaker    kafka     2         2021-09-28 23:08:14.917899922 -0600 -0600 deployed  cpe-kafka-mm2-0.1.0           0.1.0
nginx                       kafka     2         2021-09-28 23:09:23.254961918 -0600 -0600 deployed  nginx-5.1.13                  1.17.9
strmzi                      kafka     3         2021-09-28 23:04:26.583610586 -0600 -0600 deployed  strimzi-kafka-operator-0.19.0 0.19.0
[21:January:2022:04:58:38]:(prod_ap-mumbai-1_dataplane):~/galorndon/osvc-platform
○ (main) $ keitn kafka shared-kafka-dr-kafka-0 -- bash

set -o vi
Defaulted container "kafka" out of: kafka, tls-sidecar, kafka-init (init)
[kafka@shared-kafka-dr-kafka-0 kafka]$
[kafka@shared-kafka-dr-kafka-0 kafka]$ set -o vi
[kafka@shared-kafka-dr-kafka-0 kafka]$ ./bin/kafka-topics.sh --zookeeper localhost:2182 --list > /tmp/kafkatopic.list
Exception in thread "main" kafka.zookeeper.ZooKeeperClientTimeoutException: Timed out waiting for connection while in state: CONNECTING
  at kafka.zookeeper.ZooKeeperClient.$anonfun$waitUntilConnected$3(ZooKeeperClient.scala:262)
  at kafka.zookeeper.ZooKeeperClient.waitUntilConnected(ZooKeeperClient.scala:258)
  at kafka.zookeeper.ZooKeeperClient.<init>(ZooKeeperClient.scala:119)
  at kafka.zk.KafkaZkClient$.apply(KafkaZkClient.scala:1863)
  at kafka.admin.TopicCommand$ZookeeperTopicService$.apply(TopicCommand.scala:341)
  at kafka.admin.TopicCommand$.main(TopicCommand.scala:55)
  at kafka.admin.TopicCommand.main(TopicCommand.scala)
[kafka@shared-kafka-dr-kafka-0 kafka]$
[kafka@shared-kafka-dr-kafka-0 kafka]$ #./bin/kafka-topics.sh --zookeeper localhost:2182 --list > /tmp/kafkatopic.list
[kafka@shared-kafka-dr-kafka-0 kafka]$ ll /tmp/kafkatopic.list
-rw-r--r--. 1 kafka root 0 Jan 21 04:59 /tmp/kafkatopic.list
[kafka@shared-kafka-dr-kafka-0 kafka]$ #./bin/kafka-topics.sh --zookeeper localhost:2181 --list > /tmp/kafkatopic.list
[kafka@shared-kafka-dr-kafka-0 kafka]$ ll /tmp/kafkatopic.list
-rw-r--r--. 1 kafka root 0 Jan 21 04:59 /tmp/kafkatopic.list
[kafka@shared-kafka-dr-kafka-0 kafka]$ cat /tmp/kafkatopic.list
[kafka@shared-kafka-dr-kafka-0 kafka]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost --list > /tmp/kafkatopics
[kafka@shared-kafka-dr-kafka-0 kafka]$ ll /tmp/kafkatopics
-rw-r--r--. 1 kafka root 7046 Jan 21 05:02 /tmp/kafkatopics
[kafka@shared-kafka-dr-kafka-0 kafka]$ grep -ir tms /tmp/kafkatopics
ap-hyderabad-1.tms.application.map
ap-hyderabad-1.tms.operation.request
ap-hyderabad-1.tms.operation.request.tracker
ap-hyderabad-1.tms.operation.response
ap-hyderabad-1.tms.processor-tenant-tms.internal.state-changelog
ap-hyderabad-1.tms.processor-tenantOperation-tms.tenant.operation-store-changelog
ap-hyderabad-1.tms.processor-tenantOperation-tms.tenant.operation-store-repartition
ap-hyderabad-1.tms.tenant.operation
ap-hyderabad-1.tms.tenant.state
[kafka@shared-kafka-dr-kafka-0 kafka]$
[kafka@shared-kafka-dr-kafka-0 kafka]$
[kafka@shared-kafka-dr-kafka-0 kafka]$ vi /tmp/kafkatopics
[kafka@shared-kafka-dr-kafka-0 kafka]$ ./bin/kafka-topics.sh --zookeeper localhost --describe --topic tms.application.map
Error while executing topic command : Topic 'tms.application.map' does not exist as expected
[2022-01-21 05:04:53,165] ERROR java.lang.IllegalArgumentException: Topic 'tms.application.map' does not exist as expected
  at kafka.admin.TopicCommand$.kafka$admin$TopicCommand$$ensureTopicExists(TopicCommand.scala:504)
  at kafka.admin.TopicCommand$ZookeeperTopicService.describeTopic(TopicCommand.scala:410)
  at kafka.admin.TopicCommand$.main(TopicCommand.scala:68)
  at kafka.admin.TopicCommand.main(TopicCommand.scala)
 (kafka.admin.TopicCommand$)
[kafka@shared-kafka-dr-kafka-0 kafka]$ ./bin/kafka-topics.sh --zookeeper localhost --describe --topic ap-hyderabad-1.tms.application.map
Topic: ap-hyderabad-1.tms.application.map PartitionCount: 3 ReplicationFactor: 3  Configs: cleanup.policy=compact
  Topic: ap-hyderabad-1.tms.application.map Partition: 0  Leader: 2 Replicas: 2,0,1 Isr: 2,0,1
  Topic: ap-hyderabad-1.tms.application.map Partition: 1  Leader: 0 Replicas: 0,1,2 Isr: 0,1,2
  Topic: ap-hyderabad-1.tms.application.map Partition: 2  Leader: 1 Replicas: 1,2,0 Isr: 1,2,0
[kafka@shared-kafka-dr-kafka-0 kafka]$ grep -ir mumbai /tmp/kafkatopics
[kafka@shared-kafka-dr-kafka-0 kafka]$
[kafka@shared-kafka-dr-kafka-0 kafka]$ kgc
bash: kgc: command not found
[kafka@shared-kafka-dr-kafka-0 kafka]$ exit
exit
command terminated with exit code 127
[21:January:2022:05:06:00]:(prod_ap-mumbai-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgc
CURRENT   NAME                               CLUSTER               AUTHINFO           NAMESPACE
          prod_ap-hyderabad-1_controlplane   cluster-cbjr52qcfoa   user-cbjr52qcfoa
          prod_ap-hyderabad-1_dataplane      cluster-cpj2zxktnda   user-cpj2zxktnda
          prod_ap-melbourne-1_controlplane   cluster-czgcn3bmjtg   user-czgcn3bmjtg
          prod_ap-melbourne-1_dataplane      cluster-c4dazdggfst   user-c4dazdggfst
          prod_ap-mumbai-1_controlplane      cluster-cjiru3mifia   user-cjiru3mifia
*         prod_ap-mumbai-1_dataplane         cluster-ci3ypr6tmga   user-ci3ypr6tmga
          prod_ap-osaka-1_controlplane       cluster-cp5327xtmfa   user-cp5327xtmfa
          prod_ap-osaka-1_dataplane          cluster-cqldck6xatq   user-cqldck6xatq
          prod_ap-sydney-1_controlplane      cluster-c4wembugqyt   user-c4wembugqyt
          prod_ap-sydney-1_dataplane         cluster-cswezbzgyyg   user-cswezbzgyyg
          prod_ap-tokyo-1_controlplane       cluster-cz54pc3z4wa   user-cz54pc3z4wa
          prod_ap-tokyo-1_dataplane          cluster-cyjtbtbbjfq   user-cyjtbtbbjfq
          prod_ca-montreal-1_controlplane    cluster-czdoolemnqw   user-czdoolemnqw
          prod_ca-montreal-1_dataplane       cluster-cqtkytbgq3d   user-cqtkytbgq3d
          prod_ca-toronto-1_controlplane     cluster-cygcmjxhfsd   user-cygcmjxhfsd
          prod_ca-toronto-1_dataplane        cluster-cywmodfmm3d   user-cywmodfmm3d
          prod_eu-amsterdam-1_controlplane   cluster-c4tkmjwgbrt   user-c4tkmjwgbrt
          prod_eu-amsterdam-1_dataplane      cluster-cstsy3bmjrd   user-cstsy3bmjrd
          prod_eu-frankfurt-1_controlplane   cluster-cydcytcg43d   user-cydcytcg43d
          prod_eu-frankfurt-1_dataplane      cluster-crdgzbrmjsd   user-crdgzbrmjsd
          prod_eu-zurich-1_controlplane      cluster-cwmoxn3bfwq   user-cwmoxn3bfwq
          prod_eu-zurich-1_dataplane         cluster-chtoe5bnsza   user-chtoe5bnsza
          prod_me-dubai-1_controlplane       cluster-ce5puidc4ya   user-ce5puidc4ya
          prod_me-dubai-1_dataplane          cluster-c77hcd3vizq   user-c77hcd3vizq
          prod_me-jeddah-1_controlplane      cluster-cpzagsoukeq   user-cpzagsoukeq
          prod_me-jeddah-1_dataplane         cluster-cf4f222mrfa   user-cf4f222mrfa
          prod_sa-santiago-1_controlplane    cluster-czxpimgibeq   user-czxpimgibeq
          prod_sa-santiago-1_dataplane       cluster-cuutxfvj2ea   user-cuutxfvj2ea
          prod_sa-saopaulo-1_controlplane    cluster-cc75n7hujpq   user-cc75n7hujpq
          prod_sa-saopaulo-1_dataplane       cluster-cpjqrjtbotq   user-cpjqrjtbotq
          prod_sa-vinhedo-1_controlplane     cluster-cihx6aprjua   user-cihx6aprjua
          prod_sa-vinhedo-1_dataplane        cluster-ciqmbmn6wta   user-ciqmbmn6wta
          prod_uk-cardiff-1_controlplane     cluster-ctggmtbmfrg   user-ctggmtbmfrg
          prod_uk-cardiff-1_dataplane        cluster-cqwmnrqmztd   user-cqwmnrqmztd
          prod_uk-london-1_controlplane      cluster-cywgnlcmqzd   user-cywgnlcmqzd
          prod_uk-london-1_dataplane         cluster-c3tazjsmjsd   user-c3tazjsmjsd
          prod_us-ashburn-1_controlplane     cluster-czwczjzgrsw   user-czwczjzgrsw
          prod_us-ashburn-1_dataplane        cluster-c3wkyjyguzt   user-c3wkyjyguzt
          prod_us-phoenix-1_controlplane     cluster-c3tkobugrst   user-c3tkobugrst
          prod_us-phoenix-1_dataplane        cluster-csdqylfmi2g   user-csdqylfmi2g
          prod_us-phoenix-1_deployment       cluster-c4tgolgmjsd   user-c4tgolgmjsd
[21:January:2022:05:07:53]:(prod_ap-mumbai-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kuc prod_us-ashburn-1_dataplane
Switched to context "prod_us-ashburn-1_dataplane".
[21:January:2022:05:07:58]:(prod_ap-mumbai-1_dataplane):~/galorndon/osvc-platform
○ (main) $ rp
[21:January:2022:05:08:00]:(prod_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpnk
NAME                                                             READY   STATUS      RESTARTS   AGE   IP             NODE          NOMINATED NODE   READINESS GATES
cpe-cronjob-sr-backup-sr-backup-1642739100-xrlmg                 0/1     Completed   0          45m   10.245.0.31    10.20.8.206   <none>           <none>
cpe-cronjob-sr-backup-sr-backup-1642740000-hgp7x                 0/1     Completed   0          30m   10.245.0.32    10.20.8.206   <none>           <none>
cpe-cronjob-sr-backup-sr-backup-1642740900-fln97                 0/1     Completed   0          15m   10.245.0.33    10.20.8.206   <none>           <none>
cpe-cronjob-sr-backup-sr-backup-1642741800-zs2j5                 1/1     Running     0          24s   10.245.0.34    10.20.8.206   <none>           <none>
schemaregistry-proxy-78c6f88b54-svrl2                            2/2     Running     0          58d   10.245.3.15    10.20.8.60    <none>           <none>
shared-kafka-dr-kafka-0                                          2/2     Running     0          58d   10.245.5.136   10.20.9.243   <none>           <none>
shared-kafka-dr-kafka-1                                          2/2     Running     0          50d   10.245.1.117   10.20.9.76    <none>           <none>
shared-kafka-dr-kafka-2                                          2/2     Running     0          58d   10.245.6.49    10.20.9.247   <none>           <none>
shared-kafka-dr-kafka-exporter-85b8994ff8-lxgzx                  1/1     Running     0          58d   10.245.6.157   10.20.8.5     <none>           <none>
shared-kafka-dr-zookeeper-0                                      1/1     Running     0          58d   10.245.5.190   10.20.9.243   <none>           <none>
shared-kafka-dr-zookeeper-1                                      1/1     Running     0          58d   10.245.3.85    10.20.8.60    <none>           <none>
shared-kafka-dr-zookeeper-2                                      1/1     Running     0          58d   10.245.5.99    10.20.8.167   <none>           <none>
shared-kafka-entity-operator-648fd59b49-hp7jk                    3/3     Running     1          58d   10.245.2.70    10.20.8.83    <none>           <none>
shared-kafka-kafka-0                                             2/2     Running     0          58d   10.245.4.7     10.20.8.122   <none>           <none>
shared-kafka-kafka-1                                             2/2     Running     0          58d   10.245.7.15    10.20.9.254   <none>           <none>
shared-kafka-kafka-2                                             2/2     Running     0          58d   10.245.5.134   10.20.9.243   <none>           <none>
shared-kafka-kafka-exporter-5d6bcfd8c9-x2zcp                     1/1     Running     0          58d   10.245.2.68    10.20.8.83    <none>           <none>
shared-kafka-mm2-dr-us-ashburn-1-mirrormaker2-76556cc845-dz8gw   1/1     Running     1          50d   10.245.6.74    10.20.9.247   <none>           <none>
shared-kafka-mm2-dr-us-ashburn-1-mirrormaker2-76556cc845-qcnbv   1/1     Running     0          58d   10.245.5.153   10.20.9.243   <none>           <none>
shared-kafka-mm2-us-ashburn-1-mirrormaker2-5cf49c796-mtc2r       1/1     Running     0          58d   10.245.3.20    10.20.8.60    <none>           <none>
shared-kafka-mm2-us-ashburn-1-mirrormaker2-5cf49c796-zbh4n       1/1     Running     0          58d   10.245.6.185   10.20.8.5     <none>           <none>
shared-kafka-zookeeper-0                                         1/1     Running     0          58d   10.245.3.87    10.20.8.60    <none>           <none>
shared-kafka-zookeeper-1                                         1/1     Running     0          50d   10.245.1.116   10.20.9.76    <none>           <none>
shared-kafka-zookeeper-2                                         1/1     Running     0          58d   10.245.2.87    10.20.8.83    <none>           <none>
shared-schemaregistry-84b98cd5d-j9ct6                            2/2     Running     0          58d   10.245.0.181   10.20.8.153   <none>           <none>
shared-schemaregistry-84b98cd5d-zsp6x                            2/2     Running     0          58d   10.245.3.12    10.20.8.60    <none>           <none>
strimzi-cluster-operator-84f4b84c84-wtgh7                        1/1     Running     11         58d   10.245.2.67    10.20.8.83    <none>           <none>
[21:January:2022:05:10:26]:(prod_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ keitn kafka shared-kafka-kafka-0  -- bash
Defaulted container "kafka" out of: kafka, tls-sidecar, kafka-init (init)
[kafka@shared-kafka-kafka-0 kafka]$ set -o vi
[kafka@shared-kafka-kafka-0 kafka]$
[kafka@shared-kafka-kafka-0 kafka]$ bin/kafka-acls.sh --list --authorizer-properties zookeeper.connect=localhost:2181
Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=me-dubai-1.tms.tenant.state, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=data-pipeline, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ca-montreal-1.tms.tenant.operation, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-hyderabad-1.tms.tenant.state, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-tokyo-1.tms.operation.request, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=sa-vinhedo-1.tms.operation.response, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios-psr.external.event, patternType=LITERAL)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ca-montreal-1.tms.operation.request, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=opaec.grp, patternType=PREFIXED)`:
  (principal=User:CN=opaec-opaecgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios-psr.dead.event, patternType=LITERAL)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios.send.event, patternType=LITERAL)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=analytics.queue.state, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-osaka-1.tms.tenant.state, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=uk-london-1.tms.operation.request, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ca-montreal-1.tms.operation.response, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios-psr.error.event, patternType=LITERAL)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=me-dubai-1.tms.operation.request.tracker, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=analytics.queued-report-executor, patternType=PREFIXED)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=tms.producer, patternType=PREFIXED)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=sa-vinhedo-1.tms.application.map, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=eu-amsterdam-1.tms.tenant.operation, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-sydney-1.tms.tenant.operation, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=uk-london-1.tms.operation.response, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=helios-psr, patternType=PREFIXED)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=helios.grp, patternType=PREFIXED)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=analytics.tenant, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=us-phoenix-1.tms.application.map, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=analytics.message.publisher.specs, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=data-pipeline.raw2com, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-sydney-1.tms.operation.request.tracker, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=sa-vinhedo-1.tms.tenant.state, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-hyderabad-1.tms.application.map, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-osaka-1.tms.operation.request.tracker, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ca-toronto-1.tms.operation.response, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios.source.data, patternType=LITERAL)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=analytics, patternType=PREFIXED)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=me-dubai-1.data-pipeline, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=tms.tenant.state, patternType=PREFIXED)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=READ, permissionType=ALLOW)
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=READ, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=eu-frankfurt-1.tms.operation.request, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=analytics.analytics-provisioning-service, patternType=PREFIXED)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios.error.event, patternType=LITERAL)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=mercury-mercurygeneric-rw, host=*, operation=READ, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=sa-saopaulo-1.tms.tenant.operation, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=data-pipeline.connector-provisioning, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios-psr.publish.event, patternType=LITERAL)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=uk-cardiff-1.tms.application.map, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=us-phoenix-1.tms.operation.response, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=sa-saopaulo-1.tms.tenant.state, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=analytics.tenant.schema.mapping, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=eu-zurich-1.tms.tenant.operation, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=eu-amsterdam-1.tms.operation.request, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-tokyo-1.tms.operation.request.tracker, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=me-jeddah-1.tms.operation.request, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=sa-vinhedo-1.data-pipeline, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-tokyo-1.tms.tenant.state, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=sa-vinhedo-1.tms.operation.request.tracker, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=eu-amsterdam-1.tms.application.map, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-osaka-1.tms.operation.request, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=eu-frankfurt-1.data-pipeline, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ca-montreal-1.data-pipeline, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=karapace-schema-registry, patternType=LITERAL)`:
  (principal=User:CN=karapaceschemaregistry, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=analytics.connect.offsets, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios.webhook.dead.event.map, patternType=LITERAL)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=*, patternType=LITERAL)`:
  (principal=User:CN=admin-tools-admintoolsgeneric-r, host=*, operation=READ, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=analytics.queue, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=analytics.tenant-schema-deployment-response, patternType=PREFIXED)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=data-pipeline.com.object.event, patternType=PREFIXED)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=READ, permissionType=ALLOW)
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=READ, permissionType=ALLOW)
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=READ, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ca-toronto-1.tms.application.map, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=analytics.tenant-deployment, patternType=PREFIXED)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=me-dubai-1.tms.operation.request, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-sydney-1.data-pipeline, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=uk-cardiff-1.data-pipeline, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=analytics.message-publisher, patternType=PREFIXED)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-tokyo-1.data-pipeline, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=sitedashboard.pdb-provisioning-response, patternType=PREFIXED)`:
  (principal=User:CN=sitedashboard-sitedashboardgeneric-rw, host=*, operation=READ, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=me-jeddah-1.tms.operation.response, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ca-montreal-1.tms.operation.request.tracker, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=us-phoenix-1.data-pipeline, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios-psr.webhook.data, patternType=LITERAL)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios.death.event, patternType=LITERAL)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=mercury-mercurygeneric-rw, host=*, operation=READ, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios.external.event, patternType=LITERAL)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=mercury-mercurygeneric-rw, host=*, operation=WRITE, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=analytics.tenant.schema.metadata, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=uk-london-1.tms.tenant.state, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=acs2.pdb-provisioning-request, patternType=PREFIXED)`:
  (principal=User:CN=acs2-acs2generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=me-jeddah-1.data-pipeline, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=uk-london-1.tms.application.map, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=visitorservicegrp, patternType=PREFIXED)`:
  (principal=User:CN=visitorservice-visitorservicegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios.rule.data, patternType=LITERAL)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=tms.application.map, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-sydney-1.tms.operation.request, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-mumbai-1.tms.tenant.operation, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=sa-saopaulo-1.tms.application.map, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=eu-frankfurt-1.tms.tenant.operation, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=kweetgrp, patternType=PREFIXED)`:
  (principal=User:CN=mercury-mercurygeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=uk-london-1.data-pipeline, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=uk-london-1.tms.operation.request.tracker, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=dms.tenant.response, patternType=LITERAL)`:
  (principal=User:CN=pdb-provisioning-pdbprovisioninggeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=xo-xogeneric-rw, host=*, operation=READ, permissionType=ALLOW)
  (principal=User:CN=agora-agorageneric-rw, host=*, operation=READ, permissionType=ALLOW)
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=READ, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios-psr.status.event, patternType=LITERAL)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=analytics.report-executor, patternType=PREFIXED)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=dms.consumer, patternType=PREFIXED)`:
  (principal=User:CN=pdb-provisioning-pdbprovisioninggeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-sydney-1.tms.tenant.state, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-melbourne-1.tms.operation.request.tracker, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios-psr.webhook.rule.map, patternType=LITERAL)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=analytics.connect.configs, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-sydney-1.tms.application.map, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=uk-cardiff-1.tms.operation.response, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=sa-vinhedo-1.tms.operation.request, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-hyderabad-1.tms.tenant.operation, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-hyderabad-1.data-pipeline, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=eu-frankfurt-1.tms.operation.response, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=analytics.tenant.schema.deploy.response, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios, patternType=PREFIXED)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=sa-saopaulo-1.data-pipeline, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=analytics.data, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=*, patternType=LITERAL)`:
  (principal=User:CN=admin-tools-admintoolsgeneric-r, host=*, operation=READ, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=pdb.consumer, patternType=PREFIXED)`:
  (principal=User:CN=pdb-provisioning-pdbprovisioninggeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=us-phoenix-1.tms.tenant.state, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios-psr.event.rule.map, patternType=LITERAL)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=us-phoenix-1.tms.tenant.operation, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=data-pipeline.com.object.event, patternType=LITERAL)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=READ, permissionType=ALLOW)
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=READ, permissionType=ALLOW)
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=READ, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=eu-amsterdam-1.tms.operation.response, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=us-phoenix-1.tms.operation.request, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios-psr.priority.external.event, patternType=LITERAL)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=analytics.pdb-provisioning-request, patternType=PREFIXED)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-mumbai-1.tms.tenant.state, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=analytics.tenant-schema-metadata, patternType=PREFIXED)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=analytics.schema.mapping, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=analytics.connect.status, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=analytics.connect-group, patternType=PREFIXED)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-osaka-1.tms.application.map, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=tms.operation.request.tracker, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=eu-zurich-1.tms.application.map, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-mumbai-1.tms.operation.request.tracker, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios.event.data, patternType=LITERAL)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=dms.processor, patternType=PREFIXED)`:
  (principal=User:CN=pdb-provisioning-pdbprovisioninggeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=analytics.tenant-schema-deployment, patternType=PREFIXED)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=agoragrp, patternType=PREFIXED)`:
  (principal=User:CN=agora-agorageneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=functional-test, patternType=LITERAL)`:
  (principal=User:CN=functional-test, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios-psr.source.data, patternType=LITERAL)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios-psr.source.event.map, patternType=LITERAL)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios.publish.event, patternType=LITERAL)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-tokyo-1.tms.application.map, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios-psr.grp, patternType=PREFIXED)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios.analytics.event, patternType=LITERAL)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=uk-cardiff-1.tms.operation.request, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ca-toronto-1.data-pipeline, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-osaka-1.tms.tenant.operation, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=uk-london-1.tms.tenant.operation, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=sa-saopaulo-1.tms.operation.response, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=eu-zurich-1.tms.operation.response, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=analytics.connector.template, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=data-pipeline.binlog-kafka-connect, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-mumbai-1.data-pipeline, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios.dead.event, patternType=LITERAL)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=eu-amsterdam-1.tms.tenant.state, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ca-toronto-1.tms.tenant.state, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=analytics.definition, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=sa-vinhedo-1.tms.tenant.operation, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=me-dubai-1.tms.tenant.operation, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-mumbai-1.tms.operation.response, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=me-dubai-1.tms.application.map, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=uk-cardiff-1.tms.tenant.state, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=mercurygrp, patternType=PREFIXED)`:
  (principal=User:CN=mercury-mercurygeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=eu-zurich-1.tms.tenant.state, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=eu-frankfurt-1.tms.operation.request.tracker, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=uk-cardiff-1.tms.operation.request.tracker, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=us-phoenix-1.tms.operation.request.tracker, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=xo.pdb-provisioning-request, patternType=PREFIXED)`:
  (principal=User:CN=xo-xogeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=test, patternType=LITERAL)`:
  (principal=User:CN=mirrormaker-iad, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=mirrormaker-dr-iad, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=mirrormaker-phx, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=mirrormaker, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=tms.processor, patternType=PREFIXED)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=analytics.report-schema-service, patternType=PREFIXED)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=sitedashboard.pdb-provisioning-request, patternType=PREFIXED)`:
  (principal=User:CN=sitedashboard-sitedashboardgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=analytics.tenant.provision.info, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=tms.tenant.operation, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=_karapaceschemas, patternType=LITERAL)`:
  (principal=User:CN=karapaceschemaregistry, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=data-pipeline.com.schema.metadata, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=READ, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=eu-zurich-1.tms.operation.request.tracker, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios.priority.external.event, patternType=LITERAL)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=mercury-mercurygeneric-rw, host=*, operation=WRITE, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=dms.processor, patternType=PREFIXED)`:
  (principal=User:CN=pdb-provisioning-pdbprovisioninggeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=sa-saopaulo-1.tms.operation.request, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=analytics.schema-mapping-process, patternType=PREFIXED)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios-psr.send.event, patternType=LITERAL)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios.source.webhook.map, patternType=LITERAL)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=as-app-insights, patternType=PREFIXED)`:
  (principal=User:CN=bui-bui-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=eu-amsterdam-1.data-pipeline, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios-psr.rule.data, patternType=LITERAL)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=tms.operation.request, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=analytics.message.publisher.runs, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-melbourne-1.tms.operation.request, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=data-pipeline.schema-service, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=pdb.provision.request, patternType=LITERAL)`:
  (principal=User:CN=xo-xogeneric-rw, host=*, operation=WRITE, permissionType=ALLOW)
  (principal=User:CN=agora-agorageneric-rw, host=*, operation=WRITE, permissionType=ALLOW)
  (principal=User:CN=visitorservice-visitorservicegeneric-rw, host=*, operation=WRITE, permissionType=ALLOW)
  (principal=User:CN=pdb-provisioning-pdbprovisioninggeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=acs2-acs2generic-rw, host=*, operation=WRITE, permissionType=ALLOW)
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=WRITE, permissionType=ALLOW)
  (principal=User:CN=sitedashboard-sitedashboardgeneric-rw, host=*, operation=WRITE, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=mercurygrp, patternType=PREFIXED)`:
  (principal=User:CN=mercury-mercurygeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=schema-registry, patternType=LITERAL)`:
  (principal=User:CN=schemaregistry, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=analytics.report.classification, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=functional-test, patternType=LITERAL)`:
  (principal=User:CN=functional-test, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=analytics.audit, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-melbourne-1.tms.tenant.state, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-osaka-1.tms.operation.response, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=analytics.schema, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-melbourne-1.data-pipeline, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=uk-cardiff-1.tms.tenant.operation, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-hyderabad-1.tms.operation.response, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios-psr.event.data, patternType=LITERAL)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=analytics.tenant.schema.deploy, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ca-montreal-1.tms.application.map, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios-psr.source.webhook.map, patternType=LITERAL)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=helios, patternType=PREFIXED)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=eu-amsterdam-1.tms.operation.request.tracker, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=dms.analytics.tenant.request, patternType=LITERAL)`:
  (principal=User:CN=pdb-provisioning-pdbprovisioninggeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=WRITE, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=sa-saopaulo-1.tms.operation.request.tracker, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios.grp, patternType=PREFIXED)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios-psr.death.event, patternType=LITERAL)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios.event.rule.map, patternType=LITERAL)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=me-dubai-1.tms.operation.response, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=_schemas, patternType=LITERAL)`:
  (principal=User:CN=schemaregistry, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=karapaceschemaregistry, host=*, operation=READ, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=com.oracle.osvc.insights.metadata, patternType=LITERAL)`:
  (principal=User:CN=bui-bui-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ca-montreal-1.tms.tenant.state, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=connect-, patternType=PREFIXED)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-hyderabad-1.tms.operation.request.tracker, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-melbourne-1.tms.tenant.operation, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios.source.event.map, patternType=LITERAL)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=me-jeddah-1.tms.tenant.state, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-melbourne-1.tms.application.map, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios-psr.webhook.dead.event.map, patternType=LITERAL)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=tms.processor, patternType=PREFIXED)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=analytics.report-classification, patternType=PREFIXED)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-hyderabad-1.tms.operation.request, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=xo, patternType=PREFIXED)`:
  (principal=User:CN=xo-xogeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=eu-frankfurt-1.tms.tenant.state, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios.status.event, patternType=LITERAL)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=analytics.message.publisher.run.queue, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ca-toronto-1.tms.operation.request.tracker, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=me-jeddah-1.tms.application.map, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-tokyo-1.tms.tenant.operation, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-tokyo-1.tms.operation.response, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=tms.consumer, patternType=PREFIXED)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=analytics.tenant.schema, patternType=LITERAL)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=me-jeddah-1.tms.tenant.operation, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=eu-frankfurt-1.tms.application.map, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios-psr.analytics.event, patternType=LITERAL)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=helios-psr.grp, patternType=PREFIXED)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=tms.tenant.state, patternType=LITERAL)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=READ, permissionType=ALLOW)
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=READ, permissionType=ALLOW)
  (principal=User:CN=visitorservice-visitorservicegeneric-rw, host=*, operation=READ, permissionType=ALLOW)
  (principal=User:CN=xo-xogeneric-rw, host=*, operation=READ, permissionType=ALLOW)
  (principal=User:CN=helios-generic-rw, host=*, operation=READ, permissionType=ALLOW)
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=READ, permissionType=ALLOW)
  (principal=User:CN=mercury-mercurygeneric-rw, host=*, operation=READ, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=agora-agorageneric-rw, host=*, operation=READ, permissionType=ALLOW)
  (principal=User:CN=opaec-opaecgeneric-rw, host=*, operation=READ, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=xoservicegrp, patternType=PREFIXED)`:
  (principal=User:CN=xo-xogeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios.webhook.data, patternType=LITERAL)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=tms.operation.response, patternType=LITERAL)`:
  (principal=User:CN=xo-xogeneric-rw, host=*, operation=WRITE, permissionType=ALLOW)
  (principal=User:CN=agora-agorageneric-rw, host=*, operation=WRITE, permissionType=ALLOW)
  (principal=User:CN=visitorservice-visitorservicegeneric-rw, host=*, operation=WRITE, permissionType=ALLOW)
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=WRITE, permissionType=ALLOW)
  (principal=User:CN=opaec-opaecgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=WRITE, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=mercury-mercurygeneric-rw, host=*, operation=WRITE, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=eu-zurich-1.data-pipeline, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios.webhook.rule.map, patternType=LITERAL)`:
  (principal=User:CN=helios-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=pdb.processor, patternType=PREFIXED)`:
  (principal=User:CN=pdb-provisioning-pdbprovisioninggeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ca-toronto-1.tms.tenant.operation, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-osaka-1.data-pipeline, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=helios-psr, patternType=PREFIXED)`:
  (principal=User:CN=helios-psr-generic-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=me-jeddah-1.tms.operation.request.tracker, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=pdb.processor, patternType=PREFIXED)`:
  (principal=User:CN=pdb-provisioning-pdbprovisioninggeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=data-pipeline.metadata-publisher, patternType=PREFIXED)`:
  (principal=User:CN=data-pipeline-datapipelinegeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=analytics.report-processor, patternType=PREFIXED)`:
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=dms.tenant.request, patternType=LITERAL)`:
  (principal=User:CN=pdb-provisioning-pdbprovisioninggeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=xo-xogeneric-rw, host=*, operation=WRITE, permissionType=ALLOW)
  (principal=User:CN=agora-agorageneric-rw, host=*, operation=WRITE, permissionType=ALLOW)
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=WRITE, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-mumbai-1.tms.application.map, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=eu-zurich-1.tms.operation.request, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ca-toronto-1.tms.operation.request, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=pdb.provision.response, patternType=LITERAL)`:
  (principal=User:CN=visitorservice-visitorservicegeneric-rw, host=*, operation=READ, permissionType=ALLOW)
  (principal=User:CN=xo-xogeneric-rw, host=*, operation=READ, permissionType=ALLOW)
  (principal=User:CN=pdb-provisioning-pdbprovisioninggeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=analytics-analyticsgeneric-rw, host=*, operation=READ, permissionType=ALLOW)
  (principal=User:CN=sitedashboard-sitedashboardgeneric-rw, host=*, operation=READ, permissionType=ALLOW)
  (principal=User:CN=acs2-acs2generic-rw, host=*, operation=READ, permissionType=ALLOW)
  (principal=User:CN=agora-agorageneric-rw, host=*, operation=READ, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-sydney-1.tms.operation.response, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-melbourne-1.tms.operation.response, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=ap-mumbai-1.tms.operation.request, patternType=LITERAL)`:
  (principal=User:CN=tms-tmsvault-rw, host=*, operation=ALL, permissionType=ALLOW)
  (principal=User:CN=tms-tmsgeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=kweet-, patternType=PREFIXED)`:
  (principal=User:CN=mercury-mercurygeneric-rw, host=*, operation=ALL, permissionType=ALLOW)

[kafka@shared-kafka-kafka-0 kafka]$ ./bin/kafka-topics.sh --zookeeper localhost --list > /tmp/kafkatopics.list
[kafka@shared-kafka-kafka-0 kafka]$
[kafka@shared-kafka-kafka-0 kafka]$ grep -i tms /tmp/kafkatopics.list
tms.application.map
tms.operation.request
tms.operation.request.tracker
tms.operation.response
tms.processor-tenant-tms.internal.state-changelog
tms.processor-tenantOperation-tms.tenant.operation-store-changelog
tms.processor-tenantOperation-tms.tenant.operation-store-repartition
tms.tenant.operation
tms.tenant.state
us-phoenix-1.tms.application.map
us-phoenix-1.tms.operation.request
us-phoenix-1.tms.operation.request.tracker
us-phoenix-1.tms.operation.response
us-phoenix-1.tms.patdemouk
us-phoenix-1.tms.processor-tenant-tms.internal.state-changelog
us-phoenix-1.tms.processor-tenantOperation-tms.tenant.operation-store-changelog
us-phoenix-1.tms.processor-tenantOperation-tms.tenant.operation-store-repartition
us-phoenix-1.tms.tenant.operation
us-phoenix-1.tms.tenant.state
[kafka@shared-kafka-kafka-0 kafka]$ exit
exit
[21:January:2022:05:13:41]:(prod_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[21:January:2022:05:13:55]:(prod_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kuc prod_uk-london-1_dataplane
Switched to context "prod_uk-london-1_dataplane".
[21:January:2022:05:13:56]:(prod_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ rp
[21:January:2022:05:13:58]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ #keitn kafka shared-kafka-kafka-0  -- bash
[21:January:2022:05:14:04]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpnk

keitn kafka shared-kafka-kaNAME                                                            READY   STATUS    RESTARTS   AGE    IP              NODE           NOMINATED NODE   READINESS GATES
schemaregistry-proxy-55cfd8645-7t8w2                            2/2     Running   0          87d    10.245.9.12     10.20.48.66    <none>           <none>
shared-kafka-dr-kafka-0                                         2/2     Running   238        87d    10.245.6.154    10.20.49.200   <none>           <none>
shared-kafka-dr-kafka-1                                         2/2     Running   228        87d    10.245.9.140    10.20.48.184   <none>           <none>
shared-kafka-dr-kafka-2                                         2/2     Running   0          7d5h   10.245.1.197    10.20.49.231   <none>           <none>
shared-kafka-dr-kafka-exporter-55c8bdd594-mh7zq                 1/1     Running   0          7d5h   10.245.10.74    10.20.48.38    <none>           <none>
shared-kafka-dr-zookeeper-0                                     1/1     Running   185        87d    10.245.7.157    10.20.48.86    <none>           <none>
shared-kafka-dr-zookeeper-1                                     1/1     Running   175        87d    10.245.10.11    10.20.48.38    <none>           <none>
shared-kafka-dr-zookeeper-2                                     1/1     Running   185        87d    10.245.1.153    10.20.49.231   <none>           <none>
shared-kafka-entity-operator-667b7b785c-2s29g                   3/3     Running   543        87d    10.245.10.5     10.20.48.38    <none>           <none>
shared-kafka-kafka-0                                            2/2     Running   168        87d    10.245.6.150    10.20.49.200   <none>           <none>
shared-kafka-kafka-1                                            2/2     Running   157        87d    10.245.10.17    10.20.48.38    <none>           <none>
shared-kafka-kafka-2                                            2/2     Running   0          7d5h   10.245.1.202    10.20.49.231   <none>           <none>
shared-kafka-kafka-exporter-65b4499f6c-tfxls                    1/1     Running   0          7d5h   10.245.7.137    10.20.48.86    <none>           <none>
shared-kafka-mm2-dr-uk-london-1-mirrormaker2-7f79c4c6d9-8jgjv   1/1     Running   178        87d    10.245.1.138    10.20.49.231   <none>           <none>
shared-kafka-mm2-dr-uk-london-1-mirrormaker2-7f79c4c6d9-sv65b   1/1     Running   164        87d    10.245.10.9     10.20.48.38    <none>           <none>
shared-kafka-mm2-uk-london-1-mirrormaker2-555d5669b-f2zvx       1/1     Running   178        87d    10.245.1.136    10.20.49.231   <none>           <none>
shared-kafka-mm2-uk-london-1-mirrormaker2-555d5669b-sr5zx       1/1     Running   164        87d    10.245.10.8     10.20.48.38    <none>           <none>
shared-kafka-zookeeper-0                                        1/1     Running   176        87d    10.245.7.154    10.20.48.86    <none>           <none>
shared-kafka-zookeeper-1                                        1/1     Running   165        87d    10.245.10.15    10.20.48.38    <none>           <none>
shared-kafka-zookeeper-2                                        1/1     Running   179        87d    10.245.1.151    10.20.49.231   <none>           <none>
shared-schemaregistry-647976bf8c-4zqc5                          2/2     Running   0          72d    10.245.8.85     10.20.49.88    <none>           <none>
shared-schemaregistry-647976bf8c-8ls86                          2/2     Running   0          72d    10.245.10.209   10.20.48.129   <none>           <none>
strimzi-cluster-operator-84f4b84c84-gtqlf                       1/1     Running   1          7d5h   10.245.10.73    10.20.48.38    <none>           <none>
[21:January:2022:05:14:13]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[21:January:2022:05:14:13]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ keitn kafka shared-kafka-kafka-0 -- bash
Defaulted container "kafka" out of: kafka, tls-sidecar, kafka-init (init)
[kafka@shared-kafka-kafka-0 kafka]$ ./bin/kafka-topics.sh --zookeeper localhost --list | grep -i tms
tms.application.map
tms.operation.request
tms.operation.request.tracker
tms.operation.response
tms.processor-tenant-tms.internal.state-changelog
tms.processor-tenantOperation-tms.tenant.operation-store-changelog
tms.processor-tenantOperation-tms.tenant.operation-store-repartition
tms.tenant.operation
tms.tenant.state
[kafka@shared-kafka-kafka-0 kafka]$ exit
exit
[21:January:2022:05:14:50]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kuc kprod_ca-toronto-1_dataplane
error: no context exists with the name: "kprod_ca-toronto-1_dataplane"
[21:January:2022:05:15:11]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ rp
[21:January:2022:05:15:11]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kuc prod_ca-toronto-1_dataplane
Switched to context "prod_ca-toronto-1_dataplane".
[21:January:2022:05:15:14]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ rp
[21:January:2022:05:15:16]:(prod_ca-toronto-1_dataplane):~/galorndon/osvc-platform
○ (main) $ #keitn kafka shared-kafka-kafka-0 -- /home/opc/bin/kafka-topics.sh --zookeeper localhost --list | grep -i tms
[21:January:2022:05:16:00]:(prod_ca-toronto-1_dataplane):~/galorndon/osvc-platform
○ (main) $ #keitn kafka shared-kafka-kafka-0 -- bash
[21:January:2022:05:16:07]:(prod_ca-toronto-1_dataplane):~/galorndon/osvc-platform
○ (main) $ keitn kafka shared-kafka-kafka-0 -- bash
pwd
Defaulted container "kafka" out of: kafka, tls-sidecar, kafka-init (init)
[kafka@shared-kafka-kafka-0 kafka]$ pwd
/opt/kafka
[kafka@shared-kafka-kafka-0 kafka]$ exit
exit
[21:January:2022:05:16:20]:(prod_ca-toronto-1_dataplane):~/galorndon/osvc-platform
○ (main) $ keitn kafka shared-kafka-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --zookeeper localhost --list | grep -i tms
Defaulted container "kafka" out of: kafka, tls-sidecar, kafka-init (init)
tms.application.map
tms.operation.request
tms.operation.request.tracker
tms.operation.response
tms.processor-tenant-tms.internal.state-changelog
tms.processor-tenantOperation-tms.tenant.operation-store-changelog
tms.processor-tenantOperation-tms.tenant.operation-store-repartition
tms.tenant.operation
tms.tenant.state
[21:January:2022:05:16:44]:(prod_ca-toronto-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[21:January:2022:05:19:08]:(prod_ca-toronto-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kuc prod_ap-hyderabad-1_dataplane
Switched to context "prod_ap-hyderabad-1_dataplane".
[21:January:2022:05:19:22]:(prod_ca-toronto-1_dataplane):~/galorndon/osvc-platform
○ (main) $ rp
[21:January:2022:05:19:23]:(prod_ap-hyderabad-1_dataplane):~/galorndon/osvc-platform
○ (main) $ #keitn kafka shared-kafka-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --zookeeper localhost --list | grep -i tms
[21:January:2022:05:19:29]:(prod_ap-hyderabad-1_dataplane):~/galorndon/osvc-platform
○ (main) $ #for context in $(kgc | aw
[21:January:2022:05:20:09]:(prod_ap-hyderabad-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgc | awk '{print $2}' | grep -v NAME
cluster-cbjr52qcfoa
prod_ap-hyderabad-1_dataplane
cluster-czgcn3bmjtg
cluster-c4dazdggfst
cluster-cjiru3mifia
cluster-ci3ypr6tmga
cluster-cp5327xtmfa
cluster-cqldck6xatq
cluster-c4wembugqyt
cluster-cswezbzgyyg
cluster-cz54pc3z4wa
cluster-cyjtbtbbjfq
cluster-czdoolemnqw
cluster-cqtkytbgq3d
cluster-cygcmjxhfsd
cluster-cywmodfmm3d
cluster-c4tkmjwgbrt
cluster-cstsy3bmjrd
cluster-cydcytcg43d
cluster-crdgzbrmjsd
cluster-cwmoxn3bfwq
cluster-chtoe5bnsza
cluster-ce5puidc4ya
cluster-c77hcd3vizq
cluster-cpzagsoukeq
cluster-cf4f222mrfa
cluster-czxpimgibeq
cluster-cuutxfvj2ea
cluster-cc75n7hujpq
cluster-cpjqrjtbotq
cluster-cihx6aprjua
cluster-ciqmbmn6wta
cluster-ctggmtbmfrg
cluster-cqwmnrqmztd
cluster-cywgnlcmqzd
cluster-c3tazjsmjsd
cluster-czwczjzgrsw
cluster-c3wkyjyguzt
cluster-c3tkobugrst
cluster-csdqylfmi2g
cluster-c4tgolgmjsd
[21:January:2022:05:20:27]:(prod_ap-hyderabad-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgc | awk '{print $1}' | grep -v NAME
CURRENT
prod_ap-hyderabad-1_controlplane
*
prod_ap-melbourne-1_controlplane
prod_ap-melbourne-1_dataplane
prod_ap-mumbai-1_controlplane
prod_ap-mumbai-1_dataplane
prod_ap-osaka-1_controlplane
prod_ap-osaka-1_dataplane
prod_ap-sydney-1_controlplane
prod_ap-sydney-1_dataplane
prod_ap-tokyo-1_controlplane
prod_ap-tokyo-1_dataplane
prod_ca-montreal-1_controlplane
prod_ca-montreal-1_dataplane
prod_ca-toronto-1_controlplane
prod_ca-toronto-1_dataplane
prod_eu-amsterdam-1_controlplane
prod_eu-amsterdam-1_dataplane
prod_eu-frankfurt-1_controlplane
prod_eu-frankfurt-1_dataplane
prod_eu-zurich-1_controlplane
prod_eu-zurich-1_dataplane
prod_me-dubai-1_controlplane
prod_me-dubai-1_dataplane
prod_me-jeddah-1_controlplane
prod_me-jeddah-1_dataplane
prod_sa-santiago-1_controlplane
prod_sa-santiago-1_dataplane
prod_sa-saopaulo-1_controlplane
prod_sa-saopaulo-1_dataplane
prod_sa-vinhedo-1_controlplane
prod_sa-vinhedo-1_dataplane
prod_uk-cardiff-1_controlplane
prod_uk-cardiff-1_dataplane
prod_uk-london-1_controlplane
prod_uk-london-1_dataplane
prod_us-ashburn-1_controlplane
prod_us-ashburn-1_dataplane
prod_us-phoenix-1_controlplane
prod_us-phoenix-1_dataplane
prod_us-phoenix-1_deployment
[21:January:2022:05:20:34]:(prod_ap-hyderabad-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgc | awk '{print $1}' | egrep -v "NAME|\*"
CURRENT
prod_ap-hyderabad-1_controlplane
prod_ap-melbourne-1_controlplane
prod_ap-melbourne-1_dataplane
prod_ap-mumbai-1_controlplane
prod_ap-mumbai-1_dataplane
prod_ap-osaka-1_controlplane
prod_ap-osaka-1_dataplane
prod_ap-sydney-1_controlplane
prod_ap-sydney-1_dataplane
prod_ap-tokyo-1_controlplane
prod_ap-tokyo-1_dataplane
prod_ca-montreal-1_controlplane
prod_ca-montreal-1_dataplane
prod_ca-toronto-1_controlplane
prod_ca-toronto-1_dataplane
prod_eu-amsterdam-1_controlplane
prod_eu-amsterdam-1_dataplane
prod_eu-frankfurt-1_controlplane
prod_eu-frankfurt-1_dataplane
prod_eu-zurich-1_controlplane
prod_eu-zurich-1_dataplane
prod_me-dubai-1_controlplane
prod_me-dubai-1_dataplane
prod_me-jeddah-1_controlplane
prod_me-jeddah-1_dataplane
prod_sa-santiago-1_controlplane
prod_sa-santiago-1_dataplane
prod_sa-saopaulo-1_controlplane
prod_sa-saopaulo-1_dataplane
prod_sa-vinhedo-1_controlplane
prod_sa-vinhedo-1_dataplane
prod_uk-cardiff-1_controlplane
prod_uk-cardiff-1_dataplane
prod_uk-london-1_controlplane
prod_uk-london-1_dataplane
prod_us-ashburn-1_controlplane
prod_us-ashburn-1_dataplane
prod_us-phoenix-1_controlplane
prod_us-phoenix-1_dataplane
prod_us-phoenix-1_deployment
[21:January:2022:05:20:51]:(prod_ap-hyderabad-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgc | awk '{print $1}' | egrep -v "NAME|\*|CURRENT"
prod_ap-hyderabad-1_controlplane
prod_ap-melbourne-1_controlplane
prod_ap-melbourne-1_dataplane
prod_ap-mumbai-1_controlplane
prod_ap-mumbai-1_dataplane
prod_ap-osaka-1_controlplane
prod_ap-osaka-1_dataplane
prod_ap-sydney-1_controlplane
prod_ap-sydney-1_dataplane
prod_ap-tokyo-1_controlplane
prod_ap-tokyo-1_dataplane
prod_ca-montreal-1_controlplane
prod_ca-montreal-1_dataplane
prod_ca-toronto-1_controlplane
prod_ca-toronto-1_dataplane
prod_eu-amsterdam-1_controlplane
prod_eu-amsterdam-1_dataplane
prod_eu-frankfurt-1_controlplane
prod_eu-frankfurt-1_dataplane
prod_eu-zurich-1_controlplane
prod_eu-zurich-1_dataplane
prod_me-dubai-1_controlplane
prod_me-dubai-1_dataplane
prod_me-jeddah-1_controlplane
prod_me-jeddah-1_dataplane
prod_sa-santiago-1_controlplane
prod_sa-santiago-1_dataplane
prod_sa-saopaulo-1_controlplane
prod_sa-saopaulo-1_dataplane
prod_sa-vinhedo-1_controlplane
prod_sa-vinhedo-1_dataplane
prod_uk-cardiff-1_controlplane
prod_uk-cardiff-1_dataplane
prod_uk-london-1_controlplane
prod_uk-london-1_dataplane
prod_us-ashburn-1_controlplane
prod_us-ashburn-1_dataplane
prod_us-phoenix-1_controlplane
prod_us-phoenix-1_dataplane
prod_us-phoenix-1_deployment
[21:January:2022:05:21:04]:(prod_ap-hyderabad-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgc | awk '{print $1}' | egrep -v "NAME|\*|CURRENT|controlplane|deployment"
prod_ap-melbourne-1_dataplane
prod_ap-mumbai-1_dataplane
prod_ap-osaka-1_dataplane
prod_ap-sydney-1_dataplane
prod_ap-tokyo-1_dataplane
prod_ca-montreal-1_dataplane
prod_ca-toronto-1_dataplane
prod_eu-amsterdam-1_dataplane
prod_eu-frankfurt-1_dataplane
prod_eu-zurich-1_dataplane
prod_me-dubai-1_dataplane
prod_me-jeddah-1_dataplane
prod_sa-santiago-1_dataplane
prod_sa-saopaulo-1_dataplane
prod_sa-vinhedo-1_dataplane
prod_uk-cardiff-1_dataplane
prod_uk-london-1_dataplane
prod_us-ashburn-1_dataplane
prod_us-phoenix-1_dataplane
[21:January:2022:05:21:28]:(prod_ap-hyderabad-1_dataplane):~/galorndon/osvc-platform
○ (main) $ for dataplane in $(kgc | awk '{print $1}' | egrep -v "NAME|\*|CURRENT|controlplane|deployment"); do echo "Region Cluster name: $dataplane"; keitn kafka shared-kafka-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --zookeeper localhost --list | grep -i tms; echo "==========================";done
Region Cluster name: prod_ap-melbourne-1_dataplane
E0121 05:22:56.274211    3368 request.go:1027] Unexpected error when reading response body: net/http: request canceled (Client.Timeout or context cancellation while reading body)
W0121 05:22:56.274894    3368 transport.go:260] Unable to cancel request for *exec.roundTripper
^C
[21:January:2022:05:23:06]:(prod_ap-hyderabad-1_dataplane):~/galorndon/osvc-platform
○ (main) $ for dataplane in $(kgc | awk '{print $1}' | egrep -v "NAME|\*|CURRENT|controlplane|deployment"); do echo "Region Cluster name: $dataplane"; kuc $dataplane ; keitn kafka shared-kafka-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --zookeeper localhost --list | grep -i tms; echo "==========================";done
Region Cluster name: prod_ap-melbourne-1_dataplane
Switched to context "prod_ap-melbourne-1_dataplane".
Defaulted container "kafka" out of: kafka, tls-sidecar, kafka-init (init)
tms.application.map
tms.operation.request
tms.operation.request.tracker
tms.operation.response
tms.processor-tenant-tms.internal.state-changelog
tms.processor-tenantOperation-tms.tenant.operation-store-changelog
tms.processor-tenantOperation-tms.tenant.operation-store-repartition
tms.tenant.operation
tms.tenant.state
==========================
Region Cluster name: prod_ap-mumbai-1_dataplane
Switched to context "prod_ap-mumbai-1_dataplane".
Defaulted container "kafka" out of: kafka, tls-sidecar, kafka-init (init)
tms.application.map
tms.operation.request
tms.operation.request.tracker
tms.operation.response
tms.processor-tenant-tms.internal.state-changelog
tms.processor-tenantOperation-tms.tenant.operation-store-changelog
tms.processor-tenantOperation-tms.tenant.operation-store-repartition
tms.tenant.operation
tms.tenant.state
==========================
Region Cluster name: prod_ap-osaka-1_dataplane
Switched to context "prod_ap-osaka-1_dataplane".
W0121 05:24:44.577373    3443 transport.go:260] Unable to cancel request for *exec.roundTripper
E0121 05:24:44.579854    3443 request.go:1027] Unexpected error when reading response body: net/http: request canceled (Client.Timeout or context cancellation while reading body)
W0121 05:25:16.555497    3443 transport.go:260] Unable to cancel request for *exec.roundTripper
E0121 05:25:16.558456    3443 request.go:1027] Unexpected error when reading response body: net/http: request canceled (Client.Timeout or context cancellation while reading body)
^C
[21:January:2022:05:25:23]:(prod_ap-hyderabad-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kuc prod_ap-mumbai-1_dataplane
Switched to context "prod_ap-mumbai-1_dataplane".
[21:January:2022:05:25:27]:(prod_ap-hyderabad-1_dataplane):~/galorndon/osvc-platform
○ (main) $ rp
[21:January:2022:05:25:29]:(prod_ap-mumbai-1_dataplane):~/galorndon/osvc-platform
○ (main) $ keitn kafka shared-kafka-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --zookeeper localhost --list | grep -i tms
Defaulted container "kafka" out of: kafka, tls-sidecar, kafka-init (init)
tms.application.map
tms.operation.request
tms.operation.request.tracker
tms.operation.response
tms.processor-tenant-tms.internal.state-changelog
tms.processor-tenantOperation-tms.tenant.operation-store-changelog
tms.processor-tenantOperation-tms.tenant.operation-store-repartition
tms.tenant.operation
tms.tenant.state
[21:January:2022:05:25:48]:(prod_ap-mumbai-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpnk
NAME                                                            READY   STATUS    RESTARTS   AGE    IP             NODE           NOMINATED NODE   READINESS GATES
schemaregistry-proxy-8b5879994-l4ngc                            2/2     Running   0          118d   10.245.5.155   10.20.96.73    <none>           <none>
shared-kafka-dr-kafka-0                                         2/2     Running   0          118d   10.245.3.141   10.20.97.210   <none>           <none>
shared-kafka-dr-kafka-1                                         2/2     Running   0          107d   10.245.6.146   10.20.96.199   <none>           <none>
shared-kafka-dr-kafka-2                                         2/2     Running   0          118d   10.245.5.5     10.20.96.97    <none>           <none>
shared-kafka-dr-kafka-exporter-cd8bb568c-tpn7v                  1/1     Running   0          118d   10.245.1.139   10.20.97.125   <none>           <none>
shared-kafka-dr-zookeeper-0                                     1/1     Running   0          118d   10.245.3.139   10.20.97.210   <none>           <none>
shared-kafka-dr-zookeeper-1                                     1/1     Running   0          118d   10.245.3.136   10.20.97.210   <none>           <none>
shared-kafka-dr-zookeeper-2                                     1/1     Running   0          118d   10.245.1.141   10.20.97.125   <none>           <none>
shared-kafka-entity-operator-b8fffd85d-fk2w9                    3/3     Running   4          118d   10.245.1.133   10.20.97.125   <none>           <none>
shared-kafka-kafka-0                                            2/2     Running   0          118d   10.245.1.138   10.20.97.125   <none>           <none>
shared-kafka-kafka-1                                            2/2     Running   0          118d   10.245.3.140   10.20.97.210   <none>           <none>
shared-kafka-kafka-2                                            2/2     Running   0          118d   10.245.3.137   10.20.97.210   <none>           <none>
shared-kafka-kafka-exporter-78cb89cf4d-9k9dx                    1/1     Running   0          118d   10.245.3.132   10.20.97.210   <none>           <none>
shared-kafka-mm2-ap-mumbai-1-mirrormaker2-6b45b89764-bmwjz      1/1     Running   0          118d   10.245.3.135   10.20.97.210   <none>           <none>
shared-kafka-mm2-ap-mumbai-1-mirrormaker2-6b45b89764-gxcnc      1/1     Running   0          118d   10.245.1.140   10.20.97.125   <none>           <none>
shared-kafka-mm2-dr-ap-mumbai-1-mirrormaker2-599754f974-qj5k7   1/1     Running   0          118d   10.245.3.133   10.20.97.210   <none>           <none>
shared-kafka-mm2-dr-ap-mumbai-1-mirrormaker2-599754f974-rzdvx   1/1     Running   0          118d   10.245.3.138   10.20.97.210   <none>           <none>
shared-kafka-zookeeper-0                                        1/1     Running   0          118d   10.245.1.144   10.20.97.125   <none>           <none>
shared-kafka-zookeeper-1                                        1/1     Running   0          118d   10.245.1.142   10.20.97.125   <none>           <none>
shared-kafka-zookeeper-2                                        1/1     Running   0          118d   10.245.5.6     10.20.96.97    <none>           <none>
shared-schemaregistry-54d5f95b64-9zj6m                          2/2     Running   0          118d   10.245.4.143   10.20.97.252   <none>           <none>
shared-schemaregistry-54d5f95b64-w62k9                          2/2     Running   0          118d   10.245.5.143   10.20.96.73    <none>           <none>
strimzi-cluster-operator-856b5f5768-drfnd                       1/1     Running   3          32d    10.245.2.11    10.20.96.79    <none>           <none>
[21:January:2022:05:26:18]:(prod_ap-mumbai-1_dataplane):~/galorndon/osvc-platform
○ (main) $ keitn kafka shared-kafka-kafka-0 -- bash

set -o viDefaulted container "kafka" out of: kafka, tls-sidecar, kafka-init (init)
[kafka@shared-kafka-kafka-0 kafka]$
[kafka@shared-kafka-kafka-0 kafka]$ set -o vi
[kafka@shared-kafka-kafka-0 kafka]$
[kafka@shared-kafka-kafka-0 kafka]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost --list | grep -i tms
tms.application.map
tms.operation.request
tms.operation.request.tracker
tms.operation.response
tms.processor-tenant-tms.internal.state-changelog
tms.processor-tenantOperation-tms.tenant.operation-store-changelog
tms.processor-tenantOperation-tms.tenant.operation-store-repartition
tms.tenant.operation
tms.tenant.state
[kafka@shared-kafka-kafka-0 kafka]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost --describe --topic tms.application.map
Topic: tms.application.map  PartitionCount: 3 ReplicationFactor: 3  Configs: cleanup.policy=compact
  Topic: tms.application.map  Partition: 0  Leader: 1 Replicas: 1,0,2 Isr: 1,0,2
  Topic: tms.application.map  Partition: 1  Leader: 2 Replicas: 2,1,0 Isr: 2,1,0
  Topic: tms.application.map  Partition: 2  Leader: 0 Replicas: 0,2,1 Isr: 0,2,1
[kafka@shared-kafka-kafka-0 kafka]$ exit
exit
[21:January:2022:05:28:03]:(prod_ap-mumbai-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kuc prod_us-phoenix-1_dataplane
Switched to context "prod_us-phoenix-1_dataplane".
[21:January:2022:05:30:31]:(prod_ap-mumbai-1_dataplane):~/galorndon/osvc-platform
○ (main) $ rp
[21:January:2022:05:30:31]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn mercury-psr mercury-enrichment-service -oyaml
Error from server (NotFound): pods "mercury-enrichment-service" not found
[21:January:2022:05:30:50]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn mercury-psr mercury-enrichment-service -oyaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
    meta.helm.sh/release-name: mercury
    meta.helm.sh/release-namespace: mercury-psr
  creationTimestamp: "2022-01-19T22:53:28Z"
  generation: 1
  labels:
    app.kubernetes.io/instance: mercury
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: enrichment-service
    helm.sh/chart: enrichment-service-22.01.18-REL21.11-318
    monitoring: "true"
    project: mercury
    version: "0"
  name: mercury-enrichment-service
  namespace: mercury-psr
  resourceVersion: "292930226"
  selfLink: /apis/apps/v1/namespaces/mercury-psr/deployments/mercury-enrichment-service
  uid: 447daecc-ca5f-43b2-9ede-b4cdf71b8c69
spec:
  progressDeadlineSeconds: 600
  replicas: 3
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/name: enrichment-service
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      annotations:
        vault.security.banzaicloud.io/log-level: warn
        vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
        vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
        vault.security.banzaicloud.io/vault-env-daemon: "true"
        vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
        vault.security.banzaicloud.io/vault-role: mercury-psr
        vault.security.banzaicloud.io/vault-skip-verify: "true"
      creationTimestamp: null
      labels:
        app.kubernetes.io/instance: mercury
        app.kubernetes.io/name: enrichment-service
        chronos.enabled: "True"
        chronos.maxPodLifetime: "1440"
        chronos.minAvailable: "0"
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - enrichment-service
                - key: release
                  operator: In
                  values:
                  - mercury
              topologyKey: failure-domain.beta.kubernetes.io/zone
            weight: 100
      containers:
      - env:
        - name: profile
          value: oci-k8s
        - name: DATACENTER
          value: OCI
        - name: RELEASE_NAME
          value: mercury
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: POD_SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.serviceAccountName
        - name: MERCURY_URL_PATTERN
          value: https://engagement-psr.us1.channels.ocs.oraclecloud.com/mercury-psr
        - name: JAEGER_REMOTE_REPORTER
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.hostIP
        - name: VAULT_TOKEN
          value: vault:login
        - name: HOST_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.hostIP
        - name: CONSUL_HTTP_ADDR
          value: https://$(HOST_IP):8501
        - name: CONSUL_TOKEN
          value: vault:/cpe_consul/creds/mercury-psr#token
        - name: JAVA_OPTS
          value: '-Dvault.enabled=true -Dvault.namespace=mercury-psr -Dvault.transit.kafkaMessageEncryption.path=transit_mercury
            -Dvault.base.address=https://vault.query.prod.consul:8200 -Dvault.shared.kv.engine=/kv
            -Dthidwick.vault.uri=https://vault.query.prod.consul:8200 -Dthidwick.vault.transit.cache-expiration=300
            -Dthidwick.vault.kv.cache-expiration=300 -Dthidwick.vault.serviceaccount=mercurygeneric
            -Dlog4j2.formatMsgNoLookups=true -Dmercury.cpeVersion=v2 -Dfile.token.enabled=true
            -Dvault.authentication=TOKEN -Dvault.role=mercury-psr -Dvault.path=k8s-phx-dataplane
            -Dkafka.log_level=WARN -Dthidwick-server.logback.level=INFO -Dthidwick-server.logback.service.log_level=DEBUG
            -Dthidwick.kafka.message.encryption.cipher=AES/GCM -Djwt.tenant.validation.enabled=false
            -Dthidwick.kafka.schema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul
            -Dthidwick.kafka.bootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443
            -Dthidwick.tms.base.tmsStateStream.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443
            -Djavax.net.ssl.trustStore=/keystores/truststore.jks -Djavax.net.ssl.keyStore=/keystores/keystore.jks
            -Dhttp.security.headers.enabled=true -Dhttp.rest.security.headers.enabled=true
            -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Xms1536m   -Xmx4096m   -Dmercury.tenant.name.filter="^(evai)"                 -Dtenant.realm=engagement-psr.us1.channels.ocs.oraclecloud.com '
        image: iad.ocir.io/osvcstage/mercury/enrichment-service:22.01.18-REL21.11-318
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 1
          httpGet:
            path: /health/checks
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 120
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 1
        name: enrichment-service
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        readinessProbe:
          failureThreshold: 1
          httpGet:
            path: /health/ping
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 120
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          limits:
            cpu: "1"
            ephemeral-storage: 10Gi
            memory: 6Gi
          requests:
            cpu: 100m
            ephemeral-storage: 2Gi
            memory: 2Gi
        startupProbe:
          failureThreshold: 120
          httpGet:
            path: /health/checks
            port: 8080
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /keystores
          name: keystore-volume
        - mountPath: /tmp/keystore-pass
          name: keystore-pass
      dnsPolicy: ClusterFirst
      imagePullSecrets:
      - name: osvcstage-ocirsecret
      initContainers:
      - command:
        - /bin/bash
        - -c
        - /init/init-script.sh
        env:
        - name: AUTO_REFRESH_CONSUL_TOKEN
          value: vault:cpe_consul/creds/mercury-psr#token
        - name: KAFKA_TEMPLATED_PKI
          value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
            .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
            "ttl": "720h"}'
        - name: KEYSTORE_LOCATION
          value: /keystores
        - name: TRUSTSTORE_NAME
          value: truststore.jks
        - name: KEYSTORE_SECRETPATH
          value: /tmp/keystore-pass
        - name: KEYSTORE_TMPPASSFILENAME
          value: props/kafka.properties
        - name: KEYSTORE_NAME
          value: keystore.jks
        - name: SERVICEACCOUNT_SECRETPATH
          value: /var/run/secrets/kubernetes.io/serviceaccount
        - name: VAULT_ADDR
          value: https://vault.query.prod.consul:8200
        - name: VAULT_SECRETPATH
          value: /tmp/vaultca
        - name: PKI_VAULT_ENGINE_PATH
          value: v1/infra_pki/issue/kafka_client
        - name: CERT_CN
          value: mercury-psr-mercurygeneric-rw
        - name: KAFKA_SECURITY_PROTOCOL
          value: SSL
        - name: SA_ROLE
          value: mercury-psr_mercurygeneric
        - name: VAULT_SERVICEACCOUNT
          value: mercurygeneric
        image: iad.ocir.io/osvcstage/mercury/init-container/prod:1.0.0.18
        imagePullPolicy: IfNotPresent
        name: init
        resources:
          limits:
            cpu: "1"
            ephemeral-storage: 10Gi
            memory: 6Gi
          requests:
            cpu: 100m
            ephemeral-storage: 2Gi
            memory: 2Gi
        securityContext:
          runAsUser: 0
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /keystores
          name: keystore-volume
        - mountPath: /tmp/vaultca
          name: vaultca
          readOnly: true
        - mountPath: /tmp/keystore-pass
          name: keystore-pass
          readOnly: true
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: mercurygeneric
      serviceAccountName: mercurygeneric
      terminationGracePeriodSeconds: 30
      volumes:
      - name: vaultca
        secret:
          defaultMode: 420
          secretName: vault-tls
      - name: keystore-pass
        secret:
          defaultMode: 420
          secretName: mercury-enrichment-service-keystore
      - emptyDir: {}
        name: keystore-volume
status:
  availableReplicas: 3
  conditions:
  - lastTransitionTime: "2022-01-19T22:57:59Z"
    lastUpdateTime: "2022-01-19T22:57:59Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2022-01-19T22:53:33Z"
    lastUpdateTime: "2022-01-19T22:57:59Z"
    message: ReplicaSet "mercury-enrichment-service-84c964b545" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 1
  readyReplicas: 3
  replicas: 3
  updatedReplicas: 3
[21:January:2022:05:31:01]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[21:January:2022:05:32:10]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn mercury-psr mercury-session-housekeeping-processor -oyaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
    meta.helm.sh/release-name: mercury
    meta.helm.sh/release-namespace: mercury-psr
  creationTimestamp: "2022-01-19T22:53:29Z"
  generation: 1
  labels:
    app.kubernetes.io/instance: mercury
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: session-housekeeping-processor
    helm.sh/chart: session-housekeeping-processor-21.11.09-TRUNK-39
    monitoring: "true"
    project: mercury
    version: "0"
  name: mercury-session-housekeeping-processor
  namespace: mercury-psr
  resourceVersion: "292928757"
  selfLink: /apis/apps/v1/namespaces/mercury-psr/deployments/mercury-session-housekeeping-processor
  uid: c1996302-9457-4e74-97d1-7c7dc1f7bea4
spec:
  progressDeadlineSeconds: 600
  replicas: 3
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/name: session-housekeeping-processor
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      annotations:
        vault.security.banzaicloud.io/log-level: warn
        vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
        vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
        vault.security.banzaicloud.io/vault-env-daemon: "true"
        vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
        vault.security.banzaicloud.io/vault-role: mercury-psr
        vault.security.banzaicloud.io/vault-skip-verify: "true"
      creationTimestamp: null
      labels:
        app.kubernetes.io/instance: mercury
        app.kubernetes.io/name: session-housekeeping-processor
        chronos.enabled: "True"
        chronos.maxPodLifetime: "1440"
        chronos.minAvailable: "0"
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - session-housekeeping-processor
                - key: release
                  operator: In
                  values:
                  - mercury
              topologyKey: failure-domain.beta.kubernetes.io/zone
            weight: 100
      containers:
      - env:
        - name: AUTO_REFRESH_CONSUL_TOKEN
          value: vault:cpe_consul/creds/mercury-psr#token
        - name: KAFKA_TEMPLATED_PKI
          value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
            .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
            "ttl": "720h"}'
        - name: KEYSTORE_LOCATION
          value: /keystores
        - name: TRUSTSTORE_NAME
          value: truststore.jks
        - name: KEYSTORE_SECRETPATH
          value: /tmp/keystore-pass
        - name: KEYSTORE_TMPPASSFILENAME
          value: props/kafka.properties
        - name: KEYSTORE_NAME
          value: keystore.jks
        - name: SERVICEACCOUNT_SECRETPATH
          value: /var/run/secrets/kubernetes.io/serviceaccount
        - name: VAULT_ADDR
          value: https://vault.query.prod.consul:8200
        - name: VAULT_SECRETPATH
          value: /tmp/vaultca
        - name: PKI_VAULT_ENGINE_PATH
          value: v1/infra_pki/issue/kafka_client
        - name: CERT_CN
          value: mercury-psr-mercurygeneric-rw
        - name: KAFKA_SECURITY_PROTOCOL
          value: SSL
        - name: SA_ROLE
          value: mercury-psr_mercurygeneric
        - name: VAULT_SERVICEACCOUNT
          value: mercurygeneric
        - name: profile
          value: oci-k8s
        - name: DATACENTER
          value: OCI
        - name: RELEASE_NAME
          value: mercury
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: POD_SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.serviceAccountName
        - name: MERCURY_URL_PATTERN
          value: https://engagement-psr.us1.channels.ocs.oraclecloud.com/mercury-psr
        - name: JAEGER_REMOTE_REPORTER
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.hostIP
        - name: VAULT_TOKEN
          value: vault:login
        - name: HOST_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.hostIP
        - name: CONSUL_HTTP_ADDR
          value: https://$(HOST_IP):8501
        - name: CONSUL_TOKEN
          value: vault:/cpe_consul/creds/mercury-psr#token
        - name: JAVA_OPTS
          value: '-Dvault.enabled=true -Dvault.namespace=mercury-psr -Dvault.transit.kafkaMessageEncryption.path=transit_mercury
            -Dvault.base.address=https://vault.query.prod.consul:8200 -Dvault.shared.kv.engine=/kv
            -Dthidwick.vault.uri=https://vault.query.prod.consul:8200 -Dthidwick.vault.transit.cache-expiration=300
            -Dthidwick.vault.kv.cache-expiration=300 -Dthidwick.vault.serviceaccount=mercurygeneric
            -Dlog4j2.formatMsgNoLookups=true -Dmercury.cpeVersion=v2 -Dfile.token.enabled=true
            -Dvault.authentication=TOKEN -Dvault.role=mercury-psr -Dvault.path=k8s-phx-dataplane
            -Dkafka.log_level=WARN -Dthidwick-server.logback.level=INFO -Dthidwick-server.logback.service.log_level=INFO
            -Dthidwick.kafka.message.encryption.cipher=AES/GCM -Djwt.tenant.validation.enabled=false
            -Dthidwick.kafka.schema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul
            -Dthidwick.kafka.bootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443
            -Dthidwick.tms.base.tmsStateStream.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443
            -Djavax.net.ssl.trustStore=/keystores/truststore.jks -Djavax.net.ssl.keyStore=/keystores/keystore.jks
            -Dhttp.security.headers.enabled=true -Dhttp.rest.security.headers.enabled=true
            -Dsession.housekeeping.watchdog.timer.ms=600000                    -Xms1536m   -Xmx4096m   -Dtenant.realm=engagement-psr.us1.channels.ocs.oraclecloud.com '
        image: iad.ocir.io/osvcstage/mercury/session-housekeeping-processor:21.11.09-TRUNK-39
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 1
          httpGet:
            path: /health/checks
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 5
        name: session-housekeeping-processor
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        readinessProbe:
          failureThreshold: 1
          httpGet:
            path: /health/ping
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          limits:
            cpu: "1"
            ephemeral-storage: 10Gi
            memory: 6Gi
          requests:
            cpu: 100m
            ephemeral-storage: 2Gi
            memory: 2Gi
        securityContext:
          runAsUser: 0
        startupProbe:
          failureThreshold: 120
          httpGet:
            path: /health/checks
            port: 8080
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /keystores
          name: keystore-volume
        - mountPath: /tmp/keystore-pass
          name: keystore-pass
        - mountPath: /tmp/vaultca
          name: vaultca
          readOnly: true
      dnsPolicy: ClusterFirst
      imagePullSecrets:
      - name: osvcstage-ocirsecret
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: mercurygeneric
      serviceAccountName: mercurygeneric
      terminationGracePeriodSeconds: 30
      volumes:
      - name: vaultca
        secret:
          defaultMode: 420
          secretName: vault-tls
      - name: keystore-pass
        secret:
          defaultMode: 420
          secretName: mercury-session-housekeeping-processor-keystore
      - emptyDir: {}
        name: keystore-volume
status:
  availableReplicas: 3
  conditions:
  - lastTransitionTime: "2022-01-19T22:56:27Z"
    lastUpdateTime: "2022-01-19T22:56:27Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2022-01-19T22:53:36Z"
    lastUpdateTime: "2022-01-19T22:56:27Z"
    message: ReplicaSet "mercury-session-housekeeping-processor-788fdfd5bb" has successfully
      progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 1
  readyReplicas: 3
  replicas: 3
  updatedReplicas: 3
[21:January:2022:05:33:33]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn mercury-psr mercury-tenant-downtime-monitor -oyaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
    meta.helm.sh/release-name: mercury
    meta.helm.sh/release-namespace: mercury-psr
  creationTimestamp: "2022-01-19T22:53:28Z"
  generation: 1
  labels:
    app.kubernetes.io/instance: mercury
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: tenant-downtime-monitor
    helm.sh/chart: tenant-downtime-monitor-22.01.18-REL21.11-153
    monitoring: "true"
    project: mercury
    version: "0"
  name: mercury-tenant-downtime-monitor
  namespace: mercury-psr
  resourceVersion: "292929220"
  selfLink: /apis/apps/v1/namespaces/mercury-psr/deployments/mercury-tenant-downtime-monitor
  uid: c84d6229-3019-4322-b56b-e8f6b1501f75
spec:
  progressDeadlineSeconds: 600
  replicas: 3
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/name: tenant-downtime-monitor
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      annotations:
        vault.security.banzaicloud.io/log-level: warn
        vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
        vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
        vault.security.banzaicloud.io/vault-env-daemon: "true"
        vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
        vault.security.banzaicloud.io/vault-role: mercury-psr
        vault.security.banzaicloud.io/vault-skip-verify: "true"
      creationTimestamp: null
      labels:
        app.kubernetes.io/instance: mercury
        app.kubernetes.io/name: tenant-downtime-monitor
        chronos.enabled: "True"
        chronos.maxPodLifetime: "1440"
        chronos.minAvailable: "0"
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - tenant-downtime-monitor
                - key: release
                  operator: In
                  values:
                  - mercury
              topologyKey: failure-domain.beta.kubernetes.io/zone
            weight: 100
      containers:
      - env:
        - name: profile
          value: oci-k8s
        - name: DATACENTER
          value: OCI
        - name: RELEASE_NAME
          value: mercury
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: POD_SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.serviceAccountName
        - name: MERCURY_URL_PATTERN
          value: https://engagement-psr.us1.channels.ocs.oraclecloud.com/mercury-psr
        - name: JAEGER_REMOTE_REPORTER
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.hostIP
        - name: VAULT_TOKEN
          value: vault:login
        - name: HOST_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.hostIP
        - name: CONSUL_HTTP_ADDR
          value: https://$(HOST_IP):8501
        - name: CONSUL_TOKEN
          value: vault:/cpe_consul/creds/mercury-psr#token
        - name: JAVA_OPTS
          value: '-Dvault.enabled=true -Dvault.namespace=mercury-psr -Dvault.transit.kafkaMessageEncryption.path=transit_mercury
            -Dvault.base.address=https://vault.query.prod.consul:8200 -Dvault.shared.kv.engine=/kv
            -Dthidwick.vault.uri=https://vault.query.prod.consul:8200 -Dthidwick.vault.transit.cache-expiration=300
            -Dthidwick.vault.kv.cache-expiration=300 -Dthidwick.vault.serviceaccount=mercurygeneric
            -Dlog4j2.formatMsgNoLookups=true -Dmercury.cpeVersion=v2 -Dfile.token.enabled=true
            -Dvault.authentication=TOKEN -Dvault.role=mercury-psr -Dvault.path=k8s-phx-dataplane
            -Dkafka.log_level=WARN -Dthidwick-server.logback.level=INFO -Dthidwick-server.logback.service.log_level=DEBUG
            -Dthidwick.kafka.message.encryption.cipher=AES/GCM -Djwt.tenant.validation.enabled=false
            -Dthidwick.kafka.schema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul
            -Dthidwick.kafka.bootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443
            -Dthidwick.tms.base.tmsStateStream.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443
            -Djavax.net.ssl.trustStore=/keystores/truststore.jks -Djavax.net.ssl.keyStore=/keystores/keystore.jks
            -Dhttp.security.headers.enabled=true -Dhttp.rest.security.headers.enabled=true
            -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Xms1536m   -Xmx4096m   -Dmercury.tenant.name.filter="^(evai)"                 -Dtenant.realm=engagement-psr.us1.channels.ocs.oraclecloud.com '
        image: iad.ocir.io/osvcstage/mercury/tenant-downtime-monitor:22.01.18-REL21.11-153
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 1
          httpGet:
            path: /health/checks
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 120
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 1
        name: tenant-downtime-monitor
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        readinessProbe:
          failureThreshold: 1
          httpGet:
            path: /health/ping
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 120
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          limits:
            cpu: "1"
            ephemeral-storage: 10Gi
            memory: 6Gi
          requests:
            cpu: 100m
            ephemeral-storage: 2Gi
            memory: 2Gi
        startupProbe:
          failureThreshold: 120
          httpGet:
            path: /health/checks
            port: 8080
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /keystores
          name: keystore-volume
        - mountPath: /tmp/keystore-pass
          name: keystore-pass
      dnsPolicy: ClusterFirst
      imagePullSecrets:
      - name: osvcstage-ocirsecret
      initContainers:
      - command:
        - /bin/bash
        - -c
        - /init/init-script.sh
        env:
        - name: AUTO_REFRESH_CONSUL_TOKEN
          value: vault:cpe_consul/creds/mercury-psr#token
        - name: KAFKA_TEMPLATED_PKI
          value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
            .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
            "ttl": "720h"}'
        - name: KEYSTORE_LOCATION
          value: /keystores
        - name: TRUSTSTORE_NAME
          value: truststore.jks
        - name: KEYSTORE_SECRETPATH
          value: /tmp/keystore-pass
        - name: KEYSTORE_TMPPASSFILENAME
          value: props/kafka.properties
        - name: KEYSTORE_NAME
          value: keystore.jks
        - name: SERVICEACCOUNT_SECRETPATH
          value: /var/run/secrets/kubernetes.io/serviceaccount
        - name: VAULT_ADDR
          value: https://vault.query.prod.consul:8200
        - name: VAULT_SECRETPATH
          value: /tmp/vaultca
        - name: PKI_VAULT_ENGINE_PATH
          value: v1/infra_pki/issue/kafka_client
        - name: CERT_CN
          value: mercury-psr-mercurygeneric-rw
        - name: KAFKA_SECURITY_PROTOCOL
          value: SSL
        - name: SA_ROLE
          value: mercury-psr_mercurygeneric
        - name: VAULT_SERVICEACCOUNT
          value: mercurygeneric
        image: iad.ocir.io/osvcstage/mercury/init-container/prod:1.0.0.18
        imagePullPolicy: IfNotPresent
        name: init
        resources:
          limits:
            cpu: "1"
            ephemeral-storage: 10Gi
            memory: 6Gi
          requests:
            cpu: 100m
            ephemeral-storage: 2Gi
            memory: 2Gi
        securityContext:
          runAsUser: 0
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /keystores
          name: keystore-volume
        - mountPath: /tmp/vaultca
          name: vaultca
          readOnly: true
        - mountPath: /tmp/keystore-pass
          name: keystore-pass
          readOnly: true
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: mercurygeneric
      serviceAccountName: mercurygeneric
      terminationGracePeriodSeconds: 30
      volumes:
      - name: vaultca
        secret:
          defaultMode: 420
          secretName: vault-tls
      - name: keystore-pass
        secret:
          defaultMode: 420
          secretName: mercury-tenant-downtime-monitor-keystore
      - emptyDir: {}
        name: keystore-volume
status:
  availableReplicas: 3
  conditions:
  - lastTransitionTime: "2022-01-19T22:57:11Z"
    lastUpdateTime: "2022-01-19T22:57:11Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2022-01-19T22:53:31Z"
    lastUpdateTime: "2022-01-19T22:57:11Z"
    message: ReplicaSet "mercury-tenant-downtime-monitor-78fdf4c974" has successfully
      progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 1
  readyReplicas: 3
  replicas: 3
  updatedReplicas: 3
[21:January:2022:05:34:54]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[21:January:2022:05:35:17]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn mercury-psr mercury-tenant-downtime-monitor-78fdf4c974-hdg4h -oyaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubernetes.io/psp: psp-protect-docker
    vault.security.banzaicloud.io/log-level: warn
    vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
    vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
    vault.security.banzaicloud.io/vault-env-daemon: "true"
    vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
    vault.security.banzaicloud.io/vault-role: mercury-psr
    vault.security.banzaicloud.io/vault-skip-verify: "true"
  creationTimestamp: "2022-01-19T22:53:47Z"
  generateName: mercury-tenant-downtime-monitor-78fdf4c974-
  labels:
    app.kubernetes.io/instance: mercury
    app.kubernetes.io/name: tenant-downtime-monitor
    chronos.enabled: "True"
    chronos.maxPodLifetime: "1440"
    chronos.minAvailable: "0"
    pod-template-hash: 78fdf4c974
  name: mercury-tenant-downtime-monitor-78fdf4c974-hdg4h
  namespace: mercury-psr
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: mercury-tenant-downtime-monitor-78fdf4c974
    uid: 5a8f3f9b-0fbe-4eab-9e22-6131e582b6b5
  resourceVersion: "292929216"
  selfLink: /api/v1/namespaces/mercury-psr/pods/mercury-tenant-downtime-monitor-78fdf4c974-hdg4h
  uid: 216cd80e-af06-4992-bea2-1fd36c8a8d4a
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - tenant-downtime-monitor
            - key: release
              operator: In
              values:
              - mercury
          topologyKey: failure-domain.beta.kubernetes.io/zone
        weight: 100
  containers:
  - args:
    - agent
    - -config
    - /vault/config/config.hcl
    env:
    - name: VAULT_ADDR
      value: https://vault.query.prod.consul:8200
    - name: VAULT_SKIP_VERIFY
      value: "true"
    image: docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5
    imagePullPolicy: IfNotPresent
    name: vault-agent
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 100m
        memory: 128Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        add:
        - IPC_LOCK
        - SYS_PTRACE
        drop:
        - NET_RAW
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /vault/
      name: vault-env
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: mercurygeneric-token-b6n4q
      readOnly: true
    - mountPath: /vault/secrets
      name: agent-secrets
    - mountPath: /vault/config/config.hcl
      name: agent-configmap
      readOnly: true
      subPath: config.hcl
  - args:
    - /tenant-downtime-monitor/bin/tenant-downtime-monitor
    command:
    - /vault/vault-env
    env:
    - name: profile
      value: oci-k8s
    - name: DATACENTER
      value: OCI
    - name: RELEASE_NAME
      value: mercury
    - name: POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.namespace
    - name: POD_IP
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.podIP
    - name: POD_SERVICE_ACCOUNT
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: spec.serviceAccountName
    - name: MERCURY_URL_PATTERN
      value: https://engagement-psr.us1.channels.ocs.oraclecloud.com/mercury-psr
    - name: JAEGER_REMOTE_REPORTER
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.hostIP
    - name: VAULT_TOKEN
      value: vault:login
    - name: HOST_IP
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.hostIP
    - name: CONSUL_HTTP_ADDR
      value: https://$(HOST_IP):8501
    - name: CONSUL_TOKEN
      value: vault:/cpe_consul/creds/mercury-psr#token
    - name: JAVA_OPTS
      value: '-Dvault.enabled=true -Dvault.namespace=mercury-psr -Dvault.transit.kafkaMessageEncryption.path=transit_mercury
        -Dvault.base.address=https://vault.query.prod.consul:8200 -Dvault.shared.kv.engine=/kv
        -Dthidwick.vault.uri=https://vault.query.prod.consul:8200 -Dthidwick.vault.transit.cache-expiration=300
        -Dthidwick.vault.kv.cache-expiration=300 -Dthidwick.vault.serviceaccount=mercurygeneric
        -Dlog4j2.formatMsgNoLookups=true -Dmercury.cpeVersion=v2 -Dfile.token.enabled=true
        -Dvault.authentication=TOKEN -Dvault.role=mercury-psr -Dvault.path=k8s-phx-dataplane
        -Dkafka.log_level=WARN -Dthidwick-server.logback.level=INFO -Dthidwick-server.logback.service.log_level=DEBUG
        -Dthidwick.kafka.message.encryption.cipher=AES/GCM -Djwt.tenant.validation.enabled=false
        -Dthidwick.kafka.schema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul
        -Dthidwick.kafka.bootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443
        -Dthidwick.tms.base.tmsStateStream.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443
        -Djavax.net.ssl.trustStore=/keystores/truststore.jks -Djavax.net.ssl.keyStore=/keystores/keystore.jks
        -Dhttp.security.headers.enabled=true -Dhttp.rest.security.headers.enabled=true
        -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Xms1536m   -Xmx4096m   -Dmercury.tenant.name.filter="^(evai)"                 -Dtenant.realm=engagement-psr.us1.channels.ocs.oraclecloud.com '
    - name: VAULT_ADDR
      value: https://vault.query.prod.consul:8200
    - name: VAULT_SKIP_VERIFY
      value: "true"
    - name: VAULT_AUTH_METHOD
      value: jwt
    - name: VAULT_PATH
      value: k8s-phx-dataplane
    - name: VAULT_ROLE
      value: mercury-psr
    - name: VAULT_IGNORE_MISSING_SECRETS
      value: "false"
    - name: VAULT_ENV_PASSTHROUGH
    - name: VAULT_JSON_LOG
      value: "false"
    - name: VAULT_CLIENT_TIMEOUT
      value: 10s
    - name: VAULT_LOG_LEVEL
      value: warn
    - name: VAULT_ENV_DAEMON
      value: "true"
    image: iad.ocir.io/osvcstage/mercury/tenant-downtime-monitor:22.01.18-REL21.11-153
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 1
      httpGet:
        path: /health/checks
        port: 8080
        scheme: HTTP
      initialDelaySeconds: 120
      periodSeconds: 30
      successThreshold: 1
      timeoutSeconds: 1
    name: tenant-downtime-monitor
    ports:
    - containerPort: 8080
      name: http
      protocol: TCP
    readinessProbe:
      failureThreshold: 1
      httpGet:
        path: /health/ping
        port: 8080
        scheme: HTTP
      initialDelaySeconds: 120
      periodSeconds: 30
      successThreshold: 1
      timeoutSeconds: 1
    resources:
      limits:
        cpu: "1"
        ephemeral-storage: 10Gi
        memory: 6Gi
      requests:
        cpu: 100m
        ephemeral-storage: 2Gi
        memory: 2Gi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - NET_RAW
    startupProbe:
      failureThreshold: 120
      httpGet:
        path: /health/checks
        port: 8080
        scheme: HTTP
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /keystores
      name: keystore-volume
    - mountPath: /tmp/keystore-pass
      name: keystore-pass
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: mercurygeneric-token-b6n4q
      readOnly: true
    - mountPath: /vault/
      name: vault-env
    - mountPath: /vault/secrets
      name: agent-secrets
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  imagePullSecrets:
  - name: osvcstage-ocirsecret
  initContainers:
  - command:
    - sh
    - -c
    - cp /usr/local/bin/vault-env /vault/
    image: docker-master.cdaas.oraclecloud.com/docker-osvc-local/banzaicloud/vault-env:1.14.3
    imagePullPolicy: IfNotPresent
    name: copy-vault-env
    resources:
      limits:
        cpu: 250m
        memory: 64Mi
      requests:
        cpu: 50m
        memory: 64Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - NET_RAW
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /vault/
      name: vault-env
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: mercurygeneric-token-b6n4q
      readOnly: true
  - args:
    - /bin/bash
    - -c
    - /init/init-script.sh
    command:
    - /vault/vault-env
    env:
    - name: AUTO_REFRESH_CONSUL_TOKEN
      value: vault:cpe_consul/creds/mercury-psr#token
    - name: KAFKA_TEMPLATED_PKI
      value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
        .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
        "ttl": "720h"}'
    - name: KEYSTORE_LOCATION
      value: /keystores
    - name: TRUSTSTORE_NAME
      value: truststore.jks
    - name: KEYSTORE_SECRETPATH
      value: /tmp/keystore-pass
    - name: KEYSTORE_TMPPASSFILENAME
      value: props/kafka.properties
    - name: KEYSTORE_NAME
      value: keystore.jks
    - name: SERVICEACCOUNT_SECRETPATH
      value: /var/run/secrets/kubernetes.io/serviceaccount
    - name: VAULT_ADDR
      value: https://vault.query.prod.consul:8200
    - name: VAULT_SECRETPATH
      value: /tmp/vaultca
    - name: PKI_VAULT_ENGINE_PATH
      value: v1/infra_pki/issue/kafka_client
    - name: CERT_CN
      value: mercury-psr-mercurygeneric-rw
    - name: KAFKA_SECURITY_PROTOCOL
      value: SSL
    - name: SA_ROLE
      value: mercury-psr_mercurygeneric
    - name: VAULT_SERVICEACCOUNT
      value: mercurygeneric
    - name: VAULT_ADDR
      value: https://vault.query.prod.consul:8200
    - name: VAULT_SKIP_VERIFY
      value: "true"
    - name: VAULT_AUTH_METHOD
      value: jwt
    - name: VAULT_PATH
      value: k8s-phx-dataplane
    - name: VAULT_ROLE
      value: mercury-psr
    - name: VAULT_IGNORE_MISSING_SECRETS
      value: "false"
    - name: VAULT_ENV_PASSTHROUGH
    - name: VAULT_JSON_LOG
      value: "false"
    - name: VAULT_CLIENT_TIMEOUT
      value: 10s
    - name: VAULT_LOG_LEVEL
      value: warn
    - name: VAULT_ENV_DAEMON
      value: "true"
    image: iad.ocir.io/osvcstage/mercury/init-container/prod:1.0.0.18
    imagePullPolicy: IfNotPresent
    name: init
    resources:
      limits:
        cpu: "1"
        ephemeral-storage: 10Gi
        memory: 6Gi
      requests:
        cpu: 100m
        ephemeral-storage: 2Gi
        memory: 2Gi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - NET_RAW
      runAsUser: 0
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /keystores
      name: keystore-volume
    - mountPath: /tmp/vaultca
      name: vaultca
      readOnly: true
    - mountPath: /tmp/keystore-pass
      name: keystore-pass
      readOnly: true
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: mercurygeneric-token-b6n4q
      readOnly: true
    - mountPath: /vault/
      name: vault-env
  nodeName: 10.20.1.247
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: mercurygeneric
  serviceAccountName: mercurygeneric
  shareProcessNamespace: true
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: vaultca
    secret:
      defaultMode: 420
      secretName: vault-tls
  - name: keystore-pass
    secret:
      defaultMode: 420
      secretName: mercury-tenant-downtime-monitor-keystore
  - emptyDir: {}
    name: keystore-volume
  - name: mercurygeneric-token-b6n4q
    secret:
      defaultMode: 420
      secretName: mercurygeneric-token-b6n4q
  - emptyDir:
      medium: Memory
    name: vault-env
  - emptyDir:
      medium: Memory
    name: agent-secrets
  - configMap:
      defaultMode: 420
      items:
      - key: config.hcl
        path: config.hcl
      name: mercury-psr-va-configmap
    name: agent-configmap
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2022-01-19T22:54:56Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2022-01-19T22:57:11Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2022-01-19T22:57:11Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2022-01-19T22:53:47Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: docker://5476d518b311bd46ce764a761595933b39b10e6f411b7c3ff995e0924fc0f15e
    image: iad.ocir.io/osvcstage/mercury/tenant-downtime-monitor:22.01.18-REL21.11-153
    imageID: docker-pullable://iad.ocir.io/osvcstage/mercury/tenant-downtime-monitor@sha256:a4cb469391247fe881b3256df54a9832bee2b5fcc9af5bbf354225dee7f4ad26
    lastState: {}
    name: tenant-downtime-monitor
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2022-01-19T22:55:04Z"
  - containerID: docker://db17b8275c9657aff022866c023ce32ae1952e8585f383eb0b9a28526a3fa422
    image: docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5
    imageID: docker-pullable://docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault@sha256:57438e97b72c05368ca06f4d5f34667e965248635718001abdea8fa11f18adad
    lastState: {}
    name: vault-agent
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2022-01-19T22:54:56Z"
  hostIP: 10.20.1.247
  initContainerStatuses:
  - containerID: docker://c3d675576a85d9d0e5807e7d38bb53db764c31f1f1811b56a55cebdaa3840b00
    image: docker-master.cdaas.oraclecloud.com/docker-osvc-local/banzaicloud/vault-env:1.14.3
    imageID: docker-pullable://docker-master.cdaas.oraclecloud.com/docker-osvc-local/banzaicloud/vault-env@sha256:7435d1c641c0aa0fd4faaff32ab189b29d3d909b100d36d4d90df3b79cee8ce3
    lastState: {}
    name: copy-vault-env
    ready: true
    restartCount: 0
    state:
      terminated:
        containerID: docker://c3d675576a85d9d0e5807e7d38bb53db764c31f1f1811b56a55cebdaa3840b00
        exitCode: 0
        finishedAt: "2022-01-19T22:53:48Z"
        reason: Completed
        startedAt: "2022-01-19T22:53:48Z"
  - containerID: docker://81d2d731e1862739ac1b5e74e0e4b749920712ee31a5f9a20ee796901cf90e31
    image: iad.ocir.io/osvcstage/mercury/init-container/prod:1.0.0.18
    imageID: docker-pullable://iad.ocir.io/osvcstage/mercury/init-container/prod@sha256:59dbfb749d6cd87a0c86711bcadf5f2328a2a611c1db4cbb31f117fe60e5d299
    lastState: {}
    name: init
    ready: true
    restartCount: 1
    state:
      terminated:
        containerID: docker://81d2d731e1862739ac1b5e74e0e4b749920712ee31a5f9a20ee796901cf90e31
        exitCode: 0
        finishedAt: "2022-01-19T22:54:56Z"
        reason: Completed
        startedAt: "2022-01-19T22:54:34Z"
  phase: Running
  podIP: 10.245.0.48
  podIPs:
  - ip: 10.245.0.48
  qosClass: Burstable
  startTime: "2022-01-19T22:53:47Z"
[21:January:2022:05:36:49]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[21:January:2022:05:37:01]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[21:January:2022:05:37:28]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn mercury-psr mercury-enrichment-service -oyaml | grep -i priority
[21:January:2022:05:39:32]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn mercury-psr mercury-enrichment-service -oyaml | grep -i web
[21:January:2022:05:39:43]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn mercury-psr mercury-enrichment-service -oyaml | grep -i javaopts
[21:January:2022:05:39:51]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[21:January:2022:05:39:59]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[21:January:2022:06:55:11]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgc
CURRENT   NAME                               CLUSTER               AUTHINFO           NAMESPACE
          prod_ap-hyderabad-1_controlplane   cluster-cbjr52qcfoa   user-cbjr52qcfoa
          prod_ap-hyderabad-1_dataplane      cluster-cpj2zxktnda   user-cpj2zxktnda
          prod_ap-melbourne-1_controlplane   cluster-czgcn3bmjtg   user-czgcn3bmjtg
          prod_ap-melbourne-1_dataplane      cluster-c4dazdggfst   user-c4dazdggfst
          prod_ap-mumbai-1_controlplane      cluster-cjiru3mifia   user-cjiru3mifia
          prod_ap-mumbai-1_dataplane         cluster-ci3ypr6tmga   user-ci3ypr6tmga
          prod_ap-osaka-1_controlplane       cluster-cp5327xtmfa   user-cp5327xtmfa
          prod_ap-osaka-1_dataplane          cluster-cqldck6xatq   user-cqldck6xatq
          prod_ap-sydney-1_controlplane      cluster-c4wembugqyt   user-c4wembugqyt
          prod_ap-sydney-1_dataplane         cluster-cswezbzgyyg   user-cswezbzgyyg
          prod_ap-tokyo-1_controlplane       cluster-cz54pc3z4wa   user-cz54pc3z4wa
          prod_ap-tokyo-1_dataplane          cluster-cyjtbtbbjfq   user-cyjtbtbbjfq
          prod_ca-montreal-1_controlplane    cluster-czdoolemnqw   user-czdoolemnqw
          prod_ca-montreal-1_dataplane       cluster-cqtkytbgq3d   user-cqtkytbgq3d
          prod_ca-toronto-1_controlplane     cluster-cygcmjxhfsd   user-cygcmjxhfsd
          prod_ca-toronto-1_dataplane        cluster-cywmodfmm3d   user-cywmodfmm3d
          prod_eu-amsterdam-1_controlplane   cluster-c4tkmjwgbrt   user-c4tkmjwgbrt
          prod_eu-amsterdam-1_dataplane      cluster-cstsy3bmjrd   user-cstsy3bmjrd
          prod_eu-frankfurt-1_controlplane   cluster-cydcytcg43d   user-cydcytcg43d
          prod_eu-frankfurt-1_dataplane      cluster-crdgzbrmjsd   user-crdgzbrmjsd
          prod_eu-zurich-1_controlplane      cluster-cwmoxn3bfwq   user-cwmoxn3bfwq
          prod_eu-zurich-1_dataplane         cluster-chtoe5bnsza   user-chtoe5bnsza
          prod_me-dubai-1_controlplane       cluster-ce5puidc4ya   user-ce5puidc4ya
          prod_me-dubai-1_dataplane          cluster-c77hcd3vizq   user-c77hcd3vizq
          prod_me-jeddah-1_controlplane      cluster-cpzagsoukeq   user-cpzagsoukeq
          prod_me-jeddah-1_dataplane         cluster-cf4f222mrfa   user-cf4f222mrfa
          prod_sa-santiago-1_controlplane    cluster-czxpimgibeq   user-czxpimgibeq
          prod_sa-santiago-1_dataplane       cluster-cuutxfvj2ea   user-cuutxfvj2ea
          prod_sa-saopaulo-1_controlplane    cluster-cc75n7hujpq   user-cc75n7hujpq
          prod_sa-saopaulo-1_dataplane       cluster-cpjqrjtbotq   user-cpjqrjtbotq
          prod_sa-vinhedo-1_controlplane     cluster-cihx6aprjua   user-cihx6aprjua
          prod_sa-vinhedo-1_dataplane        cluster-ciqmbmn6wta   user-ciqmbmn6wta
          prod_uk-cardiff-1_controlplane     cluster-ctggmtbmfrg   user-ctggmtbmfrg
          prod_uk-cardiff-1_dataplane        cluster-cqwmnrqmztd   user-cqwmnrqmztd
          prod_uk-london-1_controlplane      cluster-cywgnlcmqzd   user-cywgnlcmqzd
          prod_uk-london-1_dataplane         cluster-c3tazjsmjsd   user-c3tazjsmjsd
          prod_us-ashburn-1_controlplane     cluster-czwczjzgrsw   user-czwczjzgrsw
          prod_us-ashburn-1_dataplane        cluster-c3wkyjyguzt   user-c3wkyjyguzt
          prod_us-phoenix-1_controlplane     cluster-c3tkobugrst   user-c3tkobugrst
*         prod_us-phoenix-1_dataplane        cluster-csdqylfmi2g   user-csdqylfmi2g
          prod_us-phoenix-1_deployment       cluster-c4tgolgmjsd   user-c4tgolgmjsd
[21:January:2022:06:55:13]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kuc prod_uk-london-1_dataplane
Switched to context "prod_uk-london-1_dataplane".
[21:January:2022:06:55:17]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ clear
[21:January:2022:06:55:18]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ rp
[21:January:2022:06:55:19]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn helios
NAME                                       READY   STATUS    RESTARTS   AGE   IP             NODE           NOMINATED NODE   READINESS GATES
adapter-84dd45c4d9-586gb                   2/2     Running   1          34d   10.245.11.52   10.20.49.80    <none>           <none>
adapter-84dd45c4d9-dl48s                   2/2     Running   1          34d   10.245.8.196   10.20.48.252   <none>           <none>
adapter-84dd45c4d9-pxptw                   2/2     Running   2          34d   10.245.7.237   10.20.48.86    <none>           <none>
authentication-7db4b55f7c-2dfmj            2/2     Running   2          34d   10.245.11.53   10.20.49.80    <none>           <none>
authentication-7db4b55f7c-bdgj8            2/2     Running   3          34d   10.245.8.194   10.20.48.252   <none>           <none>
authentication-7db4b55f7c-wr98k            2/2     Running   7          34d   10.245.6.208   10.20.49.200   <none>           <none>
data-processor-85dbcc656c-pxtlm            2/2     Running   0          18h   10.245.0.12    10.20.49.192   <none>           <none>
data-provider-647845cd58-k8t6j             2/2     Running   2          34d   10.245.11.56   10.20.49.80    <none>           <none>
dead-letter-processor-6dbff4488d-78zng     2/2     Running   2          34d   10.245.6.20    10.20.48.34    <none>           <none>
dead-letter-processor-6dbff4488d-7mwf9     2/2     Running   1          34d   10.245.8.199   10.20.48.252   <none>           <none>
dead-letter-processor-6dbff4488d-nl5nb     2/2     Running   2          34d   10.245.11.59   10.20.49.80    <none>           <none>
event-scheduler-79cdc7549f-9wwc8           2/2     Running   2          34d   10.245.11.58   10.20.49.80    <none>           <none>
event-scheduler-79cdc7549f-h6c8q           2/2     Running   1          34d   10.245.8.195   10.20.48.252   <none>           <none>
event-scheduler-79cdc7549f-kkfrz           2/2     Running   2          34d   10.245.6.209   10.20.49.200   <none>           <none>
external-event-processor-686488b49-f24rr   2/2     Running   2          34d   10.245.7.235   10.20.48.86    <none>           <none>
external-event-processor-686488b49-qz5t8   2/2     Running   2          34d   10.245.8.197   10.20.48.252   <none>           <none>
external-event-processor-686488b49-x22lm   2/2     Running   1          34d   10.245.11.57   10.20.49.80    <none>           <none>
lifecycle-86896bb6c6-xhp7p                 2/2     Running   3          34d   10.245.11.54   10.20.49.80    <none>           <none>
sender-7d97857675-c6dtg                    2/2     Running   3          34d   10.245.11.55   10.20.49.80    <none>           <none>
sender-7d97857675-shgv7                    2/2     Running   3          34d   10.245.6.19    10.20.48.34    <none>           <none>
sender-7d97857675-zc7tb                    2/2     Running   2          34d   10.245.8.193   10.20.48.252   <none>           <none>
[21:January:2022:06:55:40]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ hln helios
NAME                      NAMESPACE REVISION  UPDATED                                 STATUS    CHART                           APP VERSION
adapter                   helios    5         2021-12-17 11:06:10.966179139 +0000 UTC deployed  helios-thidwick-21.12.16-1461   21.12.16-1461
authentication            helios    5         2021-12-17 11:06:11.625353478 +0000 UTC deployed  helios-vision-21.12.16-1461     21.12.16-1461
canary                    helios    5         2021-12-17 11:06:10.69394262 +0000 UTC  deployed  helios-vision-21.12.16-1461     21.12.16-1461
dashboard                 helios    5         2021-12-17 11:13:33.445725009 +0000 UTC deployed  helios-dashboard-21.12.16-1461  21.12.16-1461
data-processor            helios    5         2021-12-17 11:08:33.965707686 +0000 UTC deployed  helios-thidwick-21.12.16-1461   21.12.16-1461
data-provider             helios    5         2021-12-17 11:07:46.427807849 +0000 UTC deployed  helios-thidwick-21.12.16-1461   21.12.16-1461
dead-letter-processor     helios    5         2021-12-17 11:08:42.341220113 +0000 UTC deployed  helios-thidwick-21.12.16-1461   21.12.16-1461
event-scheduler           helios    5         2021-12-17 11:10:52.988280926 +0000 UTC deployed  helios-thidwick-21.12.16-1461   21.12.16-1461
external-event-processor  helios    5         2021-12-17 11:06:11.211195814 +0000 UTC deployed  helios-thidwick-21.12.16-1461   21.12.16-1461
lifecycle                 helios    5         2021-12-17 11:06:28.734266372 +0000 UTC deployed  helios-vision-21.12.16-1461     21.12.16-1461
sender                    helios    5         2021-12-17 11:06:11.270122066 +0000 UTC deployed  helios-vision-21.12.16-1461     21.12.16-1461
[21:January:2022:06:56:12]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn helios
NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
adapter                    3/3     3            3           169d
authentication             3/3     3            3           169d
canary                     0/0     0            0           169d
data-processor             1/1     1            1           169d
data-provider              1/1     1            1           169d
dead-letter-processor      3/3     3            3           169d
event-scheduler            3/3     3            3           169d
external-event-processor   3/3     3            3           169d
lifecycle                  1/1     1            1           169d
sender                     3/3     3            3           169d
[21:January:2022:06:57:06]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kln helios data-processor-85dbcc656c-pxtlm > ~/galorndon/ctemp/data-processor-85dbcc656c-pxtlm$dnow
error: a container name must be specified for pod data-processor-85dbcc656c-pxtlm, choose one of: [vault-agent app] or one of the init containers: [copy-vault-env]
[21:January:2022:07:00:48]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kln helios data-processor-85dbcc656c-pxtlm -c app > ~/galorndon/ctemp/data-processor-85dbcc656c-pxtlm_app$dnow
[21:January:2022:07:01:07]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ date
Fri Jan 21 07:02:08 UTC 2022
[21:January:2022:07:02:08]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn helios canary -oyml
^C
[21:January:2022:07:03:08]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn helios canary -oyaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "5"
    meta.helm.sh/release-name: canary
    meta.helm.sh/release-namespace: helios
  creationTimestamp: "2021-08-04T11:25:43Z"
  generation: 6
  labels:
    app.kubernetes.io/instance: canary
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: helios
    helm.sh/chart: helios-vision-21.12.16-1461
    monitoring: "true"
    scrapeLimit: "40000"
  name: canary
  namespace: helios
  resourceVersion: "194924142"
  selfLink: /apis/apps/v1/namespaces/helios/deployments/canary
  uid: bdbe3ec3-6f92-4d59-ad1e-023d49b17b69
spec:
  progressDeadlineSeconds: 600
  replicas: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/instance: canary
      app.kubernetes.io/name: helios
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      annotations:
        vault.security.banzaicloud.io/log-level: warn
        vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
        vault.security.banzaicloud.io/vault-agent-configmap: helios-va-configmap
        vault.security.banzaicloud.io/vault-env-daemon: "true"
        vault.security.banzaicloud.io/vault-path: k8s-lhr-dataplane
        vault.security.banzaicloud.io/vault-role: helios
        vault.security.banzaicloud.io/vault-skip-verify: "true"
      creationTimestamp: null
      labels:
        app.kubernetes.io/instance: canary
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: helios
        helm.sh/chart: helios-vision-21.12.16-1461
    spec:
      containers:
      - env:
        - name: PROFILE
          value: prod
        - name: DATACENTER
          value: helios
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: HOST_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.hostIP
        - name: POD_SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.serviceAccountName
        - name: HELIOS_PROVISIONING_APPLICATION_NAME
          value: helios
        - name: VAULT_ENDPOINT
          value: https://vault.query.prod.consul:8200
        - name: VAULT_TOKEN
          value: vault:login
        - name: VAULT_TOKEN_PATH
          value: /vault/secrets/.vault-token
        - name: KEYSTORE_LOCATION
          value: /vault/secrets
        - name: TRUSTSTORE_NAME
          value: truststore.jks
        - name: KEYSTORE_SECRETPATH
          value: /tmp/keystore-pass
        - name: KEYSTORE_TMPPASSFILENAME
          value: props/helios-sharedsvc.properties
        - name: KEYSTORE_NAME
          value: keystore.jks
        - name: SERVICEACCOUNT_SECRETPATH
          value: /var/run/secrets/kubernetes.io/serviceaccount
        - name: KAFKA_SECURITY_PROTOCOL
          value: SSL
        - name: KAFKA_SECURITY_PROTOCOL
          value: SSL
        - name: KAFKA_SERVER
          value: shared-kafka-bootstrap-kafka.service.lhr-dataplane.prod.consul:443
        - name: KAFKA_SERVER
          value: shared-kafka-bootstrap-kafka.service.lhr-dataplane.prod.consul:443
        - name: SCHEMA_REGISTRY_URL
          value: https://schemaregistry-proxy-kafka.service.lhr-dataplane.prod.consul
        - name: SCHEMA_REGISTRY_URL
          value: https://schemaregistry-proxy-kafka.service.lhr-dataplane.prod.consul
        - name: SA_ROLE
          value: helios_heliosgeneric
        - name: CONSUL_HTTP_ADDR
          value: https://$(HOST_IP):8501
        - name: CONSUL_TOKEN
          value: vault:/cpe_consul/creds/helios#token
        - name: CONSUL_GLOBAL_SCOPE
          value: global/helios
        - name: CONSUL_LOCAL_SCOPE
          value: local/helios
        - name: POD_REGION
          value: lhr
        - name: CPE_VERSION
          value: "2"
        - name: VAULT_SECRETPATH
          value: /tmp/vaultca
        - name: VAULT_ADDR
          value: https://vault.query.prod.consul:8200
        - name: VAULT_AUTHTYPE
          value: tokenPath
        - name: CERT_CN
          value: helios-heliosgeneric-rw
        - name: PKI_VAULT_ENGINE_PATH
          value: v1/infra_pki/issue/kafka_client
        - name: NODE_TLS_REJECT_UNAUTHORIZED
          value: "0"
        - name: VAULT_TOKENPATH
          value: /vault/secrets/.vault-token
        - name: VAULT_AUTHENTICATION
          value: tokenPath
        - name: INGRESS_URL
          value: helioscanary.channels.uk-london-1.ocs.oraclecloud.com
        image: docker-master.cdaas.oraclecloud.com/docker-osvc-local/com.oracle.helios/canary-service:1.21.12.16-b0238
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 12
          httpGet:
            path: /health/ping
            port: http
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: app
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /health/checks
            port: http
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          limits:
            cpu: 200m
            memory: 2Gi
          requests:
            cpu: 100m
            memory: 1Gi
        startupProbe:
          failureThreshold: 60
          httpGet:
            path: /health/ping
            port: http
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /tmp/keystore-pass
          name: keystore-pass
          readOnly: true
        - mountPath: /tmp/vaultca
          name: vaultca
          readOnly: true
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: heliosgeneric
      serviceAccountName: heliosgeneric
      terminationGracePeriodSeconds: 30
      volumes:
      - name: vaultca
        secret:
          defaultMode: 420
          secretName: vault-tls
      - name: keystore-pass
        secret:
          defaultMode: 420
          secretName: canary-keystore
      - emptyDir: {}
        name: keystore-volume
status:
  conditions:
  - lastTransitionTime: "2021-11-27T20:46:20Z"
    lastUpdateTime: "2021-11-27T20:46:20Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2021-08-04T11:25:43Z"
    lastUpdateTime: "2021-12-17T11:06:16Z"
    message: ReplicaSet "canary-7744d78d64" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 6
[21:January:2022:07:03:13]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[21:January:2022:07:03:46]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn helios
NAME                                       READY   STATUS    RESTARTS   AGE   IP             NODE           NOMINATED NODE   READINESS GATES
adapter-84dd45c4d9-586gb                   2/2     Running   1          34d   10.245.11.52   10.20.49.80    <none>           <none>
adapter-84dd45c4d9-dl48s                   2/2     Running   1          34d   10.245.8.196   10.20.48.252   <none>           <none>
adapter-84dd45c4d9-pxptw                   2/2     Running   2          34d   10.245.7.237   10.20.48.86    <none>           <none>
authentication-7db4b55f7c-2dfmj            2/2     Running   2          34d   10.245.11.53   10.20.49.80    <none>           <none>
authentication-7db4b55f7c-bdgj8            2/2     Running   3          34d   10.245.8.194   10.20.48.252   <none>           <none>
authentication-7db4b55f7c-wr98k            2/2     Running   7          34d   10.245.6.208   10.20.49.200   <none>           <none>
data-processor-85dbcc656c-pxtlm            2/2     Running   0          18h   10.245.0.12    10.20.49.192   <none>           <none>
data-provider-647845cd58-k8t6j             2/2     Running   2          34d   10.245.11.56   10.20.49.80    <none>           <none>
dead-letter-processor-6dbff4488d-78zng     2/2     Running   2          34d   10.245.6.20    10.20.48.34    <none>           <none>
dead-letter-processor-6dbff4488d-7mwf9     2/2     Running   1          34d   10.245.8.199   10.20.48.252   <none>           <none>
dead-letter-processor-6dbff4488d-nl5nb     2/2     Running   2          34d   10.245.11.59   10.20.49.80    <none>           <none>
event-scheduler-79cdc7549f-9wwc8           2/2     Running   2          34d   10.245.11.58   10.20.49.80    <none>           <none>
event-scheduler-79cdc7549f-h6c8q           2/2     Running   1          34d   10.245.8.195   10.20.48.252   <none>           <none>
event-scheduler-79cdc7549f-kkfrz           2/2     Running   2          34d   10.245.6.209   10.20.49.200   <none>           <none>
external-event-processor-686488b49-f24rr   2/2     Running   2          34d   10.245.7.235   10.20.48.86    <none>           <none>
external-event-processor-686488b49-qz5t8   2/2     Running   2          34d   10.245.8.197   10.20.48.252   <none>           <none>
external-event-processor-686488b49-x22lm   2/2     Running   1          34d   10.245.11.57   10.20.49.80    <none>           <none>
lifecycle-86896bb6c6-xhp7p                 2/2     Running   3          34d   10.245.11.54   10.20.49.80    <none>           <none>
sender-7d97857675-c6dtg                    2/2     Running   3          34d   10.245.11.55   10.20.49.80    <none>           <none>
sender-7d97857675-shgv7                    2/2     Running   3          34d   10.245.6.19    10.20.48.34    <none>           <none>
sender-7d97857675-zc7tb                    2/2     Running   2          34d   10.245.8.193   10.20.48.252   <none>           <none>
[21:January:2022:07:04:32]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgssn helios
No resources found in helios namespace.
[21:January:2022:07:04:41]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ k deployment canary --replicas=1 -n helios
Error: unknown command "deployment" for "kubectl"
Run 'kubectl --help' for usage.
[21:January:2022:07:05:07]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ k deployments canary --replicas=1 -n helios
Error: unknown command "deployments" for "kubectl"
Run 'kubectl --help' for usage.
[21:January:2022:07:05:23]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ k scale deployments canary --replicas=1 -n helios
deployment.apps/canary scaled
[21:January:2022:07:06:03]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn helios
NAME                                       READY   STATUS            RESTARTS   AGE   IP             NODE           NOMINATED NODE   READINESS GATES
adapter-84dd45c4d9-586gb                   2/2     Running           1          34d   10.245.11.52   10.20.49.80    <none>           <none>
adapter-84dd45c4d9-dl48s                   2/2     Running           1          34d   10.245.8.196   10.20.48.252   <none>           <none>
adapter-84dd45c4d9-pxptw                   2/2     Running           2          34d   10.245.7.237   10.20.48.86    <none>           <none>
authentication-7db4b55f7c-2dfmj            2/2     Running           2          34d   10.245.11.53   10.20.49.80    <none>           <none>
authentication-7db4b55f7c-bdgj8            2/2     Running           3          34d   10.245.8.194   10.20.48.252   <none>           <none>
authentication-7db4b55f7c-wr98k            2/2     Running           7          34d   10.245.6.208   10.20.49.200   <none>           <none>
canary-7744d78d64-r7zn8                    0/2     PodInitializing   0          10s   10.245.0.13    10.20.49.192   <none>           <none>
data-processor-85dbcc656c-pxtlm            2/2     Running           0          18h   10.245.0.12    10.20.49.192   <none>           <none>
data-provider-647845cd58-k8t6j             2/2     Running           2          34d   10.245.11.56   10.20.49.80    <none>           <none>
dead-letter-processor-6dbff4488d-78zng     2/2     Running           2          34d   10.245.6.20    10.20.48.34    <none>           <none>
dead-letter-processor-6dbff4488d-7mwf9     2/2     Running           1          34d   10.245.8.199   10.20.48.252   <none>           <none>
dead-letter-processor-6dbff4488d-nl5nb     2/2     Running           2          34d   10.245.11.59   10.20.49.80    <none>           <none>
event-scheduler-79cdc7549f-9wwc8           2/2     Running           2          34d   10.245.11.58   10.20.49.80    <none>           <none>
event-scheduler-79cdc7549f-h6c8q           2/2     Running           1          34d   10.245.8.195   10.20.48.252   <none>           <none>
event-scheduler-79cdc7549f-kkfrz           2/2     Running           2          34d   10.245.6.209   10.20.49.200   <none>           <none>
external-event-processor-686488b49-f24rr   2/2     Running           2          34d   10.245.7.235   10.20.48.86    <none>           <none>
external-event-processor-686488b49-qz5t8   2/2     Running           2          34d   10.245.8.197   10.20.48.252   <none>           <none>
external-event-processor-686488b49-x22lm   2/2     Running           1          34d   10.245.11.57   10.20.49.80    <none>           <none>
lifecycle-86896bb6c6-xhp7p                 2/2     Running           3          34d   10.245.11.54   10.20.49.80    <none>           <none>
sender-7d97857675-c6dtg                    2/2     Running           3          34d   10.245.11.55   10.20.49.80    <none>           <none>
sender-7d97857675-shgv7                    2/2     Running           3          34d   10.245.6.19    10.20.48.34    <none>           <none>
sender-7d97857675-zc7tb                    2/2     Running           2          34d   10.245.8.193   10.20.48.252   <none>           <none>
[21:January:2022:07:06:16]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn helios
NAME                                       READY   STATUS    RESTARTS   AGE   IP             NODE           NOMINATED NODE   READINESS GATES
adapter-84dd45c4d9-586gb                   2/2     Running   1          34d   10.245.11.52   10.20.49.80    <none>           <none>
adapter-84dd45c4d9-dl48s                   2/2     Running   1          34d   10.245.8.196   10.20.48.252   <none>           <none>
adapter-84dd45c4d9-pxptw                   2/2     Running   2          34d   10.245.7.237   10.20.48.86    <none>           <none>
authentication-7db4b55f7c-2dfmj            2/2     Running   2          34d   10.245.11.53   10.20.49.80    <none>           <none>
authentication-7db4b55f7c-bdgj8            2/2     Running   3          34d   10.245.8.194   10.20.48.252   <none>           <none>
authentication-7db4b55f7c-wr98k            2/2     Running   7          34d   10.245.6.208   10.20.49.200   <none>           <none>
canary-7744d78d64-r7zn8                    2/2     Running   0          81s   10.245.0.13    10.20.49.192   <none>           <none>
data-processor-85dbcc656c-pxtlm            2/2     Running   0          18h   10.245.0.12    10.20.49.192   <none>           <none>
data-provider-647845cd58-k8t6j             2/2     Running   2          34d   10.245.11.56   10.20.49.80    <none>           <none>
dead-letter-processor-6dbff4488d-78zng     2/2     Running   2          34d   10.245.6.20    10.20.48.34    <none>           <none>
dead-letter-processor-6dbff4488d-7mwf9     2/2     Running   1          34d   10.245.8.199   10.20.48.252   <none>           <none>
dead-letter-processor-6dbff4488d-nl5nb     2/2     Running   2          34d   10.245.11.59   10.20.49.80    <none>           <none>
event-scheduler-79cdc7549f-9wwc8           2/2     Running   2          34d   10.245.11.58   10.20.49.80    <none>           <none>
event-scheduler-79cdc7549f-h6c8q           2/2     Running   1          34d   10.245.8.195   10.20.48.252   <none>           <none>
event-scheduler-79cdc7549f-kkfrz           2/2     Running   2          34d   10.245.6.209   10.20.49.200   <none>           <none>
external-event-processor-686488b49-f24rr   2/2     Running   2          34d   10.245.7.235   10.20.48.86    <none>           <none>
external-event-processor-686488b49-qz5t8   2/2     Running   2          34d   10.245.8.197   10.20.48.252   <none>           <none>
external-event-processor-686488b49-x22lm   2/2     Running   1          34d   10.245.11.57   10.20.49.80    <none>           <none>
lifecycle-86896bb6c6-xhp7p                 2/2     Running   3          34d   10.245.11.54   10.20.49.80    <none>           <none>
sender-7d97857675-c6dtg                    2/2     Running   3          34d   10.245.11.55   10.20.49.80    <none>           <none>
sender-7d97857675-shgv7                    2/2     Running   3          34d   10.245.6.19    10.20.48.34    <none>           <none>
sender-7d97857675-zc7tb                    2/2     Running   2          34d   10.245.8.193   10.20.48.252   <none>           <none>
[21:January:2022:07:07:27]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn helios  | grep canary
canary-7744d78d64-r7zn8                    2/2     Running   0          2m42s   10.245.0.13    10.20.49.192   <none>           <none>
[21:January:2022:07:08:48]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgc
CURRENT   NAME                               CLUSTER               AUTHINFO           NAMESPACE
          prod_ap-hyderabad-1_controlplane   cluster-cbjr52qcfoa   user-cbjr52qcfoa
          prod_ap-hyderabad-1_dataplane      cluster-cpj2zxktnda   user-cpj2zxktnda
          prod_ap-melbourne-1_controlplane   cluster-czgcn3bmjtg   user-czgcn3bmjtg
          prod_ap-melbourne-1_dataplane      cluster-c4dazdggfst   user-c4dazdggfst
          prod_ap-mumbai-1_controlplane      cluster-cjiru3mifia   user-cjiru3mifia
          prod_ap-mumbai-1_dataplane         cluster-ci3ypr6tmga   user-ci3ypr6tmga
          prod_ap-osaka-1_controlplane       cluster-cp5327xtmfa   user-cp5327xtmfa
          prod_ap-osaka-1_dataplane          cluster-cqldck6xatq   user-cqldck6xatq
          prod_ap-sydney-1_controlplane      cluster-c4wembugqyt   user-c4wembugqyt
          prod_ap-sydney-1_dataplane         cluster-cswezbzgyyg   user-cswezbzgyyg
          prod_ap-tokyo-1_controlplane       cluster-cz54pc3z4wa   user-cz54pc3z4wa
          prod_ap-tokyo-1_dataplane          cluster-cyjtbtbbjfq   user-cyjtbtbbjfq
          prod_ca-montreal-1_controlplane    cluster-czdoolemnqw   user-czdoolemnqw
          prod_ca-montreal-1_dataplane       cluster-cqtkytbgq3d   user-cqtkytbgq3d
          prod_ca-toronto-1_controlplane     cluster-cygcmjxhfsd   user-cygcmjxhfsd
          prod_ca-toronto-1_dataplane        cluster-cywmodfmm3d   user-cywmodfmm3d
          prod_eu-amsterdam-1_controlplane   cluster-c4tkmjwgbrt   user-c4tkmjwgbrt
          prod_eu-amsterdam-1_dataplane      cluster-cstsy3bmjrd   user-cstsy3bmjrd
          prod_eu-frankfurt-1_controlplane   cluster-cydcytcg43d   user-cydcytcg43d
          prod_eu-frankfurt-1_dataplane      cluster-crdgzbrmjsd   user-crdgzbrmjsd
          prod_eu-zurich-1_controlplane      cluster-cwmoxn3bfwq   user-cwmoxn3bfwq
          prod_eu-zurich-1_dataplane         cluster-chtoe5bnsza   user-chtoe5bnsza
          prod_me-dubai-1_controlplane       cluster-ce5puidc4ya   user-ce5puidc4ya
          prod_me-dubai-1_dataplane          cluster-c77hcd3vizq   user-c77hcd3vizq
          prod_me-jeddah-1_controlplane      cluster-cpzagsoukeq   user-cpzagsoukeq
          prod_me-jeddah-1_dataplane         cluster-cf4f222mrfa   user-cf4f222mrfa
          prod_sa-santiago-1_controlplane    cluster-czxpimgibeq   user-czxpimgibeq
          prod_sa-santiago-1_dataplane       cluster-cuutxfvj2ea   user-cuutxfvj2ea
          prod_sa-saopaulo-1_controlplane    cluster-cc75n7hujpq   user-cc75n7hujpq
          prod_sa-saopaulo-1_dataplane       cluster-cpjqrjtbotq   user-cpjqrjtbotq
          prod_sa-vinhedo-1_controlplane     cluster-cihx6aprjua   user-cihx6aprjua
          prod_sa-vinhedo-1_dataplane        cluster-ciqmbmn6wta   user-ciqmbmn6wta
          prod_uk-cardiff-1_controlplane     cluster-ctggmtbmfrg   user-ctggmtbmfrg
          prod_uk-cardiff-1_dataplane        cluster-cqwmnrqmztd   user-cqwmnrqmztd
          prod_uk-london-1_controlplane      cluster-cywgnlcmqzd   user-cywgnlcmqzd
*         prod_uk-london-1_dataplane         cluster-c3tazjsmjsd   user-c3tazjsmjsd
          prod_us-ashburn-1_controlplane     cluster-czwczjzgrsw   user-czwczjzgrsw
          prod_us-ashburn-1_dataplane        cluster-c3wkyjyguzt   user-c3wkyjyguzt
          prod_us-phoenix-1_controlplane     cluster-c3tkobugrst   user-c3tkobugrst
          prod_us-phoenix-1_dataplane        cluster-csdqylfmi2g   user-csdqylfmi2g
          prod_us-phoenix-1_deployment       cluster-c4tgolgmjsd   user-c4tgolgmjsd
[21:January:2022:07:10:03]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kuc prod_us-ashburn-1_dataplane
Switched to context "prod_us-ashburn-1_dataplane".
[21:January:2022:07:10:08]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ rp
[21:January:2022:07:10:09]:(prod_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn mercury-psr

NAME                                               READY   UP-TO-DATE   AVAILABLE   AGE
mercury-agent-command-service                      3/3     3            3           99d
mercury-channel-api                                1/1     1            1           55d
mercury-consumer-command-service                   3/3     3            3           99d
mercury-custom-availability-service                3/3     3            3           99d
mercury-data-mask-api                              3/3     3            3           99d
mercury-engagement-queue-api                       3/3     3            3           99d
mercury-enrichment-service                         3/3     3            3           99d
mercury-event-sync-service                         3/3     3            3           99d
mercury-integration-in-processor                   3/3     3            3           99d
mercury-integration-out-processor                  3/3     3            3           99d
mercury-kweet-facebook-client                      3/3     3            3           99d
mercury-kweet-facebook-webhook                     3/3     3            3           99d
mercury-kweet-twiliosms-client                     3/3     3            3           99d
mercury-kweet-userprofiles                         3/3     3            3           99d
mercury-kweet-wechat-client                        0/0     0            0           99d
mercury-kweet-wechat-webhook                       0/0     0            0           99d
mercury-mercury-ui                                 3/3     3            3           99d
mercury-metric-aggregation-processor               3/3     3            3           99d
mercury-metric-fusion-bridge                       3/3     3            3           99d
mercury-metric-generation-processor                3/3     3            3           99d
mercury-metric-internal-translation-processor      3/3     3            3           99d
mercury-metric-proxy-service                       3/3     3            3           99d
mercury-omnichannel-assignment-processor           3/3     3            3           99d
mercury-omnichannel-offer-processor                3/3     3            3           99d
mercury-osvc-bridge-api-services                   3/3     3            3           99d
mercury-osvc-bridge-metrics-data-pipeline          3/3     3            3           99d
mercury-osvc-bridge-osvc-data-extractor            3/3     3            3           99d
mercury-osvc-bridge-provisioning-processor         1/1     1            1           99d
mercury-osvc-bridge-state-processor                3/3     3            3           99d
mercury-osvc-bridge-state-query-service            3/3     3            3           99d
mercury-osvc-bridge-task-controller                3/3     3            3           99d
mercury-provisioning-monitor                       3/3     3            3           99d
mercury-provisioning-processor                     3/3     3            3           99d
mercury-psr-kafka-entity-operator                  1/1     1            1           99d
mercury-psr-kafka-kafka-exporter                   1/1     1            1           99d
mercury-queue-agent-info-processor                 3/3     3            3           99d
mercury-realtime-channel-processor                 3/3     3            3           99d
mercury-resource-channel-processor                 3/3     3            3           99d
mercury-resource-state-processor                   3/3     3            3           99d
mercury-resource-work-processor                    3/3     3            3           99d
mercury-routing-processor-agent-assignment         3/3     3            3           99d
mercury-routing-processor-agent-events-processor   3/3     3            3           99d
mercury-routing-processor-queue-assignment         3/3     3            3           99d
mercury-routing-processor-work-events-processor    3/3     3            3           99d
mercury-session-housekeeping-processor             3/3     3            3           99d
mercury-session-processor                          3/3     3            3           99d
mercury-single-sign-on-service                     3/3     3            3           99d
mercury-social-bridge                              3/3     3            3           99d
mercury-social-config                              3/3     3            3           99d
mercury-static-assets-service                      0/3     1            0           99d
mercury-tenant-downtime-monitor                    3/3     3            3           99d
mercury-transcript-api                             3/3     3            3           99d
mercury-transcript-processor                       3/3     3            3           99d
mercury-user-preference-service                    3/3     3            3           99d
mercury-work-api                                   3/3     3            3           99d
mercury-work-processor                             3/3     3            3           99d
[21:January:2022:07:10:23]:(prod_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[21:January:2022:07:10:23]:(prod_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $
[21:January:2022:07:10:31]:(prod_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn mercury-psr mercury-enrichment-service -oyaml | grep -i priority
[21:January:2022:07:10:40]:(prod_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn mercury-psr mercury-enrichment-service -oyaml | grep -i DwebHook
[21:January:2022:07:11:19]:(prod_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn mercury-psr -oyaml | grep -i webHook
      app: kweet-facebook-webhook
      app.kubernetes.io/name: kweet-facebook-webhook
    name: mercury-kweet-facebook-webhook
    selfLink: /apis/apps/v1/namespaces/mercury-psr/deployments/mercury-kweet-facebook-webhook
        app: kweet-facebook-webhook
        app.kubernetes.io/name: kweet-facebook-webhook
          app: kweet-facebook-webhook
          app.kubernetes.io/name: kweet-facebook-webhook
                    - kweet-facebook-webhook
              -Dclient.endpoint=http://mercury-kweet-facebook-client:8080 -Dkweet/facebook/webhook/channel.url=https://graph.facebook.com/v11.0/
          image: iad.ocir.io/osvcstage/mercury/kweet-facebook-webhook:21.11.10-TRUNK-99
          name: mercury-kweet-facebook-webhook
          image: iad.ocir.io/osvcstage/mercury/kweet-facebook-webhook:21.11.10-TRUNK-99
            secretName: mercury-kweet-facebook-webhook-keystore
      message: ReplicaSet "mercury-kweet-facebook-webhook-5f55d8669f" has successfully
      app: kweet-wechat-webhook
      app.kubernetes.io/name: kweet-wechat-webhook
    name: mercury-kweet-wechat-webhook
    selfLink: /apis/apps/v1/namespaces/mercury-psr/deployments/mercury-kweet-wechat-webhook
        app: kweet-wechat-webhook
        app.kubernetes.io/name: kweet-wechat-webhook
          app: kweet-wechat-webhook
          app.kubernetes.io/name: kweet-wechat-webhook
                    - kweet-wechat-webhook
              -Dkweet/wechat/webhook/channel.url=https://api.weixin.qq.com -Ddownload.media.expiry.seconds=86400
          image: iad.ocir.io/osvcstage/mercury/kweet-wechat-webhook:21.11.11-TRUNK-57
          name: mercury-kweet-wechat-webhook
          image: iad.ocir.io/osvcstage/mercury/kweet-wechat-webhook:21.11.11-TRUNK-57
            secretName: mercury-kweet-wechat-webhook-keystore
      message: ReplicaSet "mercury-kweet-wechat-webhook-76645cfcd6" has successfully
      message: 'Internal error occurred: failed calling webhook "pods.vault-secrets-webhook.admission.banzaicloud.com":
[21:January:2022:07:11:33]:(prod_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ grep -ir Dwebhook .
[21:January:2022:07:16:32]:(prod_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kuc prod_uk-london-1_dataplane
Switched to context "prod_uk-london-1_dataplane".
[21:January:2022:07:16:53]:(prod_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ rp
[21:January:2022:07:16:54]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgpn helios  | grep canary
canary-7744d78d64-r7zn8                    2/2     Running   0          11m   10.245.0.13    10.20.49.192   <none>           <none>
[21:January:2022:07:17:31]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn helios
NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
adapter                    3/3     3            3           169d
authentication             3/3     3            3           169d
canary                     1/1     1            1           169d
data-processor             1/1     1            1           169d
data-provider              1/1     1            1           169d
dead-letter-processor      3/3     3            3           169d
event-scheduler            3/3     3            3           169d
external-event-processor   3/3     3            3           169d
lifecycle                  1/1     1            1           169d
sender                     3/3     3            3           169d
[21:January:2022:07:17:38]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ #k scale deployments canary --replicas=1 -n helios
[21:January:2022:07:17:44]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kubectl scale deployments canary --replicas=0 -n helios
deployment.apps/canary scaled
[21:January:2022:07:18:31]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn helios
NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
adapter                    3/3     3            3           169d
authentication             3/3     3            3           169d
canary                     0/0     0            0           169d
data-processor             1/1     1            1           169d
data-provider              1/1     1            1           169d
dead-letter-processor      3/3     3            3           169d
event-scheduler            3/3     3            3           169d
external-event-processor   3/3     3            3           169d
lifecycle                  1/1     1            1           169d
sender                     3/3     3            3           169d
[21:January:2022:07:18:38]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn helios | grep canary
canary                     0/0     0            0           169d
[21:January:2022:07:19:23]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kuc prod_us-ashburn-1_dataplane
Switched to context "prod_us-ashburn-1_dataplane".
[21:January:2022:07:26:41]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform
○ (main) $ rp
[21:January:2022:07:26:43]:(prod_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgdpn mercury-psr
NAME                                               READY   UP-TO-DATE   AVAILABLE   AGE
mercury-agent-command-service                      3/3     3            3           99d
mercury-channel-api                                1/1     1            1           55d
mercury-consumer-command-service                   3/3     3            3           99d
mercury-custom-availability-service                3/3     3            3           99d
mercury-data-mask-api                              3/3     3            3           99d
mercury-engagement-queue-api                       3/3     3            3           99d
mercury-enrichment-service                         3/3     3            3           99d
mercury-event-sync-service                         3/3     3            3           99d
mercury-integration-in-processor                   3/3     3            3           99d
mercury-integration-out-processor                  3/3     3            3           99d
mercury-kweet-facebook-client                      3/3     3            3           99d
mercury-kweet-facebook-webhook                     3/3     3            3           99d
mercury-kweet-twiliosms-client                     3/3     3            3           99d
mercury-kweet-userprofiles                         3/3     3            3           99d
mercury-kweet-wechat-client                        0/0     0            0           99d
mercury-kweet-wechat-webhook                       0/0     0            0           99d
mercury-mercury-ui                                 3/3     3            3           99d
mercury-metric-aggregation-processor               3/3     3            3           99d
mercury-metric-fusion-bridge                       3/3     3            3           99d
mercury-metric-generation-processor                3/3     3            3           99d
mercury-metric-internal-translation-processor      3/3     3            3           99d
mercury-metric-proxy-service                       3/3     3            3           99d
mercury-omnichannel-assignment-processor           3/3     3            3           99d
mercury-omnichannel-offer-processor                3/3     3            3           99d
mercury-osvc-bridge-api-services                   3/3     3            3           99d
mercury-osvc-bridge-metrics-data-pipeline          3/3     3            3           99d
mercury-osvc-bridge-osvc-data-extractor            3/3     3            3           99d
mercury-osvc-bridge-provisioning-processor         1/1     1            1           99d
mercury-osvc-bridge-state-processor                3/3     3            3           99d
mercury-osvc-bridge-state-query-service            3/3     3            3           99d
mercury-osvc-bridge-task-controller                3/3     3            3           99d
mercury-provisioning-monitor                       3/3     3            3           99d
mercury-provisioning-processor                     3/3     3            3           99d
mercury-psr-kafka-entity-operator                  1/1     1            1           99d
mercury-psr-kafka-kafka-exporter                   1/1     1            1           99d
mercury-queue-agent-info-processor                 3/3     3            3           99d
mercury-realtime-channel-processor                 3/3     3            3           99d
mercury-resource-channel-processor                 3/3     3            3           99d
mercury-resource-state-processor                   3/3     3            3           99d
mercury-resource-work-processor                    3/3     3            3           99d
mercury-routing-processor-agent-assignment         3/3     3            3           99d
mercury-routing-processor-agent-events-processor   3/3     3            3           99d
mercury-routing-processor-queue-assignment         3/3     3            3           99d
mercury-routing-processor-work-events-processor    3/3     3            3           99d
mercury-session-housekeeping-processor             3/3     3            3           99d
mercury-session-processor                          3/3     3            3           99d
mercury-single-sign-on-service                     3/3     3            3           99d
mercury-social-bridge                              3/3     3            3           99d
mercury-social-config                              3/3     3            3           99d
mercury-static-assets-service                      0/3     1            0           99d
mercury-tenant-downtime-monitor                    3/3     3            3           99d
mercury-transcript-api                             3/3     3            3           99d
mercury-transcript-processor                       3/3     3            3           99d
mercury-user-preference-service                    3/3     3            3           99d
mercury-work-api                                   3/3     3            3           99d
mercury-work-processor                             3/3     3            3           99d
[21:January:2022:07:26:57]:(prod_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ hln mercury-psr
NAME      NAMESPACE   REVISION  UPDATED                                 STATUS    CHART                 APP VERSION
app-kafka mercury-psr 3         2021-12-03 07:42:51.180112574 +0000 UTC deployed  cpe-kafka-0.1.0       0.1.0
appdata   mercury-psr 3         2021-12-03 07:43:01.386737085 +0000 UTC deployed  kafka-data-0.1.0
mercury   mercury-psr 11        2021-12-17 06:25:07.361328384 +0000 UTC failed    mercury-2112.16.2013  21.12.16-REL21.11-2013
[21:January:2022:07:30:23]:(prod_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kgc
CURRENT   NAME                               CLUSTER               AUTHINFO           NAMESPACE
          prod_ap-hyderabad-1_controlplane   cluster-cbjr52qcfoa   user-cbjr52qcfoa
          prod_ap-hyderabad-1_dataplane      cluster-cpj2zxktnda   user-cpj2zxktnda
          prod_ap-melbourne-1_controlplane   cluster-czgcn3bmjtg   user-czgcn3bmjtg
          prod_ap-melbourne-1_dataplane      cluster-c4dazdggfst   user-c4dazdggfst
          prod_ap-mumbai-1_controlplane      cluster-cjiru3mifia   user-cjiru3mifia
          prod_ap-mumbai-1_dataplane         cluster-ci3ypr6tmga   user-ci3ypr6tmga
          prod_ap-osaka-1_controlplane       cluster-cp5327xtmfa   user-cp5327xtmfa
          prod_ap-osaka-1_dataplane          cluster-cqldck6xatq   user-cqldck6xatq
          prod_ap-sydney-1_controlplane      cluster-c4wembugqyt   user-c4wembugqyt
          prod_ap-sydney-1_dataplane         cluster-cswezbzgyyg   user-cswezbzgyyg
          prod_ap-tokyo-1_controlplane       cluster-cz54pc3z4wa   user-cz54pc3z4wa
          prod_ap-tokyo-1_dataplane          cluster-cyjtbtbbjfq   user-cyjtbtbbjfq
          prod_ca-montreal-1_controlplane    cluster-czdoolemnqw   user-czdoolemnqw
          prod_ca-montreal-1_dataplane       cluster-cqtkytbgq3d   user-cqtkytbgq3d
          prod_ca-toronto-1_controlplane     cluster-cygcmjxhfsd   user-cygcmjxhfsd
          prod_ca-toronto-1_dataplane        cluster-cywmodfmm3d   user-cywmodfmm3d
          prod_eu-amsterdam-1_controlplane   cluster-c4tkmjwgbrt   user-c4tkmjwgbrt
          prod_eu-amsterdam-1_dataplane      cluster-cstsy3bmjrd   user-cstsy3bmjrd
          prod_eu-frankfurt-1_controlplane   cluster-cydcytcg43d   user-cydcytcg43d
          prod_eu-frankfurt-1_dataplane      cluster-crdgzbrmjsd   user-crdgzbrmjsd
          prod_eu-zurich-1_controlplane      cluster-cwmoxn3bfwq   user-cwmoxn3bfwq
          prod_eu-zurich-1_dataplane         cluster-chtoe5bnsza   user-chtoe5bnsza
          prod_me-dubai-1_controlplane       cluster-ce5puidc4ya   user-ce5puidc4ya
          prod_me-dubai-1_dataplane          cluster-c77hcd3vizq   user-c77hcd3vizq
          prod_me-jeddah-1_controlplane      cluster-cpzagsoukeq   user-cpzagsoukeq
          prod_me-jeddah-1_dataplane         cluster-cf4f222mrfa   user-cf4f222mrfa
          prod_sa-santiago-1_controlplane    cluster-czxpimgibeq   user-czxpimgibeq
          prod_sa-santiago-1_dataplane       cluster-cuutxfvj2ea   user-cuutxfvj2ea
          prod_sa-saopaulo-1_controlplane    cluster-cc75n7hujpq   user-cc75n7hujpq
          prod_sa-saopaulo-1_dataplane       cluster-cpjqrjtbotq   user-cpjqrjtbotq
          prod_sa-vinhedo-1_controlplane     cluster-cihx6aprjua   user-cihx6aprjua
          prod_sa-vinhedo-1_dataplane        cluster-ciqmbmn6wta   user-ciqmbmn6wta
          prod_uk-cardiff-1_controlplane     cluster-ctggmtbmfrg   user-ctggmtbmfrg
          prod_uk-cardiff-1_dataplane        cluster-cqwmnrqmztd   user-cqwmnrqmztd
          prod_uk-london-1_controlplane      cluster-cywgnlcmqzd   user-cywgnlcmqzd
          prod_uk-london-1_dataplane         cluster-c3tazjsmjsd   user-c3tazjsmjsd
          prod_us-ashburn-1_controlplane     cluster-czwczjzgrsw   user-czwczjzgrsw
*         prod_us-ashburn-1_dataplane        cluster-c3wkyjyguzt   user-c3wkyjyguzt
          prod_us-phoenix-1_controlplane     cluster-c3tkobugrst   user-c3tkobugrst
          prod_us-phoenix-1_dataplane        cluster-csdqylfmi2g   user-csdqylfmi2g
          prod_us-phoenix-1_deployment       cluster-c4tgolgmjsd   user-c4tgolgmjsd
[21:January:2022:07:30:27]:(prod_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kuc
Sets the current-context in a kubeconfig file

Aliases:
use-context, use

Examples:
  # Use the context for the minikube cluster
  kubectl config use-context minikube

Usage:
  kubectl config use-context CONTEXT_NAME [options]

Use "kubectl options" for a list of global command-line options (applies to all commands).
error: Unexpected args: []
[21:January:2022:07:30:31]:(prod_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ kuc prod_us-phoenix-1_dataplane
Switched to context "prod_us-phoenix-1_dataplane".
[21:January:2022:07:30:33]:(prod_us-ashburn-1_dataplane):~/galorndon/osvc-platform
○ (main) $ rp
[21:January:2022:07:30:34]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ hln mercury-psr
NAME      NAMESPACE   REVISION  UPDATED                                 STATUS    CHART                 APP VERSION
app-kafka mercury-psr 1         2022-01-19 22:48:59.456337456 +0000 UTC deployed  cpe-kafka-0.1.0       0.1.0
appdata   mercury-psr 1         2022-01-19 22:49:17.75707579 +0000 UTC  deployed  kafka-data-0.1.0      null
mercury   mercury-psr 1         2022-01-19 22:50:04.789572242 +0000 UTC deployed  mercury-2201.18.1305  22.01.18-REL21.11-1305
[21:January:2022:07:30:41]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ git log
commit 02359e1c59b3d6f9f2472fae400bf1abd1059a6c
Merge: 1371364 1a654df
Author: don_lind <don.lind@oracle.com>
Date:   Thu Jan 20 04:31:17 2022 +0000

    Merge branch 'as-lorax-preprod-32201180002-20220119-08322678-nlfn' into 'main'

    Lorax-okeph01-32201180002 preprod version dock-2022.01.18-16.56.48-5c7063ff

    See merge request osvc-sre-dev/osvc-platform!3063

commit 1a654dfe08156fca79317894031ead2b6a1d6c92
Author: bunker_us <27430-bunker_us@users.noreply.orahub.oci.oraclecorp.com>
Date:   Thu Jan 20 04:31:16 2022 +0000

    Lorax-okeph01-32201180002 preprod version dock-2022.01.18-16.56.48-5c7063ff

commit 1371364686f8c9e0247e7479af49d21903575adb
Merge: d3562bc a7d56d7
Author: don_lind <don.lind@oracle.com>
Date:   Thu Jan 20 00:52:27 2022 +0000

    Merge branch 'as-horton-prod-32201180002-20220119-15113929-k49c' into 'main'

    Horton-okeph01-32201180002 prod version dock-2022.01.17-13.51.02-38840ad9

    See merge request osvc-sre-dev/osvc-platform!3074

commit a7d56d7c377b1dc0387b696608f54c8b6a525604
Author: bunker_us <27430-bunker_us@users.noreply.orahub.oci.oraclecorp.com>
Date:   Thu Jan 20 00:52:27 2022 +0000

    Horton-okeph01-32201180002 prod version dock-2022.01.17-13.51.02-38840ad9

commit d3562bc12867370310e38c58c94f9dd0794bacd4
Merge: 1b2f3a4 70ff202
Author: don_lind <don.lind@oracle.com>
Date:   Thu Jan 20 00:52:24 2022 +0000

    Merge branch 'as-lorax-prod-32201180002-20220119-15115727-tuok' into 'main'

    Lorax-okeph01 prod version dock-2022.01.18-16.56.48-5c7063ff

    See merge request osvc-sre-dev/osvc-platform!3073

commit 70ff202b923600928f213669dfad3d43e29c274b
Author: bunker_us <27430-bunker_us@users.noreply.orahub.oci.oraclecorp.com>
Date:   Thu Jan 20 00:52:23 2022 +0000

    Lorax-okeph01 prod version dock-2022.01.18-16.56.48-5c7063ff
[21:January:2022:07:31:12]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ gup
Already on 'main'
git pull --ff-only
Use of the Oracle network and applications is intended solely for Oracle's authorized users. The use of these resources by Oracle employees and contractors is subject to company policies, including the Code of Conduct, Acceptable Use Policy and Information Protection Policy; access may be monitored and logged, to the extent permitted by law, in accordance with Oracle policies. Unauthorized use may result in termination of your access, disciplinary action and/or civil and criminal penalties.
remote: Enumerating objects: 175, done.
remote: Counting objects: 100% (175/175), done.
remote: Compressing objects: 100% (119/119), done.
remote: Total 314 (delta 98), reused 74 (delta 53), pack-reused 139
Receiving objects: 100% (314/314), 59.70 KiB | 0 bytes/s, done.
Resolving deltas: 100% (156/156), completed with 13 local objects.
From orahub.oci.oraclecorp.com:osvc-sre-dev/osvc-platform
   02359e1..a615983  main       -> origin/main
   2e16571..b9c9a0f  210131-000002-cxu-integration -> origin/210131-000002-cxu-integration
   0758d64..a1b2eea  210518-000014_SourceLevelMetrics -> origin/210518-000014_SourceLevelMetrics
 * [new branch]      211027-000025-janus-dashboard -> origin/211027-000025-janus-dashboard
 * [new branch]      211207-000131_to_kafka280 -> origin/211207-000131_to_kafka280
 * [new branch]      220120-000133 -> origin/220120-000133
 * [new branch]      220120-000140 -> origin/220120-000140
   057e972..e45e932  QA211208-000109-Increase_Partitions -> origin/QA211208-000109-Increase_Partitions
 * [new branch]      acs2-dev3-memory-increase -> origin/acs2-dev3-memory-increase
 * [new branch]      data-pipeline-0.4.5 -> origin/data-pipeline-0.4.5
 * [new branch]      dcs-tracker-multi-module-setup-84701a3b-1144919-3862764-preprod -> origin/dcs-tracker-multi-module-setup-84701a3b-1144919-3862764-preprod
   7dcb5a0..bd01ba3  opaec_corp_main -> origin/opaec_corp_main
 * [new branch]      opaec_snapshot_copy -> origin/opaec_snapshot_copy
   e8530cf..d18865b  opaec_snapshot_main -> origin/opaec_snapshot_main
   4bb16b0..36e7322  rthazhek_lv14844 -> origin/rthazhek_lv14844
 * [new branch]      visitorservice_kafka_2.8.0 -> origin/visitorservice_kafka_2.8.0
Updating 02359e1..a615983
Fast-forward
 helm/vars/charts/acs2/corp/values.yaml                        |  2 ++
 helm/vars/charts/consul-acl/corp/cxemailservice.yaml          |  8 ++++++++
 helm/vars/charts/consul-acl/corp/ia-ci.yaml                   | 13 +++++++++++++
 helm/vars/charts/consul-acl/corp/ia-sandbox1.yaml             |  9 +++++++++
 helm/vars/charts/consul-acl/corp/ia-sandbox2.yaml             |  9 +++++++++
 helm/vars/charts/consul-acl/corp/ia-sandbox3.yaml             |  9 +++++++++
 helm/vars/charts/consul-acl/corp/ia.yaml                      |  9 +++++++++
 helm/vars/charts/cpe-kafka/corp/agora-ci.yaml                 |  4 +---
 helm/vars/charts/cpe-kafka/corp/agora-psr.yaml                |  4 +---
 helm/vars/charts/cpe-kafka/corp/agora.yaml                    |  4 +---
 helm/vars/charts/cpe-kafka/corp/mercury-psr.yaml              | 29 +----------------------------
 helm/vars/charts/cpe-kafka/corp/visitorservice-psr.yaml       |  4 +---
 helm/vars/charts/oracle-namespace/corp/acs2-dev2.yaml         |  4 ++--
 helm/vars/charts/oracle-namespace/corp/agora-psr.yaml         |  4 ++--
 helm/vars/charts/oracle-namespace/corp/analytics-e2e.yaml     |  2 +-
 helm/vars/charts/oracle-namespace/corp/data-pipeline-e2e.yaml |  4 ++--
 helm/vars/charts/oracle-namespace/corp/mercury-chate2e.yaml   |  4 ++--
 helm/vars/charts/vault-config/corp/cxemailservice.yaml        | 80 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 18 files changed, 153 insertions(+), 49 deletions(-)
 create mode 100644 helm/vars/charts/consul-acl/corp/cxemailservice.yaml
 create mode 100644 helm/vars/charts/consul-acl/corp/ia-ci.yaml
 create mode 100644 helm/vars/charts/consul-acl/corp/ia-sandbox1.yaml
 create mode 100644 helm/vars/charts/consul-acl/corp/ia-sandbox2.yaml
 create mode 100644 helm/vars/charts/consul-acl/corp/ia-sandbox3.yaml
 create mode 100644 helm/vars/charts/consul-acl/corp/ia.yaml
 create mode 100644 helm/vars/charts/vault-config/corp/cxemailservice.yaml
/home/opc/galorndon/osvc-platform
[21:January:2022:07:32:22]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ git checkout -b "rthazhek_lv14844" "origin/rthazhek_lv14844"
Branch rthazhek_lv14844 set up to track remote branch rthazhek_lv14844 from origin.
Switched to a new branch 'rthazhek_lv14844'
[21:January:2022:07:32:27]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ git log
commit 36e73227b61be4c44c5d5d7e5f5065247e526a33
Author: Ramesh Ramakrishnan <ramesh.ramakrishnan@oracle.com>
Date:   Thu Jan 20 14:46:58 2022 +0530

    Manually populate the env variables

commit 4bb16b0eae935deaa5cf87b873971df3a987e10c
Merge: 9d45863 9dc6ae1
Author: Ramesh Ramakrishnan <ramesh.ramakrishnan@oracle.com>
Date:   Wed Jan 19 10:40:55 2022 +0530

    Merge branch 'main' into rthazhek_lv14844

commit 9d4586305cb9b1bc4550bf3832fb6e2fe8d1aac7
Author: Ramesh Ramakrishnan <ramesh.ramakrishnan@oracle.com>
Date:   Wed Jan 19 09:18:54 2022 +0530

    Syncing the file with mercury-psr

commit c22a2c733cee2f3a3f6387092e7f81f045b30752
Author: Ramesh Ramakrishnan <ramesh.ramakrishnan@oracle.com>
Date:   Wed Jan 19 09:14:06 2022 +0530

    Sync the file with mercury-psr.yaml

commit 4fe383f4352e521e45576a58fde72b4adbc14f98
Author: Ramesh Ramakrishnan <ramesh.ramakrishnan@oracle.com>
Date:   Wed Jan 19 08:47:01 2022 +0530

    Syncing the file with deployment.yaml

commit ef2c2c3ee23cb955cee96d7e6672795336becb76
Merge: 7891738 7f68210
Author: Ramesh Ramakrishnan <ramesh.ramakrishnan@oracle.com>
Date:   Wed Jan 19 02:50:21 2022 +0530

    Merge branch 'mercury_prod_phx_mercury-psr' into rthazhek_lv14844

commit 78917386da6d24edb2ca35718cc55d62f9a586ef
Author: Ramesh Ramakrishnan <ramesh.ramakrishnan@oracle.com>
Date:   Tue Jan 18 23:34:07 2022 +0530

    Updating the chart version

commit 9dc6ae1e4b10c04353d15c7b49fea5dfecd25f47
Merge: 911e124 c9a190a
Author: kevin_flood <kevin.flood@oracle.com>
Date:   Tue Jan 18 17:59:44 2022 +0000

    Merge branch 'shared-kafka-pint-corp-280' into 'main'
[21:January:2022:07:33:26]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ gdf helm/vars/charts/mercury/prod/mercury-psr/us-phoenix-1_deployment.yaml
[21:January:2022:07:33:29]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ ll helm/vars/charts/mercury/prod/mercury-psr/us-phoenix-1_deployment.yaml
-rw-r--r-- 1 opc opc 27142 Jan 21 07:32 helm/vars/charts/mercury/prod/mercury-psr/us-phoenix-1_deployment.yaml
[21:January:2022:07:33:34]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ git log helm/vars/charts/mercury/prod/mercury-psr/us-phoenix-1_deployment.yaml
commit 36e73227b61be4c44c5d5d7e5f5065247e526a33
Author: Ramesh Ramakrishnan <ramesh.ramakrishnan@oracle.com>
Date:   Thu Jan 20 14:46:58 2022 +0530

    Manually populate the env variables

commit 4fe383f4352e521e45576a58fde72b4adbc14f98
Author: Ramesh Ramakrishnan <ramesh.ramakrishnan@oracle.com>
Date:   Wed Jan 19 08:47:01 2022 +0530

    Syncing the file with deployment.yaml

commit ef2c2c3ee23cb955cee96d7e6672795336becb76
Merge: 7891738 7f68210
Author: Ramesh Ramakrishnan <ramesh.ramakrishnan@oracle.com>
Date:   Wed Jan 19 02:50:21 2022 +0530

    Merge branch 'mercury_prod_phx_mercury-psr' into rthazhek_lv14844

commit 135e4cb2ea62c3322deca9841721095c2085f66d
Author: Ramesh Ramakrishnan <ramesh.ramakrishnan@oracle.com>
Date:   Thu Jan 13 15:53:37 2022 +0530

    LV-14844 Adding mercury-psr specific details for PHX Prod

commit ff0526a1483e122e3e91797e33f4ed4b0a9ca187
Author: Dana <dana.fagerstrom@oracle.com>
Date:   Fri Jan 7 11:01:48 2022 -0500

    added prod,phx,mercury-psr

commit a32cbc7f9e0c7651bbdd380dd551af571b330c01
Author: Ramesh Ramakrishnan <ramesh.ramakrishnan@oracle.com>
Date:   Fri Jan 7 11:26:19 2022 +0530

    LV-14844 New PSR namespace for Prod PHX
[21:January:2022:07:33:39]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ helm ls -n mercury-psr
NAME      NAMESPACE   REVISION  UPDATED                                 STATUS    CHART                 APP VERSION
app-kafka mercury-psr 1         2022-01-19 22:48:59.456337456 +0000 UTC deployed  cpe-kafka-0.1.0       0.1.0
appdata   mercury-psr 1         2022-01-19 22:49:17.75707579 +0000 UTC  deployed  kafka-data-0.1.0      null
mercury   mercury-psr 1         2022-01-19 22:50:04.789572242 +0000 UTC deployed  mercury-2201.18.1305  22.01.18-REL21.11-1305
[21:January:2022:07:36:09]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ helm repo list
Error: no repositories to show
[21:January:2022:07:36:22]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ cd helm/
charts/            helmfile-releases/ scripts/           vars/
[21:January:2022:07:36:22]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform
○ (main) $ cd helm/helmfile-releases/


[21:January:2022:07:36:34]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[21:January:2022:07:36:41]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[21:January:2022:07:36:41]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[21:January:2022:07:36:41]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[21:January:2022:07:36:41]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ ls -ltr
total 24
lrwxr-xr-x 1 opc opc    7 Oct 13  2020 vars -> ../vars
lrwxr-xr-x 1 opc opc   10 Oct 13  2020 scripts -> ../scripts
lrwxr-xr-x 1 opc opc    9 Oct 13  2020 charts -> ../charts
drwxr-xr-x 3 opc opc   96 Mar  9  2021 ssapp
drwxr-xr-x 3 opc opc   96 Apr  9  2021 analytics
drwxr-xr-x 4 opc opc  128 Jun  2  2021 cxu-integration-sandbox3
drwxr-xr-x 4 opc opc  128 Jun  2  2021 cxu-integration-sandbox2
drwxr-xr-x 4 opc opc  128 Jun  2  2021 cxu-integration-sandbox1
drwxr-xr-x 4 opc opc  128 Jun  2  2021 cxu-integration-ci
drwxr-xr-x 4 opc opc  128 Jun  2  2021 cxu-integration
drwxr-xr-x 4 opc opc  128 Jun  2  2021 acs2-ci
drwxr-xr-x 3 opc opc   96 Jun  3  2021 consul-acl
drwxr-xr-x 4 opc opc  128 Aug 11 06:16 xo-psr
drwxr-xr-x 4 opc opc  128 Aug 11 06:16 xo-e2e
drwxr-xr-x 4 opc opc  128 Aug 11 06:16 xo-ci-branch
drwxr-xr-x 4 opc opc  128 Aug 11 06:16 xo-ci
drwxr-xr-x 4 opc opc  128 Aug 11 06:16 surveys
drwxr-xr-x 3 opc opc   96 Aug 11 06:16 sdc
-rwxr-xr-x 1 opc opc 5218 Aug 11 06:16 helmfile-wrapper.yaml
-rwxr-xr-x 1 opc opc 5024 Aug 11 06:16 helmfile-standard.yaml
drwxr-xr-x 3 opc opc   96 Aug 11 06:16 ergon
drwxr-xr-x 3 opc opc   96 Aug 11 06:16 abs
drwxr-xr-x 3 opc opc   96 Aug 12 10:43 psrkafka
-rw-r--r-- 1 opc opc  897 Sep  1 06:17 helmfile-repos.yaml
drwxr-xr-x 4 opc opc  128 Sep 21 05:40 xo
drwxr-xr-x 6 opc opc  192 Sep 21 05:40 vault-config
drwxr-xr-x 5 opc opc  160 Nov  8 10:58 opaec-rc
drwxr-xr-x 4 opc opc  128 Nov  8 10:58 acs2
drwxr-xr-x 4 opc opc  128 Dec 20 16:29 cxservice
drwxr-xr-x 5 opc opc  160 Dec 20 16:29 bui
drwxr-xr-x 5 opc opc  160 Dec 20 16:29 analytics-psr
drwxr-xr-x 8 opc opc  256 Dec 20 16:29 ai-apps-sandbox1
drwxr-xr-x 5 opc opc  160 Dec 20 16:29 ai-apps-dev-mp
drwxr-xr-x 4 opc opc  128 Dec 20 16:29 agora-psr
drwxr-xr-x 4 opc opc  128 Dec 20 16:29 agora-ci
drwxr-xr-x 4 opc opc  128 Dec 20 16:29 agora
drwxr-xr-x 5 opc opc  160 Dec 20 16:29 acs2-qa
drwxr-xr-x 5 opc opc  160 Dec 20 16:29 acs2-psr
drwxr-xr-x 5 opc opc  160 Dec 20 16:29 acs2-e2e-ci
drwxr-xr-x 5 opc opc  160 Dec 20 16:29 acs2-e2e
drwxr-xr-x 5 opc opc  160 Dec 20 16:29 acs2-dev3
drwxr-xr-x 5 opc opc  160 Dec 20 16:29 acs2-dev2
drwxr-xr-x 5 opc opc  160 Dec 20 16:29 acs2-dev1
drwxr-xr-x 7 opc opc  224 Dec 20 16:29 mercury-kweet-dev
drwxr-xr-x 6 opc opc  192 Dec 20 16:29 mercury-dev
drwxr-xr-x 7 opc opc  224 Dec 20 16:29 mercury-chate2e
drwxr-xr-x 7 opc opc  224 Dec 20 16:29 mercury-cert
drwxr-xr-x 6 opc opc  192 Dec 20 16:29 mercury
drwxr-xr-x 5 opc opc  160 Dec 20 16:29 kweet-develop
drwxr-xr-x 5 opc opc  160 Dec 20 16:29 kweet-dev
drwxr-xr-x 3 opc opc   96 Dec 20 16:29 kafka
drwxr-xr-x 3 opc opc   96 Dec 20 16:29 helios-psr
drwxr-xr-x 5 opc opc  160 Dec 20 16:29 escalation-sandbox1
drwxr-xr-x 5 opc opc  160 Dec 20 16:29 escalation-ci
drwxr-xr-x 5 opc opc  160 Dec 20 16:29 escalation
-rw-r--r-- 1 opc opc  652 Dec 20 16:29 environments.yaml
drwxr-xr-x 4 opc opc  128 Dec 20 16:29 data-pipeline-psr
drwxr-xr-x 4 opc opc  128 Dec 20 16:29 data-pipeline-precorp
drwxr-xr-x 5 opc opc  160 Dec 20 16:29 visitorservice
drwxr-xr-x 3 opc opc   96 Dec 20 16:29 sitedashboard
drwxr-xr-x 3 opc opc   96 Dec 20 16:29 shared-kafka-pint
drwxr-xr-x 3 opc opc   96 Dec 20 16:29 osvcprovisioner
drwxr-xr-x 3 opc opc   96 Dec 20 16:29 oracle-namespace
drwxr-xr-x 5 opc opc  160 Dec 20 16:29 opaec-sandbox1
drwxr-xr-x 5 opc opc  160 Dec 20 16:29 opaec-psr
drwxr-xr-x 5 opc opc  160 Dec 20 16:29 opaec-ci
drwxr-xr-x 5 opc opc  160 Dec 20 16:29 opaec
drwxr-xr-x 3 opc opc   96 Jan 18 07:45 webproxy
drwxr-xr-x 5 opc opc  160 Jan 18 07:45 visitorservice-psr
drwxr-xr-x 3 opc opc   96 Jan 18 07:45 iris
drwxr-xr-x 3 opc opc   96 Jan 18 07:45 helios
drwxr-xr-x 3 opc opc   96 Jan 18 07:45 data-pipeline
drwxr-xr-x 6 opc opc  192 Jan 18 07:45 ai-apps-dev
drwxr-xr-x 6 opc opc  192 Jan 18 07:45 ai-apps
drwxr-xr-x 6 opc opc  192 Jan 21 07:32 mercury-psr
[21:January:2022:07:36:43]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ ls -ltr *repo*
-rw-r--r-- 1 opc opc 897 Sep  1 06:17 helmfile-repos.yaml
[21:January:2022:07:36:48]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ #helm repo
[21:January:2022:07:37:19]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ helmfile -e prod -f helmfile-repos.yaml repos
Adding repo stable https://charts.helm.sh/stable
"stable" has been added to your repositories

Adding repo incubator https://charts.helm.sh/incubator
"incubator" has been added to your repositories

Adding repo bitnami https://charts.bitnami.com/bitnami
"bitnami" has been added to your repositories

Adding repo hashicorp https://helm.releases.hashicorp.com
"hashicorp" has been added to your repositories

Adding repo cdaas https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc
"cdaas" has been added to your repositories

Adding repo helm-osvc https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc
"helm-osvc" has been added to your repositories

Adding repo osvc-helm-virtual https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc
"osvc-helm-virtual" has been added to your repositories

err: no releases found that matches specified selector() and environment(prod), in any helmfile
[21:January:2022:07:39:09]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ helm repo list
NAME              URL
stable            https://charts.helm.sh/stable
incubator         https://charts.helm.sh/incubator
bitnami           https://charts.bitnami.com/bitnami
hashicorp         https://helm.releases.hashicorp.com
cdaas             https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc
helm-osvc         https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc
osvc-helm-virtual https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc
[21:January:2022:07:39:17]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "hashicorp" chart repository
...Successfully got an update from the "incubator" chart repository
...Successfully got an update from the "stable" chart repository
...Successfully got an update from the "bitnami" chart repository
index.go:339: skipping loading invalid entry for chart "api-tracker" "version.0.1" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "version.0.1" is invalid
index.go:339: skipping loading invalid entry for chart "kweet-config" "2111.08.1834-01" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "2111.08.1834-01" is invalid
index.go:339: skipping loading invalid entry for chart "escalation-aggregator-service" "1-21.03.04-b0019-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1-21.03.04-b0019-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "dcs-api" "version.0.1" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "version.0.1" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.02.24-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.02.24-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0001-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.04-b0001-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.04-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0001-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.05-b0001-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.05-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0003-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.05-b0003-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.06-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.06-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.12-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.12-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.13-b0004" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.13-b0004" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.14-b0002" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.14-b0002" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.21-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.21-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.28-b0005" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.28-b0005" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.03-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0006" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.03-b0006" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.06-b0002" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.06-b0002" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.10-b0002" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.10-b0002" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.02.24-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.02.24-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0001-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.04-b0001-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.04-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0001-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.05-b0001-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.05-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0003-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.05-b0003-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.06-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.06-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.12-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.12-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.13-b0004" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.13-b0004" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.14-b0002" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.14-b0002" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.21-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.21-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.28-b0005" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.28-b0005" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.03-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0006" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.03-b0006" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.06-b0002" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.06-b0002" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.10-b0002" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.10-b0002" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.03-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.03.03-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.04-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.03.04-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0006-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.18-b0006-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0007-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.18-b0007-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.19-b0005-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.19-b0005-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "escalation-aggregator-service" "1-21.03.04-b0019-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1-21.03.04-b0019-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "kweet-config" "2111.08.1834-01" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "2111.08.1834-01" is invalid
index.go:339: skipping loading invalid entry for chart "dcs-api" "version.0.1" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "version.0.1" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.03-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.03.03-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.04-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.03.04-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0006-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.18-b0006-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0007-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.18-b0007-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.19-b0005-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.19-b0005-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "api-tracker" "version.0.1" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "version.0.1" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.03-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.03.03-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.04-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.03.04-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0006-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.18-b0006-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0007-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.18-b0007-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.19-b0005-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.19-b0005-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "kweet-config" "2111.08.1834-01" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "2111.08.1834-01" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.02.24-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.02.24-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0001-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.04-b0001-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.04-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0001-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.05-b0001-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.05-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0003-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.05-b0003-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "escalation-aggregator-service" "1-21.03.04-b0019-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1-21.03.04-b0019-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "api-tracker" "version.0.1" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "version.0.1" is invalid
index.go:339: skipping loading invalid entry for chart "dcs-api" "version.0.1" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "version.0.1" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.06-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.06-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.12-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.12-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.13-b0004" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.13-b0004" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.14-b0002" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.14-b0002" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.21-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.21-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.28-b0005" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.28-b0005" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.03-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0006" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.03-b0006" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.06-b0002" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.06-b0002" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.10-b0002" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.10-b0002" is invalid
...Successfully got an update from the "helm-osvc" chart repository
...Successfully got an update from the "osvc-helm-virtual" chart repository
...Successfully got an update from the "cdaas" chart repository
Update Complete. ⎈Happy Helming!⎈
[21:January:2022:07:39:49]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ helm repo list
NAME              URL
stable            https://charts.helm.sh/stable
incubator         https://charts.helm.sh/incubator
bitnami           https://charts.bitnami.com/bitnami
hashicorp         https://helm.releases.hashicorp.com
cdaas             https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc
helm-osvc         https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc
osvc-helm-virtual https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc
[21:January:2022:07:40:06]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgc
CURRENT   NAME                               CLUSTER               AUTHINFO           NAMESPACE
          prod_ap-hyderabad-1_controlplane   cluster-cbjr52qcfoa   user-cbjr52qcfoa
          prod_ap-hyderabad-1_dataplane      cluster-cpj2zxktnda   user-cpj2zxktnda
          prod_ap-melbourne-1_controlplane   cluster-czgcn3bmjtg   user-czgcn3bmjtg
          prod_ap-melbourne-1_dataplane      cluster-c4dazdggfst   user-c4dazdggfst
          prod_ap-mumbai-1_controlplane      cluster-cjiru3mifia   user-cjiru3mifia
          prod_ap-mumbai-1_dataplane         cluster-ci3ypr6tmga   user-ci3ypr6tmga
          prod_ap-osaka-1_controlplane       cluster-cp5327xtmfa   user-cp5327xtmfa
          prod_ap-osaka-1_dataplane          cluster-cqldck6xatq   user-cqldck6xatq
          prod_ap-sydney-1_controlplane      cluster-c4wembugqyt   user-c4wembugqyt
          prod_ap-sydney-1_dataplane         cluster-cswezbzgyyg   user-cswezbzgyyg
          prod_ap-tokyo-1_controlplane       cluster-cz54pc3z4wa   user-cz54pc3z4wa
          prod_ap-tokyo-1_dataplane          cluster-cyjtbtbbjfq   user-cyjtbtbbjfq
          prod_ca-montreal-1_controlplane    cluster-czdoolemnqw   user-czdoolemnqw
          prod_ca-montreal-1_dataplane       cluster-cqtkytbgq3d   user-cqtkytbgq3d
          prod_ca-toronto-1_controlplane     cluster-cygcmjxhfsd   user-cygcmjxhfsd
          prod_ca-toronto-1_dataplane        cluster-cywmodfmm3d   user-cywmodfmm3d
          prod_eu-amsterdam-1_controlplane   cluster-c4tkmjwgbrt   user-c4tkmjwgbrt
          prod_eu-amsterdam-1_dataplane      cluster-cstsy3bmjrd   user-cstsy3bmjrd
          prod_eu-frankfurt-1_controlplane   cluster-cydcytcg43d   user-cydcytcg43d
          prod_eu-frankfurt-1_dataplane      cluster-crdgzbrmjsd   user-crdgzbrmjsd
          prod_eu-zurich-1_controlplane      cluster-cwmoxn3bfwq   user-cwmoxn3bfwq
          prod_eu-zurich-1_dataplane         cluster-chtoe5bnsza   user-chtoe5bnsza
          prod_me-dubai-1_controlplane       cluster-ce5puidc4ya   user-ce5puidc4ya
          prod_me-dubai-1_dataplane          cluster-c77hcd3vizq   user-c77hcd3vizq
          prod_me-jeddah-1_controlplane      cluster-cpzagsoukeq   user-cpzagsoukeq
          prod_me-jeddah-1_dataplane         cluster-cf4f222mrfa   user-cf4f222mrfa
          prod_sa-santiago-1_controlplane    cluster-czxpimgibeq   user-czxpimgibeq
          prod_sa-santiago-1_dataplane       cluster-cuutxfvj2ea   user-cuutxfvj2ea
          prod_sa-saopaulo-1_controlplane    cluster-cc75n7hujpq   user-cc75n7hujpq
          prod_sa-saopaulo-1_dataplane       cluster-cpjqrjtbotq   user-cpjqrjtbotq
          prod_sa-vinhedo-1_controlplane     cluster-cihx6aprjua   user-cihx6aprjua
          prod_sa-vinhedo-1_dataplane        cluster-ciqmbmn6wta   user-ciqmbmn6wta
          prod_uk-cardiff-1_controlplane     cluster-ctggmtbmfrg   user-ctggmtbmfrg
          prod_uk-cardiff-1_dataplane        cluster-cqwmnrqmztd   user-cqwmnrqmztd
          prod_uk-london-1_controlplane      cluster-cywgnlcmqzd   user-cywgnlcmqzd
          prod_uk-london-1_dataplane         cluster-c3tazjsmjsd   user-c3tazjsmjsd
          prod_us-ashburn-1_controlplane     cluster-czwczjzgrsw   user-czwczjzgrsw
          prod_us-ashburn-1_dataplane        cluster-c3wkyjyguzt   user-c3wkyjyguzt
          prod_us-phoenix-1_controlplane     cluster-c3tkobugrst   user-c3tkobugrst
*         prod_us-phoenix-1_dataplane        cluster-csdqylfmi2g   user-csdqylfmi2g
          prod_us-phoenix-1_deployment       cluster-c4tgolgmjsd   user-c4tgolgmjsd
[21:January:2022:07:40:10]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ oong
{
  "data": "osvcprod"
}
[21:January:2022:07:40:19]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ helm ls -n mercury-psr
NAME      NAMESPACE   REVISION  UPDATED                                 STATUS    CHART                 APP VERSION
app-kafka mercury-psr 1         2022-01-19 22:48:59.456337456 +0000 UTC deployed  cpe-kafka-0.1.0       0.1.0
appdata   mercury-psr 1         2022-01-19 22:49:17.75707579 +0000 UTC  deployed  kafka-data-0.1.0      null
mercury   mercury-psr 1         2022-01-19 22:50:04.789572242 +0000 UTC deployed  mercury-2201.18.1305  22.01.18-REL21.11-1305
[21:January:2022:07:40:34]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ ##helmfile -e prod -f osvc-platform/helm/helmfile-releases/mercury-psr/helmfile-mercury-psr-apps.yaml -l region=us-phoenix-1 diff --concurrency 3
[21:January:2022:07:40:38]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ ll mercury-psr/helmfile-mercury-psr-apps.yaml
-rw-r--r-- 1 opc opc 7132 Jan 21 07:32 mercury-psr/helmfile-mercury-psr-apps.yaml
[21:January:2022:07:40:54]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[21:January:2022:07:41:16]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[21:January:2022:07:41:17]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[21:January:2022:07:41:17]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[21:January:2022:07:41:17]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[21:January:2022:07:41:17]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[21:January:2022:07:41:17]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[21:January:2022:07:41:18]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ ###helmfile -e prod -f osvc-platform/helm/helmfile-releases/mercury-psr/helmfile-mercury-psr-apps.yaml -l region=us-phoenix-1 diff --concurrency 3
[21:January:2022:07:41:25]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ gbr
git branch
  abs-ocigroupname
  add-idc-engineers
  develop
  main
* rthazhek_lv14844
[21:January:2022:07:41:34]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ helmfile -e prod -f ./mercury-psr/helmfile-mercury-psr-apps.yaml -l region=us-phoenix-1 diff --concurrency 3
Comparing release=mercury, chart=osvc-helm-virtual/mercury
mercury-psr, mercury-agent-command-service-keystore, Secret (v1) has changed:
  # Source: mercury/charts/agent-command-service/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-agent-command-service-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-channel-api-keystore, Secret (v1) has changed:
  # Source: mercury/charts/channel-api/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-channel-api-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-consumer-command-service-keystore, Secret (v1) has changed:
  # Source: mercury/charts/consumer-command-service/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-consumer-command-service-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-custom-availability-service-keystore, Secret (v1) has changed:
  # Source: mercury/charts/custom-availability-service/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-custom-availability-service-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-data-mask-api-keystore, Secret (v1) has changed:
  # Source: mercury/charts/data-mask-api/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-data-mask-api-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-engagement-queue-api-keystore, Secret (v1) has changed:
  # Source: mercury/charts/engagement-queue-api/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-engagement-queue-api-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-enrichment-service, Deployment (apps) has changed:
  # Source: mercury/charts/enrichment-service/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: mercury-enrichment-service
    labels:
      project: mercury
      version: "0"
      app.kubernetes.io/name: enrichment-service
      helm.sh/chart: enrichment-service-22.01.18-REL21.11-318
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/managed-by: Helm
      monitoring: "true"
    annotations:
  spec:
    replicas: 3
    selector:
      matchLabels:
        app.kubernetes.io/name: enrichment-service
        app.kubernetes.io/instance: mercury
    strategy:
      type: RollingUpdate
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
    template:
      metadata:
        annotations:
          vault.security.banzaicloud.io/log-level: warn
          vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
          vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
          vault.security.banzaicloud.io/vault-env-daemon: "true"
          vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
          vault.security.banzaicloud.io/vault-role: mercury-psr
          vault.security.banzaicloud.io/vault-skip-verify: "true"
        labels:
          app.kubernetes.io/name: enrichment-service
          app.kubernetes.io/instance: mercury
          chronos.enabled: "True"
          chronos.maxPodLifetime: "1440"
          chronos.minAvailable: "0"
      spec:
        serviceAccountName: mercurygeneric
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - "enrichment-service"
                  - key: release
                    operator: In
                    values:
                    - mercury
                topologyKey: "failure-domain.beta.kubernetes.io/zone"
        imagePullSecrets:
          - name: osvcstage-ocirsecret
        volumes:
        - name: vaultca
          secret:
            secretName: vault-tls
        - name: keystore-pass
          secret:
            secretName: mercury-enrichment-service-keystore
        - name: keystore-volume
          emptyDir: {}
        initContainers:
        - name: init
          image: iad.ocir.io/osvcstage/mercury/init-container/prod:1.0.0.18
          imagePullPolicy: IfNotPresent
          command: ['/bin/bash', '-c', '/init/init-script.sh']
          # Need to run the init container as root (also we're forced to use its UID, i.e. 0)
          securityContext:
            runAsUser: 0
          env:
          - name: AUTO_REFRESH_CONSUL_TOKEN
            value: vault:cpe_consul/creds/mercury-psr#token
          - name: KAFKA_TEMPLATED_PKI
            value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
              .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
              "ttl": "720h"}'
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: TRUSTSTORE_NAME
            value: truststore.jks
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: props/kafka.properties
          - name: KEYSTORE_NAME
            value: keystore.jks
          - name: SERVICEACCOUNT_SECRETPATH
            value: /var/run/secrets/kubernetes.io/serviceaccount
          - name: VAULT_ADDR
            value: "https://vault.query.prod.consul:8200"
          - name: VAULT_SECRETPATH
            value: /tmp/vaultca
          - name: PKI_VAULT_ENGINE_PATH
            value: "v1/infra_pki/issue/kafka_client"
          - name: CERT_CN
            value: "mercury-psr-mercurygeneric-rw"
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          - name: VAULT_SERVICEACCOUNT
            value: mercurygeneric
          volumeMounts:
          - name: keystore-volume
            mountPath: /keystores
          - name: vaultca
            mountPath: /tmp/vaultca
            readOnly: true
          - name: keystore-pass
            mountPath: /tmp/keystore-pass
            readOnly: true
          resources:
              limits:
                cpu: "1"
                ephemeral-storage: 10Gi
                memory: 6Gi
              requests:
                cpu: 100m
                ephemeral-storage: 2Gi
                memory: 2Gi
        containers:
          - name: enrichment-service
            image: "iad.ocir.io/osvcstage/mercury/enrichment-service:22.01.18-REL21.11-318"
            imagePullPolicy: IfNotPresent
            env:
            - name: profile
              value: oci-k8s
            - name: DATACENTER
              value: OCI
            - name: RELEASE_NAME
              value: mercury
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: POD_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            - name: MERCURY_URL_PATTERN
              value: https://engagement-psr.us1.channels.ocs.oraclecloud.com/mercury-psr
            - name: JAEGER_REMOTE_REPORTER
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: VAULT_TOKEN
              value: vault:login
            - name: HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: CONSUL_HTTP_ADDR
              value: https://$(HOST_IP):8501
            - name: CONSUL_TOKEN
              value: vault:/cpe_consul/creds/mercury-psr#token
            - name: JAVA_OPTS
              value: "-Dvault.enabled=true \
                      -Dvault.namespace=mercury-psr \
                      -Dvault.transit.kafkaMessageEncryption.path=transit_mercury \
                      -Dvault.base.address=https://vault.query.prod.consul:8200 \
                      -Dvault.shared.kv.engine=/kv \
                      -Dthidwick.vault.uri=https://vault.query.prod.consul:8200 \
                      -Dthidwick.vault.transit.cache-expiration=300 \
                      -Dthidwick.vault.kv.cache-expiration=300 \
                      -Dthidwick.vault.serviceaccount=mercurygeneric \
                      -Dlog4j2.formatMsgNoLookups=true \
                      -Dmercury.cpeVersion=v2
                      -Dfile.token.enabled=true \
                      -Dvault.authentication=TOKEN \
                      -Dvault.role=mercury-psr \
                      -Dvault.path=k8s-phx-dataplane
                      -Dkafka.log_level=WARN \
                      -Dthidwick-server.logback.level=INFO \
                      -Dthidwick-server.logback.service.log_level=DEBUG \
                      -Dthidwick.kafka.message.encryption.cipher=AES/GCM \
                      -Djwt.tenant.validation.enabled=false \
                      -Dthidwick.kafka.schema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul \
                      -Dthidwick.kafka.bootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443 \
                      -Dthidwick.tms.base.tmsStateStream.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443 \
                      -Djavax.net.ssl.trustStore=/keystores/truststore.jks \
                      -Djavax.net.ssl.keyStore=/keystores/keystore.jks \
                      -Dhttp.security.headers.enabled=true \
                      -Dhttp.rest.security.headers.enabled=true \
-                     -XX:+UseG1GC -XX:MaxGCPauseMillis=250 \
-                     -Xms1536m \
+                     -DwebHook.topic.name=helios-psr.priority.external.event -Dhelios.error.event=helios-psr.error.event                    -Xms1536m \
                      -Xmx4096m \
                      -Dmercury.tenant.name.filter=\"^(evai)\"                 -Dtenant.realm=engagement-psr.us1.channels.ocs.oraclecloud.com "
            volumeMounts:
            - mountPath: /keystores
              name: keystore-volume
            - mountPath: /tmp/keystore-pass
              name: keystore-pass
            ports:
              - name: http
                containerPort: 8080
                protocol: TCP
            startupProbe:
              periodSeconds: 10
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 120
              httpGet:
                path: /health/checks
                port: 8080
            livenessProbe:
              initialDelaySeconds: 120
              periodSeconds: 30
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 1
              httpGet:
                path: /health/checks
                port: 8080
            readinessProbe:
              initialDelaySeconds: 120
              periodSeconds: 30
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 1
              httpGet:
                path: /health/ping
                port: 8080
            # Service deployment provided resources
            resources:
              limits:
                cpu: "1"
                ephemeral-storage: 10Gi
                memory: 6Gi
              requests:
                cpu: 100m
                ephemeral-storage: 2Gi
                memory: 2Gi
mercury-psr, mercury-enrichment-service-keystore, Secret (v1) has changed:
  # Source: mercury/charts/enrichment-service/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-enrichment-service-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-event-sync-service, Deployment (apps) has changed:
  # Source: mercury/charts/event-sync-service/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: mercury-event-sync-service
    labels:
      project: mercury
      version: "0"
      app.kubernetes.io/name: event-sync-service
      helm.sh/chart: event-sync-service-22.01.18-REL21.11-184
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/managed-by: Helm
      monitoring: "true"
    annotations:
  spec:
    replicas: 3
    selector:
      matchLabels:
        app.kubernetes.io/name: event-sync-service
        app.kubernetes.io/instance: mercury
    strategy:
      type: RollingUpdate
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
    template:
      metadata:
        annotations:
          vault.security.banzaicloud.io/log-level: warn
          vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
          vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
          vault.security.banzaicloud.io/vault-env-daemon: "true"
          vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
          vault.security.banzaicloud.io/vault-role: mercury-psr
          vault.security.banzaicloud.io/vault-skip-verify: "true"
        labels:
          app.kubernetes.io/name: event-sync-service
          app.kubernetes.io/instance: mercury
          chronos.enabled: "True"
          chronos.maxPodLifetime: "1440"
          chronos.minAvailable: "0"
      spec:
        serviceAccountName: mercurygeneric
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - "event-sync-service"
                  - key: release
                    operator: In
                    values:
                    - mercury
                topologyKey: "failure-domain.beta.kubernetes.io/zone"
        imagePullSecrets:
          - name: osvcstage-ocirsecret
        volumes:
        - name: vaultca
          secret:
            secretName: vault-tls
        - name: keystore-pass
          secret:
            secretName: mercury-event-sync-service-keystore
        - name: keystore-volume
          emptyDir: {}
        initContainers:
        - name: init
          image: iad.ocir.io/osvcstage/mercury/init-container/prod:1.0.0.18
          imagePullPolicy: IfNotPresent
          command: ['/bin/bash', '-c', '/init/init-script.sh']
          # Need to run the init container as root (also we're forced to use its UID, i.e. 0)
          securityContext:
            runAsUser: 0
          env:
          - name: AUTO_REFRESH_CONSUL_TOKEN
            value: vault:cpe_consul/creds/mercury-psr#token
          - name: KAFKA_TEMPLATED_PKI
            value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
              .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
              "ttl": "720h"}'
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: TRUSTSTORE_NAME
            value: truststore.jks
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: props/kafka.properties
          - name: KEYSTORE_NAME
            value: keystore.jks
          - name: SERVICEACCOUNT_SECRETPATH
            value: /var/run/secrets/kubernetes.io/serviceaccount
          - name: VAULT_ADDR
            value: "https://vault.query.prod.consul:8200"
          - name: VAULT_SECRETPATH
            value: /tmp/vaultca
          - name: PKI_VAULT_ENGINE_PATH
            value: "v1/infra_pki/issue/kafka_client"
          - name: CERT_CN
            value: "mercury-psr-mercurygeneric-rw"
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          - name: VAULT_SERVICEACCOUNT
            value: mercurygeneric
          volumeMounts:
          - name: keystore-volume
            mountPath: /keystores
          - name: vaultca
            mountPath: /tmp/vaultca
            readOnly: true
          - name: keystore-pass
            mountPath: /tmp/keystore-pass
            readOnly: true
          resources:
              limits:
                cpu: "1"
                ephemeral-storage: 10Gi
                memory: 6Gi
              requests:
                cpu: 100m
                ephemeral-storage: 2Gi
                memory: 2Gi
        containers:
          - name: event-sync-service
            image: "iad.ocir.io/osvcstage/mercury/event-sync-service:22.01.18-REL21.11-184"
            imagePullPolicy: IfNotPresent
            env:
            - name: profile
              value: oci-k8s
            - name: DATACENTER
              value: OCI
            - name: RELEASE_NAME
              value: mercury
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: POD_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            - name: MERCURY_URL_PATTERN
              value: https://engagement-psr.us1.channels.ocs.oraclecloud.com/mercury-psr
            - name: JAEGER_REMOTE_REPORTER
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: VAULT_TOKEN
              value: vault:login
            - name: HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: CONSUL_HTTP_ADDR
              value: https://$(HOST_IP):8501
            - name: CONSUL_TOKEN
              value: vault:/cpe_consul/creds/mercury-psr#token
            - name: JAVA_OPTS
              value: "-Dvault.enabled=true \
                      -Dvault.namespace=mercury-psr \
                      -Dvault.transit.kafkaMessageEncryption.path=transit_mercury \
                      -Dvault.base.address=https://vault.query.prod.consul:8200 \
                      -Dvault.shared.kv.engine=/kv \
                      -Dthidwick.vault.uri=https://vault.query.prod.consul:8200 \
                      -Dthidwick.vault.transit.cache-expiration=300 \
                      -Dthidwick.vault.kv.cache-expiration=300 \
                      -Dthidwick.vault.serviceaccount=mercurygeneric \
                      -Dlog4j2.formatMsgNoLookups=true \
                      -Dmercury.cpeVersion=v2
                      -Dfile.token.enabled=true \
                      -Dvault.authentication=TOKEN \
                      -Dvault.role=mercury-psr \
                      -Dvault.path=k8s-phx-dataplane
                      -Dkafka.log_level=WARN \
                      -Dthidwick-server.logback.level=INFO \
                      -Dthidwick-server.logback.service.log_level=DEBUG \
                      -Dthidwick.kafka.message.encryption.cipher=AES/GCM \
                      -Djwt.tenant.validation.enabled=false \
                      -Dthidwick.kafka.schema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul \
                      -Dthidwick.kafka.bootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443 \
                      -Dthidwick.tms.base.tmsStateStream.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443 \
                      -Djavax.net.ssl.trustStore=/keystores/truststore.jks \
                      -Djavax.net.ssl.keyStore=/keystores/keystore.jks \
                      -Dhttp.security.headers.enabled=true \
                      -Dhttp.rest.security.headers.enabled=true \
-                     -XX:+UseG1GC -XX:MaxGCPauseMillis=250 \
-                     -Xms1536m \
+                     -Dwebhook.topic.name=helios-psr.external.event -Dhelios.error.event=helios-psr.error.event                    -Xms1536m \
                      -Xmx4096m \
                      -Dmercury.tenant.name.filter=\"^(evai)\"                 -Dtenant.realm=engagement-psr.us1.channels.ocs.oraclecloud.com "
            volumeMounts:
            - mountPath: /keystores
              name: keystore-volume
            - mountPath: /tmp/keystore-pass
              name: keystore-pass
            ports:
              - name: http
                containerPort: 8080
                protocol: TCP
            startupProbe:
              periodSeconds: 10
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 120
              httpGet:
                path: /health/checks
                port: 8080
            livenessProbe:
              initialDelaySeconds: 120
              periodSeconds: 30
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 1
              httpGet:
                path: /health/checks
                port: 8080
            readinessProbe:
              initialDelaySeconds: 120
              periodSeconds: 30
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 1
              httpGet:
                path: /health/ping
                port: 8080
            # Service deployment provided resources
            resources:
              limits:
                cpu: "1"
                ephemeral-storage: 10Gi
                memory: 6Gi
              requests:
                cpu: 100m
                ephemeral-storage: 2Gi
                memory: 2Gi
mercury-psr, mercury-event-sync-service-keystore, Secret (v1) has changed:
  # Source: mercury/charts/event-sync-service/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-event-sync-service-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-integration-in-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/integration-in-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-integration-in-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-integration-out-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/integration-out-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-integration-out-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-kweet-facebook-client, Deployment (apps) has changed:
  # Source: mercury/charts/kweet-facebook-client/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: mercury-kweet-facebook-client
    namespace: mercury-psr
    labels:
      app: kweet-facebook-client
      project: mercury
      release: mercury
      version: "0"
      app.kubernetes.io/name: kweet-facebook-client
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/managed-by: Helm
    annotations:
-     "checksum/config": 557a9b077e47b7bed81b4a0b11dfc5837bcbea4159ae19f83493f582cda25484
+     "checksum/config": 5519767815f3917824ba33bcf9a3e370a0bd5953e98f5048d376f703caa91e70
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: kweet-facebook-client
        release: mercury
        app.kubernetes.io/name: kweet-facebook-client
        app.kubernetes.io/instance: mercury
    template:
      metadata:
        annotations:
          vault.security.banzaicloud.io/log-level: warn
          vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
          vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
          vault.security.banzaicloud.io/vault-env-daemon: "true"
          vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
          vault.security.banzaicloud.io/vault-role: mercury-psr
          vault.security.banzaicloud.io/vault-skip-verify: "true"
        labels:
          app: kweet-facebook-client
          project: mercury
          release: mercury
          version: "0"
          app.kubernetes.io/name: kweet-facebook-client
          app.kubernetes.io/instance: mercury
      spec:
        serviceAccountName: mercurygeneric
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - "kweet-facebook-client"
                      - key: release
                        operator: In
                        values:
                          - mercury
                  topologyKey: "failure-domain.beta.kubernetes.io/zone"
        imagePullSecrets:
        - name: osvcstage-ocirsecret
        volumes:
        - name: keystore-volume
          emptyDir: {}
        - name: vaultca
          secret:
            secretName: vault-tls
        - name: keystore-pass
          secret:
            secretName: mercury-kweet-facebook-client-keystore
        initContainers:
        - name: init ## use kweet's init container
          image: iad.ocir.io/osvcstage/mercury/kweet-facebook-client:21.11.10-TRUNK-56
          imagePullPolicy: IfNotPresent
          command: ['/bin/bash', '/init/init-script.sh']
          # Need to run the init container as root (also we're forced to use its UID, i.e. 0)
          securityContext:
            runAsUser: 0
          env:
          - name: AUTO_REFRESH_CONSUL_TOKEN
            value: vault:cpe_consul/creds/mercury-psr#token
          - name: KAFKA_TEMPLATED_PKI
            value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
              .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
              "ttl": "720h"}'
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: TRUSTSTORE_NAME
            value: truststore.jks
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KEYSTORE_NAME
            value: kafka-client-keystore.jks
          - name: SERVICEACCOUNT_SECRETPATH
            value: /var/run/secrets/kubernetes.io/serviceaccount
          - name: VAULT_ADDR
            value: https://vault.service.consul:8200
          - name: VAULT_SECRETPATH
            value: /tmp/vaultca
          - name: PKI_VAULT_ENGINE_PATH
            value: "v1/infra_pki/issue/kafka_client"
          - name: CERT_CN
            value: "mercury-psr-mercurygeneric-rw"
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          volumeMounts:
          - name: keystore-volume
            mountPath: /keystores
          - name: vaultca
            mountPath: /tmp/vaultca
            readOnly: true
          - name: keystore-pass
            mountPath: /tmp/keystore-pass
            readOnly: true
          resources:
            limits:
              cpu: "2"
              memory: 3Gi
            requests:
              cpu: 200m
              memory: 2Gi
        containers:
        - name: mercury-kweet-facebook-client
          image: iad.ocir.io/osvcstage/mercury/kweet-facebook-client:21.11.10-TRUNK-56
          imagePullPolicy: IfNotPresent
          env:
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: HOST_IP
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
          - name: CONSUL_HTTP_ADDR
            value: https://$(HOST_IP):8501
          - name: CONSUL_TOKEN
            value: vault:/cpe_consul/creds/mercury-psr#token
          - name: POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: VAULT_TOKEN
            value: vault:login
          - name: JAVA_OPTS
            value: "-Dschema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul \
                    -Dbootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443 \
                    -Dservice.datacenter=DC-one \
                    -Dservice.defaultTenant=TMS \
                    -Dthidwick-server.port=8080 \
                    -Dthidwick-server.swagger.enabled=true \
                    -Dthidwick-server.swagger-ui.enabled=true \
                    -Dkweet.vault.enabled=true \
                    -Dkweet.vault.namespace=mercury-psr \
                    -Dkweet.vault.serviceaccount=mercurygeneric \
                    -Dkweet.vault.host=vault.service.consul \
                    -Djavax.net.ssl.trustStore=/keystores/truststore.jks \
                    -Dtopics.prefix=mercury-psr \
                    -Dsharedkafka.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443 \
                    -Dkafka.security.protocol=SSL \
                    -Dkafka.ssl.truststore.location=/keystores/truststore.jks \
                    -Dkafka.ssl.keystore.location=/keystores/kafka-client-keystore.jks \
                    -Dkafka.ssl.jks.passphrase.file=/keystores/props/kafka.properties \
                    -Dkweet.message.encryption.enabled=true \
                    -Djson.logging.enabled=false \
                    -Dthidwick-server.logback.service.log_level=INFO \
                    -Dthidwick-server.logback.level=INFO \
                    -Dmercury.enabled=true \
                    -Dslack.api.enabled=true \
                    -Dslack.api.channel=#om-alerts-prod \
                    -Dingress.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dcpe.version=v2 \
                    -Dvault.authentication=TOKEN \
                    -Dvault.role=mercury-psr \
                    -Dkweet.vault.path=mercury \
                    -Dfile.token.enabled=true \
                    -Dkweet.consul.enabled=true \
                    -Dconsul.base.address=$(CONSUL_HTTP_ADDR) \
                    -Dconsul.token=$(CONSUL_TOKEN) \
                    -Dtenant.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dkweet.vault.uri=https://vault.query.prod.consul:8200 \
                    -Dkweet/facebook/client/channel.url=https://graph.facebook.com/v11.0/ \
                    -Dbean.kafka.configurationProvider.enabled=false \
                    -Dbean.kafka.incidentProvider.enabled=false \
                    -Dconfigservice.update.endpoint=http://mercury-social-config:8080/configupdate/config \

                    -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Xms256m"
          volumeMounts:
          - mountPath: /keystores
            name: keystore-volume
          - mountPath: /tmp/keystore-pass
            name: keystore-pass
          ports:
          - name: http
            containerPort: 8080
            protocol: TCP
          livenessProbe:
            httpGet:
              path: /health/checks
              port: http
            initialDelaySeconds: 180
          readinessProbe:
            httpGet:
              path: /health/ping
              port: http
            initialDelaySeconds: 180
          resources:
            limits:
              cpu: "2"
              memory: 3Gi
            requests:
              cpu: 200m
              memory: 2Gi
mercury-psr, mercury-kweet-facebook-client-keystore, Secret (v1) has changed:
  # Source: mercury/charts/kweet-facebook-client/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-kweet-facebook-client-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-kweet-facebook-webhook, Deployment (apps) has changed:
  # Source: mercury/charts/kweet-facebook-webhook/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: mercury-kweet-facebook-webhook
    namespace: mercury-psr
    labels:
      app: kweet-facebook-webhook
      project: mercury
      release: mercury
      version: "0"
      app.kubernetes.io/name: kweet-facebook-webhook
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/managed-by: Helm
    annotations:
-     "checksum/config": fcbb312c00e65e9c7ac56102b5091095dc61f9425f959a257cca063756129333
+     "checksum/config": caba21d0deb9f2d4a87c69c71c5f4ef616f0bd994574aaf2e5823ed8786c0fc6
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: kweet-facebook-webhook
        release: mercury
        app.kubernetes.io/name: kweet-facebook-webhook
        app.kubernetes.io/instance: mercury
    template:
      metadata:
        annotations:
          vault.security.banzaicloud.io/log-level: warn
          vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
          vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
          vault.security.banzaicloud.io/vault-env-daemon: "true"
          vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
          vault.security.banzaicloud.io/vault-role: mercury-psr
          vault.security.banzaicloud.io/vault-skip-verify: "true"
        labels:
          app: kweet-facebook-webhook
          project: mercury
          release: mercury
          version: "0"
          app.kubernetes.io/name: kweet-facebook-webhook
          app.kubernetes.io/instance: mercury
      spec:
        serviceAccountName: mercurygeneric
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - "kweet-facebook-webhook"
                      - key: release
                        operator: In
                        values:
                          - mercury
                  topologyKey: "failure-domain.beta.kubernetes.io/zone"
        imagePullSecrets:
        - name: osvcstage-ocirsecret
        volumes:
        - name: keystore-volume
          emptyDir: {}
        - name: vaultca
          secret:
            secretName: vault-tls
        - name: keystore-pass
          secret:
            secretName: mercury-kweet-facebook-webhook-keystore
        initContainers:
        - name: init ## use kweet's init container
          image: iad.ocir.io/osvcstage/mercury/kweet-facebook-webhook:21.11.10-TRUNK-99
          imagePullPolicy: IfNotPresent
          command: ['/bin/bash', '/init/init-script.sh']
          # Need to run the init container as root (also we're forced to use its UID, i.e. 0)
          securityContext:
            runAsUser: 0
          env:
          - name: AUTO_REFRESH_CONSUL_TOKEN
            value: vault:cpe_consul/creds/mercury-psr#token
          - name: KAFKA_TEMPLATED_PKI
            value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
              .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
              "ttl": "720h"}'
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: TRUSTSTORE_NAME
            value: truststore.jks
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KEYSTORE_NAME
            value: kafka-client-keystore.jks
          - name: SERVICEACCOUNT_SECRETPATH
            value: /var/run/secrets/kubernetes.io/serviceaccount
          - name: VAULT_ADDR
            value: https://vault.service.consul:8200
          - name: VAULT_SECRETPATH
            value: /tmp/vaultca
          - name: PKI_VAULT_ENGINE_PATH
            value: "v1/infra_pki/issue/kafka_client"
          - name: CERT_CN
            value: "mercury-psr-mercurygeneric-rw"
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          volumeMounts:
          - name: keystore-volume
            mountPath: /keystores
          - name: vaultca
            mountPath: /tmp/vaultca
            readOnly: true
          - name: keystore-pass
            mountPath: /tmp/keystore-pass
            readOnly: true
          resources:
            limits:
              cpu: "2"
              memory: 3Gi
            requests:
              cpu: 100m
              memory: 2Gi
        containers:
        - name: mercury-kweet-facebook-webhook
          image: iad.ocir.io/osvcstage/mercury/kweet-facebook-webhook:21.11.10-TRUNK-99
          imagePullPolicy: IfNotPresent
          env:
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: HOST_IP
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
          - name: CONSUL_HTTP_ADDR
            value: https://$(HOST_IP):8501
          - name: CONSUL_TOKEN
            value: vault:/cpe_consul/creds/mercury-psr#token
          - name: POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: VAULT_TOKEN
            value: vault:login
          - name: JAVA_OPTS
            value: "-Dschema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul \
                    -Dbootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443 \
                    -Dservice.datacenter=DC-one \
                    -Dservice.defaultTenant=TMS \
                    -Dthidwick-server.port=8080 \
                    -Dthidwick-server.swagger.enabled=true \
                    -Dthidwick-server.swagger-ui.enabled=true \
                    -Dkweet.vault.enabled=true \
                    -Dkweet.vault.namespace=mercury-psr \
                    -Dkweet.vault.serviceaccount=mercurygeneric \
                    -Dkweet.vault.host=vault.service.consul \
                    -Djavax.net.ssl.trustStore=/keystores/truststore.jks \
                    -Dtopics.prefix=mercury-psr \
                    -Dsharedkafka.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443 \
                    -Dkafka.security.protocol=SSL \
                    -Dkafka.ssl.truststore.location=/keystores/truststore.jks \
                    -Dkafka.ssl.keystore.location=/keystores/kafka-client-keystore.jks \
                    -Dkafka.ssl.jks.passphrase.file=/keystores/props/kafka.properties \
                    -Dkweet.message.encryption.enabled=true \
                    -Djson.logging.enabled=false \
                    -Dthidwick-server.logback.service.log_level=INFO \
                    -Dthidwick-server.logback.level=INFO \
                    -Dmercury.enabled=true \
                    -Dslack.api.enabled=true \
                    -Dslack.api.channel=#om-alerts-prod \
                    -Dingress.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dcpe.version=v2 \
                    -Dvault.authentication=TOKEN \
                    -Dvault.role=mercury-psr \
                    -Dkweet.vault.path=mercury \
                    -Dfile.token.enabled=true \
                    -Dkweet.consul.enabled=true \
                    -Dconsul.base.address=$(CONSUL_HTTP_ADDR) \
                    -Dconsul.token=$(CONSUL_TOKEN) \
                    -Dtenant.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dkweet.vault.uri=https://vault.query.prod.consul:8200 \
                    -Dkweet.service.userprofile.host=http://kweet-userprofiles-kweet-userprofiles \
                    -Dkweet.service.userprofile.port=8080 \
                    -Dbean.service.userProfileProvider.enabled=false \
                    -Dbean.kafka.userProfileProvider.enabled=true \
                    -Dbean.kafka.incidentProvider.enabled=false \
                    -Dclient.endpoint=http://mercury-kweet-facebook-client:8080 \
                    -Dkweet/facebook/webhook/channel.url=https://graph.facebook.com/v11.0/ \

                    -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Xms256m"
          volumeMounts:
          - mountPath: /keystores
            name: keystore-volume
          - mountPath: /tmp/keystore-pass
            name: keystore-pass
          ports:
          - name: http
            containerPort: 8080
            protocol: TCP
          livenessProbe:
            httpGet:
              path: /kweet/facebook/health/checks
              port: http
            initialDelaySeconds: 180
          readinessProbe:
            httpGet:
              path: /kweet/facebook/health/ping
              port: http
            initialDelaySeconds: 180
          resources:
            limits:
              cpu: "2"
              memory: 3Gi
            requests:
              cpu: 100m
              memory: 2Gi
mercury-psr, mercury-kweet-facebook-webhook-keystore, Secret (v1) has changed:
  # Source: mercury/charts/kweet-facebook-webhook/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-kweet-facebook-webhook-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-kweet-twiliosms-client, Deployment (apps) has changed:
  # Source: mercury/charts/kweet-twiliosms-client/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: mercury-kweet-twiliosms-client
    namespace: mercury-psr
    labels:
      app: kweet-twiliosms-client
      project: mercury
      release: mercury
      version: "0"
      app.kubernetes.io/name: kweet-twiliosms-client
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/managed-by: Helm
    annotations:
-     "checksum/config": c009478eee2d5358e377c5f2fbb3f4919be518a2e8cf5b4c34637dbd85f22ec9
+     "checksum/config": 9c52fe9d473ce47b21bfd4a5fffd6fa6a152542b5e5ac01fd88f2705066ad298
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: kweet-twiliosms-client
        release: mercury
        app.kubernetes.io/name: kweet-twiliosms-client
        app.kubernetes.io/instance: mercury
    template:
      metadata:
        annotations:
          vault.security.banzaicloud.io/log-level: warn
          vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
          vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
          vault.security.banzaicloud.io/vault-env-daemon: "true"
          vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
          vault.security.banzaicloud.io/vault-role: mercury-psr
          vault.security.banzaicloud.io/vault-skip-verify: "true"
        labels:
          app: kweet-twiliosms-client
          project: mercury
          release: mercury
          version: "0"
          app.kubernetes.io/name: kweet-twiliosms-client
          app.kubernetes.io/instance: mercury
      spec:
        serviceAccountName: mercurygeneric
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - "kweet-twiliosms-client"
                      - key: release
                        operator: In
                        values:
                          - mercury
                  topologyKey: "failure-domain.beta.kubernetes.io/zone"
        imagePullSecrets:
        - name: osvcstage-ocirsecret
        volumes:
        - name: keystore-volume
          emptyDir: {}
        - name: vaultca
          secret:
            secretName: vault-tls
        - name: keystore-pass
          secret:
            secretName: mercury-kweet-twiliosms-client-keystore
        initContainers:
        - name: init ## use kweet's init container
          image: iad.ocir.io/osvcstage/mercury/kweet-twiliosms-client:21.11.11-TRUNK-70
          imagePullPolicy: IfNotPresent
          command: ['/bin/bash', '/init/init-script.sh']
          # Need to run the init container as root (also we're forced to use its UID, i.e. 0)
          securityContext:
            runAsUser: 0
          env:
          - name: AUTO_REFRESH_CONSUL_TOKEN
            value: vault:cpe_consul/creds/mercury-psr#token
          - name: KAFKA_TEMPLATED_PKI
            value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
              .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
              "ttl": "720h"}'
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: TRUSTSTORE_NAME
            value: truststore.jks
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KEYSTORE_NAME
            value: kafka-client-keystore.jks
          - name: SERVICEACCOUNT_SECRETPATH
            value: /var/run/secrets/kubernetes.io/serviceaccount
          - name: VAULT_ADDR
            value: https://vault.service.consul:8200
          - name: VAULT_SECRETPATH
            value: /tmp/vaultca
          - name: PKI_VAULT_ENGINE_PATH
            value: "v1/infra_pki/issue/kafka_client"
          - name: CERT_CN
            value: "mercury-psr-mercurygeneric-rw"
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          volumeMounts:
          - name: keystore-volume
            mountPath: /keystores
          - name: vaultca
            mountPath: /tmp/vaultca
            readOnly: true
          - name: keystore-pass
            mountPath: /tmp/keystore-pass
            readOnly: true
          resources:
            limits:
              cpu: 2
              memory: 2048Mi
            requests:
              cpu: 0.2
              memory: 384Mi
        containers:
        - name: mercury-kweet-twiliosms-client
          image: iad.ocir.io/osvcstage/mercury/kweet-twiliosms-client:21.11.11-TRUNK-70
          imagePullPolicy: IfNotPresent
          env:
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: HOST_IP
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
          - name: CONSUL_HTTP_ADDR
            value: https://$(HOST_IP):8501
          - name: CONSUL_TOKEN
            value: vault:/cpe_consul/creds/mercury-psr#token
          - name: POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: VAULT_TOKEN
            value: vault:login
          - name: JAVA_OPTS
            value: "-Dschema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul \
                    -Dbootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443 \
                    -Dservice.datacenter=DC-one \
                    -Dservice.defaultTenant=TMS \
                    -Dthidwick-server.port=8080 \
                    -Dthidwick-server.swagger.enabled=true \
                    -Dthidwick-server.swagger-ui.enabled=true \
                    -Dkweet.vault.enabled=true \
                    -Dkweet.vault.namespace=mercury-psr \
                    -Dkweet.vault.serviceaccount=mercurygeneric \
                    -Dkweet.vault.host=vault.service.consul \
                    -Djavax.net.ssl.trustStore=/keystores/truststore.jks \
                    -Dtopics.prefix=mercury-psr \
                    -Dsharedkafka.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443 \
                    -Dkafka.security.protocol=SSL \
                    -Dkafka.ssl.truststore.location=/keystores/truststore.jks \
                    -Dkafka.ssl.keystore.location=/keystores/kafka-client-keystore.jks \
                    -Dkafka.ssl.jks.passphrase.file=/keystores/props/kafka.properties \
                    -Dkweet.message.encryption.enabled=true \
                    -Djson.logging.enabled=false \
                    -Dthidwick-server.logback.service.log_level=INFO \
                    -Dthidwick-server.logback.level=INFO \
                    -Dmercury.enabled=true \
                    -Dslack.api.enabled=true \
                    -Dslack.api.channel=#om-alerts-prod \
                    -Dingress.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dcpe.version=v2 \
                    -Dvault.authentication=TOKEN \
                    -Dvault.role=mercury-psr \
                    -Dkweet.vault.path=mercury \
                    -Dfile.token.enabled=true \
                    -Dkweet.consul.enabled=true \
                    -Dconsul.base.address=$(CONSUL_HTTP_ADDR) \
                    -Dconsul.token=$(CONSUL_TOKEN) \
                    -Dtenant.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dkweet.vault.uri=https://vault.query.prod.consul:8200 \
                    -Droutingquestions.defaultpush.enabled=false \
                    -Dbean.kafka.configurationProvider.enabled=false \
                    -Dbean.kafka.incidentProvider.enabled=false \

                    -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Xms256m"
          volumeMounts:
          - mountPath: /keystores
            name: keystore-volume
          - mountPath: /tmp/keystore-pass
            name: keystore-pass
          ports:
          - name: http
            containerPort: 8080
            protocol: TCP
          livenessProbe:
            httpGet:
              path: /kweet/twiliosms/health/checks
              port: http
            initialDelaySeconds: 180
          readinessProbe:
            httpGet:
              path: /kweet/twiliosms/health/ping
              port: http
            initialDelaySeconds: 180
          resources:
            limits:
              cpu: 2
              memory: 2048Mi
            requests:
              cpu: 0.2
              memory: 384Mi
mercury-psr, mercury-kweet-twiliosms-client-keystore, Secret (v1) has changed:
  # Source: mercury/charts/kweet-twiliosms-client/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-kweet-twiliosms-client-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-kweet-userprofiles, Deployment (apps) has changed:
  # Source: mercury/charts/kweet-userprofiles/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: mercury-kweet-userprofiles
    namespace: mercury-psr
    labels:
      app: kweet-userprofiles
      project: mercury
      release: mercury
      version: "0"
      app.kubernetes.io/name: kweet-userprofiles
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/managed-by: Helm
    annotations:
-     "checksum/config": 40a9b7400c53647454f5d24bc4bc67197232a7b0e363621d8a1d9f5eaf84c8a7
+     "checksum/config": e04ccc137e4a2bb12a89d0e336f71bb9942245b9a6f27d799a357fb011534c58
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: kweet-userprofiles
        release: mercury
        app.kubernetes.io/name: kweet-userprofiles
        app.kubernetes.io/instance: mercury
    template:
      metadata:
        annotations:
          vault.security.banzaicloud.io/log-level: warn
          vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
          vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
          vault.security.banzaicloud.io/vault-env-daemon: "true"
          vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
          vault.security.banzaicloud.io/vault-role: mercury-psr
          vault.security.banzaicloud.io/vault-skip-verify: "true"
        labels:
          app: kweet-userprofiles
          project: mercury
          release: mercury
          version: "0"
          app.kubernetes.io/name: kweet-userprofiles
          app.kubernetes.io/instance: mercury
      spec:
        serviceAccountName: mercurygeneric
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - "kweet-userprofiles"
                      - key: release
                        operator: In
                        values:
                          - mercury
                  topologyKey: "failure-domain.beta.kubernetes.io/zone"
        imagePullSecrets:
        - name: osvcstage-ocirsecret
        volumes:
        - name: keystore-volume
          emptyDir: {}
        - name: vaultca
          secret:
            secretName: vault-tls
        - name: keystore-pass
          secret:
            secretName: mercury-kweet-userprofiles-keystore
        initContainers:
        - name: init ## use kweet's init container
          image: iad.ocir.io/osvcstage/mercury/kweet-userprofiles:21.11.08-TRUNK-52
          imagePullPolicy: Always
          command: ['/bin/bash', '/init/init-script.sh']
          # Need to run the init container as root (also we're forced to use its UID, i.e. 0)
          securityContext:
            runAsUser: 0
          env:
          - name: AUTO_REFRESH_CONSUL_TOKEN
            value: vault:cpe_consul/creds/mercury-psr#token
          - name: KAFKA_TEMPLATED_PKI
            value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
              .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
              "ttl": "720h"}'
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: TRUSTSTORE_NAME
            value: truststore.jks
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KEYSTORE_NAME
            value: kafka-client-keystore.jks
          - name: SERVICEACCOUNT_SECRETPATH
            value: /var/run/secrets/kubernetes.io/serviceaccount
          - name: VAULT_ADDR
            value: https://vault.service.consul:8200
          - name: VAULT_SECRETPATH
            value: /tmp/vaultca
          - name: PKI_VAULT_ENGINE_PATH
            value: "v1/infra_pki/issue/kafka_client"
          - name: CERT_CN
            value: "mercury-psr-mercurygeneric-rw"
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          volumeMounts:
          - name: keystore-volume
            mountPath: /keystores
          - name: vaultca
            mountPath: /tmp/vaultca
            readOnly: true
          - name: keystore-pass
            mountPath: /tmp/keystore-pass
            readOnly: true
          resources:
            limits:
              cpu: 1
              memory: 2Gi
            requests:
              cpu: "0.2"
              memory: 1Gi
        containers:
        - name: mercury-kweet-userprofiles
          image: iad.ocir.io/osvcstage/mercury/kweet-userprofiles:21.11.08-TRUNK-52
          imagePullPolicy: Always
          env:
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: HOST_IP
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
          - name: CONSUL_HTTP_ADDR
            value: https://$(HOST_IP):8501
          - name: CONSUL_TOKEN
            value: vault:/cpe_consul/creds/mercury-psr#token
          - name: POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: VAULT_TOKEN
            value: vault:login
          - name: JAVA_OPTS
            value: "-Dschema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul \
                    -Dbootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443 \
                    -Dservice.datacenter=DC-one \
                    -Dservice.defaultTenant=TMS \
                    -Dthidwick-server.port=8080 \
                    -Dthidwick-server.swagger.enabled=true \
                    -Dthidwick-server.swagger-ui.enabled=true \
                    -Dkweet.vault.enabled=true \
                    -Dkweet.vault.namespace=mercury-psr \
                    -Dkweet.vault.serviceaccount=mercurygeneric \
                    -Dkweet.vault.host=vault.service.consul \
                    -Djavax.net.ssl.trustStore=/keystores/truststore.jks \
                    -Dtopics.prefix=mercury-psr \
                    -Dsharedkafka.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443 \
                    -Dkafka.security.protocol=SSL \
                    -Dkafka.ssl.truststore.location=/keystores/truststore.jks \
                    -Dkafka.ssl.keystore.location=/keystores/kafka-client-keystore.jks \
                    -Dkafka.ssl.jks.passphrase.file=/keystores/props/kafka.properties \
                    -Dkweet.message.encryption.enabled=true \
                    -Djson.logging.enabled=false \
                    -Dthidwick-server.logback.service.log_level=INFO \
                    -Dthidwick-server.logback.level=INFO \
                    -Dmercury.enabled=true \
                    -Dslack.api.enabled=true \
                    -Dslack.api.channel=#om-alerts-prod \
                    -Dingress.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dcpe.version=v2 \
                    -Dvault.authentication=TOKEN \
                    -Dvault.role=mercury-psr \
                    -Dkweet.vault.path=mercury \
                    -Dfile.token.enabled=true \
                    -Dkweet.consul.enabled=true \
                    -Dconsul.base.address=$(CONSUL_HTTP_ADDR) \
                    -Dconsul.token=$(CONSUL_TOKEN) \
                    -Dtenant.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dkweet.vault.uri=https://vault.query.prod.consul:8200 \
                    -Dbogus=true \

                    -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Xms256m"
          volumeMounts:
          - mountPath: /keystores
            name: keystore-volume
          - mountPath: /tmp/keystore-pass
            name: keystore-pass
          ports:
          - name: http
            containerPort: 8080
            protocol: TCP
          livenessProbe:
            httpGet:
              path: /kweet/userprofiles/health/checks
              port: http
            initialDelaySeconds: 180
          readinessProbe:
            httpGet:
              path: /kweet/userprofiles/health/ping
              port: http
            initialDelaySeconds: 180
          resources:
            limits:
              cpu: 1
              memory: 2Gi
            requests:
              cpu: "0.2"
              memory: 1Gi
mercury-psr, mercury-kweet-userprofiles-keystore, Secret (v1) has changed:
  # Source: mercury/charts/kweet-userprofiles/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-kweet-userprofiles-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-kweet-wechat-client, Deployment (apps) has changed:
  # Source: mercury/charts/kweet-wechat-client/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: mercury-kweet-wechat-client
    namespace: mercury-psr
    labels:
      app: kweet-wechat-client
      project: mercury
      release: mercury
      version: "0"
      app.kubernetes.io/name: kweet-wechat-client
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/managed-by: Helm
    annotations:
-     "checksum/config": 13101ccbd8337c685517ea6bd21a910c3c8b3c2ce1ab9a696aa0f08bc489c5a7
+     "checksum/config": 1116d6ec8117a32f5a2ee1ba635d8bc34daac98faed246237a59b4e8ee94a1a5
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: kweet-wechat-client
        release: mercury
        app.kubernetes.io/name: kweet-wechat-client
        app.kubernetes.io/instance: mercury
    template:
      metadata:
        annotations:
          vault.security.banzaicloud.io/log-level: warn
          vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
          vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
          vault.security.banzaicloud.io/vault-env-daemon: "true"
          vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
          vault.security.banzaicloud.io/vault-role: mercury-psr
          vault.security.banzaicloud.io/vault-skip-verify: "true"
        labels:
          app: kweet-wechat-client
          project: mercury
          release: mercury
          version: "0"
          app.kubernetes.io/name: kweet-wechat-client
          app.kubernetes.io/instance: mercury
      spec:
        serviceAccountName: mercurygeneric
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - "kweet-wechat-client"
                      - key: release
                        operator: In
                        values:
                          - mercury
                  topologyKey: "failure-domain.beta.kubernetes.io/zone"
        imagePullSecrets:
        - name: osvcstage-ocirsecret
        volumes:
        - name: keystore-volume
          emptyDir: {}
        - name: vaultca
          secret:
            secretName: vault-tls
        - name: keystore-pass
          secret:
            secretName: mercury-kweet-wechat-client-keystore
        initContainers:
        - name: init ## use kweet's init container
          image: iad.ocir.io/osvcstage/mercury/kweet-wechat-client:21.11.08-TRUNK-35
          imagePullPolicy: IfNotPresent
          command: ['/bin/bash', '/init/init-script.sh']
          # Need to run the init container as root (also we're forced to use its UID, i.e. 0)
          securityContext:
            runAsUser: 0
          env:
          - name: AUTO_REFRESH_CONSUL_TOKEN
            value: vault:cpe_consul/creds/mercury-psr#token
          - name: KAFKA_TEMPLATED_PKI
            value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
              .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
              "ttl": "720h"}'
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: TRUSTSTORE_NAME
            value: truststore.jks
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KEYSTORE_NAME
            value: kafka-client-keystore.jks
          - name: SERVICEACCOUNT_SECRETPATH
            value: /var/run/secrets/kubernetes.io/serviceaccount
          - name: VAULT_ADDR
            value: https://vault.service.consul:8200
          - name: VAULT_SECRETPATH
            value: /tmp/vaultca
          - name: PKI_VAULT_ENGINE_PATH
            value: "v1/infra_pki/issue/kafka_client"
          - name: CERT_CN
            value: "mercury-psr-mercurygeneric-rw"
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          volumeMounts:
          - name: keystore-volume
            mountPath: /keystores
          - name: vaultca
            mountPath: /tmp/vaultca
            readOnly: true
          - name: keystore-pass
            mountPath: /tmp/keystore-pass
            readOnly: true
          resources:
            limits:
              cpu: "2"
              memory: 3Gi
            requests:
              cpu: 200m
              memory: 2Gi
        containers:
        - name: mercury-kweet-wechat-client
          image: iad.ocir.io/osvcstage/mercury/kweet-wechat-client:21.11.08-TRUNK-35
          imagePullPolicy: IfNotPresent
          env:
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: HOST_IP
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
          - name: CONSUL_HTTP_ADDR
            value: https://$(HOST_IP):8501
          - name: CONSUL_TOKEN
            value: vault:/cpe_consul/creds/mercury-psr#token
          - name: POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: VAULT_TOKEN
            value: vault:login
          - name: JAVA_OPTS
            value: "-Dschema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul \
                    -Dbootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443 \
                    -Dservice.datacenter=DC-one \
                    -Dservice.defaultTenant=TMS \
                    -Dthidwick-server.port=8080 \
                    -Dthidwick-server.swagger.enabled=true \
                    -Dthidwick-server.swagger-ui.enabled=true \
                    -Dkweet.vault.enabled=true \
                    -Dkweet.vault.namespace=mercury-psr \
                    -Dkweet.vault.serviceaccount=mercurygeneric \
                    -Dkweet.vault.host=vault.service.consul \
                    -Djavax.net.ssl.trustStore=/keystores/truststore.jks \
                    -Dtopics.prefix=mercury-psr \
                    -Dsharedkafka.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443 \
                    -Dkafka.security.protocol=SSL \
                    -Dkafka.ssl.truststore.location=/keystores/truststore.jks \
                    -Dkafka.ssl.keystore.location=/keystores/kafka-client-keystore.jks \
                    -Dkafka.ssl.jks.passphrase.file=/keystores/props/kafka.properties \
                    -Dkweet.message.encryption.enabled=true \
                    -Djson.logging.enabled=false \
                    -Dthidwick-server.logback.service.log_level=INFO \
                    -Dthidwick-server.logback.level=INFO \
                    -Dmercury.enabled=true \
                    -Dslack.api.enabled=true \
                    -Dslack.api.channel=#om-alerts-prod \
                    -Dingress.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dcpe.version=v2 \
                    -Dvault.authentication=TOKEN \
                    -Dvault.role=mercury-psr \
                    -Dkweet.vault.path=mercury \
                    -Dfile.token.enabled=true \
                    -Dkweet.consul.enabled=true \
                    -Dconsul.base.address=$(CONSUL_HTTP_ADDR) \
                    -Dconsul.token=$(CONSUL_TOKEN) \
                    -Dtenant.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dkweet.vault.uri=https://vault.query.prod.consul:8200 \
                    -Dkweet/wechat/client/channel.url=https://api.weixin.qq.com/ \
                    -Dbean.kafka.configurationProvider.enabled=false \
                    -Dbean.kafka.incidentProvider.enabled=false \

                    -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Xms256m"
          volumeMounts:
          - mountPath: /keystores
            name: keystore-volume
          - mountPath: /tmp/keystore-pass
            name: keystore-pass
          ports:
          - name: http
            containerPort: 8080
            protocol: TCP
          livenessProbe:
            httpGet:
              path: /health/checks
              port: http
            initialDelaySeconds: 180
          readinessProbe:
            httpGet:
              path: /health/ping
              port: http
            initialDelaySeconds: 180
          resources:
            limits:
              cpu: "2"
              memory: 3Gi
            requests:
              cpu: 200m
              memory: 2Gi
mercury-psr, mercury-kweet-wechat-client-keystore, Secret (v1) has changed:
  # Source: mercury/charts/kweet-wechat-client/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-kweet-wechat-client-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-kweet-wechat-webhook, Deployment (apps) has changed:
  # Source: mercury/charts/kweet-wechat-webhook/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: mercury-kweet-wechat-webhook
    namespace: mercury-psr
    labels:
      app: kweet-wechat-webhook
      project: mercury
      release: mercury
      version: "0"
      app.kubernetes.io/name: kweet-wechat-webhook
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/managed-by: Helm
    annotations:
-     "checksum/config": 86d069364115942ff369104df166e021d87b15944b9bf549302b8b45f4e1d5c3
+     "checksum/config": f0be465409dad51950035758086fc4aa9ecc79fd6457f7f0074b883007d8e396
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: kweet-wechat-webhook
        release: mercury
        app.kubernetes.io/name: kweet-wechat-webhook
        app.kubernetes.io/instance: mercury
    template:
      metadata:
        annotations:
          vault.security.banzaicloud.io/log-level: warn
          vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
          vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
          vault.security.banzaicloud.io/vault-env-daemon: "true"
          vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
          vault.security.banzaicloud.io/vault-role: mercury-psr
          vault.security.banzaicloud.io/vault-skip-verify: "true"
        labels:
          app: kweet-wechat-webhook
          project: mercury
          release: mercury
          version: "0"
          app.kubernetes.io/name: kweet-wechat-webhook
          app.kubernetes.io/instance: mercury
      spec:
        serviceAccountName: mercurygeneric
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - "kweet-wechat-webhook"
                      - key: release
                        operator: In
                        values:
                          - mercury
                  topologyKey: "failure-domain.beta.kubernetes.io/zone"
        imagePullSecrets:
        - name: osvcstage-ocirsecret
        volumes:
        - name: keystore-volume
          emptyDir: {}
        - name: vaultca
          secret:
            secretName: vault-tls
        - name: keystore-pass
          secret:
            secretName: mercury-kweet-wechat-webhook-keystore
        initContainers:
        - name: init ## use kweet's init container
          image: iad.ocir.io/osvcstage/mercury/kweet-wechat-webhook:21.11.11-TRUNK-57
          imagePullPolicy: IfNotPresent
          command: ['/bin/bash', '/init/init-script.sh']
          # Need to run the init container as root (also we're forced to use its UID, i.e. 0)
          securityContext:
            runAsUser: 0
          env:
          - name: AUTO_REFRESH_CONSUL_TOKEN
            value: vault:cpe_consul/creds/mercury-psr#token
          - name: KAFKA_TEMPLATED_PKI
            value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
              .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
              "ttl": "720h"}'
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: TRUSTSTORE_NAME
            value: truststore.jks
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KEYSTORE_NAME
            value: kafka-client-keystore.jks
          - name: SERVICEACCOUNT_SECRETPATH
            value: /var/run/secrets/kubernetes.io/serviceaccount
          - name: VAULT_ADDR
            value: https://vault.service.consul:8200
          - name: VAULT_SECRETPATH
            value: /tmp/vaultca
          - name: PKI_VAULT_ENGINE_PATH
            value: "v1/infra_pki/issue/kafka_client"
          - name: CERT_CN
            value: "mercury-psr-mercurygeneric-rw"
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          volumeMounts:
          - name: keystore-volume
            mountPath: /keystores
          - name: vaultca
            mountPath: /tmp/vaultca
            readOnly: true
          - name: keystore-pass
            mountPath: /tmp/keystore-pass
            readOnly: true
          resources:
            limits:
              cpu: "2"
              memory: 3Gi
            requests:
              cpu: 200m
              memory: 2Gi
        containers:
        - name: mercury-kweet-wechat-webhook
          image: iad.ocir.io/osvcstage/mercury/kweet-wechat-webhook:21.11.11-TRUNK-57
          imagePullPolicy: IfNotPresent
          env:
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: HOST_IP
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
          - name: CONSUL_HTTP_ADDR
            value: https://$(HOST_IP):8501
          - name: CONSUL_TOKEN
            value: vault:/cpe_consul/creds/mercury-psr#token
          - name: POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: VAULT_TOKEN
            value: vault:login
          - name: JAVA_OPTS
            value: "-Dschema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul \
                    -Dbootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443 \
                    -Dservice.datacenter=DC-one \
                    -Dservice.defaultTenant=TMS \
                    -Dthidwick-server.port=8080 \
                    -Dthidwick-server.swagger.enabled=true \
                    -Dthidwick-server.swagger-ui.enabled=true \
                    -Dkweet.vault.enabled=true \
                    -Dkweet.vault.namespace=mercury-psr \
                    -Dkweet.vault.serviceaccount=mercurygeneric \
                    -Dkweet.vault.host=vault.service.consul \
                    -Djavax.net.ssl.trustStore=/keystores/truststore.jks \
                    -Dtopics.prefix=mercury-psr \
                    -Dsharedkafka.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443 \
                    -Dkafka.security.protocol=SSL \
                    -Dkafka.ssl.truststore.location=/keystores/truststore.jks \
                    -Dkafka.ssl.keystore.location=/keystores/kafka-client-keystore.jks \
                    -Dkafka.ssl.jks.passphrase.file=/keystores/props/kafka.properties \
                    -Dkweet.message.encryption.enabled=true \
                    -Djson.logging.enabled=false \
                    -Dthidwick-server.logback.service.log_level=INFO \
                    -Dthidwick-server.logback.level=INFO \
                    -Dmercury.enabled=true \
                    -Dslack.api.enabled=true \
                    -Dslack.api.channel=#om-alerts-prod \
                    -Dingress.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dcpe.version=v2 \
                    -Dvault.authentication=TOKEN \
                    -Dvault.role=mercury-psr \
                    -Dkweet.vault.path=mercury \
                    -Dfile.token.enabled=true \
                    -Dkweet.consul.enabled=true \
                    -Dconsul.base.address=$(CONSUL_HTTP_ADDR) \
                    -Dconsul.token=$(CONSUL_TOKEN) \
                    -Dtenant.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dkweet.vault.uri=https://vault.query.prod.consul:8200 \
                    -Dkweet.chat.url=http://kweet-chat-kweet-chat:8080 \
                    -Dkweet.service.userprofile.host=http://kweet-userprofiles-kweet-userprofiles \
                    -Dkweet.service.userprofile.port=8080 \
                    -Dbean.service.userProfileProvider.enabled=true \
                    -Dbean.kafka.userProfileProvider.enabled=false \
                    -Dclient.endpoint=http://mercury-kweet-wechat-client:8080 \
                    -Dconfigservice.update.endpoint=http://mercury-social-config:8080/configupdate/config \
                    -Dkweet/wechat/webhook/channel.url=https://api.weixin.qq.com \
                    -Ddownload.media.expiry.seconds=86400 \
                    -Dbean.kafka.configurationProvider.enabled=false \
                    -Dbean.kafka.incidentProvider.enabled=false \

                    -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Xms256m"
          volumeMounts:
          - mountPath: /keystores
            name: keystore-volume
          - mountPath: /tmp/keystore-pass
            name: keystore-pass
          ports:
          - name: http
            containerPort: 8080
            protocol: TCP
          livenessProbe:
            httpGet:
              path: /kweet/wechat/health/checks
              port: http
            initialDelaySeconds: 180
          readinessProbe:
            httpGet:
              path: /kweet/wechat/health/ping
              port: http
            initialDelaySeconds: 180
          resources:
            limits:
              cpu: "2"
              memory: 3Gi
            requests:
              cpu: 200m
              memory: 2Gi
mercury-psr, mercury-kweet-wechat-webhook-keystore, Secret (v1) has changed:
  # Source: mercury/charts/kweet-wechat-webhook/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-kweet-wechat-webhook-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-mercury-ui-keystore, Secret (v1) has changed:
  # Source: mercury/charts/mercury-ui/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-mercury-ui-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-metric-aggregation-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/metric-aggregation-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-metric-aggregation-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-metric-fusion-bridge-keystore, Secret (v1) has changed:
  # Source: mercury/charts/metric-fusion-bridge/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-metric-fusion-bridge-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-metric-generation-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/metric-generation-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-metric-generation-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-metric-internal-translation-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/metric-internal-translation-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-metric-internal-translation-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-metric-proxy-service-keystore, Secret (v1) has changed:
  # Source: mercury/charts/metric-proxy-service/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-metric-proxy-service-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-omnichannel-assignment-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/omnichannel-assignment-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-omnichannel-assignment-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-omnichannel-offer-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/omnichannel-offer-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-omnichannel-offer-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-osvc-bridge-api-services-keystore, Secret (v1) has changed:
  # Source: mercury/charts/osvc-bridge-api-services/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-osvc-bridge-api-services-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-osvc-bridge-metrics-data-pipeline-keystore, Secret (v1) has changed:
  # Source: mercury/charts/osvc-bridge-metrics-data-pipeline/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-osvc-bridge-metrics-data-pipeline-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-osvc-bridge-osvc-data-extractor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/osvc-bridge-osvc-data-extractor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-osvc-bridge-osvc-data-extractor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-osvc-bridge-provisioning-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/osvc-bridge-provisioning-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-osvc-bridge-provisioning-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-osvc-bridge-state-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/osvc-bridge-state-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-osvc-bridge-state-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-osvc-bridge-state-query-service-keystore, Secret (v1) has changed:
  # Source: mercury/charts/osvc-bridge-state-query-service/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-osvc-bridge-state-query-service-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-osvc-bridge-task-controller-keystore, Secret (v1) has changed:
  # Source: mercury/charts/osvc-bridge-task-controller/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-osvc-bridge-task-controller-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-provisioning-monitor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/provisioning-monitor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-provisioning-monitor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-provisioning-processor, Deployment (apps) has changed:
  # Source: mercury/charts/provisioning-processor/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: mercury-provisioning-processor
    labels:
      project: mercury
      version: "0"
      app.kubernetes.io/name: provisioning-processor
      helm.sh/chart: provisioning-processor-22.01.18-REL21.11-1087
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/managed-by: Helm
      monitoring: "true"
    annotations:
  spec:
    replicas: 3
    selector:
      matchLabels:
        app.kubernetes.io/name: provisioning-processor
        app.kubernetes.io/instance: mercury
    strategy:
      type: RollingUpdate
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
    template:
      metadata:
        annotations:
          vault.security.banzaicloud.io/log-level: warn
          vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
          vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
          vault.security.banzaicloud.io/vault-env-daemon: "true"
          vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
          vault.security.banzaicloud.io/vault-role: mercury-psr
          vault.security.banzaicloud.io/vault-skip-verify: "true"
        labels:
          app.kubernetes.io/name: provisioning-processor
          app.kubernetes.io/instance: mercury
          chronos.enabled: "True"
          chronos.maxPodLifetime: "1440"
          chronos.minAvailable: "0"
      spec:
        serviceAccountName: mercurygeneric
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - "provisioning-processor"
                  - key: release
                    operator: In
                    values:
                    - mercury
                topologyKey: "failure-domain.beta.kubernetes.io/zone"
        imagePullSecrets:
          - name: osvcstage-ocirsecret
        volumes:
        - name: vaultca
          secret:
            secretName: vault-tls
        - name: keystore-pass
          secret:
            secretName: mercury-provisioning-processor-keystore
        - name: keystore-volume
          emptyDir: {}
        initContainers:
        - name: init
          image: iad.ocir.io/osvcstage/mercury/init-container/prod:1.0.0.18
          imagePullPolicy: IfNotPresent
          command: ['/bin/bash', '-c', '/init/init-script.sh']
          # Need to run the init container as root (also we're forced to use its UID, i.e. 0)
          securityContext:
            runAsUser: 0
          env:
          - name: AUTO_REFRESH_CONSUL_TOKEN
            value: vault:cpe_consul/creds/mercury-psr#token
          - name: KAFKA_TEMPLATED_PKI
            value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
              .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
              "ttl": "720h"}'
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: TRUSTSTORE_NAME
            value: truststore.jks
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: props/kafka.properties
          - name: KEYSTORE_NAME
            value: keystore.jks
          - name: SERVICEACCOUNT_SECRETPATH
            value: /var/run/secrets/kubernetes.io/serviceaccount
          - name: VAULT_ADDR
            value: "https://vault.query.prod.consul:8200"
          - name: VAULT_SECRETPATH
            value: /tmp/vaultca
          - name: PKI_VAULT_ENGINE_PATH
            value: "v1/infra_pki/issue/kafka_client"
          - name: CERT_CN
            value: "mercury-psr-mercurygeneric-rw"
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          - name: VAULT_SERVICEACCOUNT
            value: mercurygeneric
          volumeMounts:
          - name: keystore-volume
            mountPath: /keystores
          - name: vaultca
            mountPath: /tmp/vaultca
            readOnly: true
          - name: keystore-pass
            mountPath: /tmp/keystore-pass
            readOnly: true
          resources:
              limits:
                cpu: "1"
                ephemeral-storage: 10Gi
                memory: 6Gi
              requests:
                cpu: 100m
                ephemeral-storage: 2Gi
                memory: 2Gi
        containers:
          - name: provisioning-processor
            image: "iad.ocir.io/osvcstage/mercury/provisioning-processor:22.01.18-REL21.11-1087"
            imagePullPolicy: IfNotPresent
            env:
            - name: profile
              value: oci-k8s
            - name: DATACENTER
              value: OCI
            - name: RELEASE_NAME
              value: mercury
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: POD_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            - name: MERCURY_URL_PATTERN
              value: https://engagement-psr.us1.channels.ocs.oraclecloud.com/mercury-psr
            - name: JAEGER_REMOTE_REPORTER
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: VAULT_TOKEN
              value: vault:login
            - name: HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: CONSUL_HTTP_ADDR
              value: https://$(HOST_IP):8501
            - name: CONSUL_TOKEN
              value: vault:/cpe_consul/creds/mercury-psr#token
            - name: JAVA_OPTS
              value: "-Dvault.enabled=true \
                      -Dvault.namespace=mercury-psr \
                      -Dvault.transit.kafkaMessageEncryption.path=transit_mercury \
                      -Dvault.base.address=https://vault.query.prod.consul:8200 \
                      -Dvault.shared.kv.engine=/kv \
                      -Dthidwick.vault.uri=https://vault.query.prod.consul:8200 \
                      -Dthidwick.vault.transit.cache-expiration=300 \
                      -Dthidwick.vault.kv.cache-expiration=300 \
                      -Dthidwick.vault.serviceaccount=mercurygeneric \
                      -Dlog4j2.formatMsgNoLookups=true \
                      -Dmercury.cpeVersion=v2
                      -Dfile.token.enabled=true \
                      -Dvault.authentication=TOKEN \
                      -Dvault.role=mercury-psr \
                      -Dvault.path=k8s-phx-dataplane
                      -Dkafka.log_level=WARN \
                      -Dthidwick-server.logback.level=INFO \
                      -Dthidwick-server.logback.service.log_level=DEBUG \
                      -Dthidwick.kafka.message.encryption.cipher=AES/GCM \
                      -Djwt.tenant.validation.enabled=false \
                      -Dthidwick.kafka.schema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul \
                      -Dthidwick.kafka.bootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443 \
                      -Dthidwick.tms.base.tmsStateStream.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443 \
                      -Djavax.net.ssl.trustStore=/keystores/truststore.jks \
                      -Djavax.net.ssl.keyStore=/keystores/keystore.jks \
                      -Dhttp.security.headers.enabled=true \
                      -Dhttp.rest.security.headers.enabled=true \
-                     -Dmercury.provisioning.max.time.mins=29                    -Xms1536m \
+                     -Dmercury.provisioning.max.time.mins=29 -Dwebhook.service.base.uri=http://authentication-service-helios.helios-psr.svc.cluster.local:80                    -Xms1536m \
                      -Xmx4096m \
                      -Dmercury.tenant.name.filter=\"^(evai)\" -Dsingle.sign.on.service.idp.login.response.url=https://engagement-psr.us1.channels.ocs.oraclecloud.com/mercury-psr/sp/saml/response -Dsingle.sign.on.service.idp.logout.response.url=https://engagement-psr.us1.channels.ocs.oraclecloud.com/mercury-psr/sp/saml/logout/response -Dpreferences.service.url=http://mercury-user-preference-service:8080      -Dmercury.provisioning.syncUpstreamData=true        -Dtenant.realm=engagement-psr.us1.channels.ocs.oraclecloud.com "
            volumeMounts:
            - mountPath: /keystores
              name: keystore-volume
            - mountPath: /tmp/keystore-pass
              name: keystore-pass
            ports:
              - name: http
                containerPort: 8080
                protocol: TCP
            startupProbe:
              periodSeconds: 10
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 120
              httpGet:
                path: /health/checks
                port: 8080
            livenessProbe:
              initialDelaySeconds: 120
              periodSeconds: 30
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 1
              httpGet:
                path: /health/checks
                port: 8080
            readinessProbe:
              initialDelaySeconds: 120
              periodSeconds: 30
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 1
              httpGet:
                path: /health/ping
                port: 8080
            # Service deployment provided resources
            resources:
              limits:
                cpu: "1"
                ephemeral-storage: 10Gi
                memory: 6Gi
              requests:
                cpu: 100m
                ephemeral-storage: 2Gi
                memory: 2Gi
mercury-psr, mercury-provisioning-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/provisioning-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-provisioning-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-queue-agent-info-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/queue-agent-info-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-queue-agent-info-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-realtime-channel-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/realtime-channel-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-realtime-channel-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-resource-channel-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/resource-channel-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-resource-channel-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-resource-state-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/resource-state-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-resource-state-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-resource-work-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/resource-work-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-resource-work-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-routing-processor-agent-assignment-keystore, Secret (v1) has changed:
  # Source: mercury/charts/routing-processor-agent-assignment/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-routing-processor-agent-assignment-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-routing-processor-agent-events-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/routing-processor-agent-events-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-routing-processor-agent-events-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-routing-processor-queue-assignment-keystore, Secret (v1) has changed:
  # Source: mercury/charts/routing-processor-queue-assignment/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-routing-processor-queue-assignment-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-routing-processor-work-events-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/routing-processor-work-events-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-routing-processor-work-events-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-session-housekeeping-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/session-housekeeping-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-session-housekeeping-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-session-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/session-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-session-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-single-sign-on-service-keystore, Secret (v1) has changed:
  # Source: mercury/charts/single-sign-on-service/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-single-sign-on-service-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-social-bridge-keystore, Secret (v1) has changed:
  # Source: mercury/charts/social-bridge/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-social-bridge-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-social-config-keystore, Secret (v1) has changed:
  # Source: mercury/charts/social-config/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-social-config-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-static-assets-service-keystore, Secret (v1) has changed:
  # Source: mercury/charts/static-assets-service/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-static-assets-service-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-tenant-downtime-monitor, Deployment (apps) has changed:
  # Source: mercury/charts/tenant-downtime-monitor/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: mercury-tenant-downtime-monitor
    labels:
      project: mercury
      version: "0"
      app.kubernetes.io/name: tenant-downtime-monitor
      helm.sh/chart: tenant-downtime-monitor-22.01.18-REL21.11-153
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/managed-by: Helm
      monitoring: "true"
    annotations:
  spec:
    replicas: 3
    selector:
      matchLabels:
        app.kubernetes.io/name: tenant-downtime-monitor
        app.kubernetes.io/instance: mercury
    strategy:
      type: RollingUpdate
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
    template:
      metadata:
        annotations:
          vault.security.banzaicloud.io/log-level: warn
          vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
          vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
          vault.security.banzaicloud.io/vault-env-daemon: "true"
          vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
          vault.security.banzaicloud.io/vault-role: mercury-psr
          vault.security.banzaicloud.io/vault-skip-verify: "true"
        labels:
          app.kubernetes.io/name: tenant-downtime-monitor
          app.kubernetes.io/instance: mercury
          chronos.enabled: "True"
          chronos.maxPodLifetime: "1440"
          chronos.minAvailable: "0"
      spec:
        serviceAccountName: mercurygeneric
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - "tenant-downtime-monitor"
                  - key: release
                    operator: In
                    values:
                    - mercury
                topologyKey: "failure-domain.beta.kubernetes.io/zone"
        imagePullSecrets:
          - name: osvcstage-ocirsecret
        volumes:
        - name: vaultca
          secret:
            secretName: vault-tls
        - name: keystore-pass
          secret:
            secretName: mercury-tenant-downtime-monitor-keystore
        - name: keystore-volume
          emptyDir: {}
        initContainers:
        - name: init
          image: iad.ocir.io/osvcstage/mercury/init-container/prod:1.0.0.18
          imagePullPolicy: IfNotPresent
          command: ['/bin/bash', '-c', '/init/init-script.sh']
          # Need to run the init container as root (also we're forced to use its UID, i.e. 0)
          securityContext:
            runAsUser: 0
          env:
          - name: AUTO_REFRESH_CONSUL_TOKEN
            value: vault:cpe_consul/creds/mercury-psr#token
          - name: KAFKA_TEMPLATED_PKI
            value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
              .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
              "ttl": "720h"}'
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: TRUSTSTORE_NAME
            value: truststore.jks
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: props/kafka.properties
          - name: KEYSTORE_NAME
            value: keystore.jks
          - name: SERVICEACCOUNT_SECRETPATH
            value: /var/run/secrets/kubernetes.io/serviceaccount
          - name: VAULT_ADDR
            value: "https://vault.query.prod.consul:8200"
          - name: VAULT_SECRETPATH
            value: /tmp/vaultca
          - name: PKI_VAULT_ENGINE_PATH
            value: "v1/infra_pki/issue/kafka_client"
          - name: CERT_CN
            value: "mercury-psr-mercurygeneric-rw"
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          - name: VAULT_SERVICEACCOUNT
            value: mercurygeneric
          volumeMounts:
          - name: keystore-volume
            mountPath: /keystores
          - name: vaultca
            mountPath: /tmp/vaultca
            readOnly: true
          - name: keystore-pass
            mountPath: /tmp/keystore-pass
            readOnly: true
          resources:
              limits:
                cpu: "1"
                ephemeral-storage: 10Gi
                memory: 6Gi
              requests:
                cpu: 100m
                ephemeral-storage: 2Gi
                memory: 2Gi
        containers:
          - name: tenant-downtime-monitor
            image: "iad.ocir.io/osvcstage/mercury/tenant-downtime-monitor:22.01.18-REL21.11-153"
            imagePullPolicy: IfNotPresent
            env:
            - name: profile
              value: oci-k8s
            - name: DATACENTER
              value: OCI
            - name: RELEASE_NAME
              value: mercury
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: POD_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            - name: MERCURY_URL_PATTERN
              value: https://engagement-psr.us1.channels.ocs.oraclecloud.com/mercury-psr
            - name: JAEGER_REMOTE_REPORTER
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: VAULT_TOKEN
              value: vault:login
            - name: HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: CONSUL_HTTP_ADDR
              value: https://$(HOST_IP):8501
            - name: CONSUL_TOKEN
              value: vault:/cpe_consul/creds/mercury-psr#token
            - name: JAVA_OPTS
              value: "-Dvault.enabled=true \
                      -Dvault.namespace=mercury-psr \
                      -Dvault.transit.kafkaMessageEncryption.path=transit_mercury \
                      -Dvault.base.address=https://vault.query.prod.consul:8200 \
                      -Dvault.shared.kv.engine=/kv \
                      -Dthidwick.vault.uri=https://vault.query.prod.consul:8200 \
                      -Dthidwick.vault.transit.cache-expiration=300 \
                      -Dthidwick.vault.kv.cache-expiration=300 \
                      -Dthidwick.vault.serviceaccount=mercurygeneric \
                      -Dlog4j2.formatMsgNoLookups=true \
                      -Dmercury.cpeVersion=v2
                      -Dfile.token.enabled=true \
                      -Dvault.authentication=TOKEN \
                      -Dvault.role=mercury-psr \
                      -Dvault.path=k8s-phx-dataplane
                      -Dkafka.log_level=WARN \
                      -Dthidwick-server.logback.level=INFO \
                      -Dthidwick-server.logback.service.log_level=DEBUG \
                      -Dthidwick.kafka.message.encryption.cipher=AES/GCM \
                      -Djwt.tenant.validation.enabled=false \
                      -Dthidwick.kafka.schema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul \
                      -Dthidwick.kafka.bootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443 \
                      -Dthidwick.tms.base.tmsStateStream.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443 \
                      -Djavax.net.ssl.trustStore=/keystores/truststore.jks \
                      -Djavax.net.ssl.keyStore=/keystores/keystore.jks \
                      -Dhttp.security.headers.enabled=true \
                      -Dhttp.rest.security.headers.enabled=true \
-                     -XX:+UseG1GC -XX:MaxGCPauseMillis=250 \
-                     -Xms1536m \
+                     -Dwebhook.death.topic.name=helios-psr.death.event -Dwebhook.base.url=http://authentication-service-helios.helios-psr.svc.cluster.local:80                    -Xms1536m \
                      -Xmx4096m \
                      -Dmercury.tenant.name.filter=\"^(evai)\"                 -Dtenant.realm=engagement-psr.us1.channels.ocs.oraclecloud.com "
            volumeMounts:
            - mountPath: /keystores
              name: keystore-volume
            - mountPath: /tmp/keystore-pass
              name: keystore-pass
            ports:
              - name: http
                containerPort: 8080
                protocol: TCP
            startupProbe:
              periodSeconds: 10
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 120
              httpGet:
                path: /health/checks
                port: 8080
            livenessProbe:
              initialDelaySeconds: 120
              periodSeconds: 30
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 1
              httpGet:
                path: /health/checks
                port: 8080
            readinessProbe:
              initialDelaySeconds: 120
              periodSeconds: 30
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 1
              httpGet:
                path: /health/ping
                port: 8080
            # Service deployment provided resources
            resources:
              limits:
                cpu: "1"
                ephemeral-storage: 10Gi
                memory: 6Gi
              requests:
                cpu: 100m
                ephemeral-storage: 2Gi
                memory: 2Gi
mercury-psr, mercury-tenant-downtime-monitor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/tenant-downtime-monitor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-tenant-downtime-monitor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-transcript-api-keystore, Secret (v1) has changed:
  # Source: mercury/charts/transcript-api/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-transcript-api-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-transcript-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/transcript-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-transcript-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-user-preference-service-keystore, Secret (v1) has changed:
  # Source: mercury/charts/user-preference-service/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-user-preference-service-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-work-api-keystore, Secret (v1) has changed:
  # Source: mercury/charts/work-api/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-work-api-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-work-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/work-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-work-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque


[21:January:2022:07:43:18]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgc
CURRENT   NAME                               CLUSTER               AUTHINFO           NAMESPACE
          prod_ap-hyderabad-1_controlplane   cluster-cbjr52qcfoa   user-cbjr52qcfoa
          prod_ap-hyderabad-1_dataplane      cluster-cpj2zxktnda   user-cpj2zxktnda
          prod_ap-melbourne-1_controlplane   cluster-czgcn3bmjtg   user-czgcn3bmjtg
          prod_ap-melbourne-1_dataplane      cluster-c4dazdggfst   user-c4dazdggfst
          prod_ap-mumbai-1_controlplane      cluster-cjiru3mifia   user-cjiru3mifia
          prod_ap-mumbai-1_dataplane         cluster-ci3ypr6tmga   user-ci3ypr6tmga
          prod_ap-osaka-1_controlplane       cluster-cp5327xtmfa   user-cp5327xtmfa
          prod_ap-osaka-1_dataplane          cluster-cqldck6xatq   user-cqldck6xatq
          prod_ap-sydney-1_controlplane      cluster-c4wembugqyt   user-c4wembugqyt
          prod_ap-sydney-1_dataplane         cluster-cswezbzgyyg   user-cswezbzgyyg
          prod_ap-tokyo-1_controlplane       cluster-cz54pc3z4wa   user-cz54pc3z4wa
          prod_ap-tokyo-1_dataplane          cluster-cyjtbtbbjfq   user-cyjtbtbbjfq
          prod_ca-montreal-1_controlplane    cluster-czdoolemnqw   user-czdoolemnqw
          prod_ca-montreal-1_dataplane       cluster-cqtkytbgq3d   user-cqtkytbgq3d
          prod_ca-toronto-1_controlplane     cluster-cygcmjxhfsd   user-cygcmjxhfsd
          prod_ca-toronto-1_dataplane        cluster-cywmodfmm3d   user-cywmodfmm3d
          prod_eu-amsterdam-1_controlplane   cluster-c4tkmjwgbrt   user-c4tkmjwgbrt
          prod_eu-amsterdam-1_dataplane      cluster-cstsy3bmjrd   user-cstsy3bmjrd
          prod_eu-frankfurt-1_controlplane   cluster-cydcytcg43d   user-cydcytcg43d
          prod_eu-frankfurt-1_dataplane      cluster-crdgzbrmjsd   user-crdgzbrmjsd
          prod_eu-zurich-1_controlplane      cluster-cwmoxn3bfwq   user-cwmoxn3bfwq
          prod_eu-zurich-1_dataplane         cluster-chtoe5bnsza   user-chtoe5bnsza
          prod_me-dubai-1_controlplane       cluster-ce5puidc4ya   user-ce5puidc4ya
          prod_me-dubai-1_dataplane          cluster-c77hcd3vizq   user-c77hcd3vizq
          prod_me-jeddah-1_controlplane      cluster-cpzagsoukeq   user-cpzagsoukeq
          prod_me-jeddah-1_dataplane         cluster-cf4f222mrfa   user-cf4f222mrfa
          prod_sa-santiago-1_controlplane    cluster-czxpimgibeq   user-czxpimgibeq
          prod_sa-santiago-1_dataplane       cluster-cuutxfvj2ea   user-cuutxfvj2ea
          prod_sa-saopaulo-1_controlplane    cluster-cc75n7hujpq   user-cc75n7hujpq
          prod_sa-saopaulo-1_dataplane       cluster-cpjqrjtbotq   user-cpjqrjtbotq
          prod_sa-vinhedo-1_controlplane     cluster-cihx6aprjua   user-cihx6aprjua
          prod_sa-vinhedo-1_dataplane        cluster-ciqmbmn6wta   user-ciqmbmn6wta
          prod_uk-cardiff-1_controlplane     cluster-ctggmtbmfrg   user-ctggmtbmfrg
          prod_uk-cardiff-1_dataplane        cluster-cqwmnrqmztd   user-cqwmnrqmztd
          prod_uk-london-1_controlplane      cluster-cywgnlcmqzd   user-cywgnlcmqzd
          prod_uk-london-1_dataplane         cluster-c3tazjsmjsd   user-c3tazjsmjsd
          prod_us-ashburn-1_controlplane     cluster-czwczjzgrsw   user-czwczjzgrsw
          prod_us-ashburn-1_dataplane        cluster-c3wkyjyguzt   user-c3wkyjyguzt
          prod_us-phoenix-1_controlplane     cluster-c3tkobugrst   user-c3tkobugrst
*         prod_us-phoenix-1_dataplane        cluster-csdqylfmi2g   user-csdqylfmi2g
          prod_us-phoenix-1_deployment       cluster-c4tgolgmjsd   user-c4tgolgmjsd
[21:January:2022:07:57:19]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kuc prod_uk-london-1_dataplane
Switched to context "prod_uk-london-1_dataplane".
[21:January:2022:07:57:25]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ rp
[21:January:2022:07:57:26]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgpn mercury
NAME                                                              READY   STATUS    RESTARTS   AGE     IP              NODE           NOMINATED NODE   READINESS GATES
mercury-agent-command-service-9bdc9f47c-dd48h                     2/2     Running   0          7d12h   10.245.10.158   10.20.48.129   <none>           <none>
mercury-agent-command-service-9bdc9f47c-jsmxl                     2/2     Running   0          7d12h   10.245.8.3      10.20.49.88    <none>           <none>
mercury-agent-command-service-9bdc9f47c-t7whb                     2/2     Running   0          7d12h   10.245.9.94     10.20.48.66    <none>           <none>
mercury-consumer-command-service-689b975bcd-hwk5t                 2/2     Running   0          7d12h   10.245.8.203    10.20.48.252   <none>           <none>
mercury-consumer-command-service-689b975bcd-nbpvs                 2/2     Running   0          7d12h   10.245.6.25     10.20.48.34    <none>           <none>
mercury-consumer-command-service-689b975bcd-zvbh2                 2/2     Running   0          7d12h   10.245.9.95     10.20.48.66    <none>           <none>
mercury-custom-availability-service-7d45d57fd9-c8p7q              2/2     Running   0          7d7h    10.245.9.104    10.20.48.66    <none>           <none>
mercury-custom-availability-service-7d45d57fd9-n5h2c              2/2     Running   0          7d12h   10.245.6.26     10.20.48.34    <none>           <none>
mercury-custom-availability-service-7d45d57fd9-np57l              2/2     Running   0          7d12h   10.245.8.204    10.20.48.252   <none>           <none>
mercury-data-mask-api-ddf65cfc6-6zl5k                             2/2     Running   0          7d12h   10.245.11.66    10.20.49.80    <none>           <none>
mercury-data-mask-api-ddf65cfc6-cbfbq                             2/2     Running   0          7d12h   10.245.7.244    10.20.48.86    <none>           <none>
mercury-data-mask-api-ddf65cfc6-zqlhm                             2/2     Running   0          7d12h   10.245.8.205    10.20.48.252   <none>           <none>
mercury-engagement-queue-api-767969575d-gmk88                     2/2     Running   0          7d12h   10.245.11.67    10.20.49.80    <none>           <none>
mercury-engagement-queue-api-767969575d-lcrwh                     2/2     Running   0          7d12h   10.245.8.206    10.20.48.252   <none>           <none>
mercury-engagement-queue-api-767969575d-mjqvx                     2/2     Running   0          7d12h   10.245.7.245    10.20.48.86    <none>           <none>
mercury-enrichment-service-6788b45457-7rgvh                       2/2     Running   0          7d12h   10.245.11.68    10.20.49.80    <none>           <none>
mercury-enrichment-service-6788b45457-f99gj                       2/2     Running   0          7d12h   10.245.8.207    10.20.48.252   <none>           <none>
mercury-enrichment-service-6788b45457-wg9rp                       2/2     Running   0          7d12h   10.245.6.216    10.20.49.200   <none>           <none>
mercury-event-sync-service-7d74958777-2vj26                       2/2     Running   0          7d11h   10.245.11.70    10.20.49.80    <none>           <none>
mercury-event-sync-service-7d74958777-dvztj                       2/2     Running   0          7d11h   10.245.8.208    10.20.48.252   <none>           <none>
mercury-event-sync-service-7d74958777-ns69q                       2/2     Running   0          7d11h   10.245.7.246    10.20.48.86    <none>           <none>
mercury-integration-in-processor-767dcd596-5fxbf                  2/2     Running   0          7d11h   10.245.7.247    10.20.48.86    <none>           <none>
mercury-integration-in-processor-767dcd596-5l57n                  2/2     Running   0          7d11h   10.245.8.209    10.20.48.252   <none>           <none>
mercury-integration-in-processor-767dcd596-vssnk                  2/2     Running   6          7d7h    10.245.11.98    10.20.49.80    <none>           <none>
mercury-integration-out-processor-66c7f4fc5-f2mvl                 2/2     Running   0          7d11h   10.245.11.72    10.20.49.80    <none>           <none>
mercury-integration-out-processor-66c7f4fc5-k6vkz                 2/2     Running   0          7d11h   10.245.8.210    10.20.48.252   <none>           <none>
mercury-integration-out-processor-66c7f4fc5-snfwp                 2/2     Running   0          7d11h   10.245.7.248    10.20.48.86    <none>           <none>
mercury-kafka-dr-kafka-0                                          2/2     Running   0          48d     10.245.7.212    10.20.48.86    <none>           <none>
mercury-kafka-dr-kafka-1                                          2/2     Running   0          48d     10.245.9.192    10.20.48.184   <none>           <none>
mercury-kafka-dr-kafka-2                                          2/2     Running   0          7d7h    10.245.1.198    10.20.49.231   <none>           <none>
mercury-kafka-dr-kafka-3                                          2/2     Running   0          48d     10.245.7.210    10.20.48.86    <none>           <none>
mercury-kafka-dr-kafka-4                                          2/2     Running   0          48d     10.245.9.193    10.20.48.184   <none>           <none>
mercury-kafka-dr-kafka-5                                          2/2     Running   0          48d     10.245.1.185    10.20.49.231   <none>           <none>
mercury-kafka-dr-kafka-6                                          2/2     Running   0          48d     10.245.7.211    10.20.48.86    <none>           <none>
mercury-kafka-dr-kafka-7                                          2/2     Running   0          48d     10.245.9.194    10.20.48.184   <none>           <none>
mercury-kafka-dr-kafka-8                                          2/2     Running   0          48d     10.245.1.186    10.20.49.231   <none>           <none>
mercury-kafka-dr-kafka-exporter-6c6bb77b55-m9g7k                  1/1     Running   0          58d     10.245.6.167    10.20.49.200   <none>           <none>
mercury-kafka-dr-zookeeper-0                                      1/1     Running   0          42d     10.245.7.213    10.20.48.86    <none>           <none>
mercury-kafka-dr-zookeeper-1                                      1/1     Running   0          42d     10.245.10.45    10.20.48.38    <none>           <none>
mercury-kafka-dr-zookeeper-2                                      1/1     Running   1          7d7h    10.245.1.201    10.20.49.231   <none>           <none>
mercury-kafka-entity-operator-766584f5c5-6qf66                    3/3     Running   0          58d     10.245.7.176    10.20.48.86    <none>           <none>
mercury-kafka-kafka-0                                             2/2     Running   0          48d     10.245.7.209    10.20.48.86    <none>           <none>
mercury-kafka-kafka-1                                             2/2     Running   0          48d     10.245.10.40    10.20.48.38    <none>           <none>
mercury-kafka-kafka-2                                             2/2     Running   0          48d     10.245.1.188    10.20.49.231   <none>           <none>
mercury-kafka-kafka-3                                             2/2     Running   0          48d     10.245.6.203    10.20.49.200   <none>           <none>
mercury-kafka-kafka-4                                             2/2     Running   0          48d     10.245.9.195    10.20.48.184   <none>           <none>
mercury-kafka-kafka-5                                             2/2     Running   0          48d     10.245.1.183    10.20.49.231   <none>           <none>
mercury-kafka-kafka-6                                             2/2     Running   0          48d     10.245.6.202    10.20.49.200   <none>           <none>
mercury-kafka-kafka-7                                             2/2     Running   0          48d     10.245.10.41    10.20.48.38    <none>           <none>
mercury-kafka-kafka-8                                             2/2     Running   0          48d     10.245.1.184    10.20.49.231   <none>           <none>
mercury-kafka-kafka-exporter-76c799656d-hwl8g                     1/1     Running   0          58d     10.245.8.104    10.20.49.88    <none>           <none>
mercury-kafka-mm2-dr-uk-london-1-mirrormaker2-b5b7b657-4q5zn      1/1     Running   16         35d     10.245.7.233    10.20.48.86    <none>           <none>
mercury-kafka-mm2-dr-uk-london-1-mirrormaker2-b5b7b657-wpvpr      1/1     Running   16         35d     10.245.10.48    10.20.48.38    <none>           <none>
mercury-kafka-zookeeper-0                                         1/1     Running   0          58d     10.245.8.101    10.20.49.88    <none>           <none>
mercury-kafka-zookeeper-1                                         1/1     Running   0          58d     10.245.10.31    10.20.48.38    <none>           <none>
mercury-kafka-zookeeper-2                                         1/1     Running   0          7d7h    10.245.1.196    10.20.49.231   <none>           <none>
mercury-kweet-facebook-client-75964cd9cc-bwjcb                    2/2     Running   0          7d11h   10.245.11.75    10.20.49.80    <none>           <none>
mercury-kweet-facebook-client-75964cd9cc-dk47q                    2/2     Running   0          7d11h   10.245.6.27     10.20.48.34    <none>           <none>
mercury-kweet-facebook-client-75964cd9cc-msn66                    2/2     Running   0          7d11h   10.245.10.159   10.20.48.129   <none>           <none>
mercury-kweet-facebook-webhook-7468df497-552gs                    2/2     Running   0          7d11h   10.245.11.76    10.20.49.80    <none>           <none>
mercury-kweet-facebook-webhook-7468df497-w6857                    2/2     Running   0          7d11h   10.245.6.217    10.20.49.200   <none>           <none>
mercury-kweet-facebook-webhook-7468df497-zff8t                    2/2     Running   0          7d11h   10.245.8.211    10.20.48.252   <none>           <none>
mercury-kweet-twiliosms-client-c4bc4b8d6-64pmp                    2/2     Running   0          7d11h   10.245.11.77    10.20.49.80    <none>           <none>
mercury-kweet-twiliosms-client-c4bc4b8d6-7wj5t                    2/2     Running   0          7d11h   10.245.10.160   10.20.48.129   <none>           <none>
mercury-kweet-twiliosms-client-c4bc4b8d6-kjr2k                    2/2     Running   0          7d11h   10.245.7.249    10.20.48.86    <none>           <none>
mercury-kweet-userprofiles-9ddc46448-5q425                        2/2     Running   0          7d11h   10.245.11.78    10.20.49.80    <none>           <none>
mercury-kweet-userprofiles-9ddc46448-tpbzz                        2/2     Running   0          7d11h   10.245.6.28     10.20.48.34    <none>           <none>
mercury-kweet-userprofiles-9ddc46448-zrv96                        2/2     Running   0          7d11h   10.245.8.212    10.20.48.252   <none>           <none>
mercury-mercury-ui-86568bc5f4-cdfmb                               2/2     Running   0          7d11h   10.245.11.79    10.20.49.80    <none>           <none>
mercury-mercury-ui-86568bc5f4-cm476                               2/2     Running   0          7d11h   10.245.7.250    10.20.48.86    <none>           <none>
mercury-mercury-ui-86568bc5f4-tj5xv                               2/2     Running   0          7d11h   10.245.10.161   10.20.48.129   <none>           <none>
mercury-metric-aggregation-processor-6c588bcdf4-lmsxt             2/2     Running   0          7d11h   10.245.11.80    10.20.49.80    <none>           <none>
mercury-metric-aggregation-processor-6c588bcdf4-p4wzs             2/2     Running   0          7d11h   10.245.10.162   10.20.48.129   <none>           <none>
mercury-metric-aggregation-processor-6c588bcdf4-w92rx             2/2     Running   0          7d11h   10.245.7.251    10.20.48.86    <none>           <none>
mercury-metric-fusion-bridge-6cddcfc48b-bpggb                     2/2     Running   0          7d10h   10.245.11.81    10.20.49.80    <none>           <none>
mercury-metric-fusion-bridge-6cddcfc48b-d5272                     2/2     Running   0          7d11h   10.245.6.29     10.20.48.34    <none>           <none>
mercury-metric-fusion-bridge-6cddcfc48b-pwd9v                     2/2     Running   0          7d11h   10.245.8.213    10.20.48.252   <none>           <none>
mercury-metric-generation-processor-7bcc45fd94-5k8n8              2/2     Running   0          7d10h   10.245.1.194    10.20.49.231   <none>           <none>
mercury-metric-generation-processor-7bcc45fd94-pjj7w              2/2     Running   0          7d10h   10.245.7.252    10.20.48.86    <none>           <none>
mercury-metric-generation-processor-7bcc45fd94-sm9mj              2/2     Running   0          7d10h   10.245.8.214    10.20.48.252   <none>           <none>
mercury-metric-internal-translation-processor-6b9787b4-2wtbm      2/2     Running   0          7d10h   10.245.11.82    10.20.49.80    <none>           <none>
mercury-metric-internal-translation-processor-6b9787b4-8lxcd      2/2     Running   0          7d10h   10.245.10.163   10.20.48.129   <none>           <none>
mercury-metric-internal-translation-processor-6b9787b4-sqvtn      2/2     Running   0          7d10h   10.245.6.30     10.20.48.34    <none>           <none>
mercury-metric-proxy-service-7c889f58df-76tbb                     2/2     Running   5          7d7h    10.245.11.103   10.20.49.80    <none>           <none>
mercury-metric-proxy-service-7c889f58df-dmqqc                     2/2     Running   0          7d10h   10.245.7.253    10.20.48.86    <none>           <none>
mercury-metric-proxy-service-7c889f58df-dsg4v                     2/2     Running   0          7d10h   10.245.8.215    10.20.48.252   <none>           <none>
mercury-omnichannel-assignment-processor-5bc776ff65-8jq5n         2/2     Running   0          7d10h   10.245.8.216    10.20.48.252   <none>           <none>
mercury-omnichannel-assignment-processor-5bc776ff65-kn77p         2/2     Running   0          7d10h   10.245.11.83    10.20.49.80    <none>           <none>
mercury-omnichannel-assignment-processor-5bc776ff65-w8cbv         2/2     Running   0          7d10h   10.245.7.254    10.20.48.86    <none>           <none>
mercury-omnichannel-offer-processor-dfd4fcbbb-rd8sh               2/2     Running   0          7d10h   10.245.8.4      10.20.49.88    <none>           <none>
mercury-omnichannel-offer-processor-dfd4fcbbb-sq5sr               2/2     Running   0          7d10h   10.245.10.164   10.20.48.129   <none>           <none>
mercury-omnichannel-offer-processor-dfd4fcbbb-vxzfz               2/2     Running   0          7d10h   10.245.11.84    10.20.49.80    <none>           <none>
mercury-osvc-bridge-api-services-d6d7bcd98-gtx29                  2/2     Running   0          7d10h   10.245.11.85    10.20.49.80    <none>           <none>
mercury-osvc-bridge-api-services-d6d7bcd98-l8hw5                  2/2     Running   0          7d10h   10.245.6.31     10.20.48.34    <none>           <none>
mercury-osvc-bridge-api-services-d6d7bcd98-t8rnk                  2/2     Running   0          7d10h   10.245.10.165   10.20.48.129   <none>           <none>
mercury-osvc-bridge-metrics-data-pipeline-bd8757b8f-2zggp         2/2     Running   0          7d10h   10.245.6.32     10.20.48.34    <none>           <none>
mercury-osvc-bridge-metrics-data-pipeline-bd8757b8f-jsm9d         2/2     Running   0          7d10h   10.245.11.86    10.20.49.80    <none>           <none>
mercury-osvc-bridge-metrics-data-pipeline-bd8757b8f-zmljk         2/2     Running   0          7d10h   10.245.10.166   10.20.48.129   <none>           <none>
mercury-osvc-bridge-osvc-data-extractor-79d69847fc-5vfzw          2/2     Running   0          7d10h   10.245.10.167   10.20.48.129   <none>           <none>
mercury-osvc-bridge-osvc-data-extractor-79d69847fc-px67b          2/2     Running   0          7d10h   10.245.11.87    10.20.49.80    <none>           <none>
mercury-osvc-bridge-osvc-data-extractor-79d69847fc-zcj5g          2/2     Running   0          7d10h   10.245.6.33     10.20.48.34    <none>           <none>
mercury-osvc-bridge-provisioning-processor-6f94cbdbc-92qkq        2/2     Running   0          7d10h   10.245.8.217    10.20.48.252   <none>           <none>
mercury-osvc-bridge-provisioning-processor-6f94cbdbc-s5qn6        2/2     Running   0          7d9h    10.245.6.34     10.20.48.34    <none>           <none>
mercury-osvc-bridge-provisioning-processor-6f94cbdbc-x7k8m        2/2     Running   0          7d9h    10.245.11.88    10.20.49.80    <none>           <none>
mercury-osvc-bridge-state-processor-796cf55dcd-hvvrc              2/2     Running   0          7d9h    10.245.11.89    10.20.49.80    <none>           <none>
mercury-osvc-bridge-state-processor-796cf55dcd-m2jrz              2/2     Running   0          7d9h    10.245.6.35     10.20.48.34    <none>           <none>
mercury-osvc-bridge-state-processor-796cf55dcd-wcrfm              2/2     Running   0          7d9h    10.245.10.168   10.20.48.129   <none>           <none>
mercury-osvc-bridge-state-query-service-5f74d5b44d-5wcmz          2/2     Running   0          7d9h    10.245.9.96     10.20.48.66    <none>           <none>
mercury-osvc-bridge-state-query-service-5f74d5b44d-6vsq6          2/2     Running   0          7d9h    10.245.7.131    10.20.48.86    <none>           <none>
mercury-osvc-bridge-state-query-service-5f74d5b44d-cnzbp          2/2     Running   0          7d9h    10.245.10.169   10.20.48.129   <none>           <none>
mercury-osvc-bridge-task-controller-5c789b5b6f-mr7v7              2/2     Running   0          7d9h    10.245.8.218    10.20.48.252   <none>           <none>
mercury-osvc-bridge-task-controller-5c789b5b6f-qgtcs              2/2     Running   0          7d9h    10.245.7.132    10.20.48.86    <none>           <none>
mercury-osvc-bridge-task-controller-5c789b5b6f-rrts8              2/2     Running   0          7d9h    10.245.11.90    10.20.49.80    <none>           <none>
mercury-provisioning-monitor-8b8ff74-8qdcq                        2/2     Running   0          7d9h    10.245.6.218    10.20.49.200   <none>           <none>
mercury-provisioning-monitor-8b8ff74-khv2x                        2/2     Running   0          7d9h    10.245.10.170   10.20.48.129   <none>           <none>
mercury-provisioning-monitor-8b8ff74-ktxsd                        2/2     Running   0          7d9h    10.245.11.91    10.20.49.80    <none>           <none>
mercury-provisioning-processor-65f45f9844-nr4zx                   2/2     Running   0          7d9h    10.245.11.92    10.20.49.80    <none>           <none>
mercury-provisioning-processor-65f45f9844-rdcsg                   2/2     Running   0          7d9h    10.245.8.219    10.20.48.252   <none>           <none>
mercury-provisioning-processor-65f45f9844-xfbfb                   2/2     Running   0          7d9h    10.245.6.36     10.20.48.34    <none>           <none>
mercury-queue-agent-info-processor-6f9746c5cf-c65lh               2/2     Running   0          7d9h    10.245.8.5      10.20.49.88    <none>           <none>
mercury-queue-agent-info-processor-6f9746c5cf-hhc47               2/2     Running   0          7d9h    10.245.8.220    10.20.48.252   <none>           <none>
mercury-queue-agent-info-processor-6f9746c5cf-r7lvs               2/2     Running   6          7d7h    10.245.9.103    10.20.48.66    <none>           <none>
mercury-realtime-channel-processor-5d474b9956-f8s8v               2/2     Running   0          7d9h    10.245.6.37     10.20.48.34    <none>           <none>
mercury-realtime-channel-processor-5d474b9956-m9cxm               2/2     Running   5          7d7h    10.245.9.102    10.20.48.66    <none>           <none>
mercury-realtime-channel-processor-5d474b9956-pb95b               2/2     Running   0          7d9h    10.245.10.171   10.20.48.129   <none>           <none>
mercury-resource-channel-processor-657d7d67f4-hkmx4               2/2     Running   7          7d9h    10.245.8.221    10.20.48.252   <none>           <none>
mercury-resource-channel-processor-657d7d67f4-mmfkh               2/2     Running   0          7d9h    10.245.6.38     10.20.48.34    <none>           <none>
mercury-resource-channel-processor-657d7d67f4-xll9c               2/2     Running   11         7d7h    10.245.11.102   10.20.49.80    <none>           <none>
mercury-resource-state-processor-755478b579-2ptbd                 2/2     Running   0          7d9h    10.245.8.222    10.20.48.252   <none>           <none>
mercury-resource-state-processor-755478b579-cwr7l                 2/2     Running   0          7d9h    10.245.6.39     10.20.48.34    <none>           <none>
mercury-resource-state-processor-755478b579-tl8x4                 2/2     Running   0          7d9h    10.245.11.93    10.20.49.80    <none>           <none>
mercury-resource-work-processor-5cfc5f5bc7-gc69v                  2/2     Running   0          7d8h    10.245.11.94    10.20.49.80    <none>           <none>
mercury-resource-work-processor-5cfc5f5bc7-j7xzn                  2/2     Running   0          7d8h    10.245.6.40     10.20.48.34    <none>           <none>
mercury-resource-work-processor-5cfc5f5bc7-rhqsl                  2/2     Running   0          7d8h    10.245.8.223    10.20.48.252   <none>           <none>
mercury-routing-processor-agent-assignment-858c65d56f-7dhtx       2/2     Running   3          7d8h    10.245.8.224    10.20.48.252   <none>           <none>
mercury-routing-processor-agent-assignment-858c65d56f-mkjth       2/2     Running   6          7d7h    10.245.11.96    10.20.49.80    <none>           <none>
mercury-routing-processor-agent-assignment-858c65d56f-x46xf       2/2     Running   0          7d8h    10.245.6.41     10.20.48.34    <none>           <none>
mercury-routing-processor-agent-events-processor-59fc4d7d54jtxq   2/2     Running   0          7d8h    10.245.7.133    10.20.48.86    <none>           <none>
mercury-routing-processor-agent-events-processor-59fc4d7d5jhbd9   2/2     Running   0          7d8h    10.245.10.172   10.20.48.129   <none>           <none>
mercury-routing-processor-agent-events-processor-59fc4d7d5m9d98   2/2     Running   5          7d7h    10.245.11.100   10.20.49.80    <none>           <none>
mercury-routing-processor-queue-assignment-7b78db95cf-8zdg8       2/2     Running   0          7d8h    10.245.9.97     10.20.48.66    <none>           <none>
mercury-routing-processor-queue-assignment-7b78db95cf-ngkdq       2/2     Running   0          7d8h    10.245.7.134    10.20.48.86    <none>           <none>
mercury-routing-processor-queue-assignment-7b78db95cf-xdwrb       2/2     Running   0          7d8h    10.245.8.225    10.20.48.252   <none>           <none>
mercury-routing-processor-work-events-processor-7989cd4d46cv6j7   2/2     Running   0          7d8h    10.245.9.98     10.20.48.66    <none>           <none>
mercury-routing-processor-work-events-processor-7989cd4d46dh8kf   2/2     Running   0          7d8h    10.245.6.42     10.20.48.34    <none>           <none>
mercury-routing-processor-work-events-processor-7989cd4d46kzncs   2/2     Running   0          7d8h    10.245.8.226    10.20.48.252   <none>           <none>
mercury-session-housekeeping-processor-85cbb46fdd-b5gpn           2/2     Running   0          7d8h    10.245.10.173   10.20.48.129   <none>           <none>
mercury-session-housekeeping-processor-85cbb46fdd-mm8fb           2/2     Running   5          7d7h    10.245.11.97    10.20.49.80    <none>           <none>
mercury-session-housekeeping-processor-85cbb46fdd-qfjnp           2/2     Running   2          7d8h    10.245.6.43     10.20.48.34    <none>           <none>
mercury-session-processor-89dfd7468-7vbhd                         2/2     Running   0          7d8h    10.245.9.99     10.20.48.66    <none>           <none>
mercury-session-processor-89dfd7468-nntwg                         2/2     Running   0          7d8h    10.245.8.227    10.20.48.252   <none>           <none>
mercury-session-processor-89dfd7468-thmcm                         2/2     Running   0          7d8h    10.245.6.219    10.20.49.200   <none>           <none>
mercury-single-sign-on-service-d5d5f66f5-4ph6r                    2/2     Running   0          7d8h    10.245.8.228    10.20.48.252   <none>           <none>
mercury-single-sign-on-service-d5d5f66f5-f5mnf                    2/2     Running   6          7d7h    10.245.9.105    10.20.48.66    <none>           <none>
mercury-single-sign-on-service-d5d5f66f5-kvn8p                    2/2     Running   0          7d8h    10.245.7.135    10.20.48.86    <none>           <none>
mercury-social-bridge-67c94785dd-d5pr9                            2/2     Running   0          7d8h    10.245.6.44     10.20.48.34    <none>           <none>
mercury-social-bridge-67c94785dd-n4kps                            2/2     Running   5          7d7h    10.245.11.99    10.20.49.80    <none>           <none>
mercury-social-bridge-67c94785dd-w9bsp                            2/2     Running   0          7d8h    10.245.8.230    10.20.48.252   <none>           <none>
mercury-social-config-85bd7c465c-nlvxw                            2/2     Running   0          7d8h    10.245.7.136    10.20.48.86    <none>           <none>
mercury-social-config-85bd7c465c-rzdmh                            2/2     Running   0          7d8h    10.245.10.175   10.20.48.129   <none>           <none>
mercury-social-config-85bd7c465c-v8mpg                            2/2     Running   0          7d8h    10.245.11.95    10.20.49.80    <none>           <none>
mercury-static-assets-service-74ddc5dd7c-mn66w                    2/2     Running   0          7d8h    10.245.6.45     10.20.48.34    <none>           <none>
mercury-static-assets-service-74ddc5dd7c-nmxmn                    2/2     Running   1          7d7h    10.245.9.106    10.20.48.66    <none>           <none>
mercury-static-assets-service-74ddc5dd7c-xc9hr                    2/2     Running   0          7d7h    10.245.8.231    10.20.48.252   <none>           <none>
mercury-tenant-downtime-monitor-7894f4d6f6-5jgt7                  2/2     Running   0          7d7h    10.245.6.46     10.20.48.34    <none>           <none>
mercury-tenant-downtime-monitor-7894f4d6f6-dhfl6                  2/2     Running   0          7d7h    10.245.10.177   10.20.48.129   <none>           <none>
mercury-tenant-downtime-monitor-7894f4d6f6-mrt9z                  2/2     Running   0          7d7h    10.245.9.107    10.20.48.66    <none>           <none>
mercury-transcript-api-77ccfc866b-7mlc5                           2/2     Running   0          7d7h    10.245.8.232    10.20.48.252   <none>           <none>
mercury-transcript-api-77ccfc866b-jb6qg                           2/2     Running   0          7d7h    10.245.6.47     10.20.48.34    <none>           <none>
mercury-transcript-api-77ccfc866b-mncll                           2/2     Running   0          7d7h    10.245.0.5      10.20.49.192   <none>           <none>
mercury-transcript-processor-5c88b77b56-qltcc                     2/2     Running   1          7d7h    10.245.6.220    10.20.49.200   <none>           <none>
mercury-transcript-processor-5c88b77b56-rfcpc                     2/2     Running   0          7d7h    10.245.8.233    10.20.48.252   <none>           <none>
mercury-transcript-processor-5c88b77b56-x7mtr                     2/2     Running   0          7d7h    10.245.0.6      10.20.49.192   <none>           <none>
mercury-user-preference-service-5fb4785694-j6h45                  2/2     Running   0          7d7h    10.245.7.138    10.20.48.86    <none>           <none>
mercury-user-preference-service-5fb4785694-vf26l                  2/2     Running   0          7d7h    10.245.10.178   10.20.48.129   <none>           <none>
mercury-user-preference-service-5fb4785694-z9q6d                  2/2     Running   6          7d7h    10.245.11.101   10.20.49.80    <none>           <none>
mercury-work-api-8c9456c64-8trqt                                  2/2     Running   6          7d7h    10.245.9.100    10.20.48.66    <none>           <none>
mercury-work-api-8c9456c64-v46lm                                  2/2     Running   0          7d7h    10.245.6.48     10.20.48.34    <none>           <none>
mercury-work-api-8c9456c64-w26j5                                  2/2     Running   0          7d7h    10.245.10.179   10.20.48.129   <none>           <none>
mercury-work-processor-64d9c6cf4-8h8hh                            2/2     Running   0          7d7h    10.245.0.7      10.20.49.192   <none>           <none>
mercury-work-processor-64d9c6cf4-pb82j                            2/2     Running   0          7d7h    10.245.6.49     10.20.48.34    <none>           <none>
mercury-work-processor-64d9c6cf4-pcmln                            2/2     Running   0          7d7h    10.245.10.180   10.20.48.129   <none>           <none>
[21:January:2022:07:57:38]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ #kubectl port-forward mercury-provisioning-processor-65f45f9844-nr4zx 7001:8080
[21:January:2022:07:58:05]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgn mercury services
NAME                                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                               AGE
mercury-agent-command-service                       ClusterIP   10.97.249.30    <none>        8080/TCP                              171d
mercury-channel-api                                 ClusterIP   10.97.87.218    <none>        8080/TCP                              58d
mercury-consumer-command-service                    ClusterIP   10.97.232.183   <none>        8080/TCP                              171d
mercury-custom-availability-service                 ClusterIP   10.97.93.61     <none>        8080/TCP                              96d
mercury-data-mask-api                               ClusterIP   10.97.112.50    <none>        8080/TCP                              171d
mercury-engagement-queue-api                        ClusterIP   10.97.10.217    <none>        8080/TCP                              171d
mercury-enrichment-service                          ClusterIP   10.97.89.26     <none>        8080/TCP                              171d
mercury-event-sync-service                          ClusterIP   10.97.184.161   <none>        8080/TCP                              171d
mercury-integration-in-processor                    ClusterIP   10.97.142.25    <none>        8080/TCP                              171d
mercury-integration-out-processor                   ClusterIP   10.97.170.14    <none>        8080/TCP                              171d
mercury-kafka-dr-kafka-0                            ClusterIP   10.97.49.225    <none>        9094/TCP                              171d
mercury-kafka-dr-kafka-1                            ClusterIP   10.97.200.73    <none>        9094/TCP                              171d
mercury-kafka-dr-kafka-2                            ClusterIP   10.97.227.8     <none>        9094/TCP                              171d
mercury-kafka-dr-kafka-3                            ClusterIP   10.97.70.197    <none>        9094/TCP                              171d
mercury-kafka-dr-kafka-4                            ClusterIP   10.97.33.104    <none>        9094/TCP                              171d
mercury-kafka-dr-kafka-5                            ClusterIP   10.97.34.129    <none>        9094/TCP                              171d
mercury-kafka-dr-kafka-6                            ClusterIP   10.97.77.39     <none>        9094/TCP                              171d
mercury-kafka-dr-kafka-7                            ClusterIP   10.97.162.115   <none>        9094/TCP                              171d
mercury-kafka-dr-kafka-8                            ClusterIP   10.97.90.50     <none>        9094/TCP                              171d
mercury-kafka-dr-kafka-bootstrap                    ClusterIP   10.97.42.139    <none>        9091/TCP,9092/TCP,9093/TCP,9404/TCP   171d
mercury-kafka-dr-kafka-brokers                      ClusterIP   None            <none>        9091/TCP,9092/TCP,9093/TCP            171d
mercury-kafka-dr-kafka-exporter                     ClusterIP   10.97.170.177   <none>        9404/TCP                              171d
mercury-kafka-dr-kafka-external-bootstrap           ClusterIP   10.97.57.159    <none>        9094/TCP                              171d
mercury-kafka-dr-zookeeper-client                   ClusterIP   10.97.186.243   <none>        9404/TCP,2181/TCP                     171d
mercury-kafka-dr-zookeeper-nodes                    ClusterIP   None            <none>        2181/TCP,2888/TCP,3888/TCP            171d
mercury-kafka-kafka-0                               ClusterIP   10.97.142.248   <none>        9094/TCP                              171d
mercury-kafka-kafka-1                               ClusterIP   10.97.198.58    <none>        9094/TCP                              171d
mercury-kafka-kafka-2                               ClusterIP   10.97.231.96    <none>        9094/TCP                              171d
mercury-kafka-kafka-3                               ClusterIP   10.97.101.82    <none>        9094/TCP                              171d
mercury-kafka-kafka-4                               ClusterIP   10.97.77.31     <none>        9094/TCP                              171d
mercury-kafka-kafka-5                               ClusterIP   10.97.161.50    <none>        9094/TCP                              171d
mercury-kafka-kafka-6                               ClusterIP   10.97.170.56    <none>        9094/TCP                              171d
mercury-kafka-kafka-7                               ClusterIP   10.97.232.234   <none>        9094/TCP                              171d
mercury-kafka-kafka-8                               ClusterIP   10.97.62.253    <none>        9094/TCP                              171d
mercury-kafka-kafka-bootstrap                       ClusterIP   10.97.152.249   <none>        9091/TCP,9092/TCP,9093/TCP,9404/TCP   171d
mercury-kafka-kafka-brokers                         ClusterIP   None            <none>        9091/TCP,9092/TCP,9093/TCP            171d
mercury-kafka-kafka-exporter                        ClusterIP   10.97.25.151    <none>        9404/TCP                              171d
mercury-kafka-kafka-external-bootstrap              ClusterIP   10.97.19.29     <none>        9094/TCP                              171d
mercury-kafka-mm2-dr-uk-london-1-mirrormaker2-api   ClusterIP   10.97.252.154   <none>        8083/TCP,9404/TCP                     171d
mercury-kafka-zookeeper-client                      ClusterIP   10.97.172.48    <none>        9404/TCP,2181/TCP                     171d
mercury-kafka-zookeeper-nodes                       ClusterIP   None            <none>        2181/TCP,2888/TCP,3888/TCP            171d
mercury-kweet-facebook-client                       ClusterIP   10.97.163.80    <none>        8080/TCP                              171d
mercury-kweet-facebook-webhook                      ClusterIP   10.97.28.234    <none>        8080/TCP                              171d
mercury-kweet-twiliosms-client                      ClusterIP   10.97.147.121   <none>        8080/TCP                              171d
mercury-kweet-userprofiles                          ClusterIP   10.97.226.244   <none>        8080/TCP                              171d
mercury-kweet-wechat-client                         ClusterIP   10.97.9.108     <none>        8080/TCP                              171d
mercury-kweet-wechat-webhook                        ClusterIP   10.97.56.56     <none>        8080/TCP                              171d
mercury-mercury-ui                                  ClusterIP   10.97.227.11    <none>        8080/TCP                              171d
mercury-metric-aggregation-processor                ClusterIP   10.97.80.98     <none>        8080/TCP                              171d
mercury-metric-fusion-bridge                        ClusterIP   10.97.151.180   <none>        8080/TCP                              171d
mercury-metric-generation-processor                 ClusterIP   10.97.225.37    <none>        8080/TCP                              171d
mercury-metric-internal-translation-processor       ClusterIP   10.97.82.60     <none>        8080/TCP                              171d
mercury-metric-proxy-service                        ClusterIP   10.97.1.162     <none>        8080/TCP                              171d
mercury-omnichannel-assignment-processor            ClusterIP   10.97.26.196    <none>        8080/TCP                              171d
mercury-omnichannel-offer-processor                 ClusterIP   10.97.128.237   <none>        8080/TCP                              171d
mercury-osvc-bridge-api-services                    ClusterIP   10.97.224.224   <none>        8080/TCP                              171d
mercury-osvc-bridge-metrics-data-pipeline           ClusterIP   10.97.14.136    <none>        8080/TCP                              171d
mercury-osvc-bridge-osvc-data-extractor             ClusterIP   10.97.164.97    <none>        8080/TCP                              171d
mercury-osvc-bridge-provisioning-processor          ClusterIP   10.97.211.17    <none>        8080/TCP                              171d
mercury-osvc-bridge-state-processor                 ClusterIP   10.97.225.45    <none>        8080/TCP                              171d
mercury-osvc-bridge-state-query-service             ClusterIP   10.97.165.3     <none>        8080/TCP                              171d
mercury-osvc-bridge-task-controller                 ClusterIP   10.97.234.67    <none>        8080/TCP                              171d
mercury-provisioning-monitor                        ClusterIP   10.97.164.159   <none>        8080/TCP                              171d
mercury-provisioning-processor                      ClusterIP   10.97.128.159   <none>        8080/TCP                              171d
mercury-queue-agent-info-processor                  ClusterIP   10.97.220.102   <none>        8080/TCP                              171d
mercury-realtime-channel-processor                  ClusterIP   10.97.43.52     <none>        8080/TCP                              171d
mercury-resource-channel-processor                  ClusterIP   10.97.169.66    <none>        8080/TCP                              171d
mercury-resource-state-processor                    ClusterIP   10.97.254.137   <none>        8080/TCP                              171d
mercury-resource-work-processor                     ClusterIP   10.97.69.92     <none>        8080/TCP                              171d
mercury-routing-processor-agent-assignment          ClusterIP   10.97.126.149   <none>        8080/TCP                              171d
mercury-routing-processor-agent-events-processor    ClusterIP   10.97.148.184   <none>        8080/TCP                              171d
mercury-routing-processor-queue-assignment          ClusterIP   10.97.132.117   <none>        8080/TCP                              171d
mercury-routing-processor-work-events-processor     ClusterIP   10.97.58.128    <none>        8080/TCP                              171d
mercury-session-housekeeping-processor              ClusterIP   10.97.89.248    <none>        8080/TCP                              171d
mercury-session-processor                           ClusterIP   10.97.99.223    <none>        8080/TCP                              171d
mercury-single-sign-on-service                      ClusterIP   10.97.85.222    <none>        8080/TCP                              171d
mercury-social-bridge                               ClusterIP   10.97.184.31    <none>        8080/TCP                              171d
mercury-social-config                               ClusterIP   10.97.168.34    <none>        8080/TCP                              171d
mercury-static-assets-service                       ClusterIP   10.97.202.244   <none>        8080/TCP                              171d
mercury-tenant-downtime-monitor                     ClusterIP   10.97.161.117   <none>        8080/TCP                              171d
mercury-transcript-api                              ClusterIP   10.97.196.137   <none>        8080/TCP                              171d
mercury-transcript-processor                        ClusterIP   10.97.238.202   <none>        8080/TCP                              171d
mercury-user-preference-service                     ClusterIP   10.97.83.117    <none>        8080/TCP                              171d
mercury-work-api                                    ClusterIP   10.97.83.208    <none>        8080/TCP                              171d
mercury-work-processor                              ClusterIP   10.97.167.28    <none>        8080/TCP                              171d
[21:January:2022:07:58:16]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kubectl port-forward mercury-provisioning-processor-65f45f9844-nr4zx 7001:8080
Error from server (NotFound): pods "mercury-provisioning-processor-65f45f9844-nr4zx" not found
[21:January:2022:07:58:37]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kubectl port-forward mercury-provisioning-processor-65f45f9844-nr4zx 7001:8080 -n mercury
Forwarding from 127.0.0.1:7001 -> 8080

^C[21:January:2022:08:00:37]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ ls -ltr /tmp/
total 21744
-rw-r--r-- 1 opc opc  6570547 Mar 24  2021 yq_linux_amd64
-rw-r--r-- 1 opc opc 15629982 Jun 25  2021 helm-diff.tgz
drwxr-xr-x 3 opc opc     4096 Jun 25  2021 helm-diff
-rw-r--r-- 1 opc opc       56 Jan 20 08:08 osvc-platform_git_branch
-rw-r--r-- 1 opc opc    21302 Jan 21 04:56 sharedkafka.mumbai.list
-rw-r--r-- 1 opc opc       75 Jan 21 07:36 helmfile-releases_git_branch
drwx------ 2 opc opc     4096 Jan 21 07:43 helmfile895940994
drwx------ 2 opc opc     4096 Jan 21 07:43 helmfile472555769
drwx------ 2 opc opc     4096 Jan 21 07:43 helmfile429210116
drwx------ 2 opc opc     4096 Jan 21 07:43 helmfile245308368
drwx------ 2 opc opc     4096 Jan 21 07:43 helmfile053319919
drwx------ 2 opc opc     4096 Jan 21 07:43 helmfile035789715
-rw-r--r-- 1 opc opc        0 Jan 21 08:00 opc_git_branch
[21:January:2022:08:00:40]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kubectl port-forward mercury-provisioning-processor-65f45f9844-nr4zx 7001:8080 -n mercury
ERROR: Config file /home/opc/.oci/config is invalid: the key_file's value '/Users/azhekhan/.oci/oci_api_key.pem' at line 7 must be a valid file path.
ERROR: Config file /home/opc/.oci/config is invalid: the key_file's value '/Users/azhekhan/.oci/oci_api_key.pem' at line 7 must be a valid file path.
ERROR: Config file /home/opc/.oci/config is invalid: the key_file's value '/Users/azhekhan/.oci/oci_api_key.pem' at line 7 must be a valid file path.
ERROR: Config file /home/opc/.oci/config is invalid: the key_file's value '/Users/azhekhan/.oci/oci_api_key.pem' at line 7 must be a valid file path.
Unable to connect to the server: getting credentials: exec: executable oci failed with exit code 1
[21:January:2022:08:00:46]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgc
CURRENT   NAME                               CLUSTER               AUTHINFO           NAMESPACE
          prod_ap-hyderabad-1_controlplane   cluster-cbjr52qcfoa   user-cbjr52qcfoa
          prod_ap-hyderabad-1_dataplane      cluster-cpj2zxktnda   user-cpj2zxktnda
          prod_ap-melbourne-1_controlplane   cluster-czgcn3bmjtg   user-czgcn3bmjtg
          prod_ap-melbourne-1_dataplane      cluster-c4dazdggfst   user-c4dazdggfst
          prod_ap-mumbai-1_controlplane      cluster-cjiru3mifia   user-cjiru3mifia
          prod_ap-mumbai-1_dataplane         cluster-ci3ypr6tmga   user-ci3ypr6tmga
          prod_ap-osaka-1_controlplane       cluster-cp5327xtmfa   user-cp5327xtmfa
          prod_ap-osaka-1_dataplane          cluster-cqldck6xatq   user-cqldck6xatq
[DEFAULT]
#tenancy=ocid1.compartment.oc1..aaaaaaaamfzmd7dxhst3a5hbhtu6mwxnkxzacrxqwrgasecvwq2iqcg7mg3q
tenancy=ocid1.tenancy.oc1..aaaaaaaacbb4jhwb2q6tfx223i5siaiuyw6gpl3zywyosiudeimsodjkolga
region=us-ashburn-1
user=ocid1.user.oc1..aaaaaaaaft5c2fkohgco27vmkudak2i3mk5mubwmo5szyejluyso7flvkdqa
fingerprint=1e:57:b5:1f:39:03:b5:e4:16:38:87:5b:88:e5:da:e0
key_file=/Users/azhekhan/.oci/oci_api_key.pem
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
"~/.oci/config" 7L, 399C                              1,1           All
[DEFAULT]
          prod_ap-sydney-1_controlplane      cluster-c4wembugqyt   user-c4wembugqyt
          prod_ap-sydney-1_dataplane         cluster-cswezbzgyyg   user-cswezbzgyyg
          prod_ap-tokyo-1_controlplane       cluster-cz54pc3z4wa   user-cz54pc3z4wa
          prod_ap-tokyo-1_dataplane          cluster-cyjtbtbbjfq   user-cyjtbtbbjfq
          prod_ca-montreal-1_controlplane    cluster-czdoolemnqw   user-czdoolemnqw
          prod_ca-montreal-1_dataplane       cluster-cqtkytbgq3d   user-cqtkytbgq3d
          prod_ca-toronto-1_controlplane     cluster-cygcmjxhfsd   user-cygcmjxhfsd
          prod_ca-toronto-1_dataplane        cluster-cywmodfmm3d   user-cywmodfmm3d
          prod_eu-amsterdam-1_controlplane   cluster-c4tkmjwgbrt   user-c4tkmjwgbrt
          prod_eu-amsterdam-1_dataplane      cluster-cstsy3bmjrd   user-cstsy3bmjrd
          prod_eu-frankfurt-1_controlplane   cluster-cydcytcg43d   user-cydcytcg43d
          prod_eu-frankfurt-1_dataplane      cluster-crdgzbrmjsd   user-crdgzbrmjsd
          prod_eu-zurich-1_controlplane      cluster-cwmoxn3bfwq   user-cwmoxn3bfwq
          prod_eu-zurich-1_dataplane         cluster-chtoe5bnsza   user-chtoe5bnsza
          prod_me-dubai-1_controlplane       cluster-ce5puidc4ya   user-ce5puidc4ya
          prod_me-dubai-1_dataplane          cluster-c77hcd3vizq   user-c77hcd3vizq
          prod_me-jeddah-1_controlplane      cluster-cpzagsoukeq   user-cpzagsoukeq
          prod_me-jeddah-1_dataplane         cluster-cf4f222mrfa   user-cf4f222mrfa
          prod_sa-santiago-1_controlplane    cluster-czxpimgibeq   user-czxpimgibeq
*         prod_sa-santiago-1_dataplane       cluster-cuutxfvj2ea   user-cuutxfvj2ea
          prod_sa-saopaulo-1_controlplane    cluster-cc75n7hujpq   user-cc75n7hujpq
          prod_sa-saopaulo-1_dataplane       cluster-cpjqrjtbotq   user-cpjqrjtbotq
          prod_sa-vinhedo-1_controlplane     cluster-cihx6aprjua   user-cihx6aprjua
          prod_sa-vinhedo-1_dataplane        cluster-ciqmbmn6wta   user-ciqmbmn6wta
          prod_uk-cardiff-1_controlplane     cluster-ctggmtbmfrg   user-ctggmtbmfrg
          prod_uk-cardiff-1_dataplane        cluster-cqwmnrqmztd   user-cqwmnrqmztd
          prod_uk-london-1_controlplane      cluster-cywgnlcmqzd   user-cywgnlcmqzd
          prod_uk-london-1_dataplane         cluster-c3tazjsmjsd   user-c3tazjsmjsd
          prod_us-ashburn-1_controlplane     cluster-czwczjzgrsw   user-czwczjzgrsw
          prod_us-ashburn-1_dataplane        cluster-c3wkyjyguzt   user-c3wkyjyguzt
          prod_us-phoenix-1_controlplane     cluster-c3tkobugrst   user-c3tkobugrst
          prod_us-phoenix-1_dataplane        cluster-csdqylfmi2g   user-csdqylfmi2g
          prod_us-phoenix-1_deployment       cluster-c4tgolgmjsd   user-c4tgolgmjsd
[21:January:2022:08:01:04]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ oong
ERROR: Config file /home/opc/.oci/config is invalid: the key_file's value '/Users/azhekhan/.oci/oci_api_key.pem' at line 7 must be a valid file path.
[21:January:2022:08:01:07]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ cat ~/.oci/config
[DEFAULT]
#tenancy=ocid1.compartment.oc1..aaaaaaaamfzmd7dxhst3a5hbhtu6mwxnkxzacrxqwrgasecvwq2iqcg7mg3q
tenancy=ocid1.tenancy.oc1..aaaaaaaacbb4jhwb2q6tfx223i5siaiuyw6gpl3zywyosiudeimsodjkolga
region=us-ashburn-1
user=ocid1.user.oc1..aaaaaaaaft5c2fkohgco27vmkudak2i3mk5mubwmo5szyejluyso7flvkdqa
fingerprint=1e:57:b5:1f:39:03:b5:e4:16:38:87:5b:88:e5:da:e0
key_file=/Users/azhekhan/.oci/oci_api_key.pem
[21:January:2022:08:01:23]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ pwd
/home/opc/galorndon/osvc-platform/helm/helmfile-releases
[21:January:2022:08:01:29]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ vi ~/.oci/config
[21:January:2022:08:01:46]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ oong
{
  "data": "osvcstage"
}
[21:January:2022:08:01:50]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ v2prod
[21:January:2022:08:02:06]:(prod_sa-santiago-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgc
CURRENT   NAME                               CLUSTER               AUTHINFO           NAMESPACE
          prod_ap-hyderabad-1_controlplane   cluster-cbjr52qcfoa   user-cbjr52qcfoa
          prod_ap-hyderabad-1_dataplane      cluster-cpj2zxktnda   user-cpj2zxktnda
          prod_ap-melbourne-1_controlplane   cluster-czgcn3bmjtg   user-czgcn3bmjtg
          prod_ap-melbourne-1_dataplane      cluster-c4dazdggfst   user-c4dazdggfst
          prod_ap-mumbai-1_controlplane      cluster-cjiru3mifia   user-cjiru3mifia
          prod_ap-mumbai-1_dataplane         cluster-ci3ypr6tmga   user-ci3ypr6tmga
          prod_ap-osaka-1_controlplane       cluster-cp5327xtmfa   user-cp5327xtmfa
          prod_ap-osaka-1_dataplane          cluster-cqldck6xatq   user-cqldck6xatq
          prod_ap-sydney-1_controlplane      cluster-c4wembugqyt   user-c4wembugqyt
          prod_ap-sydney-1_dataplane         cluster-cswezbzgyyg   user-cswezbzgyyg
          prod_ap-tokyo-1_controlplane       cluster-cz54pc3z4wa   user-cz54pc3z4wa
          prod_ap-tokyo-1_dataplane          cluster-cyjtbtbbjfq   user-cyjtbtbbjfq
          prod_ca-montreal-1_controlplane    cluster-czdoolemnqw   user-czdoolemnqw
          prod_ca-montreal-1_dataplane       cluster-cqtkytbgq3d   user-cqtkytbgq3d
          prod_ca-toronto-1_controlplane     cluster-cygcmjxhfsd   user-cygcmjxhfsd
          prod_ca-toronto-1_dataplane        cluster-cywmodfmm3d   user-cywmodfmm3d
          prod_eu-amsterdam-1_controlplane   cluster-c4tkmjwgbrt   user-c4tkmjwgbrt
          prod_eu-amsterdam-1_dataplane      cluster-cstsy3bmjrd   user-cstsy3bmjrd
          prod_eu-frankfurt-1_controlplane   cluster-cydcytcg43d   user-cydcytcg43d
          prod_eu-frankfurt-1_dataplane      cluster-crdgzbrmjsd   user-crdgzbrmjsd
          prod_eu-zurich-1_controlplane      cluster-cwmoxn3bfwq   user-cwmoxn3bfwq
          prod_eu-zurich-1_dataplane         cluster-chtoe5bnsza   user-chtoe5bnsza
          prod_me-dubai-1_controlplane       cluster-ce5puidc4ya   user-ce5puidc4ya
          prod_me-dubai-1_dataplane          cluster-c77hcd3vizq   user-c77hcd3vizq
          prod_me-jeddah-1_controlplane      cluster-cpzagsoukeq   user-cpzagsoukeq
          prod_me-jeddah-1_dataplane         cluster-cf4f222mrfa   user-cf4f222mrfa
          prod_sa-santiago-1_controlplane    cluster-czxpimgibeq   user-czxpimgibeq
*         prod_sa-santiago-1_dataplane       cluster-cuutxfvj2ea   user-cuutxfvj2ea
          prod_sa-saopaulo-1_controlplane    cluster-cc75n7hujpq   user-cc75n7hujpq
          prod_sa-saopaulo-1_dataplane       cluster-cpjqrjtbotq   user-cpjqrjtbotq
          prod_sa-vinhedo-1_controlplane     cluster-cihx6aprjua   user-cihx6aprjua
          prod_sa-vinhedo-1_dataplane        cluster-ciqmbmn6wta   user-ciqmbmn6wta
          prod_uk-cardiff-1_controlplane     cluster-ctggmtbmfrg   user-ctggmtbmfrg
          prod_uk-cardiff-1_dataplane        cluster-cqwmnrqmztd   user-cqwmnrqmztd
          prod_uk-london-1_controlplane      cluster-cywgnlcmqzd   user-cywgnlcmqzd
          prod_uk-london-1_dataplane         cluster-c3tazjsmjsd   user-c3tazjsmjsd
          prod_us-ashburn-1_controlplane     cluster-czwczjzgrsw   user-czwczjzgrsw
          prod_us-ashburn-1_dataplane        cluster-c3wkyjyguzt   user-c3wkyjyguzt
          prod_us-phoenix-1_controlplane     cluster-c3tkobugrst   user-c3tkobugrst
          prod_us-phoenix-1_dataplane        cluster-csdqylfmi2g   user-csdqylfmi2g
          prod_us-phoenix-1_deployment       cluster-c4tgolgmjsd   user-c4tgolgmjsd
[21:January:2022:08:02:09]:(prod_sa-santiago-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kuc prod_uk-london-1_dataplane
Switched to context "prod_uk-london-1_dataplane".
[21:January:2022:08:02:15]:(prod_sa-santiago-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ rp
[21:January:2022:08:02:16]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ oong
{
  "data": "osvcprod"
}
[21:January:2022:08:02:19]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ touch /tmp/abcxyz
[21:January:2022:08:02:28]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kubectl port-forward mercury-provisioning-processor-65f45f9844-nr4zx 7001:8080 -n mercury
Forwarding from 127.0.0.1:7001 -> 8080
Handling connection for 7001
Handling connection for 7001
^C[21:January:2022:08:06:34]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ v2prod
[21:January:2022:08:06:37]:(prod_sa-santiago-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgc
CURRENT   NAME                               CLUSTER               AUTHINFO           NAMESPACE
          prod_ap-hyderabad-1_controlplane   cluster-cbjr52qcfoa   user-cbjr52qcfoa
          prod_ap-hyderabad-1_dataplane      cluster-cpj2zxktnda   user-cpj2zxktnda
          prod_ap-melbourne-1_controlplane   cluster-czgcn3bmjtg   user-czgcn3bmjtg
          prod_ap-melbourne-1_dataplane      cluster-c4dazdggfst   user-c4dazdggfst
          prod_ap-mumbai-1_controlplane      cluster-cjiru3mifia   user-cjiru3mifia
          prod_ap-mumbai-1_dataplane         cluster-ci3ypr6tmga   user-ci3ypr6tmga
          prod_ap-osaka-1_controlplane       cluster-cp5327xtmfa   user-cp5327xtmfa
          prod_ap-osaka-1_dataplane          cluster-cqldck6xatq   user-cqldck6xatq
          prod_ap-sydney-1_controlplane      cluster-c4wembugqyt   user-c4wembugqyt
          prod_ap-sydney-1_dataplane         cluster-cswezbzgyyg   user-cswezbzgyyg
          prod_ap-tokyo-1_controlplane       cluster-cz54pc3z4wa   user-cz54pc3z4wa
          prod_ap-tokyo-1_dataplane          cluster-cyjtbtbbjfq   user-cyjtbtbbjfq
          prod_ca-montreal-1_controlplane    cluster-czdoolemnqw   user-czdoolemnqw
          prod_ca-montreal-1_dataplane       cluster-cqtkytbgq3d   user-cqtkytbgq3d
          prod_ca-toronto-1_controlplane     cluster-cygcmjxhfsd   user-cygcmjxhfsd
          prod_ca-toronto-1_dataplane        cluster-cywmodfmm3d   user-cywmodfmm3d
          prod_eu-amsterdam-1_controlplane   cluster-c4tkmjwgbrt   user-c4tkmjwgbrt
          prod_eu-amsterdam-1_dataplane      cluster-cstsy3bmjrd   user-cstsy3bmjrd
          prod_eu-frankfurt-1_controlplane   cluster-cydcytcg43d   user-cydcytcg43d
          prod_eu-frankfurt-1_dataplane      cluster-crdgzbrmjsd   user-crdgzbrmjsd
          prod_eu-zurich-1_controlplane      cluster-cwmoxn3bfwq   user-cwmoxn3bfwq
          prod_eu-zurich-1_dataplane         cluster-chtoe5bnsza   user-chtoe5bnsza
          prod_me-dubai-1_controlplane       cluster-ce5puidc4ya   user-ce5puidc4ya
          prod_me-dubai-1_dataplane          cluster-c77hcd3vizq   user-c77hcd3vizq
          prod_me-jeddah-1_controlplane      cluster-cpzagsoukeq   user-cpzagsoukeq
          prod_me-jeddah-1_dataplane         cluster-cf4f222mrfa   user-cf4f222mrfa
          prod_sa-santiago-1_controlplane    cluster-czxpimgibeq   user-czxpimgibeq
*         prod_sa-santiago-1_dataplane       cluster-cuutxfvj2ea   user-cuutxfvj2ea
          prod_sa-saopaulo-1_controlplane    cluster-cc75n7hujpq   user-cc75n7hujpq
          prod_sa-saopaulo-1_dataplane       cluster-cpjqrjtbotq   user-cpjqrjtbotq
          prod_sa-vinhedo-1_controlplane     cluster-cihx6aprjua   user-cihx6aprjua
          prod_sa-vinhedo-1_dataplane        cluster-ciqmbmn6wta   user-ciqmbmn6wta
          prod_uk-cardiff-1_controlplane     cluster-ctggmtbmfrg   user-ctggmtbmfrg
          prod_uk-cardiff-1_dataplane        cluster-cqwmnrqmztd   user-cqwmnrqmztd
          prod_uk-london-1_controlplane      cluster-cywgnlcmqzd   user-cywgnlcmqzd
          prod_uk-london-1_dataplane         cluster-c3tazjsmjsd   user-c3tazjsmjsd
          prod_us-ashburn-1_controlplane     cluster-czwczjzgrsw   user-czwczjzgrsw
          prod_us-ashburn-1_dataplane        cluster-c3wkyjyguzt   user-c3wkyjyguzt
          prod_us-phoenix-1_controlplane     cluster-c3tkobugrst   user-c3tkobugrst
          prod_us-phoenix-1_dataplane        cluster-csdqylfmi2g   user-csdqylfmi2g
          prod_us-phoenix-1_deployment       cluster-c4tgolgmjsd   user-c4tgolgmjsd
[21:January:2022:08:06:39]:(prod_sa-santiago-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kuc prod_uk-london-1_dataplane
Switched to context "prod_uk-london-1_dataplane".
[21:January:2022:08:06:44]:(prod_sa-santiago-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ rp
[21:January:2022:08:06:45]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ oong
{
  "data": "osvcprod"
}
[21:January:2022:08:09:55]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ git pull
Use of the Oracle network and applications is intended solely for Oracle's authorized users. The use of these resources by Oracle employees and contractors is subject to company policies, including the Code of Conduct, Acceptable Use Policy and Information Protection Policy; access may be monitored and logged, to the extent permitted by law, in accordance with Oracle policies. Unauthorized use may result in termination of your access, disciplinary action and/or civil and criminal penalties.
remote: Enumerating objects: 31, done.
remote: Counting objects: 100% (31/31), done.
remote: Compressing objects: 100% (31/31), done.
remote: Total 31 (delta 6), reused 5 (delta 0), pack-reused 0
Unpacking objects: 100% (31/31), done.
From orahub.oci.oraclecorp.com:osvc-sre-dev/osvc-platform
   36e7322..4b408ff  rthazhek_lv14844 -> origin/rthazhek_lv14844
 * [new branch]      configure-sandbox-as-aicxservice -> origin/configure-sandbox-as-aicxservice
   d18865b..64df7a3  opaec_snapshot_main -> origin/opaec_snapshot_main
Updating 36e7322..4b408ff
Fast-forward
 helm/vars/charts/mercury/prod/mercury-psr/us-phoenix-1_deployment.yaml | 6 +++---
 1 file changed, 3 insertions(+), 3 deletions(-)
[21:January:2022:08:11:06]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ cat helm/vars/charts/mercury/prod/mercury-psr/us-phoenix-1_deployment.yaml
cat: helm/vars/charts/mercury/prod/mercury-psr/us-phoenix-1_deployment.yaml: No such file or directory
[21:January:2022:08:11:20]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ #cat helm/vars/charts/mercury/prod/mercury-psr/us-phoenix-1_deployment.yaml
[21:January:2022:08:11:26]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ cd ..

[21:January:2022:08:11:29]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm
○ (rthazhek_lv14844) $
[21:January:2022:08:11:36]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm
○ (rthazhek_lv14844) $ cd -
/home/opc/galorndon/osvc-platform/helm/helmfile-releases
[21:January:2022:08:11:38]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ cat ../../helm/vars/charts/mercury/prod/mercury-psr/us-phoenix-1_deployment.yaml
# Mercury deployment - large footprint - prod, phoenix, mercury-psr

#By default services are allocated thusly:
#resources:
#  requests:
#    cpu: "100m"
#    memory: "2Gi"
#    ephemeral-storage: "2Gi"
#  limits:
#    cpu: "1"
#    memory: "6Gi"
#    ephemeral-storage: "10Gi"

global:
  tenantNameFilterValue: "^(evai)"

agent-command-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 4
      memory: 4Gi
      ephemeral-storage: 2Gi
    limits:
      cpu: 6
      memory: 10Gi
      ephemeral-storage: 50Gi
  addOsvcBridgeApiServicesURL: true
  ingress2:
    enabled: false
  ingress3:
    enabled: false
  ingress4:
    enabled: false

channel-api:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  # disable channel API until LX is officially supported
  replicaCount: 0

consumer-command-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 2
      memory: 3Gi
      ephemeral-storage: 2Gi
    limits:
      cpu: 4
      memory: 6Gi
      ephemeral-storage: 50Gi
  ingress2:
    enabled: false
  ingress3:
    enabled: false
  ingress4:
    enabled: false

custom-availability-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

data-mask-api:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    limits:
      cpu: 800m
      ephemeral-storage: 1Gi
      memory: 6Gi
    requests:
      cpu: 100m
      ephemeral-storage: 1Gi
      memory: 3Gi
  ingress:
    enabled: true
  ingress1:
    enabled: false

engagement-queue-api:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

enrichment-service:
  javaopts: "-XX:+UseG1GC -XX:MaxGCPauseMillis=250 -DwebHook.topic.name=helios-psr.priority.external.event -Dhelios.error.event=helios-psr.error.event"
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  mercuryCommon:
    tenantNameFilter: true
  global:
    webhookTopicName: "helios-psr.priority.external.event"
    webhookErrorTopicName: "helios-psr.error.event"

event-sync-service:
  javaopts: "-XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Dwebhook.topic.name=helios-psr.external.event -Dhelios.error.event=helios-psr.error.event"
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  mercuryCommon:
    tenantNameFilter: true
  global:
    webhookTopicName: "helios-psr.external.event"
    webhookErrorTopicName: "helios-psr.error.event"

integration-in-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 1500m
      memory: 4Gi
      ephemeral-storage: 10Gi

integration-out-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 1500m
      memory: 4Gi
      ephemeral-storage: 10Gi

kweet-facebook-client:
  replicaCount: 3
  resources:
    limits:
      cpu: "2"
      memory: 3Gi
    requests:
      cpu: 200m
      memory: 2Gi
  global:
    service:
      livenessProbe:
        initialDelaySeconds: 180
      readinessProbe:
        initialDelaySeconds: 180

kweet-facebook-webhook:
  replicaCount: 3
  resources:
    limits:
      cpu: "2"
      memory: 3Gi
    requests:
      cpu: 100m
      memory: 2Gi
  global:
    service:
      livenessProbe:
        initialDelaySeconds: 180
      readinessProbe:
        initialDelaySeconds: 180

kweet-twiliosms-client:
  replicaCount: 3
  global:
    service:
      livenessProbe:
        initialDelaySeconds: 180
      readinessProbe:
        initialDelaySeconds: 180

kweet-userprofiles:
  replicaCount: 3
  global:
    service:
      livenessProbe:
        initialDelaySeconds: 180
      readinessProbe:
        initialDelaySeconds: 180

kweet-wechat-client:
  # Until WeChat is supported, do not deploy the services in production
  replicaCount: 0
  resources:
    limits:
      cpu: "2"
      memory: 3Gi
    requests:
      cpu: 200m
      memory: 2Gi
  global:
    service:
      livenessProbe:
        initialDelaySeconds: 180
      readinessProbe:
        initialDelaySeconds: 180

kweet-wechat-webhook:
  # Until WeChat is supported, do not deploy the services in production
  replicaCount: 0
  resources:
    limits:
      cpu: "2"
      memory: 3Gi
    requests:
      cpu: 200m
      memory: 2Gi
  global:
    service:
      livenessProbe:
        initialDelaySeconds: 180
      readinessProbe:
        initialDelaySeconds: 180

mercury-ui:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  mercuryCommon:
    http:
      rest:
        security:
          headers:
            enabled: false
  singleSignOn:
    authentication: true

metric-aggregation-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    limits:
      cpu: 2
      ephemeral-storage: 5Gi
      memory: 6Gi
    requests:
      cpu: 1
      ephemeral-storage: 1Gi
      memory: 3Gi

metric-fusion-bridge:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

metric-generation-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    limits:
      cpu: 2
      ephemeral-storage: 10Gi
      memory: 6Gi
    requests:
      cpu: 500m
      ephemeral-storage: 2Gi
      memory: 3Gi

metric-internal-translation-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

metric-proxy-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold:
  replicaCount: 3
  resources:
    requests:
      cpu: 1
      memory: 3Gi
      ephemeral-storage: 2Gi
    limits:
      ephemeral-storage: 15Gi

omnichannel-assignment-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 1
      ephemeral-storage: 2Gi
    limits:
      cpu: 2
      ephemeral-storage: 10Gi

omnichannel-offer-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 1
      ephemeral-storage: 2Gi
    limits:
      cpu: 2
      ephemeral-storage: 10Gi

osvc-bridge-api-services:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

osvc-bridge-metrics-data-pipeline:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

osvc-bridge-osvc-data-extractor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  # To resolve the url while using umbrella chart.
  addOsvcBridgeStateQueryURL: true

osvc-bridge-provisioning-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 1

osvc-bridge-state-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

osvc-bridge-state-query-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

osvc-bridge-task-controller:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

provisioning-monitor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

provisioning-processor:
  javaopts: "-Dmercury.provisioning.max.time.mins=29 -Dwebhook.service.base.uri=http://authentication-service-helios.helios-psr.svc.cluster.local:80"
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  provisioning:
    syncUpstreamData: true
    provisionPdb: false
    whiteListIp : false
    whiteListIpAddress : ""
  singleSignOn:
    samlURLs: true
  mercuryCommon:
    tenantNameFilter: true
  global:
    webhookBaseUri: "http://authentication-service-helios.helios-psr.svc.cluster.local:80"

queue-agent-info-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 1
      memory: 3Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 2
      memory: 6Gi
      ephemeral-storage: 10Gi

realtime-channel-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 800m
      memory: 3Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 2
      memory: 4Gi
      ephemeral-storage: 10Gi

resource-channel-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 2
      memory: 4Gi
      ephemeral-storage: 10Gi

resource-work-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 1
      memory: 2Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 2
      memory: 4Gi
      ephemeral-storage: 10Gi

resource-state-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

routing-processor-agent-assignment:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 500m
    limits:
      cpu: 2

routing-processor-agent-events-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold:
  replicaCount: 3
  resources:
    requests:
      cpu: 500m
    limits:
      cpu: 2

routing-processor-queue-assignment:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 500m

routing-processor-work-events-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 500m

session-housekeeping-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  javaopts: "-Dsession.housekeeping.watchdog.timer.ms=600000"

session-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

single-sign-on-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  singleSignOn:
    samlURLs: true

social-bridge:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

social-config:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 150m
      memory: 3Gi
    limits:
      cpu: 400m
      memory: 6Gi

static-assets-service:
  replicaCount: 3
  resources:
    requests:
      cpu: 100m
      ephemeral-storage: 2Gi
      memory: 1Gi
    limits:
      cpu: 500m
      ephemeral-storage: 10Gi
      memory: 2Gi

tenant-downtime-monitor:
  javaopts: "-XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Dwebhook.death.topic.name=helios-psr.death.event -Dwebhook.base.url=http://authentication-service-helios.helios-psr.svc.cluster.local:80"
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  global:
    webhookDeathTopicName: "helios-psr.death.event"
    webhookBaseUri: "http://authentication-service-helios.helios-psr.svc.cluster.local:80"

transcript-api:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  ingress1:
    enabled: false
  ingress2:
    enabled: false
  ingress3:
    enabled: false

transcript-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 1
      memory: 6Gi
      ephemeral-storage: 50Gi

user-preference-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

work-api:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  ingress1:
    enabled: false
  ingress2:
    enabled: false
  ingress3:
    enabled: false
  ingress4:
    enabled: false

work-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 2
      memory: 3Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 3
      memory: 8Gi
      ephemeral-storage: 10Gi
[21:January:2022:08:11:45]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[21:January:2022:08:12:25]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ helm repo list
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_prod.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_prod.config
NAME              URL
stable            https://charts.helm.sh/stable
incubator         https://charts.helm.sh/incubator
bitnami           https://charts.bitnami.com/bitnami
hashicorp         https://helm.releases.hashicorp.com
cdaas             https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc
helm-osvc         https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc
osvc-helm-virtual https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc
[21:January:2022:08:12:30]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgc
CURRENT   NAME                               CLUSTER               AUTHINFO           NAMESPACE
          prod_ap-hyderabad-1_controlplane   cluster-cbjr52qcfoa   user-cbjr52qcfoa
          prod_ap-hyderabad-1_dataplane      cluster-cpj2zxktnda   user-cpj2zxktnda
          prod_ap-melbourne-1_controlplane   cluster-czgcn3bmjtg   user-czgcn3bmjtg
          prod_ap-melbourne-1_dataplane      cluster-c4dazdggfst   user-c4dazdggfst
          prod_ap-mumbai-1_controlplane      cluster-cjiru3mifia   user-cjiru3mifia
          prod_ap-mumbai-1_dataplane         cluster-ci3ypr6tmga   user-ci3ypr6tmga
          prod_ap-osaka-1_controlplane       cluster-cp5327xtmfa   user-cp5327xtmfa
          prod_ap-osaka-1_dataplane          cluster-cqldck6xatq   user-cqldck6xatq
          prod_ap-sydney-1_controlplane      cluster-c4wembugqyt   user-c4wembugqyt
          prod_ap-sydney-1_dataplane         cluster-cswezbzgyyg   user-cswezbzgyyg
          prod_ap-tokyo-1_controlplane       cluster-cz54pc3z4wa   user-cz54pc3z4wa
          prod_ap-tokyo-1_dataplane          cluster-cyjtbtbbjfq   user-cyjtbtbbjfq
          prod_ca-montreal-1_controlplane    cluster-czdoolemnqw   user-czdoolemnqw
          prod_ca-montreal-1_dataplane       cluster-cqtkytbgq3d   user-cqtkytbgq3d
          prod_ca-toronto-1_controlplane     cluster-cygcmjxhfsd   user-cygcmjxhfsd
          prod_ca-toronto-1_dataplane        cluster-cywmodfmm3d   user-cywmodfmm3d
          prod_eu-amsterdam-1_controlplane   cluster-c4tkmjwgbrt   user-c4tkmjwgbrt
          prod_eu-amsterdam-1_dataplane      cluster-cstsy3bmjrd   user-cstsy3bmjrd
          prod_eu-frankfurt-1_controlplane   cluster-cydcytcg43d   user-cydcytcg43d
          prod_eu-frankfurt-1_dataplane      cluster-crdgzbrmjsd   user-crdgzbrmjsd
          prod_eu-zurich-1_controlplane      cluster-cwmoxn3bfwq   user-cwmoxn3bfwq
          prod_eu-zurich-1_dataplane         cluster-chtoe5bnsza   user-chtoe5bnsza
          prod_me-dubai-1_controlplane       cluster-ce5puidc4ya   user-ce5puidc4ya
          prod_me-dubai-1_dataplane          cluster-c77hcd3vizq   user-c77hcd3vizq
          prod_me-jeddah-1_controlplane      cluster-cpzagsoukeq   user-cpzagsoukeq
          prod_me-jeddah-1_dataplane         cluster-cf4f222mrfa   user-cf4f222mrfa
          prod_sa-santiago-1_controlplane    cluster-czxpimgibeq   user-czxpimgibeq
          prod_sa-santiago-1_dataplane       cluster-cuutxfvj2ea   user-cuutxfvj2ea
          prod_sa-saopaulo-1_controlplane    cluster-cc75n7hujpq   user-cc75n7hujpq
          prod_sa-saopaulo-1_dataplane       cluster-cpjqrjtbotq   user-cpjqrjtbotq
          prod_sa-vinhedo-1_controlplane     cluster-cihx6aprjua   user-cihx6aprjua
          prod_sa-vinhedo-1_dataplane        cluster-ciqmbmn6wta   user-ciqmbmn6wta
          prod_uk-cardiff-1_controlplane     cluster-ctggmtbmfrg   user-ctggmtbmfrg
          prod_uk-cardiff-1_dataplane        cluster-cqwmnrqmztd   user-cqwmnrqmztd
          prod_uk-london-1_controlplane      cluster-cywgnlcmqzd   user-cywgnlcmqzd
*         prod_uk-london-1_dataplane         cluster-c3tazjsmjsd   user-c3tazjsmjsd
          prod_us-ashburn-1_controlplane     cluster-czwczjzgrsw   user-czwczjzgrsw
          prod_us-ashburn-1_dataplane        cluster-c3wkyjyguzt   user-c3wkyjyguzt
          prod_us-phoenix-1_controlplane     cluster-c3tkobugrst   user-c3tkobugrst
          prod_us-phoenix-1_dataplane        cluster-csdqylfmi2g   user-csdqylfmi2g
          prod_us-phoenix-1_deployment       cluster-c4tgolgmjsd   user-c4tgolgmjsd
[21:January:2022:08:12:35]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kuc prod_us-phoenix-1_dataplane
Switched to context "prod_us-phoenix-1_dataplane".
[21:January:2022:08:12:39]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ rp
[21:January:2022:08:12:41]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgpn mercury-psr
NAME                                                              READY   STATUS    RESTARTS   AGE   IP             NODE          NOMINATED NODE   READINESS GATES
mercury-agent-command-service-59dcc858fc-gtl4m                    2/2     Running   0          33h   10.245.1.100   10.20.1.38    <none>           <none>
mercury-agent-command-service-59dcc858fc-ldzzz                    2/2     Running   0          33h   10.245.3.49    10.20.0.7     <none>           <none>
mercury-agent-command-service-59dcc858fc-zmkng                    2/2     Running   3          33h   10.245.8.145   10.20.1.158   <none>           <none>
mercury-channel-api-6f9c9c9dc5-f89lq                              2/2     Running   3          33h   10.245.8.143   10.20.1.158   <none>           <none>
mercury-consumer-command-service-76d45c9fcc-v89kt                 2/2     Running   0          33h   10.245.4.142   10.20.1.227   <none>           <none>
mercury-consumer-command-service-76d45c9fcc-x2dv2                 2/2     Running   0          33h   10.245.6.11    10.20.1.44    <none>           <none>
mercury-consumer-command-service-76d45c9fcc-xt68z                 2/2     Running   0          33h   10.245.2.40    10.20.1.35    <none>           <none>
mercury-custom-availability-service-569d4cf687-7dx62              2/2     Running   0          33h   10.245.5.197   10.20.0.143   <none>           <none>
mercury-custom-availability-service-569d4cf687-gsnqf              2/2     Running   3          33h   10.245.6.12    10.20.1.44    <none>           <none>
mercury-custom-availability-service-569d4cf687-w4qwz              2/2     Running   2          33h   10.245.4.139   10.20.1.227   <none>           <none>
mercury-data-mask-api-59584bc855-jhs6j                            2/2     Running   0          33h   10.245.4.25    10.20.1.27    <none>           <none>
mercury-data-mask-api-59584bc855-sl229                            2/2     Running   0          33h   10.245.4.147   10.20.1.227   <none>           <none>
mercury-data-mask-api-59584bc855-zq6bv                            2/2     Running   0          33h   10.245.3.240   10.20.0.250   <none>           <none>
mercury-engagement-queue-api-68fd4c96df-8slrm                     2/2     Running   2          33h   10.245.3.55    10.20.0.7     <none>           <none>
mercury-engagement-queue-api-68fd4c96df-z46rc                     2/2     Running   2          33h   10.245.2.140   10.20.1.191   <none>           <none>
mercury-engagement-queue-api-68fd4c96df-zcmfj                     2/2     Running   2          33h   10.245.2.42    10.20.1.35    <none>           <none>
mercury-enrichment-service-84c964b545-clw7g                       2/2     Running   0          33h   10.245.3.243   10.20.0.250   <none>           <none>
mercury-enrichment-service-84c964b545-gqvh2                       2/2     Running   0          33h   10.245.1.224   10.20.0.29    <none>           <none>
mercury-enrichment-service-84c964b545-zhkmp                       2/2     Running   0          33h   10.245.0.240   10.20.0.244   <none>           <none>
mercury-event-sync-service-57d9bd5765-96x4k                       2/2     Running   0          33h   10.245.4.29    10.20.1.27    <none>           <none>
mercury-event-sync-service-57d9bd5765-cbfqt                       2/2     Running   0          33h   10.245.7.181   10.20.1.224   <none>           <none>
mercury-event-sync-service-57d9bd5765-wpzkk                       2/2     Running   0          33h   10.245.0.50    10.20.1.247   <none>           <none>
mercury-integration-in-processor-849dff9756-4gcgz                 2/2     Running   3          33h   10.245.7.180   10.20.1.224   <none>           <none>
mercury-integration-in-processor-849dff9756-dl2p8                 2/2     Running   3          33h   10.245.3.241   10.20.0.250   <none>           <none>
mercury-integration-in-processor-849dff9756-pm6md                 2/2     Running   0          33h   10.245.4.41    10.20.1.27    <none>           <none>
mercury-integration-out-processor-689bdb99c4-66xvr                2/2     Running   3          33h   10.245.3.242   10.20.0.250   <none>           <none>
mercury-integration-out-processor-689bdb99c4-bldbg                2/2     Running   2          33h   10.245.4.30    10.20.1.27    <none>           <none>
mercury-integration-out-processor-689bdb99c4-vl4cv                2/2     Running   2          33h   10.245.7.182   10.20.1.224   <none>           <none>
mercury-kweet-facebook-client-7fddbc78f9-242wn                    2/2     Running   0          33h   10.245.8.141   10.20.1.158   <none>           <none>
mercury-kweet-facebook-client-7fddbc78f9-ckf75                    2/2     Running   0          33h   10.245.4.11    10.20.1.27    <none>           <none>
mercury-kweet-facebook-client-7fddbc78f9-nprbf                    2/2     Running   2          33h   10.245.1.94    10.20.1.38    <none>           <none>
mercury-kweet-facebook-webhook-5bc7986869-2npxp                   2/2     Running   0          33h   10.245.4.21    10.20.1.27    <none>           <none>
mercury-kweet-facebook-webhook-5bc7986869-bdtjb                   2/2     Running   3          33h   10.245.4.138   10.20.1.227   <none>           <none>
mercury-kweet-facebook-webhook-5bc7986869-d2r8q                   2/2     Running   3          33h   10.245.2.39    10.20.1.35    <none>           <none>
mercury-kweet-twiliosms-client-6b8f6ffc78-46p9m                   2/2     Running   0          33h   10.245.8.142   10.20.1.158   <none>           <none>
mercury-kweet-twiliosms-client-6b8f6ffc78-5zv2v                   2/2     Running   0          33h   10.245.6.7     10.20.1.44    <none>           <none>
mercury-kweet-twiliosms-client-6b8f6ffc78-tfqd8                   2/2     Running   0          33h   10.245.1.225   10.20.0.29    <none>           <none>
mercury-kweet-userprofiles-d9c5cdc6b-gqdhb                        2/2     Running   0          33h   10.245.8.144   10.20.1.158   <none>           <none>
mercury-kweet-userprofiles-d9c5cdc6b-jmwwb                        2/2     Running   0          33h   10.245.1.96    10.20.1.38    <none>           <none>
mercury-kweet-userprofiles-d9c5cdc6b-vzb7b                        2/2     Running   3          33h   10.245.4.14    10.20.1.27    <none>           <none>
mercury-mercury-ui-6f58d49b55-92tl8                               2/2     Running   0          33h   10.245.5.182   10.20.0.143   <none>           <none>
mercury-mercury-ui-6f58d49b55-jdvnq                               2/2     Running   0          33h   10.245.4.23    10.20.1.27    <none>           <none>
mercury-mercury-ui-6f58d49b55-zc7m2                               2/2     Running   0          33h   10.245.4.143   10.20.1.227   <none>           <none>
mercury-metric-aggregation-processor-68ff54fdb9-bkf8j             2/2     Running   0          33h   10.245.4.22    10.20.1.27    <none>           <none>
mercury-metric-aggregation-processor-68ff54fdb9-pbf9c             2/2     Running   0          33h   10.245.4.137   10.20.1.227   <none>           <none>
mercury-metric-aggregation-processor-68ff54fdb9-rldmm             2/2     Running   0          33h   10.245.5.180   10.20.0.143   <none>           <none>
mercury-metric-fusion-bridge-7f49b75ffb-gpj4g                     2/2     Running   0          33h   10.245.3.64    10.20.0.7     <none>           <none>
mercury-metric-fusion-bridge-7f49b75ffb-s7x7t                     2/2     Running   2          33h   10.245.4.140   10.20.1.227   <none>           <none>
mercury-metric-fusion-bridge-7f49b75ffb-wqf2r                     2/2     Running   0          33h   10.245.8.148   10.20.1.158   <none>           <none>
mercury-metric-generation-processor-54447b6657-gjjqd              2/2     Running   0          33h   10.245.2.36    10.20.1.35    <none>           <none>
mercury-metric-generation-processor-54447b6657-mbz9f              2/2     Running   0          33h   10.245.6.9     10.20.1.44    <none>           <none>
mercury-metric-generation-processor-54447b6657-qpcqc              2/2     Running   0          33h   10.245.1.98    10.20.1.38    <none>           <none>
mercury-metric-internal-translation-processor-57cf478c6-l2sb4     2/2     Running   2          33h   10.245.7.185   10.20.1.224   <none>           <none>
mercury-metric-internal-translation-processor-57cf478c6-lc8df     2/2     Running   2          33h   10.245.6.169   10.20.1.239   <none>           <none>
mercury-metric-internal-translation-processor-57cf478c6-zwlmn     2/2     Running   0          33h   10.245.0.53    10.20.1.247   <none>           <none>
mercury-metric-proxy-service-576df9c79f-6wgr6                     2/2     Running   0          33h   10.245.4.20    10.20.1.27    <none>           <none>
mercury-metric-proxy-service-576df9c79f-hsvfd                     2/2     Running   3          33h   10.245.1.101   10.20.1.38    <none>           <none>
mercury-metric-proxy-service-576df9c79f-vsfzd                     2/2     Running   0          33h   10.245.2.38    10.20.1.35    <none>           <none>
mercury-omnichannel-assignment-processor-587b8c9b6b-2675k         2/2     Running   2          33h   10.245.1.103   10.20.1.38    <none>           <none>
mercury-omnichannel-assignment-processor-587b8c9b6b-5nlch         2/2     Running   2          33h   10.245.8.147   10.20.1.158   <none>           <none>
mercury-omnichannel-assignment-processor-587b8c9b6b-fh9pr         2/2     Running   3          33h   10.245.4.26    10.20.1.27    <none>           <none>
mercury-omnichannel-offer-processor-76f947c586-9qfh6              2/2     Running   3          33h   10.245.3.52    10.20.0.7     <none>           <none>
mercury-omnichannel-offer-processor-76f947c586-l2c5q              2/2     Running   2          33h   10.245.2.133   10.20.1.191   <none>           <none>
mercury-omnichannel-offer-processor-76f947c586-x9sgc              2/2     Running   3          33h   10.245.8.146   10.20.1.158   <none>           <none>
mercury-osvc-bridge-api-services-756ff69dbd-b89zr                 2/2     Running   3          33h   10.245.5.187   10.20.0.143   <none>           <none>
mercury-osvc-bridge-api-services-756ff69dbd-jhlqf                 2/2     Running   2          33h   10.245.3.50    10.20.0.7     <none>           <none>
mercury-osvc-bridge-api-services-756ff69dbd-t2prz                 2/2     Running   3          33h   10.245.1.104   10.20.1.38    <none>           <none>
mercury-osvc-bridge-metrics-data-pipeline-59bfff645f-dbwpm        2/2     Running   0          33h   10.245.3.65    10.20.0.7     <none>           <none>
mercury-osvc-bridge-metrics-data-pipeline-59bfff645f-p6v69        2/2     Running   0          33h   10.245.2.137   10.20.1.191   <none>           <none>
mercury-osvc-bridge-metrics-data-pipeline-59bfff645f-vc5gq        2/2     Running   0          33h   10.245.5.192   10.20.0.143   <none>           <none>
mercury-osvc-bridge-osvc-data-extractor-855f9bd5bc-28rkm          2/2     Running   0          33h   10.245.2.136   10.20.1.191   <none>           <none>
mercury-osvc-bridge-osvc-data-extractor-855f9bd5bc-g9p5r          2/2     Running   1          33h   10.245.6.16    10.20.1.44    <none>           <none>
mercury-osvc-bridge-osvc-data-extractor-855f9bd5bc-grdg9          2/2     Running   3          33h   10.245.5.188   10.20.0.143   <none>           <none>
mercury-osvc-bridge-provisioning-processor-67cb69489-9llvc        2/2     Running   0          33h   10.245.0.51    10.20.1.247   <none>           <none>
mercury-osvc-bridge-state-processor-b655b7468-8k4xw               2/2     Running   0          33h   10.245.4.40    10.20.1.27    <none>           <none>
mercury-osvc-bridge-state-processor-b655b7468-ldjqw               2/2     Running   0          33h   10.245.2.143   10.20.1.191   <none>           <none>
mercury-osvc-bridge-state-processor-b655b7468-qtnjl               2/2     Running   0          33h   10.245.8.36    10.20.0.134   <none>           <none>
mercury-osvc-bridge-state-query-service-64dd4bbdc8-6lxwl          2/2     Running   3          33h   10.245.5.183   10.20.0.143   <none>           <none>
mercury-osvc-bridge-state-query-service-64dd4bbdc8-gx2x4          2/2     Running   2          33h   10.245.1.102   10.20.1.38    <none>           <none>
mercury-osvc-bridge-state-query-service-64dd4bbdc8-t7lxk          2/2     Running   3          33h   10.245.6.14    10.20.1.44    <none>           <none>
mercury-osvc-bridge-task-controller-569d4f4ff-67vcr               2/2     Running   0          33h   10.245.5.189   10.20.0.143   <none>           <none>
mercury-osvc-bridge-task-controller-569d4f4ff-69gtw               2/2     Running   1          33h   10.245.1.106   10.20.1.38    <none>           <none>
mercury-osvc-bridge-task-controller-569d4f4ff-8g589               2/2     Running   1          33h   10.245.6.17    10.20.1.44    <none>           <none>
mercury-provisioning-monitor-676848bb67-dqx88                     2/2     Running   0          33h   10.245.0.49    10.20.1.247   <none>           <none>
mercury-provisioning-monitor-676848bb67-k7sxg                     2/2     Running   0          33h   10.245.1.107   10.20.1.38    <none>           <none>
mercury-provisioning-monitor-676848bb67-kfwjp                     2/2     Running   0          33h   10.245.0.239   10.20.0.244   <none>           <none>
mercury-provisioning-processor-769945d9d8-6ddck                   2/2     Running   3          33h   10.245.4.17    10.20.1.27    <none>           <none>
mercury-provisioning-processor-769945d9d8-csqcn                   2/2     Running   3          33h   10.245.2.37    10.20.1.35    <none>           <none>
mercury-provisioning-processor-769945d9d8-dd7wr                   2/2     Running   0          33h   10.245.1.112   10.20.1.38    <none>           <none>
mercury-psr-kafka-entity-operator-858f557bb9-xzhlh                3/3     Running   0          33h   10.245.5.14    10.20.0.4     <none>           <none>
mercury-psr-kafka-kafka-0                                         2/2     Running   0          33h   10.245.5.11    10.20.0.4     <none>           <none>
mercury-psr-kafka-kafka-1                                         2/2     Running   0          33h   10.245.8.138   10.20.1.158   <none>           <none>
mercury-psr-kafka-kafka-2                                         2/2     Running   0          33h   10.245.1.223   10.20.0.29    <none>           <none>
mercury-psr-kafka-kafka-3                                         2/2     Running   0          33h   10.245.5.12    10.20.0.4     <none>           <none>
mercury-psr-kafka-kafka-4                                         2/2     Running   0          33h   10.245.8.139   10.20.1.158   <none>           <none>
mercury-psr-kafka-kafka-5                                         2/2     Running   0          33h   10.245.7.179   10.20.1.224   <none>           <none>
mercury-psr-kafka-kafka-6                                         2/2     Running   0          33h   10.245.5.13    10.20.0.4     <none>           <none>
mercury-psr-kafka-kafka-7                                         2/2     Running   0          33h   10.245.8.140   10.20.1.158   <none>           <none>
mercury-psr-kafka-kafka-8                                         2/2     Running   0          33h   10.245.2.131   10.20.1.191   <none>           <none>
mercury-psr-kafka-kafka-exporter-75f8b86dc4-czj8f                 1/1     Running   0          33h   10.245.5.15    10.20.0.4     <none>           <none>
mercury-psr-kafka-zookeeper-0                                     1/1     Running   0          33h   10.245.5.10    10.20.0.4     <none>           <none>
mercury-psr-kafka-zookeeper-1                                     1/1     Running   0          33h   10.245.8.137   10.20.1.158   <none>           <none>
mercury-psr-kafka-zookeeper-2                                     1/1     Running   0          33h   10.245.7.17    10.20.1.52    <none>           <none>
mercury-queue-agent-info-processor-5b4fcbb58c-pfm42               2/2     Running   3          33h   10.245.6.18    10.20.1.44    <none>           <none>
mercury-queue-agent-info-processor-5b4fcbb58c-v4dm6               2/2     Running   3          33h   10.245.4.149   10.20.1.227   <none>           <none>
mercury-queue-agent-info-processor-5b4fcbb58c-x5jch               2/2     Running   3          33h   10.245.5.191   10.20.0.143   <none>           <none>
mercury-realtime-channel-processor-64755f8cc9-4k5kj               2/2     Running   0          33h   10.245.4.144   10.20.1.227   <none>           <none>
mercury-realtime-channel-processor-64755f8cc9-jdzdd               2/2     Running   0          33h   10.245.6.15    10.20.1.44    <none>           <none>
mercury-realtime-channel-processor-64755f8cc9-tjzrw               2/2     Running   0          33h   10.245.5.184   10.20.0.143   <none>           <none>
mercury-resource-channel-processor-7d96c47886-4lnnz               2/2     Running   3          33h   10.245.2.142   10.20.1.191   <none>           <none>
mercury-resource-channel-processor-7d96c47886-hpvkk               2/2     Running   3          33h   10.245.2.44    10.20.1.35    <none>           <none>
mercury-resource-channel-processor-7d96c47886-khkqw               2/2     Running   2          33h   10.245.0.241   10.20.0.244   <none>           <none>
mercury-resource-state-processor-6c47fbbcb5-4kvhf                 2/2     Running   0          33h   10.245.3.53    10.20.0.7     <none>           <none>
mercury-resource-state-processor-6c47fbbcb5-mz22q                 2/2     Running   0          33h   10.245.2.41    10.20.1.35    <none>           <none>
mercury-resource-state-processor-6c47fbbcb5-x2w2s                 2/2     Running   0          33h   10.245.4.150   10.20.1.227   <none>           <none>
mercury-resource-work-processor-d447bfd4c-6lhkn                   2/2     Running   0          33h   10.245.5.179   10.20.0.143   <none>           <none>
mercury-resource-work-processor-d447bfd4c-ch5nj                   2/2     Running   0          33h   10.245.6.10    10.20.1.44    <none>           <none>
mercury-resource-work-processor-d447bfd4c-ctd2n                   2/2     Running   0          33h   10.245.4.136   10.20.1.227   <none>           <none>
mercury-routing-processor-agent-assignment-774885f97c-64tct       2/2     Running   2          33h   10.245.4.33    10.20.1.27    <none>           <none>
mercury-routing-processor-agent-assignment-774885f97c-8jbsz       2/2     Running   0          33h   10.245.2.144   10.20.1.191   <none>           <none>
mercury-routing-processor-agent-assignment-774885f97c-mfjdq       2/2     Running   2          33h   10.245.0.52    10.20.1.247   <none>           <none>
mercury-routing-processor-agent-events-processor-cd4c7689c7xct7   2/2     Running   0          33h   10.245.6.20    10.20.1.44    <none>           <none>
mercury-routing-processor-agent-events-processor-cd4c7689ccjmqv   2/2     Running   0          33h   10.245.3.245   10.20.0.250   <none>           <none>
mercury-routing-processor-agent-events-processor-cd4c7689cq6db2   2/2     Running   0          33h   10.245.4.152   10.20.1.227   <none>           <none>
mercury-routing-processor-queue-assignment-5464cd88bf-8j8pj       2/2     Running   2          33h   10.245.3.244   10.20.0.250   <none>           <none>
mercury-routing-processor-queue-assignment-5464cd88bf-bq6j9       2/2     Running   2          33h   10.245.6.19    10.20.1.44    <none>           <none>
mercury-routing-processor-queue-assignment-5464cd88bf-jnzl6       2/2     Running   3          33h   10.245.4.151   10.20.1.227   <none>           <none>
mercury-routing-processor-work-events-processor-7b6df456d826f79   2/2     Running   3          33h   10.245.2.132   10.20.1.191   <none>           <none>
mercury-routing-processor-work-events-processor-7b6df456d8h5n8x   2/2     Running   0          33h   10.245.5.198   10.20.0.143   <none>           <none>
mercury-routing-processor-work-events-processor-7b6df456d8hl7vv   2/2     Running   2          33h   10.245.0.238   10.20.0.244   <none>           <none>
mercury-session-housekeeping-processor-788fdfd5bb-2ncbw           2/2     Running   3          33h   10.245.5.196   10.20.0.143   <none>           <none>
mercury-session-housekeeping-processor-788fdfd5bb-l9z25           2/2     Running   2          33h   10.245.7.186   10.20.1.224   <none>           <none>
mercury-session-housekeeping-processor-788fdfd5bb-zhv58           2/2     Running   2          33h   10.245.4.35    10.20.1.27    <none>           <none>
mercury-session-processor-66d5445868-chdph                        2/2     Running   2          33h   10.245.5.195   10.20.0.143   <none>           <none>
mercury-session-processor-66d5445868-k67fv                        2/2     Running   2          33h   10.245.7.184   10.20.1.224   <none>           <none>
mercury-session-processor-66d5445868-pmv5w                        2/2     Running   2          33h   10.245.3.58    10.20.0.7     <none>           <none>
mercury-single-sign-on-service-5cd6974f66-7k458                   2/2     Running   2          33h   10.245.2.138   10.20.1.191   <none>           <none>
mercury-single-sign-on-service-5cd6974f66-dd78x                   2/2     Running   3          33h   10.245.5.193   10.20.0.143   <none>           <none>
mercury-single-sign-on-service-5cd6974f66-wmcb7                   2/2     Running   3          33h   10.245.4.32    10.20.1.27    <none>           <none>
mercury-social-bridge-5947d4fbb-6q7vt                             2/2     Running   3          33h   10.245.4.31    10.20.1.27    <none>           <none>
mercury-social-bridge-5947d4fbb-89x22                             2/2     Running   0          33h   10.245.8.37    10.20.0.134   <none>           <none>
mercury-social-bridge-5947d4fbb-v485v                             2/2     Running   2          33h   10.245.1.108   10.20.1.38    <none>           <none>
mercury-social-config-6b4cd9cb4d-dsmf5                            2/2     Running   3          33h   10.245.4.34    10.20.1.27    <none>           <none>
mercury-social-config-6b4cd9cb4d-m6txv                            2/2     Running   3          33h   10.245.1.109   10.20.1.38    <none>           <none>
mercury-social-config-6b4cd9cb4d-nlzkl                            2/2     Running   2          33h   10.245.5.194   10.20.0.143   <none>           <none>
mercury-static-assets-service-5b54b4c7d7-778vj                    2/2     Running   0          33h   10.245.8.35    10.20.0.134   <none>           <none>
mercury-static-assets-service-5b54b4c7d7-pbb5x                    2/2     Running   0          33h   10.245.3.63    10.20.0.7     <none>           <none>
mercury-static-assets-service-5b54b4c7d7-qd4hr                    2/2     Running   0          33h   10.245.1.110   10.20.1.38    <none>           <none>
mercury-tenant-downtime-monitor-78fdf4c974-hdg4h                  2/2     Running   0          33h   10.245.0.48    10.20.1.247   <none>           <none>
mercury-tenant-downtime-monitor-78fdf4c974-tp6gz                  2/2     Running   1          33h   10.245.1.105   10.20.1.38    <none>           <none>
mercury-tenant-downtime-monitor-78fdf4c974-w9bsj                  2/2     Running   1          33h   10.245.4.28    10.20.1.27    <none>           <none>
mercury-transcript-api-66568cb7cd-lj6n6                           2/2     Running   3          33h   10.245.4.148   10.20.1.227   <none>           <none>
mercury-transcript-api-66568cb7cd-n96zs                           2/2     Running   0          33h   10.245.5.185   10.20.0.143   <none>           <none>
mercury-transcript-api-66568cb7cd-qkngj                           2/2     Running   3          33h   10.245.0.237   10.20.0.244   <none>           <none>
mercury-transcript-processor-64779cbcd8-c4mx4                     2/2     Running   2          33h   10.245.1.111   10.20.1.38    <none>           <none>
mercury-transcript-processor-64779cbcd8-kkfbl                     2/2     Running   2          33h   10.245.8.149   10.20.1.158   <none>           <none>
mercury-transcript-processor-64779cbcd8-qq7k4                     2/2     Running   2          33h   10.245.4.39    10.20.1.27    <none>           <none>
mercury-user-preference-service-54c6587fc8-2h87d                  2/2     Running   0          33h   10.245.1.97    10.20.1.38    <none>           <none>
mercury-user-preference-service-54c6587fc8-ddlrk                  2/2     Running   0          33h   10.245.2.35    10.20.1.35    <none>           <none>
mercury-user-preference-service-54c6587fc8-k72kq                  2/2     Running   0          33h   10.245.6.8     10.20.1.44    <none>           <none>
mercury-work-api-b468f7db7-fcf62                                  2/2     Running   3          33h   10.245.2.43    10.20.1.35    <none>           <none>
mercury-work-api-b468f7db7-qsm7h                                  2/2     Running   3          33h   10.245.3.56    10.20.0.7     <none>           <none>
mercury-work-api-b468f7db7-zccn4                                  2/2     Running   3          33h   10.245.7.183   10.20.1.224   <none>           <none>
mercury-work-processor-6b888f5bbf-2twtx                           2/2     Running   0          33h   10.245.4.13    10.20.1.27    <none>           <none>
mercury-work-processor-6b888f5bbf-fwhks                           2/2     Running   0          33h   10.245.5.178   10.20.0.143   <none>           <none>
mercury-work-processor-6b888f5bbf-qnxpp                           2/2     Running   0          33h   10.245.4.135   10.20.1.227   <none>           <none>
[21:January:2022:08:12:53]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[21:January:2022:08:13:04]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ hln mercury-psr
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_prod.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_prod.config
NAME      NAMESPACE   REVISION  UPDATED                                 STATUS    CHART                 APP VERSION
app-kafka mercury-psr 1         2022-01-19 22:48:59.456337456 +0000 UTC deployed  cpe-kafka-0.1.0       0.1.0
appdata   mercury-psr 1         2022-01-19 22:49:17.75707579 +0000 UTC  deployed  kafka-data-0.1.0      null
mercury   mercury-psr 1         2022-01-19 22:50:04.789572242 +0000 UTC deployed  mercury-2201.18.1305  22.01.18-REL21.11-1305
[21:January:2022:08:13:17]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ helmfile -e prod -f ./mercury-psr/helmfile-mercury-psr-apps.yaml -l region=us-phoenix-1 diff --concurrency 3
Comparing release=mercury, chart=osvc-helm-virtual/mercury
mercury-psr, mercury-agent-command-service-keystore, Secret (v1) has changed:
  # Source: mercury/charts/agent-command-service/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-agent-command-service-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-channel-api-keystore, Secret (v1) has changed:
  # Source: mercury/charts/channel-api/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-channel-api-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-consumer-command-service-keystore, Secret (v1) has changed:
  # Source: mercury/charts/consumer-command-service/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-consumer-command-service-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-custom-availability-service-keystore, Secret (v1) has changed:
  # Source: mercury/charts/custom-availability-service/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-custom-availability-service-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-data-mask-api-keystore, Secret (v1) has changed:
  # Source: mercury/charts/data-mask-api/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-data-mask-api-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-engagement-queue-api-keystore, Secret (v1) has changed:
  # Source: mercury/charts/engagement-queue-api/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-engagement-queue-api-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-enrichment-service, Deployment (apps) has changed:
  # Source: mercury/charts/enrichment-service/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: mercury-enrichment-service
    labels:
      project: mercury
      version: "0"
      app.kubernetes.io/name: enrichment-service
      helm.sh/chart: enrichment-service-22.01.18-REL21.11-318
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/managed-by: Helm
      monitoring: "true"
    annotations:
  spec:
    replicas: 3
    selector:
      matchLabels:
        app.kubernetes.io/name: enrichment-service
        app.kubernetes.io/instance: mercury
    strategy:
      type: RollingUpdate
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
    template:
      metadata:
        annotations:
          vault.security.banzaicloud.io/log-level: warn
          vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
          vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
          vault.security.banzaicloud.io/vault-env-daemon: "true"
          vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
          vault.security.banzaicloud.io/vault-role: mercury-psr
          vault.security.banzaicloud.io/vault-skip-verify: "true"
        labels:
          app.kubernetes.io/name: enrichment-service
          app.kubernetes.io/instance: mercury
          chronos.enabled: "True"
          chronos.maxPodLifetime: "1440"
          chronos.minAvailable: "0"
      spec:
        serviceAccountName: mercurygeneric
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - "enrichment-service"
                  - key: release
                    operator: In
                    values:
                    - mercury
                topologyKey: "failure-domain.beta.kubernetes.io/zone"
        imagePullSecrets:
          - name: osvcstage-ocirsecret
        volumes:
        - name: vaultca
          secret:
            secretName: vault-tls
        - name: keystore-pass
          secret:
            secretName: mercury-enrichment-service-keystore
        - name: keystore-volume
          emptyDir: {}
        initContainers:
        - name: init
          image: iad.ocir.io/osvcstage/mercury/init-container/prod:1.0.0.18
          imagePullPolicy: IfNotPresent
          command: ['/bin/bash', '-c', '/init/init-script.sh']
          # Need to run the init container as root (also we're forced to use its UID, i.e. 0)
          securityContext:
            runAsUser: 0
          env:
          - name: AUTO_REFRESH_CONSUL_TOKEN
            value: vault:cpe_consul/creds/mercury-psr#token
          - name: KAFKA_TEMPLATED_PKI
            value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
              .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
              "ttl": "720h"}'
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: TRUSTSTORE_NAME
            value: truststore.jks
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: props/kafka.properties
          - name: KEYSTORE_NAME
            value: keystore.jks
          - name: SERVICEACCOUNT_SECRETPATH
            value: /var/run/secrets/kubernetes.io/serviceaccount
          - name: VAULT_ADDR
            value: "https://vault.query.prod.consul:8200"
          - name: VAULT_SECRETPATH
            value: /tmp/vaultca
          - name: PKI_VAULT_ENGINE_PATH
            value: "v1/infra_pki/issue/kafka_client"
          - name: CERT_CN
            value: "mercury-psr-mercurygeneric-rw"
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          - name: VAULT_SERVICEACCOUNT
            value: mercurygeneric
          volumeMounts:
          - name: keystore-volume
            mountPath: /keystores
          - name: vaultca
            mountPath: /tmp/vaultca
            readOnly: true
          - name: keystore-pass
            mountPath: /tmp/keystore-pass
            readOnly: true
          resources:
              limits:
                cpu: "1"
                ephemeral-storage: 10Gi
                memory: 6Gi
              requests:
                cpu: 100m
                ephemeral-storage: 2Gi
                memory: 2Gi
        containers:
          - name: enrichment-service
            image: "iad.ocir.io/osvcstage/mercury/enrichment-service:22.01.18-REL21.11-318"
            imagePullPolicy: IfNotPresent
            env:
            - name: profile
              value: oci-k8s
            - name: DATACENTER
              value: OCI
            - name: RELEASE_NAME
              value: mercury
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: POD_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            - name: MERCURY_URL_PATTERN
              value: https://engagement-psr.us1.channels.ocs.oraclecloud.com/mercury-psr
            - name: JAEGER_REMOTE_REPORTER
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: VAULT_TOKEN
              value: vault:login
            - name: HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: CONSUL_HTTP_ADDR
              value: https://$(HOST_IP):8501
            - name: CONSUL_TOKEN
              value: vault:/cpe_consul/creds/mercury-psr#token
            - name: JAVA_OPTS
              value: "-Dvault.enabled=true \
                      -Dvault.namespace=mercury-psr \
                      -Dvault.transit.kafkaMessageEncryption.path=transit_mercury \
                      -Dvault.base.address=https://vault.query.prod.consul:8200 \
                      -Dvault.shared.kv.engine=/kv \
                      -Dthidwick.vault.uri=https://vault.query.prod.consul:8200 \
                      -Dthidwick.vault.transit.cache-expiration=300 \
                      -Dthidwick.vault.kv.cache-expiration=300 \
                      -Dthidwick.vault.serviceaccount=mercurygeneric \
                      -Dlog4j2.formatMsgNoLookups=true \
                      -Dmercury.cpeVersion=v2
                      -Dfile.token.enabled=true \
                      -Dvault.authentication=TOKEN \
                      -Dvault.role=mercury-psr \
                      -Dvault.path=k8s-phx-dataplane
                      -Dkafka.log_level=WARN \
                      -Dthidwick-server.logback.level=INFO \
                      -Dthidwick-server.logback.service.log_level=DEBUG \
                      -Dthidwick.kafka.message.encryption.cipher=AES/GCM \
                      -Djwt.tenant.validation.enabled=false \
                      -Dthidwick.kafka.schema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul \
                      -Dthidwick.kafka.bootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443 \
                      -Dthidwick.tms.base.tmsStateStream.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443 \
                      -Djavax.net.ssl.trustStore=/keystores/truststore.jks \
                      -Djavax.net.ssl.keyStore=/keystores/keystore.jks \
                      -Dhttp.security.headers.enabled=true \
                      -Dhttp.rest.security.headers.enabled=true \
-                     -XX:+UseG1GC -XX:MaxGCPauseMillis=250 \
-                     -Xms1536m \
+                     -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -DwebHook.topic.name=helios-psr.priority.external.event -Dhelios.error.event=helios-psr.error.event                    -Xms1536m \
                      -Xmx4096m \
                      -Dmercury.tenant.name.filter=\"^(evai)\"                 -Dtenant.realm=engagement-psr.us1.channels.ocs.oraclecloud.com "
            volumeMounts:
            - mountPath: /keystores
              name: keystore-volume
            - mountPath: /tmp/keystore-pass
              name: keystore-pass
            ports:
              - name: http
                containerPort: 8080
                protocol: TCP
            startupProbe:
              periodSeconds: 10
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 120
              httpGet:
                path: /health/checks
                port: 8080
            livenessProbe:
              initialDelaySeconds: 120
              periodSeconds: 30
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 1
              httpGet:
                path: /health/checks
                port: 8080
            readinessProbe:
              initialDelaySeconds: 120
              periodSeconds: 30
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 1
              httpGet:
                path: /health/ping
                port: 8080
            # Service deployment provided resources
            resources:
              limits:
                cpu: "1"
                ephemeral-storage: 10Gi
                memory: 6Gi
              requests:
                cpu: 100m
                ephemeral-storage: 2Gi
                memory: 2Gi
mercury-psr, mercury-enrichment-service-keystore, Secret (v1) has changed:
  # Source: mercury/charts/enrichment-service/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-enrichment-service-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-event-sync-service, Deployment (apps) has changed:
  # Source: mercury/charts/event-sync-service/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: mercury-event-sync-service
    labels:
      project: mercury
      version: "0"
      app.kubernetes.io/name: event-sync-service
      helm.sh/chart: event-sync-service-22.01.18-REL21.11-184
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/managed-by: Helm
      monitoring: "true"
    annotations:
  spec:
    replicas: 3
    selector:
      matchLabels:
        app.kubernetes.io/name: event-sync-service
        app.kubernetes.io/instance: mercury
    strategy:
      type: RollingUpdate
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
    template:
      metadata:
        annotations:
          vault.security.banzaicloud.io/log-level: warn
          vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
          vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
          vault.security.banzaicloud.io/vault-env-daemon: "true"
          vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
          vault.security.banzaicloud.io/vault-role: mercury-psr
          vault.security.banzaicloud.io/vault-skip-verify: "true"
        labels:
          app.kubernetes.io/name: event-sync-service
          app.kubernetes.io/instance: mercury
          chronos.enabled: "True"
          chronos.maxPodLifetime: "1440"
          chronos.minAvailable: "0"
      spec:
        serviceAccountName: mercurygeneric
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - "event-sync-service"
                  - key: release
                    operator: In
                    values:
                    - mercury
                topologyKey: "failure-domain.beta.kubernetes.io/zone"
        imagePullSecrets:
          - name: osvcstage-ocirsecret
        volumes:
        - name: vaultca
          secret:
            secretName: vault-tls
        - name: keystore-pass
          secret:
            secretName: mercury-event-sync-service-keystore
        - name: keystore-volume
          emptyDir: {}
        initContainers:
        - name: init
          image: iad.ocir.io/osvcstage/mercury/init-container/prod:1.0.0.18
          imagePullPolicy: IfNotPresent
          command: ['/bin/bash', '-c', '/init/init-script.sh']
          # Need to run the init container as root (also we're forced to use its UID, i.e. 0)
          securityContext:
            runAsUser: 0
          env:
          - name: AUTO_REFRESH_CONSUL_TOKEN
            value: vault:cpe_consul/creds/mercury-psr#token
          - name: KAFKA_TEMPLATED_PKI
            value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
              .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
              "ttl": "720h"}'
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: TRUSTSTORE_NAME
            value: truststore.jks
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: props/kafka.properties
          - name: KEYSTORE_NAME
            value: keystore.jks
          - name: SERVICEACCOUNT_SECRETPATH
            value: /var/run/secrets/kubernetes.io/serviceaccount
          - name: VAULT_ADDR
            value: "https://vault.query.prod.consul:8200"
          - name: VAULT_SECRETPATH
            value: /tmp/vaultca
          - name: PKI_VAULT_ENGINE_PATH
            value: "v1/infra_pki/issue/kafka_client"
          - name: CERT_CN
            value: "mercury-psr-mercurygeneric-rw"
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          - name: VAULT_SERVICEACCOUNT
            value: mercurygeneric
          volumeMounts:
          - name: keystore-volume
            mountPath: /keystores
          - name: vaultca
            mountPath: /tmp/vaultca
            readOnly: true
          - name: keystore-pass
            mountPath: /tmp/keystore-pass
            readOnly: true
          resources:
              limits:
                cpu: "1"
                ephemeral-storage: 10Gi
                memory: 6Gi
              requests:
                cpu: 100m
                ephemeral-storage: 2Gi
                memory: 2Gi
        containers:
          - name: event-sync-service
            image: "iad.ocir.io/osvcstage/mercury/event-sync-service:22.01.18-REL21.11-184"
            imagePullPolicy: IfNotPresent
            env:
            - name: profile
              value: oci-k8s
            - name: DATACENTER
              value: OCI
            - name: RELEASE_NAME
              value: mercury
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: POD_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            - name: MERCURY_URL_PATTERN
              value: https://engagement-psr.us1.channels.ocs.oraclecloud.com/mercury-psr
            - name: JAEGER_REMOTE_REPORTER
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: VAULT_TOKEN
              value: vault:login
            - name: HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: CONSUL_HTTP_ADDR
              value: https://$(HOST_IP):8501
            - name: CONSUL_TOKEN
              value: vault:/cpe_consul/creds/mercury-psr#token
            - name: JAVA_OPTS
              value: "-Dvault.enabled=true \
                      -Dvault.namespace=mercury-psr \
                      -Dvault.transit.kafkaMessageEncryption.path=transit_mercury \
                      -Dvault.base.address=https://vault.query.prod.consul:8200 \
                      -Dvault.shared.kv.engine=/kv \
                      -Dthidwick.vault.uri=https://vault.query.prod.consul:8200 \
                      -Dthidwick.vault.transit.cache-expiration=300 \
                      -Dthidwick.vault.kv.cache-expiration=300 \
                      -Dthidwick.vault.serviceaccount=mercurygeneric \
                      -Dlog4j2.formatMsgNoLookups=true \
                      -Dmercury.cpeVersion=v2
                      -Dfile.token.enabled=true \
                      -Dvault.authentication=TOKEN \
                      -Dvault.role=mercury-psr \
                      -Dvault.path=k8s-phx-dataplane
                      -Dkafka.log_level=WARN \
                      -Dthidwick-server.logback.level=INFO \
                      -Dthidwick-server.logback.service.log_level=DEBUG \
                      -Dthidwick.kafka.message.encryption.cipher=AES/GCM \
                      -Djwt.tenant.validation.enabled=false \
                      -Dthidwick.kafka.schema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul \
                      -Dthidwick.kafka.bootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443 \
                      -Dthidwick.tms.base.tmsStateStream.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443 \
                      -Djavax.net.ssl.trustStore=/keystores/truststore.jks \
                      -Djavax.net.ssl.keyStore=/keystores/keystore.jks \
                      -Dhttp.security.headers.enabled=true \
                      -Dhttp.rest.security.headers.enabled=true \
-                     -XX:+UseG1GC -XX:MaxGCPauseMillis=250 \
-                     -Xms1536m \
+                     -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Dwebhook.topic.name=helios-psr.external.event -Dhelios.error.event=helios-psr.error.event                    -Xms1536m \
                      -Xmx4096m \
                      -Dmercury.tenant.name.filter=\"^(evai)\"                 -Dtenant.realm=engagement-psr.us1.channels.ocs.oraclecloud.com "
            volumeMounts:
            - mountPath: /keystores
              name: keystore-volume
            - mountPath: /tmp/keystore-pass
              name: keystore-pass
            ports:
              - name: http
                containerPort: 8080
                protocol: TCP
            startupProbe:
              periodSeconds: 10
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 120
              httpGet:
                path: /health/checks
                port: 8080
            livenessProbe:
              initialDelaySeconds: 120
              periodSeconds: 30
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 1
              httpGet:
                path: /health/checks
                port: 8080
            readinessProbe:
              initialDelaySeconds: 120
              periodSeconds: 30
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 1
              httpGet:
                path: /health/ping
                port: 8080
            # Service deployment provided resources
            resources:
              limits:
                cpu: "1"
                ephemeral-storage: 10Gi
                memory: 6Gi
              requests:
                cpu: 100m
                ephemeral-storage: 2Gi
                memory: 2Gi
mercury-psr, mercury-event-sync-service-keystore, Secret (v1) has changed:
  # Source: mercury/charts/event-sync-service/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-event-sync-service-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-integration-in-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/integration-in-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-integration-in-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-integration-out-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/integration-out-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-integration-out-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-kweet-facebook-client, Deployment (apps) has changed:
  # Source: mercury/charts/kweet-facebook-client/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: mercury-kweet-facebook-client
    namespace: mercury-psr
    labels:
      app: kweet-facebook-client
      project: mercury
      release: mercury
      version: "0"
      app.kubernetes.io/name: kweet-facebook-client
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/managed-by: Helm
    annotations:
-     "checksum/config": 557a9b077e47b7bed81b4a0b11dfc5837bcbea4159ae19f83493f582cda25484
+     "checksum/config": e5275b43f4f4285ca58fcbdf2777c6a1472b0dc769bee19f4cdf5dac74fc765a
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: kweet-facebook-client
        release: mercury
        app.kubernetes.io/name: kweet-facebook-client
        app.kubernetes.io/instance: mercury
    template:
      metadata:
        annotations:
          vault.security.banzaicloud.io/log-level: warn
          vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
          vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
          vault.security.banzaicloud.io/vault-env-daemon: "true"
          vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
          vault.security.banzaicloud.io/vault-role: mercury-psr
          vault.security.banzaicloud.io/vault-skip-verify: "true"
        labels:
          app: kweet-facebook-client
          project: mercury
          release: mercury
          version: "0"
          app.kubernetes.io/name: kweet-facebook-client
          app.kubernetes.io/instance: mercury
      spec:
        serviceAccountName: mercurygeneric
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - "kweet-facebook-client"
                      - key: release
                        operator: In
                        values:
                          - mercury
                  topologyKey: "failure-domain.beta.kubernetes.io/zone"
        imagePullSecrets:
        - name: osvcstage-ocirsecret
        volumes:
        - name: keystore-volume
          emptyDir: {}
        - name: vaultca
          secret:
            secretName: vault-tls
        - name: keystore-pass
          secret:
            secretName: mercury-kweet-facebook-client-keystore
        initContainers:
        - name: init ## use kweet's init container
          image: iad.ocir.io/osvcstage/mercury/kweet-facebook-client:21.11.10-TRUNK-56
          imagePullPolicy: IfNotPresent
          command: ['/bin/bash', '/init/init-script.sh']
          # Need to run the init container as root (also we're forced to use its UID, i.e. 0)
          securityContext:
            runAsUser: 0
          env:
          - name: AUTO_REFRESH_CONSUL_TOKEN
            value: vault:cpe_consul/creds/mercury-psr#token
          - name: KAFKA_TEMPLATED_PKI
            value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
              .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
              "ttl": "720h"}'
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: TRUSTSTORE_NAME
            value: truststore.jks
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KEYSTORE_NAME
            value: kafka-client-keystore.jks
          - name: SERVICEACCOUNT_SECRETPATH
            value: /var/run/secrets/kubernetes.io/serviceaccount
          - name: VAULT_ADDR
            value: https://vault.service.consul:8200
          - name: VAULT_SECRETPATH
            value: /tmp/vaultca
          - name: PKI_VAULT_ENGINE_PATH
            value: "v1/infra_pki/issue/kafka_client"
          - name: CERT_CN
            value: "mercury-psr-mercurygeneric-rw"
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          volumeMounts:
          - name: keystore-volume
            mountPath: /keystores
          - name: vaultca
            mountPath: /tmp/vaultca
            readOnly: true
          - name: keystore-pass
            mountPath: /tmp/keystore-pass
            readOnly: true
          resources:
            limits:
              cpu: "2"
              memory: 3Gi
            requests:
              cpu: 200m
              memory: 2Gi
        containers:
        - name: mercury-kweet-facebook-client
          image: iad.ocir.io/osvcstage/mercury/kweet-facebook-client:21.11.10-TRUNK-56
          imagePullPolicy: IfNotPresent
          env:
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: HOST_IP
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
          - name: CONSUL_HTTP_ADDR
            value: https://$(HOST_IP):8501
          - name: CONSUL_TOKEN
            value: vault:/cpe_consul/creds/mercury-psr#token
          - name: POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: VAULT_TOKEN
            value: vault:login
          - name: JAVA_OPTS
            value: "-Dschema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul \
                    -Dbootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443 \
                    -Dservice.datacenter=DC-one \
                    -Dservice.defaultTenant=TMS \
                    -Dthidwick-server.port=8080 \
                    -Dthidwick-server.swagger.enabled=true \
                    -Dthidwick-server.swagger-ui.enabled=true \
                    -Dkweet.vault.enabled=true \
                    -Dkweet.vault.namespace=mercury-psr \
                    -Dkweet.vault.serviceaccount=mercurygeneric \
                    -Dkweet.vault.host=vault.service.consul \
                    -Djavax.net.ssl.trustStore=/keystores/truststore.jks \
                    -Dtopics.prefix=mercury-psr \
                    -Dsharedkafka.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443 \
                    -Dkafka.security.protocol=SSL \
                    -Dkafka.ssl.truststore.location=/keystores/truststore.jks \
                    -Dkafka.ssl.keystore.location=/keystores/kafka-client-keystore.jks \
                    -Dkafka.ssl.jks.passphrase.file=/keystores/props/kafka.properties \
                    -Dkweet.message.encryption.enabled=true \
                    -Djson.logging.enabled=false \
                    -Dthidwick-server.logback.service.log_level=INFO \
                    -Dthidwick-server.logback.level=INFO \
                    -Dmercury.enabled=true \
                    -Dslack.api.enabled=true \
                    -Dslack.api.channel=#om-alerts-prod \
                    -Dingress.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dcpe.version=v2 \
                    -Dvault.authentication=TOKEN \
                    -Dvault.role=mercury-psr \
                    -Dkweet.vault.path=mercury \
                    -Dfile.token.enabled=true \
                    -Dkweet.consul.enabled=true \
                    -Dconsul.base.address=$(CONSUL_HTTP_ADDR) \
                    -Dconsul.token=$(CONSUL_TOKEN) \
                    -Dtenant.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dkweet.vault.uri=https://vault.query.prod.consul:8200 \
                    -Dkweet/facebook/client/channel.url=https://graph.facebook.com/v11.0/ \
                    -Dbean.kafka.configurationProvider.enabled=false \
                    -Dbean.kafka.incidentProvider.enabled=false \
                    -Dconfigservice.update.endpoint=http://mercury-social-config:8080/configupdate/config \

                    -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Xms256m"
          volumeMounts:
          - mountPath: /keystores
            name: keystore-volume
          - mountPath: /tmp/keystore-pass
            name: keystore-pass
          ports:
          - name: http
            containerPort: 8080
            protocol: TCP
          livenessProbe:
            httpGet:
              path: /health/checks
              port: http
            initialDelaySeconds: 180
          readinessProbe:
            httpGet:
              path: /health/ping
              port: http
            initialDelaySeconds: 180
          resources:
            limits:
              cpu: "2"
              memory: 3Gi
            requests:
              cpu: 200m
              memory: 2Gi
mercury-psr, mercury-kweet-facebook-client-keystore, Secret (v1) has changed:
  # Source: mercury/charts/kweet-facebook-client/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-kweet-facebook-client-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-kweet-facebook-webhook, Deployment (apps) has changed:
  # Source: mercury/charts/kweet-facebook-webhook/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: mercury-kweet-facebook-webhook
    namespace: mercury-psr
    labels:
      app: kweet-facebook-webhook
      project: mercury
      release: mercury
      version: "0"
      app.kubernetes.io/name: kweet-facebook-webhook
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/managed-by: Helm
    annotations:
-     "checksum/config": fcbb312c00e65e9c7ac56102b5091095dc61f9425f959a257cca063756129333
+     "checksum/config": 0144ddc70f83394142d8f3eed5254025fc7e3006056a783560077232b383164b
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: kweet-facebook-webhook
        release: mercury
        app.kubernetes.io/name: kweet-facebook-webhook
        app.kubernetes.io/instance: mercury
    template:
      metadata:
        annotations:
          vault.security.banzaicloud.io/log-level: warn
          vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
          vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
          vault.security.banzaicloud.io/vault-env-daemon: "true"
          vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
          vault.security.banzaicloud.io/vault-role: mercury-psr
          vault.security.banzaicloud.io/vault-skip-verify: "true"
        labels:
          app: kweet-facebook-webhook
          project: mercury
          release: mercury
          version: "0"
          app.kubernetes.io/name: kweet-facebook-webhook
          app.kubernetes.io/instance: mercury
      spec:
        serviceAccountName: mercurygeneric
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - "kweet-facebook-webhook"
                      - key: release
                        operator: In
                        values:
                          - mercury
                  topologyKey: "failure-domain.beta.kubernetes.io/zone"
        imagePullSecrets:
        - name: osvcstage-ocirsecret
        volumes:
        - name: keystore-volume
          emptyDir: {}
        - name: vaultca
          secret:
            secretName: vault-tls
        - name: keystore-pass
          secret:
            secretName: mercury-kweet-facebook-webhook-keystore
        initContainers:
        - name: init ## use kweet's init container
          image: iad.ocir.io/osvcstage/mercury/kweet-facebook-webhook:21.11.10-TRUNK-99
          imagePullPolicy: IfNotPresent
          command: ['/bin/bash', '/init/init-script.sh']
          # Need to run the init container as root (also we're forced to use its UID, i.e. 0)
          securityContext:
            runAsUser: 0
          env:
          - name: AUTO_REFRESH_CONSUL_TOKEN
            value: vault:cpe_consul/creds/mercury-psr#token
          - name: KAFKA_TEMPLATED_PKI
            value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
              .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
              "ttl": "720h"}'
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: TRUSTSTORE_NAME
            value: truststore.jks
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KEYSTORE_NAME
            value: kafka-client-keystore.jks
          - name: SERVICEACCOUNT_SECRETPATH
            value: /var/run/secrets/kubernetes.io/serviceaccount
          - name: VAULT_ADDR
            value: https://vault.service.consul:8200
          - name: VAULT_SECRETPATH
            value: /tmp/vaultca
          - name: PKI_VAULT_ENGINE_PATH
            value: "v1/infra_pki/issue/kafka_client"
          - name: CERT_CN
            value: "mercury-psr-mercurygeneric-rw"
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          volumeMounts:
          - name: keystore-volume
            mountPath: /keystores
          - name: vaultca
            mountPath: /tmp/vaultca
            readOnly: true
          - name: keystore-pass
            mountPath: /tmp/keystore-pass
            readOnly: true
          resources:
            limits:
              cpu: "2"
              memory: 3Gi
            requests:
              cpu: 100m
              memory: 2Gi
        containers:
        - name: mercury-kweet-facebook-webhook
          image: iad.ocir.io/osvcstage/mercury/kweet-facebook-webhook:21.11.10-TRUNK-99
          imagePullPolicy: IfNotPresent
          env:
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: HOST_IP
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
          - name: CONSUL_HTTP_ADDR
            value: https://$(HOST_IP):8501
          - name: CONSUL_TOKEN
            value: vault:/cpe_consul/creds/mercury-psr#token
          - name: POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: VAULT_TOKEN
            value: vault:login
          - name: JAVA_OPTS
            value: "-Dschema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul \
                    -Dbootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443 \
                    -Dservice.datacenter=DC-one \
                    -Dservice.defaultTenant=TMS \
                    -Dthidwick-server.port=8080 \
                    -Dthidwick-server.swagger.enabled=true \
                    -Dthidwick-server.swagger-ui.enabled=true \
                    -Dkweet.vault.enabled=true \
                    -Dkweet.vault.namespace=mercury-psr \
                    -Dkweet.vault.serviceaccount=mercurygeneric \
                    -Dkweet.vault.host=vault.service.consul \
                    -Djavax.net.ssl.trustStore=/keystores/truststore.jks \
                    -Dtopics.prefix=mercury-psr \
                    -Dsharedkafka.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443 \
                    -Dkafka.security.protocol=SSL \
                    -Dkafka.ssl.truststore.location=/keystores/truststore.jks \
                    -Dkafka.ssl.keystore.location=/keystores/kafka-client-keystore.jks \
                    -Dkafka.ssl.jks.passphrase.file=/keystores/props/kafka.properties \
                    -Dkweet.message.encryption.enabled=true \
                    -Djson.logging.enabled=false \
                    -Dthidwick-server.logback.service.log_level=INFO \
                    -Dthidwick-server.logback.level=INFO \
                    -Dmercury.enabled=true \
                    -Dslack.api.enabled=true \
                    -Dslack.api.channel=#om-alerts-prod \
                    -Dingress.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dcpe.version=v2 \
                    -Dvault.authentication=TOKEN \
                    -Dvault.role=mercury-psr \
                    -Dkweet.vault.path=mercury \
                    -Dfile.token.enabled=true \
                    -Dkweet.consul.enabled=true \
                    -Dconsul.base.address=$(CONSUL_HTTP_ADDR) \
                    -Dconsul.token=$(CONSUL_TOKEN) \
                    -Dtenant.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dkweet.vault.uri=https://vault.query.prod.consul:8200 \
                    -Dkweet.service.userprofile.host=http://kweet-userprofiles-kweet-userprofiles \
                    -Dkweet.service.userprofile.port=8080 \
                    -Dbean.service.userProfileProvider.enabled=false \
                    -Dbean.kafka.userProfileProvider.enabled=true \
                    -Dbean.kafka.incidentProvider.enabled=false \
                    -Dclient.endpoint=http://mercury-kweet-facebook-client:8080 \
                    -Dkweet/facebook/webhook/channel.url=https://graph.facebook.com/v11.0/ \

                    -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Xms256m"
          volumeMounts:
          - mountPath: /keystores
            name: keystore-volume
          - mountPath: /tmp/keystore-pass
            name: keystore-pass
          ports:
          - name: http
            containerPort: 8080
            protocol: TCP
          livenessProbe:
            httpGet:
              path: /kweet/facebook/health/checks
              port: http
            initialDelaySeconds: 180
          readinessProbe:
            httpGet:
              path: /kweet/facebook/health/ping
              port: http
            initialDelaySeconds: 180
          resources:
            limits:
              cpu: "2"
              memory: 3Gi
            requests:
              cpu: 100m
              memory: 2Gi
mercury-psr, mercury-kweet-facebook-webhook-keystore, Secret (v1) has changed:
  # Source: mercury/charts/kweet-facebook-webhook/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-kweet-facebook-webhook-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-kweet-twiliosms-client, Deployment (apps) has changed:
  # Source: mercury/charts/kweet-twiliosms-client/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: mercury-kweet-twiliosms-client
    namespace: mercury-psr
    labels:
      app: kweet-twiliosms-client
      project: mercury
      release: mercury
      version: "0"
      app.kubernetes.io/name: kweet-twiliosms-client
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/managed-by: Helm
    annotations:
-     "checksum/config": c009478eee2d5358e377c5f2fbb3f4919be518a2e8cf5b4c34637dbd85f22ec9
+     "checksum/config": a4706baa76177a754ce5ac8c61b4dcc57bdd00a30447f0bec06610ec81b008ba
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: kweet-twiliosms-client
        release: mercury
        app.kubernetes.io/name: kweet-twiliosms-client
        app.kubernetes.io/instance: mercury
    template:
      metadata:
        annotations:
          vault.security.banzaicloud.io/log-level: warn
          vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
          vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
          vault.security.banzaicloud.io/vault-env-daemon: "true"
          vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
          vault.security.banzaicloud.io/vault-role: mercury-psr
          vault.security.banzaicloud.io/vault-skip-verify: "true"
        labels:
          app: kweet-twiliosms-client
          project: mercury
          release: mercury
          version: "0"
          app.kubernetes.io/name: kweet-twiliosms-client
          app.kubernetes.io/instance: mercury
      spec:
        serviceAccountName: mercurygeneric
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - "kweet-twiliosms-client"
                      - key: release
                        operator: In
                        values:
                          - mercury
                  topologyKey: "failure-domain.beta.kubernetes.io/zone"
        imagePullSecrets:
        - name: osvcstage-ocirsecret
        volumes:
        - name: keystore-volume
          emptyDir: {}
        - name: vaultca
          secret:
            secretName: vault-tls
        - name: keystore-pass
          secret:
            secretName: mercury-kweet-twiliosms-client-keystore
        initContainers:
        - name: init ## use kweet's init container
          image: iad.ocir.io/osvcstage/mercury/kweet-twiliosms-client:21.11.11-TRUNK-70
          imagePullPolicy: IfNotPresent
          command: ['/bin/bash', '/init/init-script.sh']
          # Need to run the init container as root (also we're forced to use its UID, i.e. 0)
          securityContext:
            runAsUser: 0
          env:
          - name: AUTO_REFRESH_CONSUL_TOKEN
            value: vault:cpe_consul/creds/mercury-psr#token
          - name: KAFKA_TEMPLATED_PKI
            value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
              .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
              "ttl": "720h"}'
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: TRUSTSTORE_NAME
            value: truststore.jks
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KEYSTORE_NAME
            value: kafka-client-keystore.jks
          - name: SERVICEACCOUNT_SECRETPATH
            value: /var/run/secrets/kubernetes.io/serviceaccount
          - name: VAULT_ADDR
            value: https://vault.service.consul:8200
          - name: VAULT_SECRETPATH
            value: /tmp/vaultca
          - name: PKI_VAULT_ENGINE_PATH
            value: "v1/infra_pki/issue/kafka_client"
          - name: CERT_CN
            value: "mercury-psr-mercurygeneric-rw"
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          volumeMounts:
          - name: keystore-volume
            mountPath: /keystores
          - name: vaultca
            mountPath: /tmp/vaultca
            readOnly: true
          - name: keystore-pass
            mountPath: /tmp/keystore-pass
            readOnly: true
          resources:
            limits:
              cpu: 2
              memory: 2048Mi
            requests:
              cpu: 0.2
              memory: 384Mi
        containers:
        - name: mercury-kweet-twiliosms-client
          image: iad.ocir.io/osvcstage/mercury/kweet-twiliosms-client:21.11.11-TRUNK-70
          imagePullPolicy: IfNotPresent
          env:
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: HOST_IP
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
          - name: CONSUL_HTTP_ADDR
            value: https://$(HOST_IP):8501
          - name: CONSUL_TOKEN
            value: vault:/cpe_consul/creds/mercury-psr#token
          - name: POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: VAULT_TOKEN
            value: vault:login
          - name: JAVA_OPTS
            value: "-Dschema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul \
                    -Dbootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443 \
                    -Dservice.datacenter=DC-one \
                    -Dservice.defaultTenant=TMS \
                    -Dthidwick-server.port=8080 \
                    -Dthidwick-server.swagger.enabled=true \
                    -Dthidwick-server.swagger-ui.enabled=true \
                    -Dkweet.vault.enabled=true \
                    -Dkweet.vault.namespace=mercury-psr \
                    -Dkweet.vault.serviceaccount=mercurygeneric \
                    -Dkweet.vault.host=vault.service.consul \
                    -Djavax.net.ssl.trustStore=/keystores/truststore.jks \
                    -Dtopics.prefix=mercury-psr \
                    -Dsharedkafka.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443 \
                    -Dkafka.security.protocol=SSL \
                    -Dkafka.ssl.truststore.location=/keystores/truststore.jks \
                    -Dkafka.ssl.keystore.location=/keystores/kafka-client-keystore.jks \
                    -Dkafka.ssl.jks.passphrase.file=/keystores/props/kafka.properties \
                    -Dkweet.message.encryption.enabled=true \
                    -Djson.logging.enabled=false \
                    -Dthidwick-server.logback.service.log_level=INFO \
                    -Dthidwick-server.logback.level=INFO \
                    -Dmercury.enabled=true \
                    -Dslack.api.enabled=true \
                    -Dslack.api.channel=#om-alerts-prod \
                    -Dingress.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dcpe.version=v2 \
                    -Dvault.authentication=TOKEN \
                    -Dvault.role=mercury-psr \
                    -Dkweet.vault.path=mercury \
                    -Dfile.token.enabled=true \
                    -Dkweet.consul.enabled=true \
                    -Dconsul.base.address=$(CONSUL_HTTP_ADDR) \
                    -Dconsul.token=$(CONSUL_TOKEN) \
                    -Dtenant.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dkweet.vault.uri=https://vault.query.prod.consul:8200 \
                    -Droutingquestions.defaultpush.enabled=false \
                    -Dbean.kafka.configurationProvider.enabled=false \
                    -Dbean.kafka.incidentProvider.enabled=false \

                    -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Xms256m"
          volumeMounts:
          - mountPath: /keystores
            name: keystore-volume
          - mountPath: /tmp/keystore-pass
            name: keystore-pass
          ports:
          - name: http
            containerPort: 8080
            protocol: TCP
          livenessProbe:
            httpGet:
              path: /kweet/twiliosms/health/checks
              port: http
            initialDelaySeconds: 180
          readinessProbe:
            httpGet:
              path: /kweet/twiliosms/health/ping
              port: http
            initialDelaySeconds: 180
          resources:
            limits:
              cpu: 2
              memory: 2048Mi
            requests:
              cpu: 0.2
              memory: 384Mi
mercury-psr, mercury-kweet-twiliosms-client-keystore, Secret (v1) has changed:
  # Source: mercury/charts/kweet-twiliosms-client/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-kweet-twiliosms-client-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-kweet-userprofiles, Deployment (apps) has changed:
  # Source: mercury/charts/kweet-userprofiles/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: mercury-kweet-userprofiles
    namespace: mercury-psr
    labels:
      app: kweet-userprofiles
      project: mercury
      release: mercury
      version: "0"
      app.kubernetes.io/name: kweet-userprofiles
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/managed-by: Helm
    annotations:
-     "checksum/config": 40a9b7400c53647454f5d24bc4bc67197232a7b0e363621d8a1d9f5eaf84c8a7
+     "checksum/config": 0116d5410fcc883922e371ea70c7eb0053d5eb8918c679de5831f518972816e6
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: kweet-userprofiles
        release: mercury
        app.kubernetes.io/name: kweet-userprofiles
        app.kubernetes.io/instance: mercury
    template:
      metadata:
        annotations:
          vault.security.banzaicloud.io/log-level: warn
          vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
          vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
          vault.security.banzaicloud.io/vault-env-daemon: "true"
          vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
          vault.security.banzaicloud.io/vault-role: mercury-psr
          vault.security.banzaicloud.io/vault-skip-verify: "true"
        labels:
          app: kweet-userprofiles
          project: mercury
          release: mercury
          version: "0"
          app.kubernetes.io/name: kweet-userprofiles
          app.kubernetes.io/instance: mercury
      spec:
        serviceAccountName: mercurygeneric
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - "kweet-userprofiles"
                      - key: release
                        operator: In
                        values:
                          - mercury
                  topologyKey: "failure-domain.beta.kubernetes.io/zone"
        imagePullSecrets:
        - name: osvcstage-ocirsecret
        volumes:
        - name: keystore-volume
          emptyDir: {}
        - name: vaultca
          secret:
            secretName: vault-tls
        - name: keystore-pass
          secret:
            secretName: mercury-kweet-userprofiles-keystore
        initContainers:
        - name: init ## use kweet's init container
          image: iad.ocir.io/osvcstage/mercury/kweet-userprofiles:21.11.08-TRUNK-52
          imagePullPolicy: Always
          command: ['/bin/bash', '/init/init-script.sh']
          # Need to run the init container as root (also we're forced to use its UID, i.e. 0)
          securityContext:
            runAsUser: 0
          env:
          - name: AUTO_REFRESH_CONSUL_TOKEN
            value: vault:cpe_consul/creds/mercury-psr#token
          - name: KAFKA_TEMPLATED_PKI
            value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
              .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
              "ttl": "720h"}'
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: TRUSTSTORE_NAME
            value: truststore.jks
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KEYSTORE_NAME
            value: kafka-client-keystore.jks
          - name: SERVICEACCOUNT_SECRETPATH
            value: /var/run/secrets/kubernetes.io/serviceaccount
          - name: VAULT_ADDR
            value: https://vault.service.consul:8200
          - name: VAULT_SECRETPATH
            value: /tmp/vaultca
          - name: PKI_VAULT_ENGINE_PATH
            value: "v1/infra_pki/issue/kafka_client"
          - name: CERT_CN
            value: "mercury-psr-mercurygeneric-rw"
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          volumeMounts:
          - name: keystore-volume
            mountPath: /keystores
          - name: vaultca
            mountPath: /tmp/vaultca
            readOnly: true
          - name: keystore-pass
            mountPath: /tmp/keystore-pass
            readOnly: true
          resources:
            limits:
              cpu: 1
              memory: 2Gi
            requests:
              cpu: "0.2"
              memory: 1Gi
        containers:
        - name: mercury-kweet-userprofiles
          image: iad.ocir.io/osvcstage/mercury/kweet-userprofiles:21.11.08-TRUNK-52
          imagePullPolicy: Always
          env:
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: HOST_IP
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
          - name: CONSUL_HTTP_ADDR
            value: https://$(HOST_IP):8501
          - name: CONSUL_TOKEN
            value: vault:/cpe_consul/creds/mercury-psr#token
          - name: POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: VAULT_TOKEN
            value: vault:login
          - name: JAVA_OPTS
            value: "-Dschema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul \
                    -Dbootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443 \
                    -Dservice.datacenter=DC-one \
                    -Dservice.defaultTenant=TMS \
                    -Dthidwick-server.port=8080 \
                    -Dthidwick-server.swagger.enabled=true \
                    -Dthidwick-server.swagger-ui.enabled=true \
                    -Dkweet.vault.enabled=true \
                    -Dkweet.vault.namespace=mercury-psr \
                    -Dkweet.vault.serviceaccount=mercurygeneric \
                    -Dkweet.vault.host=vault.service.consul \
                    -Djavax.net.ssl.trustStore=/keystores/truststore.jks \
                    -Dtopics.prefix=mercury-psr \
                    -Dsharedkafka.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443 \
                    -Dkafka.security.protocol=SSL \
                    -Dkafka.ssl.truststore.location=/keystores/truststore.jks \
                    -Dkafka.ssl.keystore.location=/keystores/kafka-client-keystore.jks \
                    -Dkafka.ssl.jks.passphrase.file=/keystores/props/kafka.properties \
                    -Dkweet.message.encryption.enabled=true \
                    -Djson.logging.enabled=false \
                    -Dthidwick-server.logback.service.log_level=INFO \
                    -Dthidwick-server.logback.level=INFO \
                    -Dmercury.enabled=true \
                    -Dslack.api.enabled=true \
                    -Dslack.api.channel=#om-alerts-prod \
                    -Dingress.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dcpe.version=v2 \
                    -Dvault.authentication=TOKEN \
                    -Dvault.role=mercury-psr \
                    -Dkweet.vault.path=mercury \
                    -Dfile.token.enabled=true \
                    -Dkweet.consul.enabled=true \
                    -Dconsul.base.address=$(CONSUL_HTTP_ADDR) \
                    -Dconsul.token=$(CONSUL_TOKEN) \
                    -Dtenant.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dkweet.vault.uri=https://vault.query.prod.consul:8200 \
                    -Dbogus=true \

                    -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Xms256m"
          volumeMounts:
          - mountPath: /keystores
            name: keystore-volume
          - mountPath: /tmp/keystore-pass
            name: keystore-pass
          ports:
          - name: http
            containerPort: 8080
            protocol: TCP
          livenessProbe:
            httpGet:
              path: /kweet/userprofiles/health/checks
              port: http
            initialDelaySeconds: 180
          readinessProbe:
            httpGet:
              path: /kweet/userprofiles/health/ping
              port: http
            initialDelaySeconds: 180
          resources:
            limits:
              cpu: 1
              memory: 2Gi
            requests:
              cpu: "0.2"
              memory: 1Gi
mercury-psr, mercury-kweet-userprofiles-keystore, Secret (v1) has changed:
  # Source: mercury/charts/kweet-userprofiles/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-kweet-userprofiles-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-kweet-wechat-client, Deployment (apps) has changed:
  # Source: mercury/charts/kweet-wechat-client/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: mercury-kweet-wechat-client
    namespace: mercury-psr
    labels:
      app: kweet-wechat-client
      project: mercury
      release: mercury
      version: "0"
      app.kubernetes.io/name: kweet-wechat-client
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/managed-by: Helm
    annotations:
-     "checksum/config": 13101ccbd8337c685517ea6bd21a910c3c8b3c2ce1ab9a696aa0f08bc489c5a7
+     "checksum/config": 1206e79315cedd6e793b55551c7b2302e704b663ef8cab9b68c30d6dab5e52bd
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: kweet-wechat-client
        release: mercury
        app.kubernetes.io/name: kweet-wechat-client
        app.kubernetes.io/instance: mercury
    template:
      metadata:
        annotations:
          vault.security.banzaicloud.io/log-level: warn
          vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
          vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
          vault.security.banzaicloud.io/vault-env-daemon: "true"
          vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
          vault.security.banzaicloud.io/vault-role: mercury-psr
          vault.security.banzaicloud.io/vault-skip-verify: "true"
        labels:
          app: kweet-wechat-client
          project: mercury
          release: mercury
          version: "0"
          app.kubernetes.io/name: kweet-wechat-client
          app.kubernetes.io/instance: mercury
      spec:
        serviceAccountName: mercurygeneric
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - "kweet-wechat-client"
                      - key: release
                        operator: In
                        values:
                          - mercury
                  topologyKey: "failure-domain.beta.kubernetes.io/zone"
        imagePullSecrets:
        - name: osvcstage-ocirsecret
        volumes:
        - name: keystore-volume
          emptyDir: {}
        - name: vaultca
          secret:
            secretName: vault-tls
        - name: keystore-pass
          secret:
            secretName: mercury-kweet-wechat-client-keystore
        initContainers:
        - name: init ## use kweet's init container
          image: iad.ocir.io/osvcstage/mercury/kweet-wechat-client:21.11.08-TRUNK-35
          imagePullPolicy: IfNotPresent
          command: ['/bin/bash', '/init/init-script.sh']
          # Need to run the init container as root (also we're forced to use its UID, i.e. 0)
          securityContext:
            runAsUser: 0
          env:
          - name: AUTO_REFRESH_CONSUL_TOKEN
            value: vault:cpe_consul/creds/mercury-psr#token
          - name: KAFKA_TEMPLATED_PKI
            value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
              .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
              "ttl": "720h"}'
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: TRUSTSTORE_NAME
            value: truststore.jks
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KEYSTORE_NAME
            value: kafka-client-keystore.jks
          - name: SERVICEACCOUNT_SECRETPATH
            value: /var/run/secrets/kubernetes.io/serviceaccount
          - name: VAULT_ADDR
            value: https://vault.service.consul:8200
          - name: VAULT_SECRETPATH
            value: /tmp/vaultca
          - name: PKI_VAULT_ENGINE_PATH
            value: "v1/infra_pki/issue/kafka_client"
          - name: CERT_CN
            value: "mercury-psr-mercurygeneric-rw"
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          volumeMounts:
          - name: keystore-volume
            mountPath: /keystores
          - name: vaultca
            mountPath: /tmp/vaultca
            readOnly: true
          - name: keystore-pass
            mountPath: /tmp/keystore-pass
            readOnly: true
          resources:
            limits:
              cpu: "2"
              memory: 3Gi
            requests:
              cpu: 200m
              memory: 2Gi
        containers:
        - name: mercury-kweet-wechat-client
          image: iad.ocir.io/osvcstage/mercury/kweet-wechat-client:21.11.08-TRUNK-35
          imagePullPolicy: IfNotPresent
          env:
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: HOST_IP
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
          - name: CONSUL_HTTP_ADDR
            value: https://$(HOST_IP):8501
          - name: CONSUL_TOKEN
            value: vault:/cpe_consul/creds/mercury-psr#token
          - name: POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: VAULT_TOKEN
            value: vault:login
          - name: JAVA_OPTS
            value: "-Dschema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul \
                    -Dbootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443 \
                    -Dservice.datacenter=DC-one \
                    -Dservice.defaultTenant=TMS \
                    -Dthidwick-server.port=8080 \
                    -Dthidwick-server.swagger.enabled=true \
                    -Dthidwick-server.swagger-ui.enabled=true \
                    -Dkweet.vault.enabled=true \
                    -Dkweet.vault.namespace=mercury-psr \
                    -Dkweet.vault.serviceaccount=mercurygeneric \
                    -Dkweet.vault.host=vault.service.consul \
                    -Djavax.net.ssl.trustStore=/keystores/truststore.jks \
                    -Dtopics.prefix=mercury-psr \
                    -Dsharedkafka.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443 \
                    -Dkafka.security.protocol=SSL \
                    -Dkafka.ssl.truststore.location=/keystores/truststore.jks \
                    -Dkafka.ssl.keystore.location=/keystores/kafka-client-keystore.jks \
                    -Dkafka.ssl.jks.passphrase.file=/keystores/props/kafka.properties \
                    -Dkweet.message.encryption.enabled=true \
                    -Djson.logging.enabled=false \
                    -Dthidwick-server.logback.service.log_level=INFO \
                    -Dthidwick-server.logback.level=INFO \
                    -Dmercury.enabled=true \
                    -Dslack.api.enabled=true \
                    -Dslack.api.channel=#om-alerts-prod \
                    -Dingress.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dcpe.version=v2 \
                    -Dvault.authentication=TOKEN \
                    -Dvault.role=mercury-psr \
                    -Dkweet.vault.path=mercury \
                    -Dfile.token.enabled=true \
                    -Dkweet.consul.enabled=true \
                    -Dconsul.base.address=$(CONSUL_HTTP_ADDR) \
                    -Dconsul.token=$(CONSUL_TOKEN) \
                    -Dtenant.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dkweet.vault.uri=https://vault.query.prod.consul:8200 \
                    -Dkweet/wechat/client/channel.url=https://api.weixin.qq.com/ \
                    -Dbean.kafka.configurationProvider.enabled=false \
                    -Dbean.kafka.incidentProvider.enabled=false \

                    -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Xms256m"
          volumeMounts:
          - mountPath: /keystores
            name: keystore-volume
          - mountPath: /tmp/keystore-pass
            name: keystore-pass
          ports:
          - name: http
            containerPort: 8080
            protocol: TCP
          livenessProbe:
            httpGet:
              path: /health/checks
              port: http
            initialDelaySeconds: 180
          readinessProbe:
            httpGet:
              path: /health/ping
              port: http
            initialDelaySeconds: 180
          resources:
            limits:
              cpu: "2"
              memory: 3Gi
            requests:
              cpu: 200m
              memory: 2Gi
mercury-psr, mercury-kweet-wechat-client-keystore, Secret (v1) has changed:
  # Source: mercury/charts/kweet-wechat-client/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-kweet-wechat-client-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-kweet-wechat-webhook, Deployment (apps) has changed:
  # Source: mercury/charts/kweet-wechat-webhook/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: mercury-kweet-wechat-webhook
    namespace: mercury-psr
    labels:
      app: kweet-wechat-webhook
      project: mercury
      release: mercury
      version: "0"
      app.kubernetes.io/name: kweet-wechat-webhook
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/managed-by: Helm
    annotations:
-     "checksum/config": 86d069364115942ff369104df166e021d87b15944b9bf549302b8b45f4e1d5c3
+     "checksum/config": f8296e576d64374f5642b92c9b5ff1bc393688af25662d5b1622b24f020a8956
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: kweet-wechat-webhook
        release: mercury
        app.kubernetes.io/name: kweet-wechat-webhook
        app.kubernetes.io/instance: mercury
    template:
      metadata:
        annotations:
          vault.security.banzaicloud.io/log-level: warn
          vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
          vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
          vault.security.banzaicloud.io/vault-env-daemon: "true"
          vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
          vault.security.banzaicloud.io/vault-role: mercury-psr
          vault.security.banzaicloud.io/vault-skip-verify: "true"
        labels:
          app: kweet-wechat-webhook
          project: mercury
          release: mercury
          version: "0"
          app.kubernetes.io/name: kweet-wechat-webhook
          app.kubernetes.io/instance: mercury
      spec:
        serviceAccountName: mercurygeneric
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - "kweet-wechat-webhook"
                      - key: release
                        operator: In
                        values:
                          - mercury
                  topologyKey: "failure-domain.beta.kubernetes.io/zone"
        imagePullSecrets:
        - name: osvcstage-ocirsecret
        volumes:
        - name: keystore-volume
          emptyDir: {}
        - name: vaultca
          secret:
            secretName: vault-tls
        - name: keystore-pass
          secret:
            secretName: mercury-kweet-wechat-webhook-keystore
        initContainers:
        - name: init ## use kweet's init container
          image: iad.ocir.io/osvcstage/mercury/kweet-wechat-webhook:21.11.11-TRUNK-57
          imagePullPolicy: IfNotPresent
          command: ['/bin/bash', '/init/init-script.sh']
          # Need to run the init container as root (also we're forced to use its UID, i.e. 0)
          securityContext:
            runAsUser: 0
          env:
          - name: AUTO_REFRESH_CONSUL_TOKEN
            value: vault:cpe_consul/creds/mercury-psr#token
          - name: KAFKA_TEMPLATED_PKI
            value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
              .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
              "ttl": "720h"}'
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: TRUSTSTORE_NAME
            value: truststore.jks
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KEYSTORE_NAME
            value: kafka-client-keystore.jks
          - name: SERVICEACCOUNT_SECRETPATH
            value: /var/run/secrets/kubernetes.io/serviceaccount
          - name: VAULT_ADDR
            value: https://vault.service.consul:8200
          - name: VAULT_SECRETPATH
            value: /tmp/vaultca
          - name: PKI_VAULT_ENGINE_PATH
            value: "v1/infra_pki/issue/kafka_client"
          - name: CERT_CN
            value: "mercury-psr-mercurygeneric-rw"
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          volumeMounts:
          - name: keystore-volume
            mountPath: /keystores
          - name: vaultca
            mountPath: /tmp/vaultca
            readOnly: true
          - name: keystore-pass
            mountPath: /tmp/keystore-pass
            readOnly: true
          resources:
            limits:
              cpu: "2"
              memory: 3Gi
            requests:
              cpu: 200m
              memory: 2Gi
        containers:
        - name: mercury-kweet-wechat-webhook
          image: iad.ocir.io/osvcstage/mercury/kweet-wechat-webhook:21.11.11-TRUNK-57
          imagePullPolicy: IfNotPresent
          env:
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: kafka.properties
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: HOST_IP
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
          - name: CONSUL_HTTP_ADDR
            value: https://$(HOST_IP):8501
          - name: CONSUL_TOKEN
            value: vault:/cpe_consul/creds/mercury-psr#token
          - name: POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: VAULT_TOKEN
            value: vault:login
          - name: JAVA_OPTS
            value: "-Dschema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul \
                    -Dbootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443 \
                    -Dservice.datacenter=DC-one \
                    -Dservice.defaultTenant=TMS \
                    -Dthidwick-server.port=8080 \
                    -Dthidwick-server.swagger.enabled=true \
                    -Dthidwick-server.swagger-ui.enabled=true \
                    -Dkweet.vault.enabled=true \
                    -Dkweet.vault.namespace=mercury-psr \
                    -Dkweet.vault.serviceaccount=mercurygeneric \
                    -Dkweet.vault.host=vault.service.consul \
                    -Djavax.net.ssl.trustStore=/keystores/truststore.jks \
                    -Dtopics.prefix=mercury-psr \
                    -Dsharedkafka.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443 \
                    -Dkafka.security.protocol=SSL \
                    -Dkafka.ssl.truststore.location=/keystores/truststore.jks \
                    -Dkafka.ssl.keystore.location=/keystores/kafka-client-keystore.jks \
                    -Dkafka.ssl.jks.passphrase.file=/keystores/props/kafka.properties \
                    -Dkweet.message.encryption.enabled=true \
                    -Djson.logging.enabled=false \
                    -Dthidwick-server.logback.service.log_level=INFO \
                    -Dthidwick-server.logback.level=INFO \
                    -Dmercury.enabled=true \
                    -Dslack.api.enabled=true \
                    -Dslack.api.channel=#om-alerts-prod \
                    -Dingress.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dcpe.version=v2 \
                    -Dvault.authentication=TOKEN \
                    -Dvault.role=mercury-psr \
                    -Dkweet.vault.path=mercury \
                    -Dfile.token.enabled=true \
                    -Dkweet.consul.enabled=true \
                    -Dconsul.base.address=$(CONSUL_HTTP_ADDR) \
                    -Dconsul.token=$(CONSUL_TOKEN) \
                    -Dtenant.host=engagement-psr.us1.channels.ocs.oraclecloud.com \
                    -Dkweet.vault.uri=https://vault.query.prod.consul:8200 \
                    -Dkweet.chat.url=http://kweet-chat-kweet-chat:8080 \
                    -Dkweet.service.userprofile.host=http://kweet-userprofiles-kweet-userprofiles \
                    -Dkweet.service.userprofile.port=8080 \
                    -Dbean.service.userProfileProvider.enabled=true \
                    -Dbean.kafka.userProfileProvider.enabled=false \
                    -Dclient.endpoint=http://mercury-kweet-wechat-client:8080 \
                    -Dconfigservice.update.endpoint=http://mercury-social-config:8080/configupdate/config \
                    -Dkweet/wechat/webhook/channel.url=https://api.weixin.qq.com \
                    -Ddownload.media.expiry.seconds=86400 \
                    -Dbean.kafka.configurationProvider.enabled=false \
                    -Dbean.kafka.incidentProvider.enabled=false \

                    -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Xms256m"
          volumeMounts:
          - mountPath: /keystores
            name: keystore-volume
          - mountPath: /tmp/keystore-pass
            name: keystore-pass
          ports:
          - name: http
            containerPort: 8080
            protocol: TCP
          livenessProbe:
            httpGet:
              path: /kweet/wechat/health/checks
              port: http
            initialDelaySeconds: 180
          readinessProbe:
            httpGet:
              path: /kweet/wechat/health/ping
              port: http
            initialDelaySeconds: 180
          resources:
            limits:
              cpu: "2"
              memory: 3Gi
            requests:
              cpu: 200m
              memory: 2Gi
mercury-psr, mercury-kweet-wechat-webhook-keystore, Secret (v1) has changed:
  # Source: mercury/charts/kweet-wechat-webhook/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-kweet-wechat-webhook-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-mercury-ui-keystore, Secret (v1) has changed:
  # Source: mercury/charts/mercury-ui/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-mercury-ui-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-metric-aggregation-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/metric-aggregation-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-metric-aggregation-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-metric-fusion-bridge-keystore, Secret (v1) has changed:
  # Source: mercury/charts/metric-fusion-bridge/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-metric-fusion-bridge-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-metric-generation-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/metric-generation-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-metric-generation-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-metric-internal-translation-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/metric-internal-translation-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-metric-internal-translation-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-metric-proxy-service-keystore, Secret (v1) has changed:
  # Source: mercury/charts/metric-proxy-service/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-metric-proxy-service-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-omnichannel-assignment-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/omnichannel-assignment-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-omnichannel-assignment-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-omnichannel-offer-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/omnichannel-offer-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-omnichannel-offer-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-osvc-bridge-api-services-keystore, Secret (v1) has changed:
  # Source: mercury/charts/osvc-bridge-api-services/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-osvc-bridge-api-services-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-osvc-bridge-metrics-data-pipeline-keystore, Secret (v1) has changed:
  # Source: mercury/charts/osvc-bridge-metrics-data-pipeline/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-osvc-bridge-metrics-data-pipeline-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-osvc-bridge-osvc-data-extractor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/osvc-bridge-osvc-data-extractor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-osvc-bridge-osvc-data-extractor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-osvc-bridge-provisioning-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/osvc-bridge-provisioning-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-osvc-bridge-provisioning-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-osvc-bridge-state-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/osvc-bridge-state-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-osvc-bridge-state-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-osvc-bridge-state-query-service-keystore, Secret (v1) has changed:
  # Source: mercury/charts/osvc-bridge-state-query-service/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-osvc-bridge-state-query-service-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-osvc-bridge-task-controller-keystore, Secret (v1) has changed:
  # Source: mercury/charts/osvc-bridge-task-controller/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-osvc-bridge-task-controller-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-provisioning-monitor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/provisioning-monitor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-provisioning-monitor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-provisioning-processor, Deployment (apps) has changed:
  # Source: mercury/charts/provisioning-processor/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: mercury-provisioning-processor
    labels:
      project: mercury
      version: "0"
      app.kubernetes.io/name: provisioning-processor
      helm.sh/chart: provisioning-processor-22.01.18-REL21.11-1087
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/managed-by: Helm
      monitoring: "true"
    annotations:
  spec:
    replicas: 3
    selector:
      matchLabels:
        app.kubernetes.io/name: provisioning-processor
        app.kubernetes.io/instance: mercury
    strategy:
      type: RollingUpdate
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
    template:
      metadata:
        annotations:
          vault.security.banzaicloud.io/log-level: warn
          vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
          vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
          vault.security.banzaicloud.io/vault-env-daemon: "true"
          vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
          vault.security.banzaicloud.io/vault-role: mercury-psr
          vault.security.banzaicloud.io/vault-skip-verify: "true"
        labels:
          app.kubernetes.io/name: provisioning-processor
          app.kubernetes.io/instance: mercury
          chronos.enabled: "True"
          chronos.maxPodLifetime: "1440"
          chronos.minAvailable: "0"
      spec:
        serviceAccountName: mercurygeneric
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - "provisioning-processor"
                  - key: release
                    operator: In
                    values:
                    - mercury
                topologyKey: "failure-domain.beta.kubernetes.io/zone"
        imagePullSecrets:
          - name: osvcstage-ocirsecret
        volumes:
        - name: vaultca
          secret:
            secretName: vault-tls
        - name: keystore-pass
          secret:
            secretName: mercury-provisioning-processor-keystore
        - name: keystore-volume
          emptyDir: {}
        initContainers:
        - name: init
          image: iad.ocir.io/osvcstage/mercury/init-container/prod:1.0.0.18
          imagePullPolicy: IfNotPresent
          command: ['/bin/bash', '-c', '/init/init-script.sh']
          # Need to run the init container as root (also we're forced to use its UID, i.e. 0)
          securityContext:
            runAsUser: 0
          env:
          - name: AUTO_REFRESH_CONSUL_TOKEN
            value: vault:cpe_consul/creds/mercury-psr#token
          - name: KAFKA_TEMPLATED_PKI
            value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
              .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
              "ttl": "720h"}'
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: TRUSTSTORE_NAME
            value: truststore.jks
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: props/kafka.properties
          - name: KEYSTORE_NAME
            value: keystore.jks
          - name: SERVICEACCOUNT_SECRETPATH
            value: /var/run/secrets/kubernetes.io/serviceaccount
          - name: VAULT_ADDR
            value: "https://vault.query.prod.consul:8200"
          - name: VAULT_SECRETPATH
            value: /tmp/vaultca
          - name: PKI_VAULT_ENGINE_PATH
            value: "v1/infra_pki/issue/kafka_client"
          - name: CERT_CN
            value: "mercury-psr-mercurygeneric-rw"
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          - name: VAULT_SERVICEACCOUNT
            value: mercurygeneric
          volumeMounts:
          - name: keystore-volume
            mountPath: /keystores
          - name: vaultca
            mountPath: /tmp/vaultca
            readOnly: true
          - name: keystore-pass
            mountPath: /tmp/keystore-pass
            readOnly: true
          resources:
              limits:
                cpu: "1"
                ephemeral-storage: 10Gi
                memory: 6Gi
              requests:
                cpu: 100m
                ephemeral-storage: 2Gi
                memory: 2Gi
        containers:
          - name: provisioning-processor
            image: "iad.ocir.io/osvcstage/mercury/provisioning-processor:22.01.18-REL21.11-1087"
            imagePullPolicy: IfNotPresent
            env:
            - name: profile
              value: oci-k8s
            - name: DATACENTER
              value: OCI
            - name: RELEASE_NAME
              value: mercury
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: POD_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            - name: MERCURY_URL_PATTERN
              value: https://engagement-psr.us1.channels.ocs.oraclecloud.com/mercury-psr
            - name: JAEGER_REMOTE_REPORTER
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: VAULT_TOKEN
              value: vault:login
            - name: HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: CONSUL_HTTP_ADDR
              value: https://$(HOST_IP):8501
            - name: CONSUL_TOKEN
              value: vault:/cpe_consul/creds/mercury-psr#token
            - name: JAVA_OPTS
              value: "-Dvault.enabled=true \
                      -Dvault.namespace=mercury-psr \
                      -Dvault.transit.kafkaMessageEncryption.path=transit_mercury \
                      -Dvault.base.address=https://vault.query.prod.consul:8200 \
                      -Dvault.shared.kv.engine=/kv \
                      -Dthidwick.vault.uri=https://vault.query.prod.consul:8200 \
                      -Dthidwick.vault.transit.cache-expiration=300 \
                      -Dthidwick.vault.kv.cache-expiration=300 \
                      -Dthidwick.vault.serviceaccount=mercurygeneric \
                      -Dlog4j2.formatMsgNoLookups=true \
                      -Dmercury.cpeVersion=v2
                      -Dfile.token.enabled=true \
                      -Dvault.authentication=TOKEN \
                      -Dvault.role=mercury-psr \
                      -Dvault.path=k8s-phx-dataplane
                      -Dkafka.log_level=WARN \
                      -Dthidwick-server.logback.level=INFO \
                      -Dthidwick-server.logback.service.log_level=DEBUG \
                      -Dthidwick.kafka.message.encryption.cipher=AES/GCM \
                      -Djwt.tenant.validation.enabled=false \
                      -Dthidwick.kafka.schema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul \
                      -Dthidwick.kafka.bootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443 \
                      -Dthidwick.tms.base.tmsStateStream.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443 \
                      -Djavax.net.ssl.trustStore=/keystores/truststore.jks \
                      -Djavax.net.ssl.keyStore=/keystores/keystore.jks \
                      -Dhttp.security.headers.enabled=true \
                      -Dhttp.rest.security.headers.enabled=true \
-                     -Dmercury.provisioning.max.time.mins=29                    -Xms1536m \
+                     -Dmercury.provisioning.max.time.mins=29 -Dwebhook.service.base.uri=http://authentication-service-helios.helios-psr.svc.cluster.local:80                    -Xms1536m \
                      -Xmx4096m \
                      -Dmercury.tenant.name.filter=\"^(evai)\" -Dsingle.sign.on.service.idp.login.response.url=https://engagement-psr.us1.channels.ocs.oraclecloud.com/mercury-psr/sp/saml/response -Dsingle.sign.on.service.idp.logout.response.url=https://engagement-psr.us1.channels.ocs.oraclecloud.com/mercury-psr/sp/saml/logout/response -Dpreferences.service.url=http://mercury-user-preference-service:8080      -Dmercury.provisioning.syncUpstreamData=true        -Dtenant.realm=engagement-psr.us1.channels.ocs.oraclecloud.com "
            volumeMounts:
            - mountPath: /keystores
              name: keystore-volume
            - mountPath: /tmp/keystore-pass
              name: keystore-pass
            ports:
              - name: http
                containerPort: 8080
                protocol: TCP
            startupProbe:
              periodSeconds: 10
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 120
              httpGet:
                path: /health/checks
                port: 8080
            livenessProbe:
              initialDelaySeconds: 120
              periodSeconds: 30
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 1
              httpGet:
                path: /health/checks
                port: 8080
            readinessProbe:
              initialDelaySeconds: 120
              periodSeconds: 30
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 1
              httpGet:
                path: /health/ping
                port: 8080
            # Service deployment provided resources
            resources:
              limits:
                cpu: "1"
                ephemeral-storage: 10Gi
                memory: 6Gi
              requests:
                cpu: 100m
                ephemeral-storage: 2Gi
                memory: 2Gi
mercury-psr, mercury-provisioning-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/provisioning-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-provisioning-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-queue-agent-info-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/queue-agent-info-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-queue-agent-info-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-realtime-channel-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/realtime-channel-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-realtime-channel-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-resource-channel-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/resource-channel-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-resource-channel-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-resource-state-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/resource-state-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-resource-state-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-resource-work-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/resource-work-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-resource-work-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-routing-processor-agent-assignment-keystore, Secret (v1) has changed:
  # Source: mercury/charts/routing-processor-agent-assignment/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-routing-processor-agent-assignment-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-routing-processor-agent-events-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/routing-processor-agent-events-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-routing-processor-agent-events-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-routing-processor-queue-assignment-keystore, Secret (v1) has changed:
  # Source: mercury/charts/routing-processor-queue-assignment/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-routing-processor-queue-assignment-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-routing-processor-work-events-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/routing-processor-work-events-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-routing-processor-work-events-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-session-housekeeping-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/session-housekeeping-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-session-housekeeping-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-session-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/session-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-session-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-single-sign-on-service-keystore, Secret (v1) has changed:
  # Source: mercury/charts/single-sign-on-service/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-single-sign-on-service-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-social-bridge-keystore, Secret (v1) has changed:
  # Source: mercury/charts/social-bridge/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-social-bridge-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-social-config-keystore, Secret (v1) has changed:
  # Source: mercury/charts/social-config/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-social-config-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-static-assets-service-keystore, Secret (v1) has changed:
  # Source: mercury/charts/static-assets-service/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-static-assets-service-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-tenant-downtime-monitor, Deployment (apps) has changed:
  # Source: mercury/charts/tenant-downtime-monitor/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: mercury-tenant-downtime-monitor
    labels:
      project: mercury
      version: "0"
      app.kubernetes.io/name: tenant-downtime-monitor
      helm.sh/chart: tenant-downtime-monitor-22.01.18-REL21.11-153
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/managed-by: Helm
      monitoring: "true"
    annotations:
  spec:
    replicas: 3
    selector:
      matchLabels:
        app.kubernetes.io/name: tenant-downtime-monitor
        app.kubernetes.io/instance: mercury
    strategy:
      type: RollingUpdate
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
    template:
      metadata:
        annotations:
          vault.security.banzaicloud.io/log-level: warn
          vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
          vault.security.banzaicloud.io/vault-agent-configmap: mercury-psr-va-configmap
          vault.security.banzaicloud.io/vault-env-daemon: "true"
          vault.security.banzaicloud.io/vault-path: k8s-phx-dataplane
          vault.security.banzaicloud.io/vault-role: mercury-psr
          vault.security.banzaicloud.io/vault-skip-verify: "true"
        labels:
          app.kubernetes.io/name: tenant-downtime-monitor
          app.kubernetes.io/instance: mercury
          chronos.enabled: "True"
          chronos.maxPodLifetime: "1440"
          chronos.minAvailable: "0"
      spec:
        serviceAccountName: mercurygeneric
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - "tenant-downtime-monitor"
                  - key: release
                    operator: In
                    values:
                    - mercury
                topologyKey: "failure-domain.beta.kubernetes.io/zone"
        imagePullSecrets:
          - name: osvcstage-ocirsecret
        volumes:
        - name: vaultca
          secret:
            secretName: vault-tls
        - name: keystore-pass
          secret:
            secretName: mercury-tenant-downtime-monitor-keystore
        - name: keystore-volume
          emptyDir: {}
        initContainers:
        - name: init
          image: iad.ocir.io/osvcstage/mercury/init-container/prod:1.0.0.18
          imagePullPolicy: IfNotPresent
          command: ['/bin/bash', '-c', '/init/init-script.sh']
          # Need to run the init container as root (also we're forced to use its UID, i.e. 0)
          securityContext:
            runAsUser: 0
          env:
          - name: AUTO_REFRESH_CONSUL_TOKEN
            value: vault:cpe_consul/creds/mercury-psr#token
          - name: KAFKA_TEMPLATED_PKI
            value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
              .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
              "ttl": "720h"}'
          - name: KEYSTORE_LOCATION
            value: /keystores
          - name: TRUSTSTORE_NAME
            value: truststore.jks
          - name: KEYSTORE_SECRETPATH
            value: /tmp/keystore-pass
          - name: KEYSTORE_TMPPASSFILENAME
            value: props/kafka.properties
          - name: KEYSTORE_NAME
            value: keystore.jks
          - name: SERVICEACCOUNT_SECRETPATH
            value: /var/run/secrets/kubernetes.io/serviceaccount
          - name: VAULT_ADDR
            value: "https://vault.query.prod.consul:8200"
          - name: VAULT_SECRETPATH
            value: /tmp/vaultca
          - name: PKI_VAULT_ENGINE_PATH
            value: "v1/infra_pki/issue/kafka_client"
          - name: CERT_CN
            value: "mercury-psr-mercurygeneric-rw"
          - name: KAFKA_SECURITY_PROTOCOL
            value: SSL
          - name: SA_ROLE
            value: "mercury-psr_mercurygeneric"
          - name: VAULT_SERVICEACCOUNT
            value: mercurygeneric
          volumeMounts:
          - name: keystore-volume
            mountPath: /keystores
          - name: vaultca
            mountPath: /tmp/vaultca
            readOnly: true
          - name: keystore-pass
            mountPath: /tmp/keystore-pass
            readOnly: true
          resources:
              limits:
                cpu: "1"
                ephemeral-storage: 10Gi
                memory: 6Gi
              requests:
                cpu: 100m
                ephemeral-storage: 2Gi
                memory: 2Gi
        containers:
          - name: tenant-downtime-monitor
            image: "iad.ocir.io/osvcstage/mercury/tenant-downtime-monitor:22.01.18-REL21.11-153"
            imagePullPolicy: IfNotPresent
            env:
            - name: profile
              value: oci-k8s
            - name: DATACENTER
              value: OCI
            - name: RELEASE_NAME
              value: mercury
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: POD_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            - name: MERCURY_URL_PATTERN
              value: https://engagement-psr.us1.channels.ocs.oraclecloud.com/mercury-psr
            - name: JAEGER_REMOTE_REPORTER
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: VAULT_TOKEN
              value: vault:login
            - name: HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: CONSUL_HTTP_ADDR
              value: https://$(HOST_IP):8501
            - name: CONSUL_TOKEN
              value: vault:/cpe_consul/creds/mercury-psr#token
            - name: JAVA_OPTS
              value: "-Dvault.enabled=true \
                      -Dvault.namespace=mercury-psr \
                      -Dvault.transit.kafkaMessageEncryption.path=transit_mercury \
                      -Dvault.base.address=https://vault.query.prod.consul:8200 \
                      -Dvault.shared.kv.engine=/kv \
                      -Dthidwick.vault.uri=https://vault.query.prod.consul:8200 \
                      -Dthidwick.vault.transit.cache-expiration=300 \
                      -Dthidwick.vault.kv.cache-expiration=300 \
                      -Dthidwick.vault.serviceaccount=mercurygeneric \
                      -Dlog4j2.formatMsgNoLookups=true \
                      -Dmercury.cpeVersion=v2
                      -Dfile.token.enabled=true \
                      -Dvault.authentication=TOKEN \
                      -Dvault.role=mercury-psr \
                      -Dvault.path=k8s-phx-dataplane
                      -Dkafka.log_level=WARN \
                      -Dthidwick-server.logback.level=INFO \
                      -Dthidwick-server.logback.service.log_level=DEBUG \
                      -Dthidwick.kafka.message.encryption.cipher=AES/GCM \
                      -Djwt.tenant.validation.enabled=false \
                      -Dthidwick.kafka.schema.registry.url=https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul \
                      -Dthidwick.kafka.bootstrap.servers=mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443 \
                      -Dthidwick.tms.base.tmsStateStream.bootstrap.servers=shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443 \
                      -Djavax.net.ssl.trustStore=/keystores/truststore.jks \
                      -Djavax.net.ssl.keyStore=/keystores/keystore.jks \
                      -Dhttp.security.headers.enabled=true \
                      -Dhttp.rest.security.headers.enabled=true \
-                     -XX:+UseG1GC -XX:MaxGCPauseMillis=250 \
-                     -Xms1536m \
+                     -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Dwebhook.death.topic.name=helios-psr.death.event -Dwebhook.base.url=http://authentication-service-helios.helios-psr.svc.cluster.local:80                    -Xms1536m \
                      -Xmx4096m \
                      -Dmercury.tenant.name.filter=\"^(evai)\"                 -Dtenant.realm=engagement-psr.us1.channels.ocs.oraclecloud.com "
            volumeMounts:
            - mountPath: /keystores
              name: keystore-volume
            - mountPath: /tmp/keystore-pass
              name: keystore-pass
            ports:
              - name: http
                containerPort: 8080
                protocol: TCP
            startupProbe:
              periodSeconds: 10
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 120
              httpGet:
                path: /health/checks
                port: 8080
            livenessProbe:
              initialDelaySeconds: 120
              periodSeconds: 30
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 1
              httpGet:
                path: /health/checks
                port: 8080
            readinessProbe:
              initialDelaySeconds: 120
              periodSeconds: 30
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 1
              httpGet:
                path: /health/ping
                port: 8080
            # Service deployment provided resources
            resources:
              limits:
                cpu: "1"
                ephemeral-storage: 10Gi
                memory: 6Gi
              requests:
                cpu: 100m
                ephemeral-storage: 2Gi
                memory: 2Gi
mercury-psr, mercury-tenant-downtime-monitor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/tenant-downtime-monitor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-tenant-downtime-monitor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-transcript-api-keystore, Secret (v1) has changed:
  # Source: mercury/charts/transcript-api/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-transcript-api-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-transcript-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/transcript-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-transcript-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-user-preference-service-keystore, Secret (v1) has changed:
  # Source: mercury/charts/user-preference-service/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-user-preference-service-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-work-api-keystore, Secret (v1) has changed:
  # Source: mercury/charts/work-api/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-work-api-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque

mercury-psr, mercury-work-processor-keystore, Secret (v1) has changed:
  # Source: mercury/charts/work-processor/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "1"
    name: mercury-work-processor-keystore
  data:
-   keystorePassphrase: '-------- # (10 bytes)'
-   truststorePassphrase: '-------- # (10 bytes)'
+   keystorePassphrase: '++++++++ # (10 bytes)'
+   truststorePassphrase: '++++++++ # (10 bytes)'
  type: Opaque


[21:January:2022:08:15:40]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kuc prod_uk-london-1_dataplane
Switched to context "prod_uk-london-1_dataplane".
[21:January:2022:08:19:08]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ rp
[21:January:2022:08:19:09]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgdpn mercury
NAME                                               READY   UP-TO-DATE   AVAILABLE   AGE
mercury-agent-command-service                      3/3     3            3           171d
mercury-channel-api                                0/0     0            0           58d
mercury-consumer-command-service                   3/3     3            3           171d
mercury-custom-availability-service                3/3     3            3           96d
mercury-data-mask-api                              3/3     3            3           171d
mercury-engagement-queue-api                       3/3     3            3           171d
mercury-enrichment-service                         3/3     3            3           171d
mercury-event-sync-service                         3/3     3            3           171d
mercury-integration-in-processor                   3/3     3            3           171d
mercury-integration-out-processor                  3/3     3            3           171d
mercury-kafka-dr-kafka-exporter                    1/1     1            1           171d
mercury-kafka-entity-operator                      1/1     1            1           171d
mercury-kafka-kafka-exporter                       1/1     1            1           171d
mercury-kafka-mm2-dr-uk-london-1-mirrormaker2      2/2     2            2           171d
mercury-kweet-facebook-client                      3/3     3            3           171d
mercury-kweet-facebook-webhook                     3/3     3            3           171d
mercury-kweet-twiliosms-client                     3/3     3            3           171d
mercury-kweet-userprofiles                         3/3     3            3           171d
mercury-kweet-wechat-client                        0/0     0            0           171d
mercury-kweet-wechat-webhook                       0/0     0            0           171d
mercury-mercury-ui                                 3/3     3            3           171d
mercury-metric-aggregation-processor               3/3     3            3           171d
mercury-metric-fusion-bridge                       3/3     3            3           171d
mercury-metric-generation-processor                3/3     3            3           171d
mercury-metric-internal-translation-processor      3/3     3            3           171d
mercury-metric-proxy-service                       3/3     3            3           171d
mercury-omnichannel-assignment-processor           3/3     3            3           171d
mercury-omnichannel-offer-processor                3/3     3            3           171d
mercury-osvc-bridge-api-services                   3/3     3            3           171d
mercury-osvc-bridge-metrics-data-pipeline          3/3     3            3           171d
mercury-osvc-bridge-osvc-data-extractor            3/3     3            3           171d
mercury-osvc-bridge-provisioning-processor         3/3     3            3           171d
mercury-osvc-bridge-state-processor                3/3     3            3           171d
mercury-osvc-bridge-state-query-service            3/3     3            3           171d
mercury-osvc-bridge-task-controller                3/3     3            3           171d
mercury-provisioning-monitor                       3/3     3            3           171d
mercury-provisioning-processor                     3/3     3            3           171d
mercury-queue-agent-info-processor                 3/3     3            3           171d
mercury-realtime-channel-processor                 3/3     3            3           171d
mercury-resource-channel-processor                 3/3     3            3           171d
mercury-resource-state-processor                   3/3     3            3           171d
mercury-resource-work-processor                    3/3     3            3           171d
mercury-routing-processor-agent-assignment         3/3     3            3           171d
mercury-routing-processor-agent-events-processor   3/3     3            3           171d
mercury-routing-processor-queue-assignment         3/3     3            3           171d
mercury-routing-processor-work-events-processor    3/3     3            3           171d
mercury-session-housekeeping-processor             3/3     3            3           171d
mercury-session-processor                          3/3     3            3           171d
mercury-single-sign-on-service                     3/3     3            3           171d
mercury-social-bridge                              3/3     3            3           171d
mercury-social-config                              3/3     3            3           171d
mercury-static-assets-service                      3/3     3            3           171d
mercury-tenant-downtime-monitor                    3/3     3            3           171d
mercury-transcript-api                             3/3     3            3           171d
mercury-transcript-processor                       3/3     3            3           171d
mercury-user-preference-service                    3/3     3            3           171d
mercury-work-api                                   3/3     3            3           171d
mercury-work-processor                             3/3     3            3           171d
[21:January:2022:08:19:39]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgdpn mercury mercury-provisioning-monitor -oyaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "4"
    meta.helm.sh/release-name: mercury
    meta.helm.sh/release-namespace: mercury
  creationTimestamp: "2021-08-02T23:55:52Z"
  generation: 5
  labels:
    app.kubernetes.io/instance: mercury
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: provisioning-monitor
    helm.sh/chart: provisioning-monitor-21.10.22-TRUNK-213
    monitoring: "true"
    project: mercury
    version: "0"
  name: mercury-provisioning-monitor
  namespace: mercury
  resourceVersion: "212739599"
  selfLink: /apis/apps/v1/namespaces/mercury/deployments/mercury-provisioning-monitor
  uid: a353f933-b92b-47de-bdb4-cb906ea7f49e
spec:
  progressDeadlineSeconds: 600
  replicas: 3
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/instance: mercury
      app.kubernetes.io/name: provisioning-monitor
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      annotations:
        vault.security.banzaicloud.io/log-level: warn
        vault.security.banzaicloud.io/vault-addr: https://vault.query.prod.consul:8200
        vault.security.banzaicloud.io/vault-agent-configmap: mercury-va-configmap
        vault.security.banzaicloud.io/vault-env-daemon: "true"
        vault.security.banzaicloud.io/vault-path: k8s-lhr-dataplane
        vault.security.banzaicloud.io/vault-role: mercury
        vault.security.banzaicloud.io/vault-skip-verify: "true"
      creationTimestamp: null
      labels:
        app.kubernetes.io/instance: mercury
        app.kubernetes.io/name: provisioning-monitor
        chronos.enabled: "True"
        chronos.maxPodLifetime: "1440"
        chronos.minAvailable: "0"
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - provisioning-monitor
                - key: release
                  operator: In
                  values:
                  - mercury
              topologyKey: failure-domain.beta.kubernetes.io/zone
            weight: 100
      containers:
      - env:
        - name: profile
          value: oci-k8s
        - name: DATACENTER
          value: OCI
        - name: RELEASE_NAME
          value: mercury
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: POD_SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.serviceAccountName
        - name: MERCURY_URL_PATTERN
          value: https://engagement.uk1.channels.ocs.oraclecloud.com/mercury
        - name: JAEGER_REMOTE_REPORTER
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.hostIP
        - name: VAULT_TOKEN
          value: vault:login
        - name: HOST_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.hostIP
        - name: CONSUL_HTTP_ADDR
          value: https://$(HOST_IP):8501
        - name: CONSUL_TOKEN
          value: vault:/cpe_consul/creds/mercury#token
        - name: JAVA_OPTS
          value: '-Dvault.enabled=true -Dvault.namespace=mercury -Dvault.transit.kafkaMessageEncryption.path=transit_mercury
            -Dvault.base.address=https://vault.query.prod.consul:8200 -Dvault.shared.kv.engine=/kv
            -Dthidwick.vault.uri=https://vault.query.prod.consul:8200 -Dthidwick.vault.transit.cache-expiration=300
            -Dthidwick.vault.kv.cache-expiration=300 -Dthidwick.vault.serviceaccount=mercurygeneric
            -Dlog4j2.formatMsgNoLookups=true -Dmercury.cpeVersion=v2 -Dfile.token.enabled=true
            -Dvault.authentication=TOKEN -Dvault.role=mercury -Dvault.path=k8s-lhr-dataplane
            -Dkafka.log_level=WARN -Dthidwick-server.logback.level=INFO -Dthidwick-server.logback.service.log_level=DEBUG
            -Dthidwick.kafka.message.encryption.cipher=AES/GCM -Djwt.tenant.validation.enabled=false
            -Dthidwick.kafka.schema.registry.url=https://schemaregistry-proxy-kafka.service.lhr-dataplane.prod.consul
            -Dthidwick.kafka.bootstrap.servers=mercury-kafka-bootstrap-mercury.service.lhr-dataplane.prod.consul:443
            -Dthidwick.tms.base.tmsStateStream.bootstrap.servers=shared-kafka-bootstrap-kafka.service.lhr-dataplane.prod.consul:443
            -Djavax.net.ssl.trustStore=/keystores/truststore.jks -Djavax.net.ssl.keyStore=/keystores/keystore.jks
            -Dhttp.security.headers.enabled=true -Dhttp.rest.security.headers.enabled=true
            -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Xms1536m   -Xmx4096m   -Dtenant.realm=engagement.uk1.channels.ocs.oraclecloud.com '
        image: iad.ocir.io/osvcstage/mercury/provisioning-monitor:21.10.22-TRUNK-213
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 1
          httpGet:
            path: /health/checks
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 120
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 1
        name: provisioning-monitor
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        readinessProbe:
          failureThreshold: 1
          httpGet:
            path: /health/ping
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 120
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          limits:
            cpu: "1"
            ephemeral-storage: 10Gi
            memory: 6Gi
          requests:
            cpu: 100m
            ephemeral-storage: 2Gi
            memory: 2Gi
        startupProbe:
          failureThreshold: 120
          httpGet:
            path: /health/checks
            port: 8080
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /keystores
          name: keystore-volume
        - mountPath: /tmp/keystore-pass
          name: keystore-pass
      dnsPolicy: ClusterFirst
      imagePullSecrets:
      - name: osvcstage-ocirsecret
      initContainers:
      - command:
        - /bin/bash
        - -c
        - /init/init-script.sh
        env:
        - name: AUTO_REFRESH_CONSUL_TOKEN
          value: vault:cpe_consul/creds/mercury#token
        - name: KAFKA_TEMPLATED_PKI
          value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
            .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
            "ttl": "720h"}'
        - name: KEYSTORE_LOCATION
          value: /keystores
        - name: TRUSTSTORE_NAME
          value: truststore.jks
        - name: KEYSTORE_SECRETPATH
          value: /tmp/keystore-pass
        - name: KEYSTORE_TMPPASSFILENAME
          value: props/kafka.properties
        - name: KEYSTORE_NAME
          value: keystore.jks
        - name: SERVICEACCOUNT_SECRETPATH
          value: /var/run/secrets/kubernetes.io/serviceaccount
        - name: VAULT_ADDR
          value: https://vault.query.prod.consul:8200
        - name: VAULT_SECRETPATH
          value: /tmp/vaultca
        - name: PKI_VAULT_ENGINE_PATH
          value: v1/infra_pki/issue/kafka_client
        - name: CERT_CN
          value: mercury-mercurygeneric-rw
        - name: KAFKA_SECURITY_PROTOCOL
          value: SSL
        - name: SA_ROLE
          value: mercury_mercurygeneric
        - name: VAULT_SERVICEACCOUNT
          value: mercurygeneric
        image: iad.ocir.io/osvcstage/mercury/init-container/prod:1.0.0.18
        imagePullPolicy: IfNotPresent
        name: init
        resources:
          limits:
            cpu: "1"
            ephemeral-storage: 10Gi
            memory: 6Gi
          requests:
            cpu: 100m
            ephemeral-storage: 2Gi
            memory: 2Gi
        securityContext:
          runAsUser: 0
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /keystores
          name: keystore-volume
        - mountPath: /tmp/vaultca
          name: vaultca
          readOnly: true
        - mountPath: /tmp/keystore-pass
          name: keystore-pass
          readOnly: true
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: mercurygeneric
      serviceAccountName: mercurygeneric
      terminationGracePeriodSeconds: 30
      volumes:
      - name: vaultca
        secret:
          defaultMode: 420
          secretName: vault-tls
      - name: keystore-pass
        secret:
          defaultMode: 420
          secretName: mercury-provisioning-monitor-keystore
      - emptyDir: {}
        name: keystore-volume
status:
  availableReplicas: 3
  conditions:
  - lastTransitionTime: "2021-11-24T06:35:56Z"
    lastUpdateTime: "2021-12-14T14:55:23Z"
    message: ReplicaSet "mercury-provisioning-monitor-8b8ff74" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  - lastTransitionTime: "2022-01-13T22:35:53Z"
    lastUpdateTime: "2022-01-13T22:35:53Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  observedGeneration: 5
  readyReplicas: 3
  replicas: 3
  updatedReplicas: 3
[21:January:2022:08:20:10]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ k scale deployments -n mercury mercury-provisioning-monitor --replicas=0
deployment.apps/mercury-provisioning-monitor scaled
[21:January:2022:08:21:15]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgdpn mercury mercury-provisioning-monitor
NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
mercury-provisioning-monitor   0/0     0            0           171d
[21:January:2022:08:21:26]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kuc prod_us-phoenix-1_dataplane
Switched to context "prod_us-phoenix-1_dataplane".
[21:January:2022:08:22:16]:(prod_uk-london-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ rp
[21:January:2022:08:22:19]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[21:January:2022:08:25:31]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[21:January:2022:08:25:32]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[21:January:2022:08:25:32]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ helm repo update
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_prod.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_prod.config
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "hashicorp" chart repository
...Successfully got an update from the "incubator" chart repository
...Successfully got an update from the "stable" chart repository
...Successfully got an update from the "bitnami" chart repository
index.go:339: skipping loading invalid entry for chart "dcs-api" "version.0.1" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "version.0.1" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.03-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.03.03-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.04-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.03.04-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0006-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.18-b0006-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0007-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.18-b0007-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.19-b0005-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.19-b0005-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.02.24-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.02.24-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0001-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.04-b0001-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.04-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0001-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.05-b0001-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.05-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0003-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.05-b0003-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "escalation-aggregator-service" "1-21.03.04-b0019-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1-21.03.04-b0019-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "kweet-config" "2111.08.1834-01" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "2111.08.1834-01" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.06-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.06-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.12-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.12-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.13-b0004" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.13-b0004" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.14-b0002" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.14-b0002" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.21-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.21-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.28-b0005" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.28-b0005" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.03-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0006" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.03-b0006" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.06-b0002" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.06-b0002" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.10-b0002" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.10-b0002" is invalid
index.go:339: skipping loading invalid entry for chart "api-tracker" "version.0.1" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "version.0.1" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.03-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.03.03-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.04-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.03.04-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0006-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.18-b0006-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0007-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.18-b0007-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.19-b0005-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.19-b0005-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.06-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.06-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.12-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.12-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.13-b0004" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.13-b0004" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.14-b0002" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.14-b0002" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.21-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.21-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.28-b0005" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.28-b0005" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.03-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0006" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.03-b0006" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.06-b0002" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.06-b0002" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.10-b0002" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.10-b0002" is invalid
index.go:339: skipping loading invalid entry for chart "api-tracker" "version.0.1" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "version.0.1" is invalid
index.go:339: skipping loading invalid entry for chart "dcs-api" "version.0.1" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "version.0.1" is invalid
index.go:339: skipping loading invalid entry for chart "kweet-config" "2111.08.1834-01" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "2111.08.1834-01" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.02.24-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.02.24-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0001-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.04-b0001-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.04-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0001-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.05-b0001-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.05-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0003-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.05-b0003-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "escalation-aggregator-service" "1-21.03.04-b0019-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1-21.03.04-b0019-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "escalation-aggregator-service" "1-21.03.04-b0019-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1-21.03.04-b0019-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.02.24-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.02.24-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0001-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.04-b0001-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.04-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0001-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.05-b0001-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.05-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0003-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "3.0.0-20.03.05-b0003-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "kweet-config" "2111.08.1834-01" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "2111.08.1834-01" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.06-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.06-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.12-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.12-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.13-b0004" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.13-b0004" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.14-b0002" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.14-b0002" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.21-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.21-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.28-b0005" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.04.28-b0005" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0001" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.03-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0006" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.03-b0006" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.06-b0002" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.06-b0002" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.10-b0002" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.10-b0002" is invalid
index.go:339: skipping loading invalid entry for chart "api-tracker" "version.0.1" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "version.0.1" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.03-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.03.03-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.04-b0002-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.03.04-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0006-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.18-b0006-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0007-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.18-b0007-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.19-b0005-SNAPSHOT" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "1.0.0-21.05.19-b0005-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "dcs-api" "version.0.1" from https://artifactory-master.cdaas.oraclecloud.com/artifactory/helm-osvc: validation: chart.metadata.version "version.0.1" is invalid
...Successfully got an update from the "helm-osvc" chart repository
...Successfully got an update from the "osvc-helm-virtual" chart repository
...Successfully got an update from the "cdaas" chart repository
Update Complete. ⎈Happy Helming!⎈
[21:January:2022:08:25:57]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ #helmfile -e prod -f ./mercury-psr/helmfile-mercury-psr-apps.yaml -l region=us-phoenix-1 diff --concurrency 3
[21:January:2022:08:26:03]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgpn mercury-psr

hln mercury-psr
NAME                                                              READY   STATUS    RESTARTS   AGE   IP             NODE          NOMINATED NODE   READINESS GATES
mercury-agent-command-service-59dcc858fc-gtl4m                    2/2     Running   0          33h   10.245.1.100   10.20.1.38    <none>           <none>
mercury-agent-command-service-59dcc858fc-ldzzz                    2/2     Running   0          33h   10.245.3.49    10.20.0.7     <none>           <none>
mercury-agent-command-service-59dcc858fc-zmkng                    2/2     Running   3          33h   10.245.8.145   10.20.1.158   <none>           <none>
mercury-channel-api-6f9c9c9dc5-f89lq                              2/2     Running   3          33h   10.245.8.143   10.20.1.158   <none>           <none>
mercury-consumer-command-service-76d45c9fcc-v89kt                 2/2     Running   0          33h   10.245.4.142   10.20.1.227   <none>           <none>
mercury-consumer-command-service-76d45c9fcc-x2dv2                 2/2     Running   0          33h   10.245.6.11    10.20.1.44    <none>           <none>
mercury-consumer-command-service-76d45c9fcc-xt68z                 2/2     Running   0          33h   10.245.2.40    10.20.1.35    <none>           <none>
mercury-custom-availability-service-569d4cf687-7dx62              2/2     Running   0          33h   10.245.5.197   10.20.0.143   <none>           <none>
mercury-custom-availability-service-569d4cf687-gsnqf              2/2     Running   3          33h   10.245.6.12    10.20.1.44    <none>           <none>
mercury-custom-availability-service-569d4cf687-w4qwz              2/2     Running   2          33h   10.245.4.139   10.20.1.227   <none>           <none>
mercury-data-mask-api-59584bc855-jhs6j                            2/2     Running   0          33h   10.245.4.25    10.20.1.27    <none>           <none>
mercury-data-mask-api-59584bc855-sl229                            2/2     Running   0          33h   10.245.4.147   10.20.1.227   <none>           <none>
mercury-data-mask-api-59584bc855-zq6bv                            2/2     Running   0          33h   10.245.3.240   10.20.0.250   <none>           <none>
mercury-engagement-queue-api-68fd4c96df-8slrm                     2/2     Running   2          33h   10.245.3.55    10.20.0.7     <none>           <none>
mercury-engagement-queue-api-68fd4c96df-z46rc                     2/2     Running   2          33h   10.245.2.140   10.20.1.191   <none>           <none>
mercury-engagement-queue-api-68fd4c96df-zcmfj                     2/2     Running   2          33h   10.245.2.42    10.20.1.35    <none>           <none>
mercury-enrichment-service-84c964b545-clw7g                       2/2     Running   0          33h   10.245.3.243   10.20.0.250   <none>           <none>
mercury-enrichment-service-84c964b545-gqvh2                       2/2     Running   0          33h   10.245.1.224   10.20.0.29    <none>           <none>
mercury-enrichment-service-84c964b545-zhkmp                       2/2     Running   0          33h   10.245.0.240   10.20.0.244   <none>           <none>
mercury-event-sync-service-57d9bd5765-96x4k                       2/2     Running   0          33h   10.245.4.29    10.20.1.27    <none>           <none>
mercury-event-sync-service-57d9bd5765-cbfqt                       2/2     Running   0          33h   10.245.7.181   10.20.1.224   <none>           <none>
mercury-event-sync-service-57d9bd5765-wpzkk                       2/2     Running   0          33h   10.245.0.50    10.20.1.247   <none>           <none>
mercury-integration-in-processor-849dff9756-4gcgz                 2/2     Running   3          33h   10.245.7.180   10.20.1.224   <none>           <none>
mercury-integration-in-processor-849dff9756-dl2p8                 2/2     Running   3          33h   10.245.3.241   10.20.0.250   <none>           <none>
mercury-integration-in-processor-849dff9756-pm6md                 2/2     Running   0          33h   10.245.4.41    10.20.1.27    <none>           <none>
mercury-integration-out-processor-689bdb99c4-66xvr                2/2     Running   3          33h   10.245.3.242   10.20.0.250   <none>           <none>
mercury-integration-out-processor-689bdb99c4-bldbg                2/2     Running   2          33h   10.245.4.30    10.20.1.27    <none>           <none>
mercury-integration-out-processor-689bdb99c4-vl4cv                2/2     Running   2          33h   10.245.7.182   10.20.1.224   <none>           <none>
mercury-kweet-facebook-client-7fddbc78f9-242wn                    2/2     Running   0          33h   10.245.8.141   10.20.1.158   <none>           <none>
mercury-kweet-facebook-client-7fddbc78f9-ckf75                    2/2     Running   0          33h   10.245.4.11    10.20.1.27    <none>           <none>
mercury-kweet-facebook-client-7fddbc78f9-nprbf                    2/2     Running   2          33h   10.245.1.94    10.20.1.38    <none>           <none>
mercury-kweet-facebook-webhook-5bc7986869-2npxp                   2/2     Running   0          33h   10.245.4.21    10.20.1.27    <none>           <none>
mercury-kweet-facebook-webhook-5bc7986869-bdtjb                   2/2     Running   3          33h   10.245.4.138   10.20.1.227   <none>           <none>
mercury-kweet-facebook-webhook-5bc7986869-d2r8q                   2/2     Running   3          33h   10.245.2.39    10.20.1.35    <none>           <none>
mercury-kweet-twiliosms-client-6b8f6ffc78-46p9m                   2/2     Running   0          33h   10.245.8.142   10.20.1.158   <none>           <none>
mercury-kweet-twiliosms-client-6b8f6ffc78-5zv2v                   2/2     Running   0          33h   10.245.6.7     10.20.1.44    <none>           <none>
mercury-kweet-twiliosms-client-6b8f6ffc78-tfqd8                   2/2     Running   0          33h   10.245.1.225   10.20.0.29    <none>           <none>
mercury-kweet-userprofiles-d9c5cdc6b-gqdhb                        2/2     Running   0          33h   10.245.8.144   10.20.1.158   <none>           <none>
mercury-kweet-userprofiles-d9c5cdc6b-jmwwb                        2/2     Running   0          33h   10.245.1.96    10.20.1.38    <none>           <none>
mercury-kweet-userprofiles-d9c5cdc6b-vzb7b                        2/2     Running   3          33h   10.245.4.14    10.20.1.27    <none>           <none>
mercury-mercury-ui-6f58d49b55-92tl8                               2/2     Running   0          33h   10.245.5.182   10.20.0.143   <none>           <none>
mercury-mercury-ui-6f58d49b55-jdvnq                               2/2     Running   0          33h   10.245.4.23    10.20.1.27    <none>           <none>
mercury-mercury-ui-6f58d49b55-zc7m2                               2/2     Running   0          33h   10.245.4.143   10.20.1.227   <none>           <none>
mercury-metric-aggregation-processor-68ff54fdb9-bkf8j             2/2     Running   0          33h   10.245.4.22    10.20.1.27    <none>           <none>
mercury-metric-aggregation-processor-68ff54fdb9-pbf9c             2/2     Running   0          33h   10.245.4.137   10.20.1.227   <none>           <none>
mercury-metric-aggregation-processor-68ff54fdb9-rldmm             2/2     Running   0          33h   10.245.5.180   10.20.0.143   <none>           <none>
mercury-metric-fusion-bridge-7f49b75ffb-gpj4g                     2/2     Running   0          33h   10.245.3.64    10.20.0.7     <none>           <none>
mercury-metric-fusion-bridge-7f49b75ffb-s7x7t                     2/2     Running   2          33h   10.245.4.140   10.20.1.227   <none>           <none>
mercury-metric-fusion-bridge-7f49b75ffb-wqf2r                     2/2     Running   0          33h   10.245.8.148   10.20.1.158   <none>           <none>
mercury-metric-generation-processor-54447b6657-gjjqd              2/2     Running   0          33h   10.245.2.36    10.20.1.35    <none>           <none>
mercury-metric-generation-processor-54447b6657-mbz9f              2/2     Running   0          33h   10.245.6.9     10.20.1.44    <none>           <none>
mercury-metric-generation-processor-54447b6657-qpcqc              2/2     Running   0          33h   10.245.1.98    10.20.1.38    <none>           <none>
mercury-metric-internal-translation-processor-57cf478c6-l2sb4     2/2     Running   2          33h   10.245.7.185   10.20.1.224   <none>           <none>
mercury-metric-internal-translation-processor-57cf478c6-lc8df     2/2     Running   2          33h   10.245.6.169   10.20.1.239   <none>           <none>
mercury-metric-internal-translation-processor-57cf478c6-zwlmn     2/2     Running   0          33h   10.245.0.53    10.20.1.247   <none>           <none>
mercury-metric-proxy-service-576df9c79f-6wgr6                     2/2     Running   0          33h   10.245.4.20    10.20.1.27    <none>           <none>
mercury-metric-proxy-service-576df9c79f-hsvfd                     2/2     Running   3          33h   10.245.1.101   10.20.1.38    <none>           <none>
mercury-metric-proxy-service-576df9c79f-vsfzd                     2/2     Running   0          33h   10.245.2.38    10.20.1.35    <none>           <none>
mercury-omnichannel-assignment-processor-587b8c9b6b-2675k         2/2     Running   2          33h   10.245.1.103   10.20.1.38    <none>           <none>
mercury-omnichannel-assignment-processor-587b8c9b6b-5nlch         2/2     Running   2          33h   10.245.8.147   10.20.1.158   <none>           <none>
mercury-omnichannel-assignment-processor-587b8c9b6b-fh9pr         2/2     Running   3          33h   10.245.4.26    10.20.1.27    <none>           <none>
mercury-omnichannel-offer-processor-76f947c586-9qfh6              2/2     Running   3          33h   10.245.3.52    10.20.0.7     <none>           <none>
mercury-omnichannel-offer-processor-76f947c586-l2c5q              2/2     Running   2          33h   10.245.2.133   10.20.1.191   <none>           <none>
mercury-omnichannel-offer-processor-76f947c586-x9sgc              2/2     Running   3          33h   10.245.8.146   10.20.1.158   <none>           <none>
mercury-osvc-bridge-api-services-756ff69dbd-b89zr                 2/2     Running   3          33h   10.245.5.187   10.20.0.143   <none>           <none>
mercury-osvc-bridge-api-services-756ff69dbd-jhlqf                 2/2     Running   2          33h   10.245.3.50    10.20.0.7     <none>           <none>
mercury-osvc-bridge-api-services-756ff69dbd-t2prz                 2/2     Running   3          33h   10.245.1.104   10.20.1.38    <none>           <none>
mercury-osvc-bridge-metrics-data-pipeline-59bfff645f-dbwpm        2/2     Running   0          33h   10.245.3.65    10.20.0.7     <none>           <none>
mercury-osvc-bridge-metrics-data-pipeline-59bfff645f-p6v69        2/2     Running   0          33h   10.245.2.137   10.20.1.191   <none>           <none>
mercury-osvc-bridge-metrics-data-pipeline-59bfff645f-vc5gq        2/2     Running   0          33h   10.245.5.192   10.20.0.143   <none>           <none>
mercury-osvc-bridge-osvc-data-extractor-855f9bd5bc-28rkm          2/2     Running   0          33h   10.245.2.136   10.20.1.191   <none>           <none>
mercury-osvc-bridge-osvc-data-extractor-855f9bd5bc-g9p5r          2/2     Running   1          33h   10.245.6.16    10.20.1.44    <none>           <none>
mercury-osvc-bridge-osvc-data-extractor-855f9bd5bc-grdg9          2/2     Running   3          33h   10.245.5.188   10.20.0.143   <none>           <none>
mercury-osvc-bridge-provisioning-processor-67cb69489-9llvc        2/2     Running   0          33h   10.245.0.51    10.20.1.247   <none>           <none>
mercury-osvc-bridge-state-processor-b655b7468-8k4xw               2/2     Running   0          33h   10.245.4.40    10.20.1.27    <none>           <none>
mercury-osvc-bridge-state-processor-b655b7468-ldjqw               2/2     Running   0          33h   10.245.2.143   10.20.1.191   <none>           <none>
mercury-osvc-bridge-state-processor-b655b7468-qtnjl               2/2     Running   0          33h   10.245.8.36    10.20.0.134   <none>           <none>
mercury-osvc-bridge-state-query-service-64dd4bbdc8-6lxwl          2/2     Running   3          33h   10.245.5.183   10.20.0.143   <none>           <none>
mercury-osvc-bridge-state-query-service-64dd4bbdc8-gx2x4          2/2     Running   2          33h   10.245.1.102   10.20.1.38    <none>           <none>
mercury-osvc-bridge-state-query-service-64dd4bbdc8-t7lxk          2/2     Running   3          33h   10.245.6.14    10.20.1.44    <none>           <none>
mercury-osvc-bridge-task-controller-569d4f4ff-67vcr               2/2     Running   0          33h   10.245.5.189   10.20.0.143   <none>           <none>
mercury-osvc-bridge-task-controller-569d4f4ff-69gtw               2/2     Running   1          33h   10.245.1.106   10.20.1.38    <none>           <none>
mercury-osvc-bridge-task-controller-569d4f4ff-8g589               2/2     Running   1          33h   10.245.6.17    10.20.1.44    <none>           <none>
mercury-provisioning-monitor-676848bb67-dqx88                     2/2     Running   0          33h   10.245.0.49    10.20.1.247   <none>           <none>
mercury-provisioning-monitor-676848bb67-k7sxg                     2/2     Running   0          33h   10.245.1.107   10.20.1.38    <none>           <none>
mercury-provisioning-monitor-676848bb67-kfwjp                     2/2     Running   0          33h   10.245.0.239   10.20.0.244   <none>           <none>
mercury-provisioning-processor-769945d9d8-6ddck                   2/2     Running   3          33h   10.245.4.17    10.20.1.27    <none>           <none>
mercury-provisioning-processor-769945d9d8-csqcn                   2/2     Running   3          33h   10.245.2.37    10.20.1.35    <none>           <none>
mercury-provisioning-processor-769945d9d8-dd7wr                   2/2     Running   0          33h   10.245.1.112   10.20.1.38    <none>           <none>
mercury-psr-kafka-entity-operator-858f557bb9-xzhlh                3/3     Running   0          33h   10.245.5.14    10.20.0.4     <none>           <none>
mercury-psr-kafka-kafka-0                                         2/2     Running   0          33h   10.245.5.11    10.20.0.4     <none>           <none>
mercury-psr-kafka-kafka-1                                         2/2     Running   0          33h   10.245.8.138   10.20.1.158   <none>           <none>
mercury-psr-kafka-kafka-2                                         2/2     Running   0          33h   10.245.1.223   10.20.0.29    <none>           <none>
mercury-psr-kafka-kafka-3                                         2/2     Running   0          33h   10.245.5.12    10.20.0.4     <none>           <none>
mercury-psr-kafka-kafka-4                                         2/2     Running   0          33h   10.245.8.139   10.20.1.158   <none>           <none>
mercury-psr-kafka-kafka-5                                         2/2     Running   0          33h   10.245.7.179   10.20.1.224   <none>           <none>
mercury-psr-kafka-kafka-6                                         2/2     Running   0          33h   10.245.5.13    10.20.0.4     <none>           <none>
mercury-psr-kafka-kafka-7                                         2/2     Running   0          33h   10.245.8.140   10.20.1.158   <none>           <none>
mercury-psr-kafka-kafka-8                                         2/2     Running   0          33h   10.245.2.131   10.20.1.191   <none>           <none>
mercury-psr-kafka-kafka-exporter-75f8b86dc4-czj8f                 1/1     Running   0          33h   10.245.5.15    10.20.0.4     <none>           <none>
mercury-psr-kafka-zookeeper-0                                     1/1     Running   0          33h   10.245.5.10    10.20.0.4     <none>           <none>
mercury-psr-kafka-zookeeper-1                                     1/1     Running   0          33h   10.245.8.137   10.20.1.158   <none>           <none>
mercury-psr-kafka-zookeeper-2                                     1/1     Running   0          33h   10.245.7.17    10.20.1.52    <none>           <none>
mercury-queue-agent-info-processor-5b4fcbb58c-pfm42               2/2     Running   3          33h   10.245.6.18    10.20.1.44    <none>           <none>
mercury-queue-agent-info-processor-5b4fcbb58c-v4dm6               2/2     Running   3          33h   10.245.4.149   10.20.1.227   <none>           <none>
mercury-queue-agent-info-processor-5b4fcbb58c-x5jch               2/2     Running   3          33h   10.245.5.191   10.20.0.143   <none>           <none>
mercury-realtime-channel-processor-64755f8cc9-4k5kj               2/2     Running   0          33h   10.245.4.144   10.20.1.227   <none>           <none>
mercury-realtime-channel-processor-64755f8cc9-jdzdd               2/2     Running   0          33h   10.245.6.15    10.20.1.44    <none>           <none>
mercury-realtime-channel-processor-64755f8cc9-tjzrw               2/2     Running   0          33h   10.245.5.184   10.20.0.143   <none>           <none>
mercury-resource-channel-processor-7d96c47886-4lnnz               2/2     Running   3          33h   10.245.2.142   10.20.1.191   <none>           <none>
mercury-resource-channel-processor-7d96c47886-hpvkk               2/2     Running   3          33h   10.245.2.44    10.20.1.35    <none>           <none>
mercury-resource-channel-processor-7d96c47886-khkqw               2/2     Running   2          33h   10.245.0.241   10.20.0.244   <none>           <none>
mercury-resource-state-processor-6c47fbbcb5-4kvhf                 2/2     Running   0          33h   10.245.3.53    10.20.0.7     <none>           <none>
mercury-resource-state-processor-6c47fbbcb5-mz22q                 2/2     Running   0          33h   10.245.2.41    10.20.1.35    <none>           <none>
mercury-resource-state-processor-6c47fbbcb5-x2w2s                 2/2     Running   0          33h   10.245.4.150   10.20.1.227   <none>           <none>
mercury-resource-work-processor-d447bfd4c-6lhkn                   2/2     Running   0          33h   10.245.5.179   10.20.0.143   <none>           <none>
mercury-resource-work-processor-d447bfd4c-ch5nj                   2/2     Running   0          33h   10.245.6.10    10.20.1.44    <none>           <none>
mercury-resource-work-processor-d447bfd4c-ctd2n                   2/2     Running   0          33h   10.245.4.136   10.20.1.227   <none>           <none>
mercury-routing-processor-agent-assignment-774885f97c-64tct       2/2     Running   2          33h   10.245.4.33    10.20.1.27    <none>           <none>
mercury-routing-processor-agent-assignment-774885f97c-8jbsz       2/2     Running   0          33h   10.245.2.144   10.20.1.191   <none>           <none>
mercury-routing-processor-agent-assignment-774885f97c-mfjdq       2/2     Running   2          33h   10.245.0.52    10.20.1.247   <none>           <none>
mercury-routing-processor-agent-events-processor-cd4c7689c7xct7   2/2     Running   0          33h   10.245.6.20    10.20.1.44    <none>           <none>
mercury-routing-processor-agent-events-processor-cd4c7689ccjmqv   2/2     Running   0          33h   10.245.3.245   10.20.0.250   <none>           <none>
mercury-routing-processor-agent-events-processor-cd4c7689cq6db2   2/2     Running   0          33h   10.245.4.152   10.20.1.227   <none>           <none>
mercury-routing-processor-queue-assignment-5464cd88bf-8j8pj       2/2     Running   2          33h   10.245.3.244   10.20.0.250   <none>           <none>
mercury-routing-processor-queue-assignment-5464cd88bf-bq6j9       2/2     Running   2          33h   10.245.6.19    10.20.1.44    <none>           <none>
mercury-routing-processor-queue-assignment-5464cd88bf-jnzl6       2/2     Running   3          33h   10.245.4.151   10.20.1.227   <none>           <none>
mercury-routing-processor-work-events-processor-7b6df456d826f79   2/2     Running   3          33h   10.245.2.132   10.20.1.191   <none>           <none>
mercury-routing-processor-work-events-processor-7b6df456d8h5n8x   2/2     Running   0          33h   10.245.5.198   10.20.0.143   <none>           <none>
mercury-routing-processor-work-events-processor-7b6df456d8hl7vv   2/2     Running   2          33h   10.245.0.238   10.20.0.244   <none>           <none>
mercury-session-housekeeping-processor-788fdfd5bb-2ncbw           2/2     Running   3          33h   10.245.5.196   10.20.0.143   <none>           <none>
mercury-session-housekeeping-processor-788fdfd5bb-l9z25           2/2     Running   2          33h   10.245.7.186   10.20.1.224   <none>           <none>
mercury-session-housekeeping-processor-788fdfd5bb-zhv58           2/2     Running   2          33h   10.245.4.35    10.20.1.27    <none>           <none>
mercury-session-processor-66d5445868-chdph                        2/2     Running   2          33h   10.245.5.195   10.20.0.143   <none>           <none>
mercury-session-processor-66d5445868-k67fv                        2/2     Running   2          33h   10.245.7.184   10.20.1.224   <none>           <none>
mercury-session-processor-66d5445868-pmv5w                        2/2     Running   2          33h   10.245.3.58    10.20.0.7     <none>           <none>
mercury-single-sign-on-service-5cd6974f66-7k458                   2/2     Running   2          33h   10.245.2.138   10.20.1.191   <none>           <none>
mercury-single-sign-on-service-5cd6974f66-dd78x                   2/2     Running   3          33h   10.245.5.193   10.20.0.143   <none>           <none>
mercury-single-sign-on-service-5cd6974f66-wmcb7                   2/2     Running   3          33h   10.245.4.32    10.20.1.27    <none>           <none>
mercury-social-bridge-5947d4fbb-6q7vt                             2/2     Running   3          33h   10.245.4.31    10.20.1.27    <none>           <none>
mercury-social-bridge-5947d4fbb-89x22                             2/2     Running   0          33h   10.245.8.37    10.20.0.134   <none>           <none>
mercury-social-bridge-5947d4fbb-v485v                             2/2     Running   2          33h   10.245.1.108   10.20.1.38    <none>           <none>
mercury-social-config-6b4cd9cb4d-dsmf5                            2/2     Running   3          33h   10.245.4.34    10.20.1.27    <none>           <none>
mercury-social-config-6b4cd9cb4d-m6txv                            2/2     Running   3          33h   10.245.1.109   10.20.1.38    <none>           <none>
mercury-social-config-6b4cd9cb4d-nlzkl                            2/2     Running   2          33h   10.245.5.194   10.20.0.143   <none>           <none>
mercury-static-assets-service-5b54b4c7d7-778vj                    2/2     Running   0          33h   10.245.8.35    10.20.0.134   <none>           <none>
mercury-static-assets-service-5b54b4c7d7-pbb5x                    2/2     Running   0          33h   10.245.3.63    10.20.0.7     <none>           <none>
mercury-static-assets-service-5b54b4c7d7-qd4hr                    2/2     Running   0          33h   10.245.1.110   10.20.1.38    <none>           <none>
mercury-tenant-downtime-monitor-78fdf4c974-hdg4h                  2/2     Running   0          33h   10.245.0.48    10.20.1.247   <none>           <none>
mercury-tenant-downtime-monitor-78fdf4c974-tp6gz                  2/2     Running   1          33h   10.245.1.105   10.20.1.38    <none>           <none>
mercury-tenant-downtime-monitor-78fdf4c974-w9bsj                  2/2     Running   1          33h   10.245.4.28    10.20.1.27    <none>           <none>
mercury-transcript-api-66568cb7cd-lj6n6                           2/2     Running   3          33h   10.245.4.148   10.20.1.227   <none>           <none>
mercury-transcript-api-66568cb7cd-n96zs                           2/2     Running   0          33h   10.245.5.185   10.20.0.143   <none>           <none>
mercury-transcript-api-66568cb7cd-qkngj                           2/2     Running   3          33h   10.245.0.237   10.20.0.244   <none>           <none>
mercury-transcript-processor-64779cbcd8-c4mx4                     2/2     Running   2          33h   10.245.1.111   10.20.1.38    <none>           <none>
mercury-transcript-processor-64779cbcd8-kkfbl                     2/2     Running   2          33h   10.245.8.149   10.20.1.158   <none>           <none>
mercury-transcript-processor-64779cbcd8-qq7k4                     2/2     Running   2          33h   10.245.4.39    10.20.1.27    <none>           <none>
mercury-user-preference-service-54c6587fc8-2h87d                  2/2     Running   0          33h   10.245.1.97    10.20.1.38    <none>           <none>
mercury-user-preference-service-54c6587fc8-ddlrk                  2/2     Running   0          33h   10.245.2.35    10.20.1.35    <none>           <none>
mercury-user-preference-service-54c6587fc8-k72kq                  2/2     Running   0          33h   10.245.6.8     10.20.1.44    <none>           <none>
mercury-work-api-b468f7db7-fcf62                                  2/2     Running   3          33h   10.245.2.43    10.20.1.35    <none>           <none>
mercury-work-api-b468f7db7-qsm7h                                  2/2     Running   3          33h   10.245.3.56    10.20.0.7     <none>           <none>
mercury-work-api-b468f7db7-zccn4                                  2/2     Running   3          33h   10.245.7.183   10.20.1.224   <none>           <none>
mercury-work-processor-6b888f5bbf-2twtx                           2/2     Running   0          33h   10.245.4.13    10.20.1.27    <none>           <none>
mercury-work-processor-6b888f5bbf-fwhks                           2/2     Running   0          33h   10.245.5.178   10.20.0.143   <none>           <none>
mercury-work-processor-6b888f5bbf-qnxpp                           2/2     Running   0          33h   10.245.4.135   10.20.1.227   <none>           <none>
[21:January:2022:08:26:20]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[21:January:2022:08:26:20]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ hln mercury-psr
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_prod.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_prod.config

kgdNAME       NAMESPACE   REVISION  UPDATED                                 STATUS    CHART                 APP VERSION
app-kafka mercury-psr 1         2022-01-19 22:48:59.456337456 +0000 UTC deployed  cpe-kafka-0.1.0       0.1.0
appdata   mercury-psr 1         2022-01-19 22:49:17.75707579 +0000 UTC  deployed  kafka-data-0.1.0      null
mercury   mercury-psr 1         2022-01-19 22:50:04.789572242 +0000 UTC deployed  mercury-2201.18.1305  22.01.18-REL21.11-1305
[21:January:2022:08:26:24]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[21:January:2022:08:26:24]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgdpn mercury psr
çError from server (NotFound): deployments.apps "psr" not found
[21:January:2022:08:26:32]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgdpn mercury-psr
NAME                                               READY   UP-TO-DATE   AVAILABLE   AGE
mercury-agent-command-service                      3/3     3            3           33h
mercury-channel-api                                1/1     1            1           33h
mercury-consumer-command-service                   3/3     3            3           33h
mercury-custom-availability-service                3/3     3            3           33h
mercury-data-mask-api                              3/3     3            3           33h
mercury-engagement-queue-api                       3/3     3            3           33h
mercury-enrichment-service                         3/3     3            3           33h
mercury-event-sync-service                         3/3     3            3           33h
mercury-integration-in-processor                   3/3     3            3           33h
mercury-integration-out-processor                  3/3     3            3           33h
mercury-kweet-facebook-client                      3/3     3            3           33h
mercury-kweet-facebook-webhook                     3/3     3            3           33h
mercury-kweet-twiliosms-client                     3/3     3            3           33h
mercury-kweet-userprofiles                         3/3     3            3           33h
mercury-kweet-wechat-client                        0/0     0            0           33h
mercury-kweet-wechat-webhook                       0/0     0            0           33h
mercury-mercury-ui                                 3/3     3            3           33h
mercury-metric-aggregation-processor               3/3     3            3           33h
mercury-metric-fusion-bridge                       3/3     3            3           33h
mercury-metric-generation-processor                3/3     3            3           33h
mercury-metric-internal-translation-processor      3/3     3            3           33h
mercury-metric-proxy-service                       3/3     3            3           33h
mercury-omnichannel-assignment-processor           3/3     3            3           33h
mercury-omnichannel-offer-processor                3/3     3            3           33h
mercury-osvc-bridge-api-services                   3/3     3            3           33h
mercury-osvc-bridge-metrics-data-pipeline          3/3     3            3           33h
mercury-osvc-bridge-osvc-data-extractor            3/3     3            3           33h
mercury-osvc-bridge-provisioning-processor         1/1     1            1           33h
mercury-osvc-bridge-state-processor                3/3     3            3           33h
mercury-osvc-bridge-state-query-service            3/3     3            3           33h
mercury-osvc-bridge-task-controller                3/3     3            3           33h
mercury-provisioning-monitor                       3/3     3            3           33h
mercury-provisioning-processor                     3/3     3            3           33h
mercury-psr-kafka-entity-operator                  1/1     1            1           33h
mercury-psr-kafka-kafka-exporter                   1/1     1            1           33h
mercury-queue-agent-info-processor                 3/3     3            3           33h
mercury-realtime-channel-processor                 3/3     3            3           33h
mercury-resource-channel-processor                 3/3     3            3           33h
mercury-resource-state-processor                   3/3     3            3           33h
mercury-resource-work-processor                    3/3     3            3           33h
mercury-routing-processor-agent-assignment         3/3     3            3           33h
mercury-routing-processor-agent-events-processor   3/3     3            3           33h
mercury-routing-processor-queue-assignment         3/3     3            3           33h
mercury-routing-processor-work-events-processor    3/3     3            3           33h
mercury-session-housekeeping-processor             3/3     3            3           33h
mercury-session-processor                          3/3     3            3           33h
mercury-single-sign-on-service                     3/3     3            3           33h
mercury-social-bridge                              3/3     3            3           33h
mercury-social-config                              3/3     3            3           33h
mercury-static-assets-service                      3/3     3            3           33h
mercury-tenant-downtime-monitor                    3/3     3            3           33h
mercury-transcript-api                             3/3     3            3           33h
mercury-transcript-processor                       3/3     3            3           33h
mercury-user-preference-service                    3/3     3            3           33h
mercury-work-api                                   3/3     3            3           33h
mercury-work-processor                             3/3     3            3           33h
[21:January:2022:08:26:41]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ helmfile -e prod -f ./mercury-psr/helmfile-mercury-psr-apps.yaml -l region=us-phoenix-1 sync --concurrency 3
Affected releases are:
  mercury (osvc-helm-virtual/mercury) UPDATED

Upgrading release=mercury, chart=osvc-helm-virtual/mercury

FAILED RELEASES:
NAME
mercury
in mercury-psr/helmfile-mercury-psr-apps.yaml: failed processing release mercury: command "/usr/local/bin/helm" exited with non-zero status:

PATH:
  /usr/local/bin/helm

ARGS:
  0: helm (4 bytes)
  1: upgrade (7 bytes)
  2: --install (9 bytes)
  3: --reset-values (14 bytes)
  4: mercury (7 bytes)
  5: osvc-helm-virtual/mercury (25 bytes)
  6: --version (9 bytes)
  7: 2201.18.1305 (12 bytes)
  8: --wait (6 bytes)
  9: --timeout (9 bytes)
  10: 900s (4 bytes)
  11: --kube-context (14 bytes)
  12: prod_us-phoenix-1_dataplane (27 bytes)
  13: --namespace (11 bytes)
  14: mercury-psr (11 bytes)
  15: --values (8 bytes)
  16: /tmp/helmfile416263449/mercury-psr-mercury-values-c85994cc5 (59 bytes)
  17: --values (8 bytes)
  18: /tmp/helmfile835825572/mercury-psr-mercury-values-578596c79f (60 bytes)
  19: --values (8 bytes)
  20: /tmp/helmfile832314035/mercury-psr-mercury-values-6c6cbcdc6d (60 bytes)
  21: --values (8 bytes)
  22: /tmp/helmfile615712886/mercury-psr-mercury-values-69b49465f (59 bytes)
  23: --values (8 bytes)
  24: /tmp/helmfile322869085/mercury-psr-mercury-values-5dccb995c8 (60 bytes)
  25: --values (8 bytes)
  26: /tmp/helmfile383833880/mercury-psr-mercury-values-6b7b84b58 (59 bytes)
  27: --history-max (13 bytes)
  28: 4 (1 bytes)

ERROR:
  exit status 1

EXIT STATUS
  1

STDERR:
  WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_prod.config
  WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_prod.config
  index.go:339: skipping loading invalid entry for chart "kweet-config" "2111.08.1834-01" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "2111.08.1834-01" is invalid
  index.go:339: skipping loading invalid entry for chart "api-tracker" "version.0.1" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "version.0.1" is invalid
  index.go:339: skipping loading invalid entry for chart "escalation-aggregator-service" "1-21.03.04-b0019-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1-21.03.04-b0019-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.06-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.06-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.12-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.12-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.13-b0004" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.13-b0004" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.14-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.14-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.21-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.21-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.28-b0005" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.28-b0005" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.03-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0006" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.03-b0006" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.06-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.06-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.10-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.10-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.03-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.03.03-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.04-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.03.04-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0006-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.18-b0006-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0007-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.18-b0007-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.19-b0005-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.19-b0005-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.02.24-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.02.24-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0001-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.04-b0001-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.04-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0001-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0001-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0003-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0003-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "dcs-api" "version.0.1" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "version.0.1" is invalid
  W0121 08:37:42.561438    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:42.853954    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:43.158963    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:44.026829    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:44.319543    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:44.621725    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:45.488063    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:45.779325    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:46.079538    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:46.947858    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:47.240727    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:47.544508    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:48.411612    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:48.702777    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:48.997247    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:49.873515    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:50.166326    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:50.463401    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:51.335032    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:51.626781    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:51.922151    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:52.791530    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:53.084543    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:53.378625    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:54.247495    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:54.542522    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:54.839924    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:55.706270    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:56.002513    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:56.298148    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:57.170808    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:57.465517    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:57.767289    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:58.643386    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:58.934553    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:59.227496    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:00.095034    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:00.396121    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:00.689237    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:01.556409    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:01.848701    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:02.152309    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:03.023404    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:03.316412    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:03.610285    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:04.482342    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:04.776538    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:05.072338    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:05.946211    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:06.242134    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:06.535743    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:07.403378    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:07.693855    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:07.985994    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:08.819491    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:09.110613    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:09.410291    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:10.278440    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:10.571492    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:10.865016    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:11.729768    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:12.025760    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:12.327841    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:13.202021    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:13.493690    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:13.793519    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:14.659525    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:14.951940    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:15.247637    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:16.114359    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:16.405644    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:16.711124    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:17.580729    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:17.874082    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:18.169413    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:19.037912    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:19.334617    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:19.628706    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  Error: UPGRADE FAILED: context deadline exceeded

COMBINED OUTPUT:
  WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_prod.config
  WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_prod.config
  index.go:339: skipping loading invalid entry for chart "kweet-config" "2111.08.1834-01" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "2111.08.1834-01" is invalid
  index.go:339: skipping loading invalid entry for chart "api-tracker" "version.0.1" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "version.0.1" is invalid
  index.go:339: skipping loading invalid entry for chart "escalation-aggregator-service" "1-21.03.04-b0019-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1-21.03.04-b0019-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.06-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.06-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.12-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.12-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.13-b0004" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.13-b0004" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.14-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.14-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.21-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.21-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.28-b0005" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.28-b0005" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.03-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0006" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.03-b0006" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.06-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.06-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.10-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.10-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.03-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.03.03-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.04-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.03.04-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0006-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.18-b0006-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0007-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.18-b0007-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.19-b0005-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.19-b0005-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.02.24-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.02.24-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0001-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.04-b0001-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.04-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0001-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0001-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0003-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0003-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "dcs-api" "version.0.1" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "version.0.1" is invalid
  W0121 08:37:42.561438    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:42.853954    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:43.158963    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:44.026829    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:44.319543    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:44.621725    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:45.488063    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:45.779325    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:46.079538    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:46.947858    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:47.240727    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:47.544508    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:48.411612    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:48.702777    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:48.997247    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:49.873515    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:50.166326    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:50.463401    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:51.335032    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:51.626781    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:51.922151    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:52.791530    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:53.084543    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:53.378625    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:54.247495    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:54.542522    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:54.839924    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:55.706270    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:56.002513    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:56.298148    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:57.170808    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:57.465517    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:57.767289    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:58.643386    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:58.934553    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:37:59.227496    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:00.095034    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:00.396121    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:00.689237    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:01.556409    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:01.848701    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:02.152309    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:03.023404    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:03.316412    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:03.610285    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:04.482342    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:04.776538    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:05.072338    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:05.946211    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:06.242134    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:06.535743    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:07.403378    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:07.693855    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:07.985994    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:08.819491    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:09.110613    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:09.410291    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:10.278440    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:10.571492    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:10.865016    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:11.729768    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:12.025760    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:12.327841    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:13.202021    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:13.493690    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:13.793519    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:14.659525    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:14.951940    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:15.247637    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:16.114359    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:16.405644    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:16.711124    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:17.580729    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:17.874082    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:18.169413    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:19.037912    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:19.334617    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 08:38:19.628706    6129 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  Error: UPGRADE FAILED: context deadline exceeded
[21:January:2022:08:53:22]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ helm ls -n mercury-psr
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_prod.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_prod.config
NAME      NAMESPACE   REVISION  UPDATED                                 STATUS    CHART                 APP VERSION
app-kafka mercury-psr 1         2022-01-19 22:48:59.456337456 +0000 UTC deployed  cpe-kafka-0.1.0       0.1.0
appdata   mercury-psr 1         2022-01-19 22:49:17.75707579 +0000 UTC  deployed  kafka-data-0.1.0      null
mercury   mercury-psr 2         2022-01-21 08:28:07.3263355 +0000 UTC   failed    mercury-2201.18.1305  22.01.18-REL21.11-1305
[21:January:2022:10:00:52]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgno
NAME          STATUS   ROLES   AGE     VERSION
10.20.0.134   Ready    node    48d     v1.19.12
10.20.0.143   Ready    node    58d     v1.19.12
10.20.0.244   Ready    node    59d     v1.19.12
10.20.0.250   Ready    node    58d     v1.19.12
10.20.0.29    Ready    node    58d     v1.19.12
10.20.0.4     Ready    node    7d10h   v1.19.12
10.20.0.7     Ready    node    58d     v1.19.12
10.20.1.158   Ready    node    7d10h   v1.19.12
10.20.1.191   Ready    node    58d     v1.19.12
10.20.1.224   Ready    node    48d     v1.19.12
10.20.1.227   Ready    node    58d     v1.19.12
10.20.1.239   Ready    node    48d     v1.19.12
10.20.1.247   Ready    node    48d     v1.19.12
10.20.1.27    Ready    node    58d     v1.19.12
10.20.1.35    Ready    node    58d     v1.19.12
10.20.1.38    Ready    node    59d     v1.19.12
10.20.1.44    Ready    node    58d     v1.19.12
10.20.1.52    Ready    node    48d     v1.19.12
[21:January:2022:10:01:47]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgpn mercury-psr
NAME                                                              READY   STATUS    RESTARTS   AGE   IP             NODE          NOMINATED NODE   READINESS GATES
mercury-agent-command-service-59dcc858fc-gtl4m                    2/2     Running   0          35h   10.245.1.100   10.20.1.38    <none>           <none>
mercury-agent-command-service-59dcc858fc-ldzzz                    2/2     Running   0          35h   10.245.3.49    10.20.0.7     <none>           <none>
mercury-agent-command-service-59dcc858fc-zmkng                    2/2     Running   3          35h   10.245.8.145   10.20.1.158   <none>           <none>
mercury-channel-api-6f9c9c9dc5-f89lq                              2/2     Running   3          35h   10.245.8.143   10.20.1.158   <none>           <none>
mercury-consumer-command-service-76d45c9fcc-v89kt                 2/2     Running   0          35h   10.245.4.142   10.20.1.227   <none>           <none>
mercury-consumer-command-service-76d45c9fcc-x2dv2                 2/2     Running   0          35h   10.245.6.11    10.20.1.44    <none>           <none>
mercury-consumer-command-service-76d45c9fcc-xt68z                 2/2     Running   0          35h   10.245.2.40    10.20.1.35    <none>           <none>
mercury-custom-availability-service-569d4cf687-7dx62              2/2     Running   0          35h   10.245.5.197   10.20.0.143   <none>           <none>
mercury-custom-availability-service-569d4cf687-gsnqf              2/2     Running   3          35h   10.245.6.12    10.20.1.44    <none>           <none>
mercury-custom-availability-service-569d4cf687-w4qwz              2/2     Running   2          35h   10.245.4.139   10.20.1.227   <none>           <none>
mercury-data-mask-api-59584bc855-jhs6j                            2/2     Running   0          35h   10.245.4.25    10.20.1.27    <none>           <none>
mercury-data-mask-api-59584bc855-sl229                            2/2     Running   0          35h   10.245.4.147   10.20.1.227   <none>           <none>
mercury-data-mask-api-59584bc855-zq6bv                            2/2     Running   0          35h   10.245.3.240   10.20.0.250   <none>           <none>
mercury-engagement-queue-api-68fd4c96df-8slrm                     2/2     Running   2          35h   10.245.3.55    10.20.0.7     <none>           <none>
mercury-engagement-queue-api-68fd4c96df-z46rc                     2/2     Running   2          35h   10.245.2.140   10.20.1.191   <none>           <none>
mercury-engagement-queue-api-68fd4c96df-zcmfj                     2/2     Running   2          35h   10.245.2.42    10.20.1.35    <none>           <none>
mercury-enrichment-service-6644f74c79-2kqpv                       2/2     Running   0          81m   10.245.0.243   10.20.0.244   <none>           <none>
mercury-enrichment-service-6644f74c79-85zs5                       2/2     Running   0          86m   10.245.3.248   10.20.0.250   <none>           <none>
mercury-enrichment-service-6644f74c79-v52hh                       2/2     Running   0          83m   10.245.1.231   10.20.0.29    <none>           <none>
mercury-event-sync-service-65b7cffbd9-k7hw4                       2/2     Running   0          81m   10.245.6.173   10.20.1.239   <none>           <none>
mercury-event-sync-service-65b7cffbd9-mqrtp                       2/2     Running   0          86m   10.245.0.65    10.20.1.247   <none>           <none>
mercury-event-sync-service-65b7cffbd9-txvhb                       2/2     Running   0          83m   10.245.7.199   10.20.1.224   <none>           <none>
mercury-integration-in-processor-849dff9756-4gcgz                 2/2     Running   3          35h   10.245.7.180   10.20.1.224   <none>           <none>
mercury-integration-in-processor-849dff9756-dl2p8                 2/2     Running   3          35h   10.245.3.241   10.20.0.250   <none>           <none>
mercury-integration-in-processor-849dff9756-pm6md                 2/2     Running   0          35h   10.245.4.41    10.20.1.27    <none>           <none>
mercury-integration-out-processor-689bdb99c4-66xvr                2/2     Running   3          35h   10.245.3.242   10.20.0.250   <none>           <none>
mercury-integration-out-processor-689bdb99c4-bldbg                2/2     Running   2          35h   10.245.4.30    10.20.1.27    <none>           <none>
mercury-integration-out-processor-689bdb99c4-vl4cv                2/2     Running   2          35h   10.245.7.182   10.20.1.224   <none>           <none>
mercury-kweet-facebook-client-7fddbc78f9-242wn                    2/2     Running   0          35h   10.245.8.141   10.20.1.158   <none>           <none>
mercury-kweet-facebook-client-7fddbc78f9-ckf75                    2/2     Running   0          35h   10.245.4.11    10.20.1.27    <none>           <none>
mercury-kweet-facebook-client-7fddbc78f9-nprbf                    2/2     Running   2          35h   10.245.1.94    10.20.1.38    <none>           <none>
mercury-kweet-facebook-webhook-5bc7986869-2npxp                   2/2     Running   0          35h   10.245.4.21    10.20.1.27    <none>           <none>
mercury-kweet-facebook-webhook-5bc7986869-bdtjb                   2/2     Running   3          35h   10.245.4.138   10.20.1.227   <none>           <none>
mercury-kweet-facebook-webhook-5bc7986869-d2r8q                   2/2     Running   3          35h   10.245.2.39    10.20.1.35    <none>           <none>
mercury-kweet-twiliosms-client-6b8f6ffc78-46p9m                   2/2     Running   0          35h   10.245.8.142   10.20.1.158   <none>           <none>
mercury-kweet-twiliosms-client-6b8f6ffc78-5zv2v                   2/2     Running   0          35h   10.245.6.7     10.20.1.44    <none>           <none>
mercury-kweet-twiliosms-client-6b8f6ffc78-tfqd8                   2/2     Running   0          35h   10.245.1.225   10.20.0.29    <none>           <none>
mercury-kweet-userprofiles-d9c5cdc6b-gqdhb                        2/2     Running   0          35h   10.245.8.144   10.20.1.158   <none>           <none>
mercury-kweet-userprofiles-d9c5cdc6b-jmwwb                        2/2     Running   0          35h   10.245.1.96    10.20.1.38    <none>           <none>
mercury-kweet-userprofiles-d9c5cdc6b-vzb7b                        2/2     Running   3          35h   10.245.4.14    10.20.1.27    <none>           <none>
mercury-mercury-ui-6f58d49b55-92tl8                               2/2     Running   0          35h   10.245.5.182   10.20.0.143   <none>           <none>
mercury-mercury-ui-6f58d49b55-jdvnq                               2/2     Running   0          35h   10.245.4.23    10.20.1.27    <none>           <none>
mercury-mercury-ui-6f58d49b55-zc7m2                               2/2     Running   0          35h   10.245.4.143   10.20.1.227   <none>           <none>
mercury-metric-aggregation-processor-68ff54fdb9-bkf8j             2/2     Running   0          35h   10.245.4.22    10.20.1.27    <none>           <none>
mercury-metric-aggregation-processor-68ff54fdb9-pbf9c             2/2     Running   0          35h   10.245.4.137   10.20.1.227   <none>           <none>
mercury-metric-aggregation-processor-68ff54fdb9-rldmm             2/2     Running   0          35h   10.245.5.180   10.20.0.143   <none>           <none>
mercury-metric-fusion-bridge-7f49b75ffb-gpj4g                     2/2     Running   0          35h   10.245.3.64    10.20.0.7     <none>           <none>
mercury-metric-fusion-bridge-7f49b75ffb-s7x7t                     2/2     Running   2          35h   10.245.4.140   10.20.1.227   <none>           <none>
mercury-metric-fusion-bridge-7f49b75ffb-wqf2r                     2/2     Running   0          35h   10.245.8.148   10.20.1.158   <none>           <none>
mercury-metric-generation-processor-54447b6657-gjjqd              2/2     Running   0          35h   10.245.2.36    10.20.1.35    <none>           <none>
mercury-metric-generation-processor-54447b6657-mbz9f              2/2     Running   0          35h   10.245.6.9     10.20.1.44    <none>           <none>
mercury-metric-generation-processor-54447b6657-qpcqc              2/2     Running   0          35h   10.245.1.98    10.20.1.38    <none>           <none>
mercury-metric-internal-translation-processor-57cf478c6-l2sb4     2/2     Running   2          35h   10.245.7.185   10.20.1.224   <none>           <none>
mercury-metric-internal-translation-processor-57cf478c6-lc8df     2/2     Running   2          35h   10.245.6.169   10.20.1.239   <none>           <none>
mercury-metric-internal-translation-processor-57cf478c6-zwlmn     2/2     Running   0          35h   10.245.0.53    10.20.1.247   <none>           <none>
mercury-metric-proxy-service-576df9c79f-6wgr6                     2/2     Running   0          35h   10.245.4.20    10.20.1.27    <none>           <none>
mercury-metric-proxy-service-576df9c79f-hsvfd                     2/2     Running   3          35h   10.245.1.101   10.20.1.38    <none>           <none>
mercury-metric-proxy-service-576df9c79f-vsfzd                     2/2     Running   0          35h   10.245.2.38    10.20.1.35    <none>           <none>
mercury-omnichannel-assignment-processor-587b8c9b6b-2675k         2/2     Running   2          35h   10.245.1.103   10.20.1.38    <none>           <none>
mercury-omnichannel-assignment-processor-587b8c9b6b-5nlch         2/2     Running   2          35h   10.245.8.147   10.20.1.158   <none>           <none>
mercury-omnichannel-assignment-processor-587b8c9b6b-fh9pr         2/2     Running   3          35h   10.245.4.26    10.20.1.27    <none>           <none>
mercury-omnichannel-offer-processor-76f947c586-9qfh6              2/2     Running   3          35h   10.245.3.52    10.20.0.7     <none>           <none>
mercury-omnichannel-offer-processor-76f947c586-l2c5q              2/2     Running   2          35h   10.245.2.133   10.20.1.191   <none>           <none>
mercury-omnichannel-offer-processor-76f947c586-x9sgc              2/2     Running   3          35h   10.245.8.146   10.20.1.158   <none>           <none>
mercury-osvc-bridge-api-services-756ff69dbd-b89zr                 2/2     Running   3          35h   10.245.5.187   10.20.0.143   <none>           <none>
mercury-osvc-bridge-api-services-756ff69dbd-jhlqf                 2/2     Running   2          35h   10.245.3.50    10.20.0.7     <none>           <none>
mercury-osvc-bridge-api-services-756ff69dbd-t2prz                 2/2     Running   3          35h   10.245.1.104   10.20.1.38    <none>           <none>
mercury-osvc-bridge-metrics-data-pipeline-59bfff645f-dbwpm        2/2     Running   0          35h   10.245.3.65    10.20.0.7     <none>           <none>
mercury-osvc-bridge-metrics-data-pipeline-59bfff645f-p6v69        2/2     Running   0          35h   10.245.2.137   10.20.1.191   <none>           <none>
mercury-osvc-bridge-metrics-data-pipeline-59bfff645f-vc5gq        2/2     Running   0          35h   10.245.5.192   10.20.0.143   <none>           <none>
mercury-osvc-bridge-osvc-data-extractor-855f9bd5bc-28rkm          2/2     Running   0          35h   10.245.2.136   10.20.1.191   <none>           <none>
mercury-osvc-bridge-osvc-data-extractor-855f9bd5bc-g9p5r          2/2     Running   1          35h   10.245.6.16    10.20.1.44    <none>           <none>
mercury-osvc-bridge-osvc-data-extractor-855f9bd5bc-grdg9          2/2     Running   3          35h   10.245.5.188   10.20.0.143   <none>           <none>
mercury-osvc-bridge-provisioning-processor-67cb69489-9llvc        2/2     Running   0          35h   10.245.0.51    10.20.1.247   <none>           <none>
mercury-osvc-bridge-state-processor-b655b7468-8k4xw               2/2     Running   0          35h   10.245.4.40    10.20.1.27    <none>           <none>
mercury-osvc-bridge-state-processor-b655b7468-ldjqw               2/2     Running   0          35h   10.245.2.143   10.20.1.191   <none>           <none>
mercury-osvc-bridge-state-processor-b655b7468-qtnjl               2/2     Running   0          35h   10.245.8.36    10.20.0.134   <none>           <none>
mercury-osvc-bridge-state-query-service-64dd4bbdc8-6lxwl          2/2     Running   3          35h   10.245.5.183   10.20.0.143   <none>           <none>
mercury-osvc-bridge-state-query-service-64dd4bbdc8-gx2x4          2/2     Running   2          35h   10.245.1.102   10.20.1.38    <none>           <none>
mercury-osvc-bridge-state-query-service-64dd4bbdc8-t7lxk          2/2     Running   3          35h   10.245.6.14    10.20.1.44    <none>           <none>
mercury-osvc-bridge-task-controller-569d4f4ff-67vcr               2/2     Running   0          35h   10.245.5.189   10.20.0.143   <none>           <none>
mercury-osvc-bridge-task-controller-569d4f4ff-69gtw               2/2     Running   1          35h   10.245.1.106   10.20.1.38    <none>           <none>
mercury-osvc-bridge-task-controller-569d4f4ff-8g589               2/2     Running   1          35h   10.245.6.17    10.20.1.44    <none>           <none>
mercury-provisioning-monitor-676848bb67-dqx88                     2/2     Running   0          35h   10.245.0.49    10.20.1.247   <none>           <none>
mercury-provisioning-monitor-676848bb67-k7sxg                     2/2     Running   0          35h   10.245.1.107   10.20.1.38    <none>           <none>
mercury-provisioning-monitor-676848bb67-kfwjp                     2/2     Running   0          35h   10.245.0.239   10.20.0.244   <none>           <none>
mercury-provisioning-processor-5f7df9bc49-kdx9h                   2/2     Running   0          85m   10.245.2.48    10.20.1.35    <none>           <none>
mercury-provisioning-processor-5f7df9bc49-lwqwq                   2/2     Running   0          80m   10.245.4.42    10.20.1.27    <none>           <none>
mercury-provisioning-processor-5f7df9bc49-pd4kg                   2/2     Running   0          83m   10.245.2.145   10.20.1.191   <none>           <none>
mercury-psr-kafka-entity-operator-858f557bb9-xzhlh                3/3     Running   0          35h   10.245.5.14    10.20.0.4     <none>           <none>
mercury-psr-kafka-kafka-0                                         2/2     Running   0          35h   10.245.5.11    10.20.0.4     <none>           <none>
mercury-psr-kafka-kafka-1                                         2/2     Running   0          35h   10.245.8.138   10.20.1.158   <none>           <none>
mercury-psr-kafka-kafka-2                                         2/2     Running   0          35h   10.245.1.223   10.20.0.29    <none>           <none>
mercury-psr-kafka-kafka-3                                         2/2     Running   0          35h   10.245.5.12    10.20.0.4     <none>           <none>
mercury-psr-kafka-kafka-4                                         2/2     Running   0          35h   10.245.8.139   10.20.1.158   <none>           <none>
mercury-psr-kafka-kafka-5                                         2/2     Running   0          35h   10.245.7.179   10.20.1.224   <none>           <none>
mercury-psr-kafka-kafka-6                                         2/2     Running   0          35h   10.245.5.13    10.20.0.4     <none>           <none>
mercury-psr-kafka-kafka-7                                         2/2     Running   0          35h   10.245.8.140   10.20.1.158   <none>           <none>
mercury-psr-kafka-kafka-8                                         2/2     Running   0          35h   10.245.2.131   10.20.1.191   <none>           <none>
mercury-psr-kafka-kafka-exporter-75f8b86dc4-czj8f                 1/1     Running   0          35h   10.245.5.15    10.20.0.4     <none>           <none>
mercury-psr-kafka-zookeeper-0                                     1/1     Running   0          35h   10.245.5.10    10.20.0.4     <none>           <none>
mercury-psr-kafka-zookeeper-1                                     1/1     Running   0          35h   10.245.8.137   10.20.1.158   <none>           <none>
mercury-psr-kafka-zookeeper-2                                     1/1     Running   0          35h   10.245.7.17    10.20.1.52    <none>           <none>
mercury-queue-agent-info-processor-5b4fcbb58c-pfm42               2/2     Running   3          35h   10.245.6.18    10.20.1.44    <none>           <none>
mercury-queue-agent-info-processor-5b4fcbb58c-v4dm6               2/2     Running   3          35h   10.245.4.149   10.20.1.227   <none>           <none>
mercury-queue-agent-info-processor-5b4fcbb58c-x5jch               2/2     Running   3          35h   10.245.5.191   10.20.0.143   <none>           <none>
mercury-realtime-channel-processor-64755f8cc9-4k5kj               2/2     Running   0          35h   10.245.4.144   10.20.1.227   <none>           <none>
mercury-realtime-channel-processor-64755f8cc9-jdzdd               2/2     Running   0          35h   10.245.6.15    10.20.1.44    <none>           <none>
mercury-realtime-channel-processor-64755f8cc9-tjzrw               2/2     Running   0          35h   10.245.5.184   10.20.0.143   <none>           <none>
mercury-resource-channel-processor-7d96c47886-4lnnz               2/2     Running   3          35h   10.245.2.142   10.20.1.191   <none>           <none>
mercury-resource-channel-processor-7d96c47886-hpvkk               2/2     Running   3          35h   10.245.2.44    10.20.1.35    <none>           <none>
mercury-resource-channel-processor-7d96c47886-khkqw               2/2     Running   2          35h   10.245.0.241   10.20.0.244   <none>           <none>
mercury-resource-state-processor-6c47fbbcb5-4kvhf                 2/2     Running   0          35h   10.245.3.53    10.20.0.7     <none>           <none>
mercury-resource-state-processor-6c47fbbcb5-mz22q                 2/2     Running   0          35h   10.245.2.41    10.20.1.35    <none>           <none>
mercury-resource-state-processor-6c47fbbcb5-x2w2s                 2/2     Running   0          35h   10.245.4.150   10.20.1.227   <none>           <none>
mercury-resource-work-processor-d447bfd4c-6lhkn                   2/2     Running   0          35h   10.245.5.179   10.20.0.143   <none>           <none>
mercury-resource-work-processor-d447bfd4c-ch5nj                   2/2     Running   0          35h   10.245.6.10    10.20.1.44    <none>           <none>
mercury-resource-work-processor-d447bfd4c-ctd2n                   2/2     Running   0          35h   10.245.4.136   10.20.1.227   <none>           <none>
mercury-routing-processor-agent-assignment-774885f97c-64tct       2/2     Running   2          35h   10.245.4.33    10.20.1.27    <none>           <none>
mercury-routing-processor-agent-assignment-774885f97c-8jbsz       2/2     Running   0          35h   10.245.2.144   10.20.1.191   <none>           <none>
mercury-routing-processor-agent-assignment-774885f97c-mfjdq       2/2     Running   2          35h   10.245.0.52    10.20.1.247   <none>           <none>
mercury-routing-processor-agent-events-processor-cd4c7689c7xct7   2/2     Running   0          35h   10.245.6.20    10.20.1.44    <none>           <none>
mercury-routing-processor-agent-events-processor-cd4c7689ccjmqv   2/2     Running   0          35h   10.245.3.245   10.20.0.250   <none>           <none>
mercury-routing-processor-agent-events-processor-cd4c7689cq6db2   2/2     Running   0          35h   10.245.4.152   10.20.1.227   <none>           <none>
mercury-routing-processor-queue-assignment-5464cd88bf-8j8pj       2/2     Running   2          35h   10.245.3.244   10.20.0.250   <none>           <none>
mercury-routing-processor-queue-assignment-5464cd88bf-bq6j9       2/2     Running   2          35h   10.245.6.19    10.20.1.44    <none>           <none>
mercury-routing-processor-queue-assignment-5464cd88bf-jnzl6       2/2     Running   3          35h   10.245.4.151   10.20.1.227   <none>           <none>
mercury-routing-processor-work-events-processor-7b6df456d826f79   2/2     Running   3          35h   10.245.2.132   10.20.1.191   <none>           <none>
mercury-routing-processor-work-events-processor-7b6df456d8h5n8x   2/2     Running   0          35h   10.245.5.198   10.20.0.143   <none>           <none>
mercury-routing-processor-work-events-processor-7b6df456d8hl7vv   2/2     Running   2          35h   10.245.0.238   10.20.0.244   <none>           <none>
mercury-session-housekeeping-processor-788fdfd5bb-2ncbw           2/2     Running   3          35h   10.245.5.196   10.20.0.143   <none>           <none>
mercury-session-housekeeping-processor-788fdfd5bb-l9z25           2/2     Running   2          35h   10.245.7.186   10.20.1.224   <none>           <none>
mercury-session-housekeeping-processor-788fdfd5bb-zhv58           2/2     Running   2          35h   10.245.4.35    10.20.1.27    <none>           <none>
mercury-session-processor-66d5445868-chdph                        2/2     Running   2          35h   10.245.5.195   10.20.0.143   <none>           <none>
mercury-session-processor-66d5445868-k67fv                        2/2     Running   2          35h   10.245.7.184   10.20.1.224   <none>           <none>
mercury-session-processor-66d5445868-pmv5w                        2/2     Running   2          35h   10.245.3.58    10.20.0.7     <none>           <none>
mercury-single-sign-on-service-5cd6974f66-7k458                   2/2     Running   2          35h   10.245.2.138   10.20.1.191   <none>           <none>
mercury-single-sign-on-service-5cd6974f66-dd78x                   2/2     Running   3          35h   10.245.5.193   10.20.0.143   <none>           <none>
mercury-single-sign-on-service-5cd6974f66-wmcb7                   2/2     Running   3          35h   10.245.4.32    10.20.1.27    <none>           <none>
mercury-social-bridge-5947d4fbb-6q7vt                             2/2     Running   3          35h   10.245.4.31    10.20.1.27    <none>           <none>
mercury-social-bridge-5947d4fbb-89x22                             2/2     Running   0          35h   10.245.8.37    10.20.0.134   <none>           <none>
mercury-social-bridge-5947d4fbb-v485v                             2/2     Running   2          35h   10.245.1.108   10.20.1.38    <none>           <none>
mercury-social-config-6b4cd9cb4d-dsmf5                            2/2     Running   3          35h   10.245.4.34    10.20.1.27    <none>           <none>
mercury-social-config-6b4cd9cb4d-m6txv                            2/2     Running   3          35h   10.245.1.109   10.20.1.38    <none>           <none>
mercury-social-config-6b4cd9cb4d-nlzkl                            2/2     Running   2          35h   10.245.5.194   10.20.0.143   <none>           <none>
mercury-static-assets-service-5b54b4c7d7-778vj                    2/2     Running   0          35h   10.245.8.35    10.20.0.134   <none>           <none>
mercury-static-assets-service-5b54b4c7d7-pbb5x                    2/2     Running   0          35h   10.245.3.63    10.20.0.7     <none>           <none>
mercury-static-assets-service-5b54b4c7d7-qd4hr                    2/2     Running   0          35h   10.245.1.110   10.20.1.38    <none>           <none>
mercury-tenant-downtime-monitor-78fdf4c974-hdg4h                  2/2     Running   0          35h   10.245.0.48    10.20.1.247   <none>           <none>
mercury-tenant-downtime-monitor-78fdf4c974-tp6gz                  2/2     Running   1          35h   10.245.1.105   10.20.1.38    <none>           <none>
mercury-tenant-downtime-monitor-78fdf4c974-w9bsj                  2/2     Running   1          35h   10.245.4.28    10.20.1.27    <none>           <none>
mercury-tenant-downtime-monitor-b58fc56f8-txfq9                   1/2     Running   4          85m   10.245.0.66    10.20.1.247   <none>           <none>
mercury-transcript-api-66568cb7cd-lj6n6                           2/2     Running   3          35h   10.245.4.148   10.20.1.227   <none>           <none>
mercury-transcript-api-66568cb7cd-n96zs                           2/2     Running   0          35h   10.245.5.185   10.20.0.143   <none>           <none>
mercury-transcript-api-66568cb7cd-qkngj                           2/2     Running   3          35h   10.245.0.237   10.20.0.244   <none>           <none>
mercury-transcript-processor-64779cbcd8-c4mx4                     2/2     Running   2          35h   10.245.1.111   10.20.1.38    <none>           <none>
mercury-transcript-processor-64779cbcd8-kkfbl                     2/2     Running   2          35h   10.245.8.149   10.20.1.158   <none>           <none>
mercury-transcript-processor-64779cbcd8-qq7k4                     2/2     Running   2          35h   10.245.4.39    10.20.1.27    <none>           <none>
mercury-user-preference-service-54c6587fc8-2h87d                  2/2     Running   0          35h   10.245.1.97    10.20.1.38    <none>           <none>
mercury-user-preference-service-54c6587fc8-ddlrk                  2/2     Running   0          35h   10.245.2.35    10.20.1.35    <none>           <none>
mercury-user-preference-service-54c6587fc8-k72kq                  2/2     Running   0          35h   10.245.6.8     10.20.1.44    <none>           <none>
mercury-work-api-b468f7db7-fcf62                                  2/2     Running   3          35h   10.245.2.43    10.20.1.35    <none>           <none>
mercury-work-api-b468f7db7-qsm7h                                  2/2     Running   3          35h   10.245.3.56    10.20.0.7     <none>           <none>
mercury-work-api-b468f7db7-zccn4                                  2/2     Running   3          35h   10.245.7.183   10.20.1.224   <none>           <none>
mercury-work-processor-6b888f5bbf-2twtx                           2/2     Running   0          35h   10.245.4.13    10.20.1.27    <none>           <none>
mercury-work-processor-6b888f5bbf-fwhks                           2/2     Running   0          35h   10.245.5.178   10.20.0.143   <none>           <none>
mercury-work-processor-6b888f5bbf-qnxpp                           2/2     Running   0          35h   10.245.4.135   10.20.1.227   <none>           <none>
[21:January:2022:10:02:37]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ #kgpn mercury-psr
[21:January:2022:10:04:42]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ ll /home/opc/.kube/pv2_prod.config
-rw-r--r-- 1 opc opc 95278 Jan 21 08:22 /home/opc/.kube/pv2_prod.config
[21:January:2022:10:04:43]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ chmod 400 /home/opc/.kube/pv2_prod.config
[21:January:2022:10:04:51]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ #helmfile -e prod -f ./mercury-psr/helmfile-mercury-psr-apps.yaml -l region=us-phoenix-1 sync --concurrency 5
[21:January:2022:10:05:22]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgc
CURRENT   NAME                               CLUSTER               AUTHINFO           NAMESPACE
          prod_ap-hyderabad-1_controlplane   cluster-cbjr52qcfoa   user-cbjr52qcfoa
          prod_ap-hyderabad-1_dataplane      cluster-cpj2zxktnda   user-cpj2zxktnda
          prod_ap-melbourne-1_controlplane   cluster-czgcn3bmjtg   user-czgcn3bmjtg
          prod_ap-melbourne-1_dataplane      cluster-c4dazdggfst   user-c4dazdggfst
          prod_ap-mumbai-1_controlplane      cluster-cjiru3mifia   user-cjiru3mifia
          prod_ap-mumbai-1_dataplane         cluster-ci3ypr6tmga   user-ci3ypr6tmga
          prod_ap-osaka-1_controlplane       cluster-cp5327xtmfa   user-cp5327xtmfa
          prod_ap-osaka-1_dataplane          cluster-cqldck6xatq   user-cqldck6xatq
          prod_ap-sydney-1_controlplane      cluster-c4wembugqyt   user-c4wembugqyt
          prod_ap-sydney-1_dataplane         cluster-cswezbzgyyg   user-cswezbzgyyg
          prod_ap-tokyo-1_controlplane       cluster-cz54pc3z4wa   user-cz54pc3z4wa
          prod_ap-tokyo-1_dataplane          cluster-cyjtbtbbjfq   user-cyjtbtbbjfq
          prod_ca-montreal-1_controlplane    cluster-czdoolemnqw   user-czdoolemnqw
          prod_ca-montreal-1_dataplane       cluster-cqtkytbgq3d   user-cqtkytbgq3d
          prod_ca-toronto-1_controlplane     cluster-cygcmjxhfsd   user-cygcmjxhfsd
          prod_ca-toronto-1_dataplane        cluster-cywmodfmm3d   user-cywmodfmm3d
          prod_eu-amsterdam-1_controlplane   cluster-c4tkmjwgbrt   user-c4tkmjwgbrt
          prod_eu-amsterdam-1_dataplane      cluster-cstsy3bmjrd   user-cstsy3bmjrd
          prod_eu-frankfurt-1_controlplane   cluster-cydcytcg43d   user-cydcytcg43d
          prod_eu-frankfurt-1_dataplane      cluster-crdgzbrmjsd   user-crdgzbrmjsd
          prod_eu-zurich-1_controlplane      cluster-cwmoxn3bfwq   user-cwmoxn3bfwq
          prod_eu-zurich-1_dataplane         cluster-chtoe5bnsza   user-chtoe5bnsza
          prod_me-dubai-1_controlplane       cluster-ce5puidc4ya   user-ce5puidc4ya
          prod_me-dubai-1_dataplane          cluster-c77hcd3vizq   user-c77hcd3vizq
          prod_me-jeddah-1_controlplane      cluster-cpzagsoukeq   user-cpzagsoukeq
          prod_me-jeddah-1_dataplane         cluster-cf4f222mrfa   user-cf4f222mrfa
          prod_sa-santiago-1_controlplane    cluster-czxpimgibeq   user-czxpimgibeq
          prod_sa-santiago-1_dataplane       cluster-cuutxfvj2ea   user-cuutxfvj2ea
          prod_sa-saopaulo-1_controlplane    cluster-cc75n7hujpq   user-cc75n7hujpq
          prod_sa-saopaulo-1_dataplane       cluster-cpjqrjtbotq   user-cpjqrjtbotq
          prod_sa-vinhedo-1_controlplane     cluster-cihx6aprjua   user-cihx6aprjua
          prod_sa-vinhedo-1_dataplane        cluster-ciqmbmn6wta   user-ciqmbmn6wta
          prod_uk-cardiff-1_controlplane     cluster-ctggmtbmfrg   user-ctggmtbmfrg
          prod_uk-cardiff-1_dataplane        cluster-cqwmnrqmztd   user-cqwmnrqmztd
          prod_uk-london-1_controlplane      cluster-cywgnlcmqzd   user-cywgnlcmqzd
          prod_uk-london-1_dataplane         cluster-c3tazjsmjsd   user-c3tazjsmjsd
          prod_us-ashburn-1_controlplane     cluster-czwczjzgrsw   user-czwczjzgrsw
          prod_us-ashburn-1_dataplane        cluster-c3wkyjyguzt   user-c3wkyjyguzt
          prod_us-phoenix-1_controlplane     cluster-c3tkobugrst   user-c3tkobugrst
*         prod_us-phoenix-1_dataplane        cluster-csdqylfmi2g   user-csdqylfmi2g
          prod_us-phoenix-1_deployment       cluster-c4tgolgmjsd   user-c4tgolgmjsd
[21:January:2022:10:05:24]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ rp
[21:January:2022:10:05:27]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ helmfile -e prod -f ./mercury-psr/helmfile-mercury-psr-apps.yaml -l region=us-phoenix-1 sync --concurrency 5
Affected releases are:
  mercury (osvc-helm-virtual/mercury) UPDATED

Upgrading release=mercury, chart=osvc-helm-virtual/mercury

FAILED RELEASES:
NAME
mercury
in mercury-psr/helmfile-mercury-psr-apps.yaml: failed processing release mercury: command "/usr/local/bin/helm" exited with non-zero status:

PATH:
  /usr/local/bin/helm

ARGS:
  0: helm (4 bytes)
  1: upgrade (7 bytes)
  2: --install (9 bytes)
  3: --reset-values (14 bytes)
  4: mercury (7 bytes)
  5: osvc-helm-virtual/mercury (25 bytes)
  6: --version (9 bytes)
  7: 2201.18.1305 (12 bytes)
  8: --wait (6 bytes)
  9: --timeout (9 bytes)
  10: 900s (4 bytes)
  11: --kube-context (14 bytes)
  12: prod_us-phoenix-1_dataplane (27 bytes)
  13: --namespace (11 bytes)
  14: mercury-psr (11 bytes)
  15: --values (8 bytes)
  16: /tmp/helmfile958074627/mercury-psr-mercury-values-c85994cc5 (59 bytes)
  17: --values (8 bytes)
  18: /tmp/helmfile480038022/mercury-psr-mercury-values-578596c79f (60 bytes)
  19: --values (8 bytes)
  20: /tmp/helmfile795260461/mercury-psr-mercury-values-6c6cbcdc6d (60 bytes)
  21: --values (8 bytes)
  22: /tmp/helmfile371787176/mercury-psr-mercury-values-69b49465f (59 bytes)
  23: --values (8 bytes)
  24: /tmp/helmfile318791399/mercury-psr-mercury-values-5dccb995c8 (60 bytes)
  25: --values (8 bytes)
  26: /tmp/helmfile021872410/mercury-psr-mercury-values-6b7b84b58 (59 bytes)
  27: --history-max (13 bytes)
  28: 4 (1 bytes)

ERROR:
  exit status 1

EXIT STATUS
  1

STDERR:
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.03-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.03.03-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.04-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.03.04-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0006-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.18-b0006-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0007-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.18-b0007-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.19-b0005-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.19-b0005-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "escalation-aggregator-service" "1-21.03.04-b0019-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1-21.03.04-b0019-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "kweet-config" "2111.08.1834-01" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "2111.08.1834-01" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.06-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.06-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.12-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.12-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.13-b0004" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.13-b0004" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.14-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.14-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.21-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.21-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.28-b0005" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.28-b0005" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.03-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0006" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.03-b0006" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.06-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.06-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.10-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.10-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "dcs-api" "version.0.1" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "version.0.1" is invalid
  index.go:339: skipping loading invalid entry for chart "api-tracker" "version.0.1" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "version.0.1" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.02.24-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.02.24-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0001-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.04-b0001-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.04-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0001-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0001-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0003-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0003-SNAPSHOT" is invalid
  W0121 10:16:06.082912    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:06.373506    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:06.668649    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:07.539583    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:07.829870    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:08.130020    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:08.997984    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:09.290869    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:09.584256    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:10.452010    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:10.748236    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:11.045665    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:11.910902    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:12.203281    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:12.504882    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:13.376345    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:13.669016    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:13.966941    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:14.832830    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:15.122490    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:15.413502    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:16.282605    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:16.573988    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:16.889141    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:17.760386    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:18.053254    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:18.350572    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:19.218227    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:19.509414    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:19.804414    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:20.674618    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:20.967920    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:21.264413    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:22.128769    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:22.420105    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:22.717948    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:23.585658    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:23.878949    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:24.174827    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:25.045358    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:25.336336    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:25.628853    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:26.503969    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:26.796369    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:27.093542    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:27.960168    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:28.252620    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:28.549342    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:29.415852    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:29.707028    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:30.002915    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:30.872865    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:31.167934    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:31.464091    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:32.345051    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:32.636232    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:32.901793    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:33.771113    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:34.074258    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:34.373466    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:35.242497    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:35.533753    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:35.827118    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:36.692511    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:36.983525    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:37.280706    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:38.147950    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:38.441557    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:38.737132    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:39.606310    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:39.899012    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:40.193798    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:41.061297    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:41.354834    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:41.653437    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:42.518040    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:42.809083    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:43.105061    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  Error: UPGRADE FAILED: Get "https://130.35.130.41:6443/api/v1/namespaces/mercury-psr/services/mercury-consumer-command-service": context deadline exceeded

COMBINED OUTPUT:
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.03-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.03.03-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.04-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.03.04-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0006-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.18-b0006-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0007-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.18-b0007-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.19-b0005-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.19-b0005-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "escalation-aggregator-service" "1-21.03.04-b0019-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1-21.03.04-b0019-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "kweet-config" "2111.08.1834-01" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "2111.08.1834-01" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.06-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.06-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.12-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.12-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.13-b0004" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.13-b0004" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.14-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.14-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.21-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.21-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.28-b0005" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.28-b0005" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.03-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0006" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.03-b0006" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.06-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.06-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.10-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.10-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "dcs-api" "version.0.1" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "version.0.1" is invalid
  index.go:339: skipping loading invalid entry for chart "api-tracker" "version.0.1" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "version.0.1" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.02.24-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.02.24-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0001-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.04-b0001-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.04-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0001-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0001-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0003-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0003-SNAPSHOT" is invalid
  W0121 10:16:06.082912    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:06.373506    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:06.668649    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:07.539583    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:07.829870    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:08.130020    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:08.997984    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:09.290869    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:09.584256    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:10.452010    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:10.748236    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:11.045665    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:11.910902    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:12.203281    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:12.504882    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:13.376345    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:13.669016    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:13.966941    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:14.832830    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:15.122490    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:15.413502    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:16.282605    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:16.573988    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:16.889141    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:17.760386    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:18.053254    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:18.350572    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:19.218227    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:19.509414    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:19.804414    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:20.674618    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:20.967920    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:21.264413    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:22.128769    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:22.420105    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:22.717948    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:23.585658    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:23.878949    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:24.174827    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:25.045358    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:25.336336    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:25.628853    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:26.503969    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:26.796369    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:27.093542    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:27.960168    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:28.252620    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:28.549342    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:29.415852    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:29.707028    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:30.002915    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:30.872865    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:31.167934    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:31.464091    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:32.345051    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:32.636232    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:32.901793    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:33.771113    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:34.074258    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:34.373466    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:35.242497    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:35.533753    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:35.827118    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:36.692511    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:36.983525    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:37.280706    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:38.147950    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:38.441557    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:38.737132    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:39.606310    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:39.899012    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:40.193798    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:41.061297    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:41.354834    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:41.653437    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:42.518040    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:42.809083    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:16:43.105061    6300 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  Error: UPGRADE FAILED: Get "https://130.35.130.41:6443/api/v1/namespaces/mercury-psr/services/mercury-consumer-command-service": context deadline exceeded
[21:January:2022:10:31:46]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ helmfile --help
NAME:
   helmfile

USAGE:
   helmfile [global options] command [command options] [arguments...]

VERSION:
   v0.138.7

COMMANDS:
   deps          update charts based on their requirements
   repos         sync repositories from state file (helm repo add && helm repo update)
   charts        DEPRECATED: sync releases from state file (helm upgrade --install)
   diff          diff releases from state file against env (helm diff)
   template      template releases from state file against env (helm template)
   write-values  write values files for releases. Similar to `helmfile template`, write values files instead of manifests.
   lint          lint charts from state file (helm lint)
   sync          sync all resources from state file (repos, releases and chart deps)
   apply         apply all resources from state file only when there are changes
   status        retrieve status of releases in state file
   delete        DEPRECATED: delete releases from state file (helm delete)
   destroy       deletes and then purges releases
   test          test releases from state file (helm test)
   build         output compiled helmfile state(s) as YAML
   list          list releases defined in state file
   version       Show the version for Helmfile.
   help, h       Shows a list of commands or help for one command

GLOBAL OPTIONS:
   --helm-binary value, -b value           path to helm binary (default: "helm")
   --file helmfile.yaml, -f helmfile.yaml  load config from file or directory. defaults to helmfile.yaml or `helmfile.d`(means `helmfile.d/*.yaml`) in this preference
   --environment value, -e value           specify the environment name. defaults to "default"
   --state-values-set value                set state values on the command line (can specify multiple or separate values with commas: key1=val1,key2=val2)
   --state-values-file value               specify state values in a YAML file
   --quiet, -q                             Silence output. Equivalent to log-level warn
   --kube-context value                    Set kubectl context. Uses current context by default
   --debug                                 Enable verbose output for Helm and set log-level to debug, this disables --quiet/-q effect
   --no-color                              Output without color
   --log-level value                       Set log level, default info
   --namespace value, -n value             Set namespace. Uses the namespace set in the context by default, and is available in templates as {{ .Namespace }}
   --selector value, -l value              Only run using the releases that match labels. Labels can take the form of foo=bar or foo!=bar.
                                           A release must match all labels in a group in order to be used. Multiple groups can be specified at once.
                                           --selector tier=frontend,tier!=proxy --selector tier=backend. Will match all frontend, non-proxy releases AND all backend releases.
                                           The name of a release can be used as a label. --selector name=myrelease
   --allow-no-matching-release             Do not exit with an error code if the provided selector has no matching releases.
   --interactive, -i                       Request confirmation before attempting to modify clusters
   --help, -h                              show help
   --version, -v                           print the version
[21:January:2022:10:32:24]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ helmfile -e prod -f ./mercury-psr/helmfile-mercury-psr-apps.yaml -l region=us-phoenix-1 template  > ~/galorndon/ctemp/PHX-helmfile-mercury-psr-apps-helmfile.template
Templating release=mercury, chart=osvc-helm-virtual/mercury
[21:January:2022:10:33:45]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ #helmfile -e prod -f ./mercury-psr/helmfile-mercury-psr-apps.yaml --debug -l region=us-phoenix-1 sync --concurrency 1
[21:January:2022:10:40:31]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ ########
[21:January:2022:10:40:34]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ ######
[21:January:2022:10:40:37]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ #######
[21:January:2022:10:40:38]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ helmfile -e prod -f ./mercury-psr/helmfile-mercury-psr-apps.yaml --debug -l region=us-phoenix-1 sync --concurrency 1
processing file "helmfile-mercury-psr-apps.yaml" in directory "mercury-psr"
changing working directory to "/home/opc/galorndon/osvc-platform/helm/helmfile-releases/mercury-psr"
first-pass rendering starting for "helmfile-mercury-psr-apps.yaml.part.0": inherited=&{prod map[] map[]}, overrode=<nil>
first-pass uses: &{prod map[] map[]}
first-pass rendering output of "helmfile-mercury-psr-apps.yaml.part.0":
 0: # Namespace: mercury-psr
 1: bases:
 2:   - ../environments.yaml

error in first-pass rendering: result of "helmfile-mercury-psr-apps.yaml.part.0":
 0: # Namespace: mercury-psr
 1: bases:
 2:   - ../environments.yaml

first-pass produced: &{prod map[] map[]}
first-pass rendering result of "helmfile-mercury-psr-apps.yaml.part.0": {prod map[] map[]}
second-pass rendering result of "helmfile-mercury-psr-apps.yaml.part.0":
 0: # Namespace: mercury-psr
 1: bases:
 2:   - ../environments.yaml

first-pass rendering starting for "../environments.yaml.part.0": inherited=&{prod map[] map[]}, overrode=<nil>
first-pass uses: &{prod map[] map[]}
first-pass rendering output of "../environments.yaml.part.0":
 0: environments:
 1:   prod:
 2:     missingFileHandler: Warn
 3:     values:
 4:       - ../vars/helmfile/realms/prod.yaml
 5:   demo:
 6:     missingFileHandler: Warn
 7:     values:
 8:       - ../vars/helmfile/realms/demo.yaml
 9:   preprod:
10:     missingFileHandler: Warn
11:     values:
12:       - ../vars/helmfile/realms/preprod.yaml
13:   dev:
14:     missingFileHandler: Warn
15:     values:
16:       - ../vars/helmfile/realms/dev.yaml
17:   corp:
18:     missingFileHandler: Warn
19:     values:
20:       - ../vars/helmfile/realms/corp.yaml
21:   gcloud:
22:     missingFileHandler: Warn
23:     values:
24:       - ../vars/helmfile/realms/gcloud.yaml
25:
26: helmDefaults:
27:   wait: true
28:   timeout: 900
29:   historyMax: 4
30:   createNamespace: false
31:

envvals_loader: loaded ../vars/helmfile/realms/prod.yaml:map[consul_dns_cluster_ip:10.97.10.10 dns_zone_name:ocs.oraclecloud.com oci_home_region:us-ashburn-1 oke_cluster_types:[dataplane] proxy:map[] realm:prod realmRegions:[map[abbreviation:us2 key:iad legacy_dns_servers:[10.134.0.21 10.134.0.63] name:us-ashburn-1 vault_sync:true] map[abbreviation:us1 deploy_region:true key:phx name:us-phoenix-1] map[abbreviation:eu2 key:ams name:eu-amsterdam-1] map[abbreviation:eu1 key:fra name:eu-frankfurt-1] map[abbreviation:uk2 key:cwl name:uk-cardiff-1] map[abbreviation:uk1 key:lhr name:uk-london-1] map[abbreviation:ap5 key:mel name:ap-melbourne-1] map[abbreviation:ap3 key:syd name:ap-sydney-1] map[abbreviation:ca1 key:yyz name:ca-toronto-1] map[abbreviation:ca2 key:yul name:ca-montreal-1] map[abbreviation:ap6 key:hyd name:ap-hyderabad-1] map[abbreviation:ap2 key:bom name:ap-mumbai-1] map[abbreviation:ap4 key:kix name:ap-osaka-1] map[abbreviation:ap1 key:nrt name:ap-tokyo-1] map[abbreviation:sa1 key:gru name:sa-saopaulo-1] map[abbreviation:sa3 key:vcp name:sa-vinhedo-1] map[abbreviation:me2 key:jed name:me-jeddah-1] map[abbreviation:me1 key:dxb name:me-dubai-1] map[abbreviation:eu3 key:zrh name:eu-zurich-1] map[abbreviation:sa2 key:scl name:sa-santiago-1]] schemaRegistryMasterRegion:us-ashburn-1 tenancy_name:osvcprod tenancy_ocid:ocid1.tenancy.oc1..aaaaaaaahqmm3ceg4kw7aqygde6vc66jlnypuxpgd6k623tzv2u6e43i7vkq]
first-pass produced: &{prod map[consul_dns_cluster_ip:10.97.10.10 dns_zone_name:ocs.oraclecloud.com oci_home_region:us-ashburn-1 oke_cluster_types:[dataplane] proxy:map[] realm:prod realmRegions:[map[abbreviation:us2 key:iad legacy_dns_servers:[10.134.0.21 10.134.0.63] name:us-ashburn-1 vault_sync:true] map[abbreviation:us1 deploy_region:true key:phx name:us-phoenix-1] map[abbreviation:eu2 key:ams name:eu-amsterdam-1] map[abbreviation:eu1 key:fra name:eu-frankfurt-1] map[abbreviation:uk2 key:cwl name:uk-cardiff-1] map[abbreviation:uk1 key:lhr name:uk-london-1] map[abbreviation:ap5 key:mel name:ap-melbourne-1] map[abbreviation:ap3 key:syd name:ap-sydney-1] map[abbreviation:ca1 key:yyz name:ca-toronto-1] map[abbreviation:ca2 key:yul name:ca-montreal-1] map[abbreviation:ap6 key:hyd name:ap-hyderabad-1] map[abbreviation:ap2 key:bom name:ap-mumbai-1] map[abbreviation:ap4 key:kix name:ap-osaka-1] map[abbreviation:ap1 key:nrt name:ap-tokyo-1] map[abbreviation:sa1 key:gru name:sa-saopaulo-1] map[abbreviation:sa3 key:vcp name:sa-vinhedo-1] map[abbreviation:me2 key:jed name:me-jeddah-1] map[abbreviation:me1 key:dxb name:me-dubai-1] map[abbreviation:eu3 key:zrh name:eu-zurich-1] map[abbreviation:sa2 key:scl name:sa-santiago-1]] schemaRegistryMasterRegion:us-ashburn-1 tenancy_name:osvcprod tenancy_ocid:ocid1.tenancy.oc1..aaaaaaaahqmm3ceg4kw7aqygde6vc66jlnypuxpgd6k623tzv2u6e43i7vkq] map[]}
first-pass rendering result of "../environments.yaml.part.0": {prod map[consul_dns_cluster_ip:10.97.10.10 dns_zone_name:ocs.oraclecloud.com oci_home_region:us-ashburn-1 oke_cluster_types:[dataplane] proxy:map[] realm:prod realmRegions:[map[abbreviation:us2 key:iad legacy_dns_servers:[10.134.0.21 10.134.0.63] name:us-ashburn-1 vault_sync:true] map[abbreviation:us1 deploy_region:true key:phx name:us-phoenix-1] map[abbreviation:eu2 key:ams name:eu-amsterdam-1] map[abbreviation:eu1 key:fra name:eu-frankfurt-1] map[abbreviation:uk2 key:cwl name:uk-cardiff-1] map[abbreviation:uk1 key:lhr name:uk-london-1] map[abbreviation:ap5 key:mel name:ap-melbourne-1] map[abbreviation:ap3 key:syd name:ap-sydney-1] map[abbreviation:ca1 key:yyz name:ca-toronto-1] map[abbreviation:ca2 key:yul name:ca-montreal-1] map[abbreviation:ap6 key:hyd name:ap-hyderabad-1] map[abbreviation:ap2 key:bom name:ap-mumbai-1] map[abbreviation:ap4 key:kix name:ap-osaka-1] map[abbreviation:ap1 key:nrt name:ap-tokyo-1] map[abbreviation:sa1 key:gru name:sa-saopaulo-1] map[abbreviation:sa3 key:vcp name:sa-vinhedo-1] map[abbreviation:me2 key:jed name:me-jeddah-1] map[abbreviation:me1 key:dxb name:me-dubai-1] map[abbreviation:eu3 key:zrh name:eu-zurich-1] map[abbreviation:sa2 key:scl name:sa-santiago-1]] schemaRegistryMasterRegion:us-ashburn-1 tenancy_name:osvcprod tenancy_ocid:ocid1.tenancy.oc1..aaaaaaaahqmm3ceg4kw7aqygde6vc66jlnypuxpgd6k623tzv2u6e43i7vkq] map[]}
vals:
map[consul_dns_cluster_ip:10.97.10.10 dns_zone_name:ocs.oraclecloud.com oci_home_region:us-ashburn-1 oke_cluster_types:[dataplane] proxy:map[] realm:prod realmRegions:[map[abbreviation:us2 key:iad legacy_dns_servers:[10.134.0.21 10.134.0.63] name:us-ashburn-1 vault_sync:true] map[abbreviation:us1 deploy_region:true key:phx name:us-phoenix-1] map[abbreviation:eu2 key:ams name:eu-amsterdam-1] map[abbreviation:eu1 key:fra name:eu-frankfurt-1] map[abbreviation:uk2 key:cwl name:uk-cardiff-1] map[abbreviation:uk1 key:lhr name:uk-london-1] map[abbreviation:ap5 key:mel name:ap-melbourne-1] map[abbreviation:ap3 key:syd name:ap-sydney-1] map[abbreviation:ca1 key:yyz name:ca-toronto-1] map[abbreviation:ca2 key:yul name:ca-montreal-1] map[abbreviation:ap6 key:hyd name:ap-hyderabad-1] map[abbreviation:ap2 key:bom name:ap-mumbai-1] map[abbreviation:ap4 key:kix name:ap-osaka-1] map[abbreviation:ap1 key:nrt name:ap-tokyo-1] map[abbreviation:sa1 key:gru name:sa-saopaulo-1] map[abbreviation:sa3 key:vcp name:sa-vinhedo-1] map[abbreviation:me2 key:jed name:me-jeddah-1] map[abbreviation:me1 key:dxb name:me-dubai-1] map[abbreviation:eu3 key:zrh name:eu-zurich-1] map[abbreviation:sa2 key:scl name:sa-santiago-1]] schemaRegistryMasterRegion:us-ashburn-1 tenancy_name:osvcprod tenancy_ocid:ocid1.tenancy.oc1..aaaaaaaahqmm3ceg4kw7aqygde6vc66jlnypuxpgd6k623tzv2u6e43i7vkq]
defaultVals:[]
second-pass rendering result of "../environments.yaml.part.0":
 0: environments:
 1:   prod:
 2:     missingFileHandler: Warn
 3:     values:
 4:       - ../vars/helmfile/realms/prod.yaml
 5:   demo:
 6:     missingFileHandler: Warn
 7:     values:
 8:       - ../vars/helmfile/realms/demo.yaml
 9:   preprod:
10:     missingFileHandler: Warn
11:     values:
12:       - ../vars/helmfile/realms/preprod.yaml
13:   dev:
14:     missingFileHandler: Warn
15:     values:
16:       - ../vars/helmfile/realms/dev.yaml
17:   corp:
18:     missingFileHandler: Warn
19:     values:
20:       - ../vars/helmfile/realms/corp.yaml
21:   gcloud:
22:     missingFileHandler: Warn
23:     values:
24:       - ../vars/helmfile/realms/gcloud.yaml
25:
26: helmDefaults:
27:   wait: true
28:   timeout: 900
29:   historyMax: 4
30:   createNamespace: false
31:

envvals_loader: loaded ../vars/helmfile/realms/prod.yaml:map[consul_dns_cluster_ip:10.97.10.10 dns_zone_name:ocs.oraclecloud.com oci_home_region:us-ashburn-1 oke_cluster_types:[dataplane] proxy:map[] realm:prod realmRegions:[map[abbreviation:us2 key:iad legacy_dns_servers:[10.134.0.21 10.134.0.63] name:us-ashburn-1 vault_sync:true] map[abbreviation:us1 deploy_region:true key:phx name:us-phoenix-1] map[abbreviation:eu2 key:ams name:eu-amsterdam-1] map[abbreviation:eu1 key:fra name:eu-frankfurt-1] map[abbreviation:uk2 key:cwl name:uk-cardiff-1] map[abbreviation:uk1 key:lhr name:uk-london-1] map[abbreviation:ap5 key:mel name:ap-melbourne-1] map[abbreviation:ap3 key:syd name:ap-sydney-1] map[abbreviation:ca1 key:yyz name:ca-toronto-1] map[abbreviation:ca2 key:yul name:ca-montreal-1] map[abbreviation:ap6 key:hyd name:ap-hyderabad-1] map[abbreviation:ap2 key:bom name:ap-mumbai-1] map[abbreviation:ap4 key:kix name:ap-osaka-1] map[abbreviation:ap1 key:nrt name:ap-tokyo-1] map[abbreviation:sa1 key:gru name:sa-saopaulo-1] map[abbreviation:sa3 key:vcp name:sa-vinhedo-1] map[abbreviation:me2 key:jed name:me-jeddah-1] map[abbreviation:me1 key:dxb name:me-dubai-1] map[abbreviation:eu3 key:zrh name:eu-zurich-1] map[abbreviation:sa2 key:scl name:sa-santiago-1]] schemaRegistryMasterRegion:us-ashburn-1 tenancy_name:osvcprod tenancy_ocid:ocid1.tenancy.oc1..aaaaaaaahqmm3ceg4kw7aqygde6vc66jlnypuxpgd6k623tzv2u6e43i7vkq]
merged environment: &{prod map[consul_dns_cluster_ip:10.97.10.10 dns_zone_name:ocs.oraclecloud.com oci_home_region:us-ashburn-1 oke_cluster_types:[dataplane] proxy:map[] realm:prod realmRegions:[map[abbreviation:us2 key:iad legacy_dns_servers:[10.134.0.21 10.134.0.63] name:us-ashburn-1 vault_sync:true] map[abbreviation:us1 deploy_region:true key:phx name:us-phoenix-1] map[abbreviation:eu2 key:ams name:eu-amsterdam-1] map[abbreviation:eu1 key:fra name:eu-frankfurt-1] map[abbreviation:uk2 key:cwl name:uk-cardiff-1] map[abbreviation:uk1 key:lhr name:uk-london-1] map[abbreviation:ap5 key:mel name:ap-melbourne-1] map[abbreviation:ap3 key:syd name:ap-sydney-1] map[abbreviation:ca1 key:yyz name:ca-toronto-1] map[abbreviation:ca2 key:yul name:ca-montreal-1] map[abbreviation:ap6 key:hyd name:ap-hyderabad-1] map[abbreviation:ap2 key:bom name:ap-mumbai-1] map[abbreviation:ap4 key:kix name:ap-osaka-1] map[abbreviation:ap1 key:nrt name:ap-tokyo-1] map[abbreviation:sa1 key:gru name:sa-saopaulo-1] map[abbreviation:sa3 key:vcp name:sa-vinhedo-1] map[abbreviation:me2 key:jed name:me-jeddah-1] map[abbreviation:me1 key:dxb name:me-dubai-1] map[abbreviation:eu3 key:zrh name:eu-zurich-1] map[abbreviation:sa2 key:scl name:sa-santiago-1]] schemaRegistryMasterRegion:us-ashburn-1 tenancy_name:osvcprod tenancy_ocid:ocid1.tenancy.oc1..aaaaaaaahqmm3ceg4kw7aqygde6vc66jlnypuxpgd6k623tzv2u6e43i7vkq] map[]}
envvals_loader: loaded ../vars/helmfile/realms/prod.yaml:map[consul_dns_cluster_ip:10.97.10.10 dns_zone_name:ocs.oraclecloud.com oci_home_region:us-ashburn-1 oke_cluster_types:[dataplane] proxy:map[] realm:prod realmRegions:[map[abbreviation:us2 key:iad legacy_dns_servers:[10.134.0.21 10.134.0.63] name:us-ashburn-1 vault_sync:true] map[abbreviation:us1 deploy_region:true key:phx name:us-phoenix-1] map[abbreviation:eu2 key:ams name:eu-amsterdam-1] map[abbreviation:eu1 key:fra name:eu-frankfurt-1] map[abbreviation:uk2 key:cwl name:uk-cardiff-1] map[abbreviation:uk1 key:lhr name:uk-london-1] map[abbreviation:ap5 key:mel name:ap-melbourne-1] map[abbreviation:ap3 key:syd name:ap-sydney-1] map[abbreviation:ca1 key:yyz name:ca-toronto-1] map[abbreviation:ca2 key:yul name:ca-montreal-1] map[abbreviation:ap6 key:hyd name:ap-hyderabad-1] map[abbreviation:ap2 key:bom name:ap-mumbai-1] map[abbreviation:ap4 key:kix name:ap-osaka-1] map[abbreviation:ap1 key:nrt name:ap-tokyo-1] map[abbreviation:sa1 key:gru name:sa-saopaulo-1] map[abbreviation:sa3 key:vcp name:sa-vinhedo-1] map[abbreviation:me2 key:jed name:me-jeddah-1] map[abbreviation:me1 key:dxb name:me-dubai-1] map[abbreviation:eu3 key:zrh name:eu-zurich-1] map[abbreviation:sa2 key:scl name:sa-santiago-1]] schemaRegistryMasterRegion:us-ashburn-1 tenancy_name:osvcprod tenancy_ocid:ocid1.tenancy.oc1..aaaaaaaahqmm3ceg4kw7aqygde6vc66jlnypuxpgd6k623tzv2u6e43i7vkq]
merged environment: &{prod map[consul_dns_cluster_ip:10.97.10.10 dns_zone_name:ocs.oraclecloud.com oci_home_region:us-ashburn-1 oke_cluster_types:[dataplane] proxy:map[] realm:prod realmRegions:[map[abbreviation:us2 key:iad legacy_dns_servers:[10.134.0.21 10.134.0.63] name:us-ashburn-1 vault_sync:true] map[abbreviation:us1 deploy_region:true key:phx name:us-phoenix-1] map[abbreviation:eu2 key:ams name:eu-amsterdam-1] map[abbreviation:eu1 key:fra name:eu-frankfurt-1] map[abbreviation:uk2 key:cwl name:uk-cardiff-1] map[abbreviation:uk1 key:lhr name:uk-london-1] map[abbreviation:ap5 key:mel name:ap-melbourne-1] map[abbreviation:ap3 key:syd name:ap-sydney-1] map[abbreviation:ca1 key:yyz name:ca-toronto-1] map[abbreviation:ca2 key:yul name:ca-montreal-1] map[abbreviation:ap6 key:hyd name:ap-hyderabad-1] map[abbreviation:ap2 key:bom name:ap-mumbai-1] map[abbreviation:ap4 key:kix name:ap-osaka-1] map[abbreviation:ap1 key:nrt name:ap-tokyo-1] map[abbreviation:sa1 key:gru name:sa-saopaulo-1] map[abbreviation:sa3 key:vcp name:sa-vinhedo-1] map[abbreviation:me2 key:jed name:me-jeddah-1] map[abbreviation:me1 key:dxb name:me-dubai-1] map[abbreviation:eu3 key:zrh name:eu-zurich-1] map[abbreviation:sa2 key:scl name:sa-santiago-1]] schemaRegistryMasterRegion:us-ashburn-1 tenancy_name:osvcprod tenancy_ocid:ocid1.tenancy.oc1..aaaaaaaahqmm3ceg4kw7aqygde6vc66jlnypuxpgd6k623tzv2u6e43i7vkq] map[]}
first-pass rendering starting for "helmfile-mercury-psr-apps.yaml.part.1": inherited=&{prod map[consul_dns_cluster_ip:10.97.10.10 dns_zone_name:ocs.oraclecloud.com oci_home_region:us-ashburn-1 oke_cluster_types:[dataplane] proxy:map[] realm:prod realmRegions:[map[abbreviation:us2 key:iad legacy_dns_servers:[10.134.0.21 10.134.0.63] name:us-ashburn-1 vault_sync:true] map[abbreviation:us1 deploy_region:true key:phx name:us-phoenix-1] map[abbreviation:eu2 key:ams name:eu-amsterdam-1] map[abbreviation:eu1 key:fra name:eu-frankfurt-1] map[abbreviation:uk2 key:cwl name:uk-cardiff-1] map[abbreviation:uk1 key:lhr name:uk-london-1] map[abbreviation:ap5 key:mel name:ap-melbourne-1] map[abbreviation:ap3 key:syd name:ap-sydney-1] map[abbreviation:ca1 key:yyz name:ca-toronto-1] map[abbreviation:ca2 key:yul name:ca-montreal-1] map[abbreviation:ap6 key:hyd name:ap-hyderabad-1] map[abbreviation:ap2 key:bom name:ap-mumbai-1] map[abbreviation:ap4 key:kix name:ap-osaka-1] map[abbreviation:ap1 key:nrt name:ap-tokyo-1] map[abbreviation:sa1 key:gru name:sa-saopaulo-1] map[abbreviation:sa3 key:vcp name:sa-vinhedo-1] map[abbreviation:me2 key:jed name:me-jeddah-1] map[abbreviation:me1 key:dxb name:me-dubai-1] map[abbreviation:eu3 key:zrh name:eu-zurich-1] map[abbreviation:sa2 key:scl name:sa-santiago-1]] schemaRegistryMasterRegion:us-ashburn-1 tenancy_name:osvcprod tenancy_ocid:ocid1.tenancy.oc1..aaaaaaaahqmm3ceg4kw7aqygde6vc66jlnypuxpgd6k623tzv2u6e43i7vkq] map[]}, overrode=<nil>
first-pass uses: &{prod map[consul_dns_cluster_ip:10.97.10.10 dns_zone_name:ocs.oraclecloud.com oci_home_region:us-ashburn-1 oke_cluster_types:[dataplane] proxy:map[] realm:prod realmRegions:[map[abbreviation:us2 key:iad legacy_dns_servers:[10.134.0.21 10.134.0.63] name:us-ashburn-1 vault_sync:true] map[abbreviation:us1 deploy_region:true key:phx name:us-phoenix-1] map[abbreviation:eu2 key:ams name:eu-amsterdam-1] map[abbreviation:eu1 key:fra name:eu-frankfurt-1] map[abbreviation:uk2 key:cwl name:uk-cardiff-1] map[abbreviation:uk1 key:lhr name:uk-london-1] map[abbreviation:ap5 key:mel name:ap-melbourne-1] map[abbreviation:ap3 key:syd name:ap-sydney-1] map[abbreviation:ca1 key:yyz name:ca-toronto-1] map[abbreviation:ca2 key:yul name:ca-montreal-1] map[abbreviation:ap6 key:hyd name:ap-hyderabad-1] map[abbreviation:ap2 key:bom name:ap-mumbai-1] map[abbreviation:ap4 key:kix name:ap-osaka-1] map[abbreviation:ap1 key:nrt name:ap-tokyo-1] map[abbreviation:sa1 key:gru name:sa-saopaulo-1] map[abbreviation:sa3 key:vcp name:sa-vinhedo-1] map[abbreviation:me2 key:jed name:me-jeddah-1] map[abbreviation:me1 key:dxb name:me-dubai-1] map[abbreviation:eu3 key:zrh name:eu-zurich-1] map[abbreviation:sa2 key:scl name:sa-santiago-1]] schemaRegistryMasterRegion:us-ashburn-1 tenancy_name:osvcprod tenancy_ocid:ocid1.tenancy.oc1..aaaaaaaahqmm3ceg4kw7aqygde6vc66jlnypuxpgd6k623tzv2u6e43i7vkq] map[]}
first-pass rendering input of "helmfile-mercury-psr-apps.yaml.part.1":
 0: {{ $realm := .Values.realm -}}
 1: # Namespace needs to be dynamic.
 2: {{ $namespace := "mercury-psr" -}}
 3: {{- $dnsZoneName := .Values.dns_zone_name -}}
 4: {{- $mercurySpec := readFile (printf "../../vars/charts/mercury/%s/chart.yaml" $realm ) | fromYaml }}
 5: {{- $deploymentValues := readFile (printf "../../vars/charts/mercury/%s/deployment.yaml" $realm ) | fromYaml }}
 6: {{ $proxyHost := "" -}}
 7: {{ $proxyPort := "" -}}
 8: {{ $noProxy := "" -}}
 9: {{ $ingressName := "" }}
10: {{ $tlsCertSecretName := "" }}
11:
12: {{ if eq .Environment.Name "corp" -}}
13: {{ $proxyParts := split "://" .Values.proxy.http_proxy -}}
14: {{ $proxyUrl := $proxyParts._1 -}}
15: {{ $proxyUrlParts := split ":" $proxyUrl -}}
16: {{ $proxyHost = $proxyUrlParts._0 -}}
17: {{ $proxyPort = $proxyUrlParts._1 -}}
18: {{ $idcsDomains := "|*.identity.c9dev2.oc9qadev.com" }}
19: {{ $noProxy = regexReplaceAll "\\.0\\.\\*" ( printf "mercury-*|%s%s" (regexReplaceAll "\\d+/\\d+" .Values.proxy.no_proxy "*" | replace ",." "|*." | replace "," "|" | replace ".0." ".*." ) $idcsDomains ) ".*" | replace ".*.*" ".*" -}}
20: {{ end -}}
21:
22: {{ if hasKey $deploymentValues "tlsCertSecretName" }}
23: {{ $tlsCertSecretName = $deploymentValues.tlsCertSecretName }}
24: {{ else }}
25: {{ $tlsCertSecretName = printf "star.channels.%s" $dnsZoneName }}
26: {{ end -}}
27:
28: templates:
29:   mercury: &mercury
30:     chart: osvc-helm-virtual/mercury
31:     namespace: "{{ $namespace }}"
32:     {{- if eq $realm "prod" }}
33:     version: 2201.18.1305
34:     {{- else }}
35:     version: {{ $mercurySpec.chart.version }}
36:     {{- end }}
37:     labels:
38:       service: "{{ $namespace }}"
39:
40: releases:
41:   {{- $realmRegions := .Values.realmRegions -}}
42:   {{- $okeClusters := .Values.oke_cluster_types -}}
43:
44:   {{- range .Values.realmRegions }}
45:   # If region is marked as deploy_region, append deployment OKE type
46:   {{- $region := .name }}
47:   {{- $region_key := .key }}
48:   # ingress region will be us-phoenix-1 for preprod and the region everywhere else
49:   {{- $ingressRegion := $region_key }}
50:   {{- if eq $realm "preprod" }}
51:   {{- $ingressRegion := $region }}
52:   {{- end }}
53:
54:   {{- if eq $realm "prod" }}
55:   # for the mapping of region to ingress, see
56:   # https://confluence.oraclecorp.com/confluence/display/CPE/Ingress%2C+DNS%2C+and+Certificates
57:   {{- if eq $region_key "phx" }}
58:   {{- $ingressName = (printf "engagement-psr.us1.channels.%s" $dnsZoneName) }}
59:   {{- else if eq $region_key "iad" }}
60:   {{- $ingressName = (printf "engagement-psr.us2.channels.%s" $dnsZoneName) }}
61:   {{- end }}
62:   {{- if eq $namespace "mercury-psr" }}
63:   {{- $tlsCertSecretName = "engagement-psr-tls-cert" }}
64:   {{- end }}
65:   {{- else }}
66:   # This is the standard format for non-production environments
67:   {{- $ingressName = (printf "engagement-%s-%s.%s.channels.%s" $namespace $ingressRegion $realm $dnsZoneName) }}
68:   {{- end }}
69:
70:   {{- range $okeClusters }}
71:   {{- $okeCluster := . }}
72:   {{- $kubeContext := (printf "%s_%s_%s" $realm $region $okeCluster) }}
73:   {{- $vaultPath := (printf "k8s-%s-%s" $region_key $okeCluster) }}
74:
75: - name: mercury
76:   createNamespace: false
77:   <<: *mercury
78:   kubeContext: "{{ $kubeContext }}"
79:   labels:
80:     region: {{ $region }}
81:     app: {{ $namespace }}
82:   missingFileHandler: Info
83:   values:
84:     - "../../vars/charts/mercury/deployment.yaml"
85:     - "../../vars/charts/mercury/{{$realm}}/deployment.yaml"
86:     - "../../vars/charts/mercury/{{$realm}}/{{$namespace}}/deployment.yaml"
87:     - "../../vars/charts/mercury/{{$realm}}/{{$namespace}}/{{$region}}_deployment.yaml"
88:     - mercuryCommon:
89:         tenantNameFilter: true
90:     - global:
91:         mercury:
92:           enabled: true
93:           cpev2: true
94:         serviceAccount: ## Required by kweet - Service account names and whether to create them
95:           create: false
96:           name: mercurygeneric
97:         {{- if ne $proxyHost "" }}
98:         javaOptsProxy: -Dhttp.proxyHost={{ $proxyHost }} -Dhttp.proxyPort={{ $proxyPort }} -Dhttps.proxyHost={{ $proxyHost }} -Dhttps.proxyPort={{ $proxyPort }} -Dhttp.nonProxyHosts={{ printf "\\\"%s\\\"" $noProxy }}
99:         {{- end }}
100:         vault:
101:           endPoint: "https://vault.query.{{ $realm }}.consul:8200"
102:           path: "{{ $vaultPath }}"
103:         consul:
104:           region:
105:             key: {{ $region_key }}
106:         webhook:
107:           host: engagement-{{ $namespace }}.{{ $realm }}.channels.{{ $dnsZoneName }}
108:         service:
109:           port: 8080
110:           targetPort: 8080
111:         {{- if or (eq $realm "prod" ) (eq $realm "preprod") }}
112:         image:
113:           repository: iad.ocir.io/osvcstage
114:           initImage: init-container/prod:1.0.0.18
115:           secret: osvcstage-ocirsecret
116:         {{- end }}
117:         ingress:
118:           class: {{ $mercurySpec.chart.ingressType }}
119:           annotations:
120:             ingress.kubernetes.io/ssl-redirect: "true"
121:           hosts:
122:             - {{ $ingressName }}
123:           tls:
124:             - secretName: {{ $tlsCertSecretName }}
125:               hosts:
126:                 - {{ $ingressName }}
127:         kafka:
128:           bootstrapServers: "{{ $namespace }}-kafka-bootstrap-{{ $namespace }}.service.{{ $region_key }}-dataplane.{{ $realm }}.consul:443"
129:           tmsBootstrapServers: "shared-kafka-bootstrap-kafka.service.{{ $region_key }}-dataplane.{{ $realm }}.consul:443"
130:         schemaRegistry:
131:           url: "https://schemaregistry-proxy-kafka.service.{{ $region_key }}-dataplane.{{ $realm }}.consul"
132:         podAnnotations:
133:           vault.security.banzaicloud.io/log-level: "warn"
134:           vault.security.banzaicloud.io/vault-addr: "https://vault.query.{{ $realm }}.consul:8200"
135:           vault.security.banzaicloud.io/vault-role: "{{ $namespace }}"
136:           vault.security.banzaicloud.io/vault-path: "{{ $vaultPath }}"
137:           vault.security.banzaicloud.io/vault-skip-verify: "true"
138:           vault.security.banzaicloud.io/vault-env-daemon: "true"
139:           vault.security.banzaicloud.io/vault-agent-configmap: "{{ $namespace }}-va-configmap"
140:         keystore:
141:           tokenPath: /vault/.vault-token
142:           location: /vault/secrets/
143:         extraEnvVars:
144:           - name: AUTO_REFRESH_CONSUL_TOKEN
145:             value: "vault:cpe_consul/creds/{{ $namespace }}#token"
146:             # These commands would have to be executed before anything else that tries to use these files
147:             # in an init container.  If not using an init container, none of this is necessary.
148:             # $ echo $KAFKA_TEMPLATED_PKI | jq .certificate -r > /tmp/certificate.crt
149:             # $ echo $KAFKA_TEMPLATED_PKI | jq .issuing_ca -r > /tmp/issuing_ca.crt
150:             # $ echo $KAFKA_TEMPLATED_PKI | jq .private_key -r > /tmp/private.key
151:           - name: KAFKA_TEMPLATED_PKI
152:             value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate" .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw", "ttl": "720h"}'
153:           {{- if $.Values.proxy }}
154:           - name: HTTP_PROXY
155:             value: "{{ $.Values.proxy.http_proxy }}"
156:           - name: HTTPS_PROXY
157:             value: "{{ $.Values.proxy.https_proxy }}"
158:           - name: NO_PROXY
159:             value: "{{ $.Values.proxy.no_proxy }}"
160:           {{- end }}
161:   {{- end }}
162: {{- end }}
163:

template syntax error: template: stringTemplate:33:11: executing "stringTemplate" at <eq $realm "prod">: error calling eq: incompatible types for comparison
first-pass rendering output of "helmfile-mercury-psr-apps.yaml.part.1":
 0: # Namespace needs to be dynamic.
 1:
 2:
 3:
 4:
 5:
 6:
 7: templates:
 8:   mercury: &mercury
 9:     chart: osvc-helm-virtual/mercury
10:     namespace: "mercury-psr"

first-pass produced: &{prod map[consul_dns_cluster_ip:10.97.10.10 dns_zone_name:ocs.oraclecloud.com oci_home_region:us-ashburn-1 oke_cluster_types:[dataplane] proxy:map[] realm:prod realmRegions:[map[abbreviation:us2 key:iad legacy_dns_servers:[10.134.0.21 10.134.0.63] name:us-ashburn-1 vault_sync:true] map[abbreviation:us1 deploy_region:true key:phx name:us-phoenix-1] map[abbreviation:eu2 key:ams name:eu-amsterdam-1] map[abbreviation:eu1 key:fra name:eu-frankfurt-1] map[abbreviation:uk2 key:cwl name:uk-cardiff-1] map[abbreviation:uk1 key:lhr name:uk-london-1] map[abbreviation:ap5 key:mel name:ap-melbourne-1] map[abbreviation:ap3 key:syd name:ap-sydney-1] map[abbreviation:ca1 key:yyz name:ca-toronto-1] map[abbreviation:ca2 key:yul name:ca-montreal-1] map[abbreviation:ap6 key:hyd name:ap-hyderabad-1] map[abbreviation:ap2 key:bom name:ap-mumbai-1] map[abbreviation:ap4 key:kix name:ap-osaka-1] map[abbreviation:ap1 key:nrt name:ap-tokyo-1] map[abbreviation:sa1 key:gru name:sa-saopaulo-1] map[abbreviation:sa3 key:vcp name:sa-vinhedo-1] map[abbreviation:me2 key:jed name:me-jeddah-1] map[abbreviation:me1 key:dxb name:me-dubai-1] map[abbreviation:eu3 key:zrh name:eu-zurich-1] map[abbreviation:sa2 key:scl name:sa-santiago-1]] schemaRegistryMasterRegion:us-ashburn-1 tenancy_name:osvcprod tenancy_ocid:ocid1.tenancy.oc1..aaaaaaaahqmm3ceg4kw7aqygde6vc66jlnypuxpgd6k623tzv2u6e43i7vkq] map[]}
first-pass rendering result of "helmfile-mercury-psr-apps.yaml.part.1": {prod map[consul_dns_cluster_ip:10.97.10.10 dns_zone_name:ocs.oraclecloud.com oci_home_region:us-ashburn-1 oke_cluster_types:[dataplane] proxy:map[] realm:prod realmRegions:[map[abbreviation:us2 key:iad legacy_dns_servers:[10.134.0.21 10.134.0.63] name:us-ashburn-1 vault_sync:true] map[abbreviation:us1 deploy_region:true key:phx name:us-phoenix-1] map[abbreviation:eu2 key:ams name:eu-amsterdam-1] map[abbreviation:eu1 key:fra name:eu-frankfurt-1] map[abbreviation:uk2 key:cwl name:uk-cardiff-1] map[abbreviation:uk1 key:lhr name:uk-london-1] map[abbreviation:ap5 key:mel name:ap-melbourne-1] map[abbreviation:ap3 key:syd name:ap-sydney-1] map[abbreviation:ca1 key:yyz name:ca-toronto-1] map[abbreviation:ca2 key:yul name:ca-montreal-1] map[abbreviation:ap6 key:hyd name:ap-hyderabad-1] map[abbreviation:ap2 key:bom name:ap-mumbai-1] map[abbreviation:ap4 key:kix name:ap-osaka-1] map[abbreviation:ap1 key:nrt name:ap-tokyo-1] map[abbreviation:sa1 key:gru name:sa-saopaulo-1] map[abbreviation:sa3 key:vcp name:sa-vinhedo-1] map[abbreviation:me2 key:jed name:me-jeddah-1] map[abbreviation:me1 key:dxb name:me-dubai-1] map[abbreviation:eu3 key:zrh name:eu-zurich-1] map[abbreviation:sa2 key:scl name:sa-santiago-1]] schemaRegistryMasterRegion:us-ashburn-1 tenancy_name:osvcprod tenancy_ocid:ocid1.tenancy.oc1..aaaaaaaahqmm3ceg4kw7aqygde6vc66jlnypuxpgd6k623tzv2u6e43i7vkq] map[]}
vals:
map[consul_dns_cluster_ip:10.97.10.10 dns_zone_name:ocs.oraclecloud.com oci_home_region:us-ashburn-1 oke_cluster_types:[dataplane] proxy:map[] realm:prod realmRegions:[map[abbreviation:us2 key:iad legacy_dns_servers:[10.134.0.21 10.134.0.63] name:us-ashburn-1 vault_sync:true] map[abbreviation:us1 deploy_region:true key:phx name:us-phoenix-1] map[abbreviation:eu2 key:ams name:eu-amsterdam-1] map[abbreviation:eu1 key:fra name:eu-frankfurt-1] map[abbreviation:uk2 key:cwl name:uk-cardiff-1] map[abbreviation:uk1 key:lhr name:uk-london-1] map[abbreviation:ap5 key:mel name:ap-melbourne-1] map[abbreviation:ap3 key:syd name:ap-sydney-1] map[abbreviation:ca1 key:yyz name:ca-toronto-1] map[abbreviation:ca2 key:yul name:ca-montreal-1] map[abbreviation:ap6 key:hyd name:ap-hyderabad-1] map[abbreviation:ap2 key:bom name:ap-mumbai-1] map[abbreviation:ap4 key:kix name:ap-osaka-1] map[abbreviation:ap1 key:nrt name:ap-tokyo-1] map[abbreviation:sa1 key:gru name:sa-saopaulo-1] map[abbreviation:sa3 key:vcp name:sa-vinhedo-1] map[abbreviation:me2 key:jed name:me-jeddah-1] map[abbreviation:me1 key:dxb name:me-dubai-1] map[abbreviation:eu3 key:zrh name:eu-zurich-1] map[abbreviation:sa2 key:scl name:sa-santiago-1]] schemaRegistryMasterRegion:us-ashburn-1 tenancy_name:osvcprod tenancy_ocid:ocid1.tenancy.oc1..aaaaaaaahqmm3ceg4kw7aqygde6vc66jlnypuxpgd6k623tzv2u6e43i7vkq]
defaultVals:[]
second-pass rendering result of "helmfile-mercury-psr-apps.yaml.part.1":
 0: # Namespace needs to be dynamic.
 1:
 2:
 3:
 4:
 5:
 6:
 7: templates:
 8:   mercury: &mercury
 9:     chart: osvc-helm-virtual/mercury
10:     namespace: "mercury-psr"
11:     version: 2201.18.1305
12:     labels:
13:       service: "mercury-psr"
14:
15: releases:
16:   # If region is marked as deploy_region, append deployment OKE type
17:   # ingress region will be us-phoenix-1 for preprod and the region everywhere else
18:   # for the mapping of region to ingress, see
19:   # https://confluence.oraclecorp.com/confluence/display/CPE/Ingress%2C+DNS%2C+and+Certificates
20:
21: - name: mercury
22:   createNamespace: false
23:   <<: *mercury
24:   kubeContext: "prod_us-ashburn-1_dataplane"
25:   labels:
26:     region: us-ashburn-1
27:     app: mercury-psr
28:   missingFileHandler: Info
29:   values:
30:     - "../../vars/charts/mercury/deployment.yaml"
31:     - "../../vars/charts/mercury/prod/deployment.yaml"
32:     - "../../vars/charts/mercury/prod/mercury-psr/deployment.yaml"
33:     - "../../vars/charts/mercury/prod/mercury-psr/us-ashburn-1_deployment.yaml"
34:     - mercuryCommon:
35:         tenantNameFilter: true
36:     - global:
37:         mercury:
38:           enabled: true
39:           cpev2: true
40:         serviceAccount: ## Required by kweet - Service account names and whether to create them
41:           create: false
42:           name: mercurygeneric
43:         vault:
44:           endPoint: "https://vault.query.prod.consul:8200"
45:           path: "k8s-iad-dataplane"
46:         consul:
47:           region:
48:             key: iad
49:         webhook:
50:           host: engagement-mercury-psr.prod.channels.ocs.oraclecloud.com
51:         service:
52:           port: 8080
53:           targetPort: 8080
54:         image:
55:           repository: iad.ocir.io/osvcstage
56:           initImage: init-container/prod:1.0.0.18
57:           secret: osvcstage-ocirsecret
58:         ingress:
59:           class: public
60:           annotations:
61:             ingress.kubernetes.io/ssl-redirect: "true"
62:           hosts:
63:             - engagement-psr.us2.channels.ocs.oraclecloud.com
64:           tls:
65:             - secretName: engagement-psr-tls-cert
66:               hosts:
67:                 - engagement-psr.us2.channels.ocs.oraclecloud.com
68:         kafka:
69:           bootstrapServers: "mercury-psr-kafka-bootstrap-mercury-psr.service.iad-dataplane.prod.consul:443"
70:           tmsBootstrapServers: "shared-kafka-bootstrap-kafka.service.iad-dataplane.prod.consul:443"
71:         schemaRegistry:
72:           url: "https://schemaregistry-proxy-kafka.service.iad-dataplane.prod.consul"
73:         podAnnotations:
74:           vault.security.banzaicloud.io/log-level: "warn"
75:           vault.security.banzaicloud.io/vault-addr: "https://vault.query.prod.consul:8200"
76:           vault.security.banzaicloud.io/vault-role: "mercury-psr"
77:           vault.security.banzaicloud.io/vault-path: "k8s-iad-dataplane"
78:           vault.security.banzaicloud.io/vault-skip-verify: "true"
79:           vault.security.banzaicloud.io/vault-env-daemon: "true"
80:           vault.security.banzaicloud.io/vault-agent-configmap: "mercury-psr-va-configmap"
81:         keystore:
82:           tokenPath: /vault/.vault-token
83:           location: /vault/secrets/
84:         extraEnvVars:
85:           - name: AUTO_REFRESH_CONSUL_TOKEN
86:             value: "vault:cpe_consul/creds/mercury-psr#token"
87:             # These commands would have to be executed before anything else that tries to use these files
88:             # in an init container.  If not using an init container, none of this is necessary.
89:             # $ echo $KAFKA_TEMPLATED_PKI | jq .certificate -r > /tmp/certificate.crt
90:             # $ echo $KAFKA_TEMPLATED_PKI | jq .issuing_ca -r > /tmp/issuing_ca.crt
91:             # $ echo $KAFKA_TEMPLATED_PKI | jq .private_key -r > /tmp/private.key
92:           - name: KAFKA_TEMPLATED_PKI
93:             value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate" .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw", "ttl": "720h"}'
94:   # If region is marked as deploy_region, append deployment OKE type
95:   # ingress region will be us-phoenix-1 for preprod and the region everywhere else
96:   # for the mapping of region to ingress, see
97:   # https://confluence.oraclecorp.com/confluence/display/CPE/Ingress%2C+DNS%2C+and+Certificates
98:
99: - name: mercury
100:   createNamespace: false
101:   <<: *mercury
102:   kubeContext: "prod_us-phoenix-1_dataplane"
103:   labels:
104:     region: us-phoenix-1
105:     app: mercury-psr
106:   missingFileHandler: Info
107:   values:
108:     - "../../vars/charts/mercury/deployment.yaml"
109:     - "../../vars/charts/mercury/prod/deployment.yaml"
110:     - "../../vars/charts/mercury/prod/mercury-psr/deployment.yaml"
111:     - "../../vars/charts/mercury/prod/mercury-psr/us-phoenix-1_deployment.yaml"
112:     - mercuryCommon:
113:         tenantNameFilter: true
114:     - global:
115:         mercury:
116:           enabled: true
117:           cpev2: true
118:         serviceAccount: ## Required by kweet - Service account names and whether to create them
119:           create: false
120:           name: mercurygeneric
121:         vault:
122:           endPoint: "https://vault.query.prod.consul:8200"
123:           path: "k8s-phx-dataplane"
124:         consul:
125:           region:
126:             key: phx
127:         webhook:
128:           host: engagement-mercury-psr.prod.channels.ocs.oraclecloud.com
129:         service:
130:           port: 8080
131:           targetPort: 8080
132:         image:
133:           repository: iad.ocir.io/osvcstage
134:           initImage: init-container/prod:1.0.0.18
135:           secret: osvcstage-ocirsecret
136:         ingress:
137:           class: public
138:           annotations:
139:             ingress.kubernetes.io/ssl-redirect: "true"
140:           hosts:
141:             - engagement-psr.us1.channels.ocs.oraclecloud.com
142:           tls:
143:             - secretName: engagement-psr-tls-cert
144:               hosts:
145:                 - engagement-psr.us1.channels.ocs.oraclecloud.com
146:         kafka:
147:           bootstrapServers: "mercury-psr-kafka-bootstrap-mercury-psr.service.phx-dataplane.prod.consul:443"
148:           tmsBootstrapServers: "shared-kafka-bootstrap-kafka.service.phx-dataplane.prod.consul:443"
149:         schemaRegistry:
150:           url: "https://schemaregistry-proxy-kafka.service.phx-dataplane.prod.consul"
151:         podAnnotations:
152:           vault.security.banzaicloud.io/log-level: "warn"
153:           vault.security.banzaicloud.io/vault-addr: "https://vault.query.prod.consul:8200"
154:           vault.security.banzaicloud.io/vault-role: "mercury-psr"
155:           vault.security.banzaicloud.io/vault-path: "k8s-phx-dataplane"
156:           vault.security.banzaicloud.io/vault-skip-verify: "true"
157:           vault.security.banzaicloud.io/vault-env-daemon: "true"
158:           vault.security.banzaicloud.io/vault-agent-configmap: "mercury-psr-va-configmap"
159:         keystore:
160:           tokenPath: /vault/.vault-token
161:           location: /vault/secrets/
162:         extraEnvVars:
163:           - name: AUTO_REFRESH_CONSUL_TOKEN
164:             value: "vault:cpe_consul/creds/mercury-psr#token"
165:             # These commands would have to be executed before anything else that tries to use these files
166:             # in an init container.  If not using an init container, none of this is necessary.
167:             # $ echo $KAFKA_TEMPLATED_PKI | jq .certificate -r > /tmp/certificate.crt
168:             # $ echo $KAFKA_TEMPLATED_PKI | jq .issuing_ca -r > /tmp/issuing_ca.crt
169:             # $ echo $KAFKA_TEMPLATED_PKI | jq .private_key -r > /tmp/private.key
170:           - name: KAFKA_TEMPLATED_PKI
171:             value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate" .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw", "ttl": "720h"}'
172:   # If region is marked as deploy_region, append deployment OKE type
173:   # ingress region will be us-phoenix-1 for preprod and the region everywhere else
174:   # for the mapping of region to ingress, see
175:   # https://confluence.oraclecorp.com/confluence/display/CPE/Ingress%2C+DNS%2C+and+Certificates
176:
177: - name: mercury
178:   createNamespace: false
179:   <<: *mercury
180:   kubeContext: "prod_eu-amsterdam-1_dataplane"
181:   labels:
182:     region: eu-amsterdam-1
183:     app: mercury-psr
184:   missingFileHandler: Info
185:   values:
186:     - "../../vars/charts/mercury/deployment.yaml"
187:     - "../../vars/charts/mercury/prod/deployment.yaml"
188:     - "../../vars/charts/mercury/prod/mercury-psr/deployment.yaml"
189:     - "../../vars/charts/mercury/prod/mercury-psr/eu-amsterdam-1_deployment.yaml"
190:     - mercuryCommon:
191:         tenantNameFilter: true
192:     - global:
193:         mercury:
194:           enabled: true
195:           cpev2: true
196:         serviceAccount: ## Required by kweet - Service account names and whether to create them
197:           create: false
198:           name: mercurygeneric
199:         vault:
200:           endPoint: "https://vault.query.prod.consul:8200"
201:           path: "k8s-ams-dataplane"
202:         consul:
203:           region:
204:             key: ams
205:         webhook:
206:           host: engagement-mercury-psr.prod.channels.ocs.oraclecloud.com
207:         service:
208:           port: 8080
209:           targetPort: 8080
210:         image:
211:           repository: iad.ocir.io/osvcstage
212:           initImage: init-container/prod:1.0.0.18
213:           secret: osvcstage-ocirsecret
214:         ingress:
215:           class: public
216:           annotations:
217:             ingress.kubernetes.io/ssl-redirect: "true"
218:           hosts:
219:             - engagement-psr.us1.channels.ocs.oraclecloud.com
220:           tls:
221:             - secretName: engagement-psr-tls-cert
222:               hosts:
223:                 - engagement-psr.us1.channels.ocs.oraclecloud.com
224:         kafka:
225:           bootstrapServers: "mercury-psr-kafka-bootstrap-mercury-psr.service.ams-dataplane.prod.consul:443"
226:           tmsBootstrapServers: "shared-kafka-bootstrap-kafka.service.ams-dataplane.prod.consul:443"
227:         schemaRegistry:
228:           url: "https://schemaregistry-proxy-kafka.service.ams-dataplane.prod.consul"
229:         podAnnotations:
230:           vault.security.banzaicloud.io/log-level: "warn"
231:           vault.security.banzaicloud.io/vault-addr: "https://vault.query.prod.consul:8200"
232:           vault.security.banzaicloud.io/vault-role: "mercury-psr"
233:           vault.security.banzaicloud.io/vault-path: "k8s-ams-dataplane"
234:           vault.security.banzaicloud.io/vault-skip-verify: "true"
235:           vault.security.banzaicloud.io/vault-env-daemon: "true"
236:           vault.security.banzaicloud.io/vault-agent-configmap: "mercury-psr-va-configmap"
237:         keystore:
238:           tokenPath: /vault/.vault-token
239:           location: /vault/secrets/
240:         extraEnvVars:
241:           - name: AUTO_REFRESH_CONSUL_TOKEN
242:             value: "vault:cpe_consul/creds/mercury-psr#token"
243:             # These commands would have to be executed before anything else that tries to use these files
244:             # in an init container.  If not using an init container, none of this is necessary.
245:             # $ echo $KAFKA_TEMPLATED_PKI | jq .certificate -r > /tmp/certificate.crt
246:             # $ echo $KAFKA_TEMPLATED_PKI | jq .issuing_ca -r > /tmp/issuing_ca.crt
247:             # $ echo $KAFKA_TEMPLATED_PKI | jq .private_key -r > /tmp/private.key
248:           - name: KAFKA_TEMPLATED_PKI
249:             value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate" .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw", "ttl": "720h"}'
250:   # If region is marked as deploy_region, append deployment OKE type
251:   # ingress region will be us-phoenix-1 for preprod and the region everywhere else
252:   # for the mapping of region to ingress, see
253:   # https://confluence.oraclecorp.com/confluence/display/CPE/Ingress%2C+DNS%2C+and+Certificates
254:
255: - name: mercury
256:   createNamespace: false
257:   <<: *mercury
258:   kubeContext: "prod_eu-frankfurt-1_dataplane"
259:   labels:
260:     region: eu-frankfurt-1
261:     app: mercury-psr
262:   missingFileHandler: Info
263:   values:
264:     - "../../vars/charts/mercury/deployment.yaml"
265:     - "../../vars/charts/mercury/prod/deployment.yaml"
266:     - "../../vars/charts/mercury/prod/mercury-psr/deployment.yaml"
267:     - "../../vars/charts/mercury/prod/mercury-psr/eu-frankfurt-1_deployment.yaml"
268:     - mercuryCommon:
269:         tenantNameFilter: true
270:     - global:
271:         mercury:
272:           enabled: true
273:           cpev2: true
274:         serviceAccount: ## Required by kweet - Service account names and whether to create them
275:           create: false
276:           name: mercurygeneric
277:         vault:
278:           endPoint: "https://vault.query.prod.consul:8200"
279:           path: "k8s-fra-dataplane"
280:         consul:
281:           region:
282:             key: fra
283:         webhook:
284:           host: engagement-mercury-psr.prod.channels.ocs.oraclecloud.com
285:         service:
286:           port: 8080
287:           targetPort: 8080
288:         image:
289:           repository: iad.ocir.io/osvcstage
290:           initImage: init-container/prod:1.0.0.18
291:           secret: osvcstage-ocirsecret
292:         ingress:
293:           class: public
294:           annotations:
295:             ingress.kubernetes.io/ssl-redirect: "true"
296:           hosts:
297:             - engagement-psr.us1.channels.ocs.oraclecloud.com
298:           tls:
299:             - secretName: engagement-psr-tls-cert
300:               hosts:
301:                 - engagement-psr.us1.channels.ocs.oraclecloud.com
302:         kafka:
303:           bootstrapServers: "mercury-psr-kafka-bootstrap-mercury-psr.service.fra-dataplane.prod.consul:443"
304:           tmsBootstrapServers: "shared-kafka-bootstrap-kafka.service.fra-dataplane.prod.consul:443"
305:         schemaRegistry:
306:           url: "https://schemaregistry-proxy-kafka.service.fra-dataplane.prod.consul"
307:         podAnnotations:
308:           vault.security.banzaicloud.io/log-level: "warn"
309:           vault.security.banzaicloud.io/vault-addr: "https://vault.query.prod.consul:8200"
310:           vault.security.banzaicloud.io/vault-role: "mercury-psr"
311:           vault.security.banzaicloud.io/vault-path: "k8s-fra-dataplane"
312:           vault.security.banzaicloud.io/vault-skip-verify: "true"
313:           vault.security.banzaicloud.io/vault-env-daemon: "true"
314:           vault.security.banzaicloud.io/vault-agent-configmap: "mercury-psr-va-configmap"
315:         keystore:
316:           tokenPath: /vault/.vault-token
317:           location: /vault/secrets/
318:         extraEnvVars:
319:           - name: AUTO_REFRESH_CONSUL_TOKEN
320:             value: "vault:cpe_consul/creds/mercury-psr#token"
321:             # These commands would have to be executed before anything else that tries to use these files
322:             # in an init container.  If not using an init container, none of this is necessary.
323:             # $ echo $KAFKA_TEMPLATED_PKI | jq .certificate -r > /tmp/certificate.crt
324:             # $ echo $KAFKA_TEMPLATED_PKI | jq .issuing_ca -r > /tmp/issuing_ca.crt
325:             # $ echo $KAFKA_TEMPLATED_PKI | jq .private_key -r > /tmp/private.key
326:           - name: KAFKA_TEMPLATED_PKI
327:             value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate" .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw", "ttl": "720h"}'
328:   # If region is marked as deploy_region, append deployment OKE type
329:   # ingress region will be us-phoenix-1 for preprod and the region everywhere else
330:   # for the mapping of region to ingress, see
331:   # https://confluence.oraclecorp.com/confluence/display/CPE/Ingress%2C+DNS%2C+and+Certificates
332:
333: - name: mercury
334:   createNamespace: false
335:   <<: *mercury
336:   kubeContext: "prod_uk-cardiff-1_dataplane"
337:   labels:
338:     region: uk-cardiff-1
339:     app: mercury-psr
340:   missingFileHandler: Info
341:   values:
342:     - "../../vars/charts/mercury/deployment.yaml"
343:     - "../../vars/charts/mercury/prod/deployment.yaml"
344:     - "../../vars/charts/mercury/prod/mercury-psr/deployment.yaml"
345:     - "../../vars/charts/mercury/prod/mercury-psr/uk-cardiff-1_deployment.yaml"
346:     - mercuryCommon:
347:         tenantNameFilter: true
348:     - global:
349:         mercury:
350:           enabled: true
351:           cpev2: true
352:         serviceAccount: ## Required by kweet - Service account names and whether to create them
353:           create: false
354:           name: mercurygeneric
355:         vault:
356:           endPoint: "https://vault.query.prod.consul:8200"
357:           path: "k8s-cwl-dataplane"
358:         consul:
359:           region:
360:             key: cwl
361:         webhook:
362:           host: engagement-mercury-psr.prod.channels.ocs.oraclecloud.com
363:         service:
364:           port: 8080
365:           targetPort: 8080
366:         image:
367:           repository: iad.ocir.io/osvcstage
368:           initImage: init-container/prod:1.0.0.18
369:           secret: osvcstage-ocirsecret
370:         ingress:
371:           class: public
372:           annotations:
373:             ingress.kubernetes.io/ssl-redirect: "true"
374:           hosts:
375:             - engagement-psr.us1.channels.ocs.oraclecloud.com
376:           tls:
377:             - secretName: engagement-psr-tls-cert
378:               hosts:
379:                 - engagement-psr.us1.channels.ocs.oraclecloud.com
380:         kafka:
381:           bootstrapServers: "mercury-psr-kafka-bootstrap-mercury-psr.service.cwl-dataplane.prod.consul:443"
382:           tmsBootstrapServers: "shared-kafka-bootstrap-kafka.service.cwl-dataplane.prod.consul:443"
383:         schemaRegistry:
384:           url: "https://schemaregistry-proxy-kafka.service.cwl-dataplane.prod.consul"
385:         podAnnotations:
386:           vault.security.banzaicloud.io/log-level: "warn"
387:           vault.security.banzaicloud.io/vault-addr: "https://vault.query.prod.consul:8200"
388:           vault.security.banzaicloud.io/vault-role: "mercury-psr"
389:           vault.security.banzaicloud.io/vault-path: "k8s-cwl-dataplane"
390:           vault.security.banzaicloud.io/vault-skip-verify: "true"
391:           vault.security.banzaicloud.io/vault-env-daemon: "true"
392:           vault.security.banzaicloud.io/vault-agent-configmap: "mercury-psr-va-configmap"
393:         keystore:
394:           tokenPath: /vault/.vault-token
395:           location: /vault/secrets/
396:         extraEnvVars:
397:           - name: AUTO_REFRESH_CONSUL_TOKEN
398:             value: "vault:cpe_consul/creds/mercury-psr#token"
399:             # These commands would have to be executed before anything else that tries to use these files
400:             # in an init container.  If not using an init container, none of this is necessary.
401:             # $ echo $KAFKA_TEMPLATED_PKI | jq .certificate -r > /tmp/certificate.crt
402:             # $ echo $KAFKA_TEMPLATED_PKI | jq .issuing_ca -r > /tmp/issuing_ca.crt
403:             # $ echo $KAFKA_TEMPLATED_PKI | jq .private_key -r > /tmp/private.key
404:           - name: KAFKA_TEMPLATED_PKI
405:             value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate" .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw", "ttl": "720h"}'
406:   # If region is marked as deploy_region, append deployment OKE type
407:   # ingress region will be us-phoenix-1 for preprod and the region everywhere else
408:   # for the mapping of region to ingress, see
409:   # https://confluence.oraclecorp.com/confluence/display/CPE/Ingress%2C+DNS%2C+and+Certificates
410:
411: - name: mercury
412:   createNamespace: false
413:   <<: *mercury
414:   kubeContext: "prod_uk-london-1_dataplane"
415:   labels:
416:     region: uk-london-1
417:     app: mercury-psr
418:   missingFileHandler: Info
419:   values:
420:     - "../../vars/charts/mercury/deployment.yaml"
421:     - "../../vars/charts/mercury/prod/deployment.yaml"
422:     - "../../vars/charts/mercury/prod/mercury-psr/deployment.yaml"
423:     - "../../vars/charts/mercury/prod/mercury-psr/uk-london-1_deployment.yaml"
424:     - mercuryCommon:
425:         tenantNameFilter: true
426:     - global:
427:         mercury:
428:           enabled: true
429:           cpev2: true
430:         serviceAccount: ## Required by kweet - Service account names and whether to create them
431:           create: false
432:           name: mercurygeneric
433:         vault:
434:           endPoint: "https://vault.query.prod.consul:8200"
435:           path: "k8s-lhr-dataplane"
436:         consul:
437:           region:
438:             key: lhr
439:         webhook:
440:           host: engagement-mercury-psr.prod.channels.ocs.oraclecloud.com
441:         service:
442:           port: 8080
443:           targetPort: 8080
444:         image:
445:           repository: iad.ocir.io/osvcstage
446:           initImage: init-container/prod:1.0.0.18
447:           secret: osvcstage-ocirsecret
448:         ingress:
449:           class: public
450:           annotations:
451:             ingress.kubernetes.io/ssl-redirect: "true"
452:           hosts:
453:             - engagement-psr.us1.channels.ocs.oraclecloud.com
454:           tls:
455:             - secretName: engagement-psr-tls-cert
456:               hosts:
457:                 - engagement-psr.us1.channels.ocs.oraclecloud.com
458:         kafka:
459:           bootstrapServers: "mercury-psr-kafka-bootstrap-mercury-psr.service.lhr-dataplane.prod.consul:443"
460:           tmsBootstrapServers: "shared-kafka-bootstrap-kafka.service.lhr-dataplane.prod.consul:443"
461:         schemaRegistry:
462:           url: "https://schemaregistry-proxy-kafka.service.lhr-dataplane.prod.consul"
463:         podAnnotations:
464:           vault.security.banzaicloud.io/log-level: "warn"
465:           vault.security.banzaicloud.io/vault-addr: "https://vault.query.prod.consul:8200"
466:           vault.security.banzaicloud.io/vault-role: "mercury-psr"
467:           vault.security.banzaicloud.io/vault-path: "k8s-lhr-dataplane"
468:           vault.security.banzaicloud.io/vault-skip-verify: "true"
469:           vault.security.banzaicloud.io/vault-env-daemon: "true"
470:           vault.security.banzaicloud.io/vault-agent-configmap: "mercury-psr-va-configmap"
471:         keystore:
472:           tokenPath: /vault/.vault-token
473:           location: /vault/secrets/
474:         extraEnvVars:
475:           - name: AUTO_REFRESH_CONSUL_TOKEN
476:             value: "vault:cpe_consul/creds/mercury-psr#token"
477:             # These commands would have to be executed before anything else that tries to use these files
478:             # in an init container.  If not using an init container, none of this is necessary.
479:             # $ echo $KAFKA_TEMPLATED_PKI | jq .certificate -r > /tmp/certificate.crt
480:             # $ echo $KAFKA_TEMPLATED_PKI | jq .issuing_ca -r > /tmp/issuing_ca.crt
481:             # $ echo $KAFKA_TEMPLATED_PKI | jq .private_key -r > /tmp/private.key
482:           - name: KAFKA_TEMPLATED_PKI
483:             value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate" .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw", "ttl": "720h"}'
484:   # If region is marked as deploy_region, append deployment OKE type
485:   # ingress region will be us-phoenix-1 for preprod and the region everywhere else
486:   # for the mapping of region to ingress, see
487:   # https://confluence.oraclecorp.com/confluence/display/CPE/Ingress%2C+DNS%2C+and+Certificates
488:
489: - name: mercury
490:   createNamespace: false
491:   <<: *mercury
492:   kubeContext: "prod_ap-melbourne-1_dataplane"
493:   labels:
494:     region: ap-melbourne-1
495:     app: mercury-psr
496:   missingFileHandler: Info
497:   values:
498:     - "../../vars/charts/mercury/deployment.yaml"
499:     - "../../vars/charts/mercury/prod/deployment.yaml"
500:     - "../../vars/charts/mercury/prod/mercury-psr/deployment.yaml"
501:     - "../../vars/charts/mercury/prod/mercury-psr/ap-melbourne-1_deployment.yaml"
502:     - mercuryCommon:
503:         tenantNameFilter: true
504:     - global:
505:         mercury:
506:           enabled: true
507:           cpev2: true
508:         serviceAccount: ## Required by kweet - Service account names and whether to create them
509:           create: false
510:           name: mercurygeneric
511:         vault:
512:           endPoint: "https://vault.query.prod.consul:8200"
513:           path: "k8s-mel-dataplane"
514:         consul:
515:           region:
516:             key: mel
517:         webhook:
518:           host: engagement-mercury-psr.prod.channels.ocs.oraclecloud.com
519:         service:
520:           port: 8080
521:           targetPort: 8080
522:         image:
523:           repository: iad.ocir.io/osvcstage
524:           initImage: init-container/prod:1.0.0.18
525:           secret: osvcstage-ocirsecret
526:         ingress:
527:           class: public
528:           annotations:
529:             ingress.kubernetes.io/ssl-redirect: "true"
530:           hosts:
531:             - engagement-psr.us1.channels.ocs.oraclecloud.com
532:           tls:
533:             - secretName: engagement-psr-tls-cert
534:               hosts:
535:                 - engagement-psr.us1.channels.ocs.oraclecloud.com
536:         kafka:
537:           bootstrapServers: "mercury-psr-kafka-bootstrap-mercury-psr.service.mel-dataplane.prod.consul:443"
538:           tmsBootstrapServers: "shared-kafka-bootstrap-kafka.service.mel-dataplane.prod.consul:443"
539:         schemaRegistry:
540:           url: "https://schemaregistry-proxy-kafka.service.mel-dataplane.prod.consul"
541:         podAnnotations:
542:           vault.security.banzaicloud.io/log-level: "warn"
543:           vault.security.banzaicloud.io/vault-addr: "https://vault.query.prod.consul:8200"
544:           vault.security.banzaicloud.io/vault-role: "mercury-psr"
545:           vault.security.banzaicloud.io/vault-path: "k8s-mel-dataplane"
546:           vault.security.banzaicloud.io/vault-skip-verify: "true"
547:           vault.security.banzaicloud.io/vault-env-daemon: "true"
548:           vault.security.banzaicloud.io/vault-agent-configmap: "mercury-psr-va-configmap"
549:         keystore:
550:           tokenPath: /vault/.vault-token
551:           location: /vault/secrets/
552:         extraEnvVars:
553:           - name: AUTO_REFRESH_CONSUL_TOKEN
554:             value: "vault:cpe_consul/creds/mercury-psr#token"
555:             # These commands would have to be executed before anything else that tries to use these files
556:             # in an init container.  If not using an init container, none of this is necessary.
557:             # $ echo $KAFKA_TEMPLATED_PKI | jq .certificate -r > /tmp/certificate.crt
558:             # $ echo $KAFKA_TEMPLATED_PKI | jq .issuing_ca -r > /tmp/issuing_ca.crt
559:             # $ echo $KAFKA_TEMPLATED_PKI | jq .private_key -r > /tmp/private.key
560:           - name: KAFKA_TEMPLATED_PKI
561:             value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate" .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw", "ttl": "720h"}'
562:   # If region is marked as deploy_region, append deployment OKE type
563:   # ingress region will be us-phoenix-1 for preprod and the region everywhere else
564:   # for the mapping of region to ingress, see
565:   # https://confluence.oraclecorp.com/confluence/display/CPE/Ingress%2C+DNS%2C+and+Certificates
566:
567: - name: mercury
568:   createNamespace: false
569:   <<: *mercury
570:   kubeContext: "prod_ap-sydney-1_dataplane"
571:   labels:
572:     region: ap-sydney-1
573:     app: mercury-psr
574:   missingFileHandler: Info
575:   values:
576:     - "../../vars/charts/mercury/deployment.yaml"
577:     - "../../vars/charts/mercury/prod/deployment.yaml"
578:     - "../../vars/charts/mercury/prod/mercury-psr/deployment.yaml"
579:     - "../../vars/charts/mercury/prod/mercury-psr/ap-sydney-1_deployment.yaml"
580:     - mercuryCommon:
581:         tenantNameFilter: true
582:     - global:
583:         mercury:
584:           enabled: true
585:           cpev2: true
586:         serviceAccount: ## Required by kweet - Service account names and whether to create them
587:           create: false
588:           name: mercurygeneric
589:         vault:
590:           endPoint: "https://vault.query.prod.consul:8200"
591:           path: "k8s-syd-dataplane"
592:         consul:
593:           region:
594:             key: syd
595:         webhook:
596:           host: engagement-mercury-psr.prod.channels.ocs.oraclecloud.com
597:         service:
598:           port: 8080
599:           targetPort: 8080
600:         image:
601:           repository: iad.ocir.io/osvcstage
602:           initImage: init-container/prod:1.0.0.18
603:           secret: osvcstage-ocirsecret
604:         ingress:
605:           class: public
606:           annotations:
607:             ingress.kubernetes.io/ssl-redirect: "true"
608:           hosts:
609:             - engagement-psr.us1.channels.ocs.oraclecloud.com
610:           tls:
611:             - secretName: engagement-psr-tls-cert
612:               hosts:
613:                 - engagement-psr.us1.channels.ocs.oraclecloud.com
614:         kafka:
615:           bootstrapServers: "mercury-psr-kafka-bootstrap-mercury-psr.service.syd-dataplane.prod.consul:443"
616:           tmsBootstrapServers: "shared-kafka-bootstrap-kafka.service.syd-dataplane.prod.consul:443"
617:         schemaRegistry:
618:           url: "https://schemaregistry-proxy-kafka.service.syd-dataplane.prod.consul"
619:         podAnnotations:
620:           vault.security.banzaicloud.io/log-level: "warn"
621:           vault.security.banzaicloud.io/vault-addr: "https://vault.query.prod.consul:8200"
622:           vault.security.banzaicloud.io/vault-role: "mercury-psr"
623:           vault.security.banzaicloud.io/vault-path: "k8s-syd-dataplane"
624:           vault.security.banzaicloud.io/vault-skip-verify: "true"
625:           vault.security.banzaicloud.io/vault-env-daemon: "true"
626:           vault.security.banzaicloud.io/vault-agent-configmap: "mercury-psr-va-configmap"
627:         keystore:
628:           tokenPath: /vault/.vault-token
629:           location: /vault/secrets/
630:         extraEnvVars:
631:           - name: AUTO_REFRESH_CONSUL_TOKEN
632:             value: "vault:cpe_consul/creds/mercury-psr#token"
633:             # These commands would have to be executed before anything else that tries to use these files
634:             # in an init container.  If not using an init container, none of this is necessary.
635:             # $ echo $KAFKA_TEMPLATED_PKI | jq .certificate -r > /tmp/certificate.crt
636:             # $ echo $KAFKA_TEMPLATED_PKI | jq .issuing_ca -r > /tmp/issuing_ca.crt
637:             # $ echo $KAFKA_TEMPLATED_PKI | jq .private_key -r > /tmp/private.key
638:           - name: KAFKA_TEMPLATED_PKI
639:             value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate" .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw", "ttl": "720h"}'
640:   # If region is marked as deploy_region, append deployment OKE type
641:   # ingress region will be us-phoenix-1 for preprod and the region everywhere else
642:   # for the mapping of region to ingress, see
643:   # https://confluence.oraclecorp.com/confluence/display/CPE/Ingress%2C+DNS%2C+and+Certificates
644:
645: - name: mercury
646:   createNamespace: false
647:   <<: *mercury
648:   kubeContext: "prod_ca-toronto-1_dataplane"
649:   labels:
650:     region: ca-toronto-1
651:     app: mercury-psr
652:   missingFileHandler: Info
653:   values:
654:     - "../../vars/charts/mercury/deployment.yaml"
655:     - "../../vars/charts/mercury/prod/deployment.yaml"
656:     - "../../vars/charts/mercury/prod/mercury-psr/deployment.yaml"
657:     - "../../vars/charts/mercury/prod/mercury-psr/ca-toronto-1_deployment.yaml"
658:     - mercuryCommon:
659:         tenantNameFilter: true
660:     - global:
661:         mercury:
662:           enabled: true
663:           cpev2: true
664:         serviceAccount: ## Required by kweet - Service account names and whether to create them
665:           create: false
666:           name: mercurygeneric
667:         vault:
668:           endPoint: "https://vault.query.prod.consul:8200"
669:           path: "k8s-yyz-dataplane"
670:         consul:
671:           region:
672:             key: yyz
673:         webhook:
674:           host: engagement-mercury-psr.prod.channels.ocs.oraclecloud.com
675:         service:
676:           port: 8080
677:           targetPort: 8080
678:         image:
679:           repository: iad.ocir.io/osvcstage
680:           initImage: init-container/prod:1.0.0.18
681:           secret: osvcstage-ocirsecret
682:         ingress:
683:           class: public
684:           annotations:
685:             ingress.kubernetes.io/ssl-redirect: "true"
686:           hosts:
687:             - engagement-psr.us1.channels.ocs.oraclecloud.com
688:           tls:
689:             - secretName: engagement-psr-tls-cert
690:               hosts:
691:                 - engagement-psr.us1.channels.ocs.oraclecloud.com
692:         kafka:
693:           bootstrapServers: "mercury-psr-kafka-bootstrap-mercury-psr.service.yyz-dataplane.prod.consul:443"
694:           tmsBootstrapServers: "shared-kafka-bootstrap-kafka.service.yyz-dataplane.prod.consul:443"
695:         schemaRegistry:
696:           url: "https://schemaregistry-proxy-kafka.service.yyz-dataplane.prod.consul"
697:         podAnnotations:
698:           vault.security.banzaicloud.io/log-level: "warn"
699:           vault.security.banzaicloud.io/vault-addr: "https://vault.query.prod.consul:8200"
700:           vault.security.banzaicloud.io/vault-role: "mercury-psr"
701:           vault.security.banzaicloud.io/vault-path: "k8s-yyz-dataplane"
702:           vault.security.banzaicloud.io/vault-skip-verify: "true"
703:           vault.security.banzaicloud.io/vault-env-daemon: "true"
704:           vault.security.banzaicloud.io/vault-agent-configmap: "mercury-psr-va-configmap"
705:         keystore:
706:           tokenPath: /vault/.vault-token
707:           location: /vault/secrets/
708:         extraEnvVars:
709:           - name: AUTO_REFRESH_CONSUL_TOKEN
710:             value: "vault:cpe_consul/creds/mercury-psr#token"
711:             # These commands would have to be executed before anything else that tries to use these files
712:             # in an init container.  If not using an init container, none of this is necessary.
713:             # $ echo $KAFKA_TEMPLATED_PKI | jq .certificate -r > /tmp/certificate.crt
714:             # $ echo $KAFKA_TEMPLATED_PKI | jq .issuing_ca -r > /tmp/issuing_ca.crt
715:             # $ echo $KAFKA_TEMPLATED_PKI | jq .private_key -r > /tmp/private.key
716:           - name: KAFKA_TEMPLATED_PKI
717:             value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate" .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw", "ttl": "720h"}'
718:   # If region is marked as deploy_region, append deployment OKE type
719:   # ingress region will be us-phoenix-1 for preprod and the region everywhere else
720:   # for the mapping of region to ingress, see
721:   # https://confluence.oraclecorp.com/confluence/display/CPE/Ingress%2C+DNS%2C+and+Certificates
722:
723: - name: mercury
724:   createNamespace: false
725:   <<: *mercury
726:   kubeContext: "prod_ca-montreal-1_dataplane"
727:   labels:
728:     region: ca-montreal-1
729:     app: mercury-psr
730:   missingFileHandler: Info
731:   values:
732:     - "../../vars/charts/mercury/deployment.yaml"
733:     - "../../vars/charts/mercury/prod/deployment.yaml"
734:     - "../../vars/charts/mercury/prod/mercury-psr/deployment.yaml"
735:     - "../../vars/charts/mercury/prod/mercury-psr/ca-montreal-1_deployment.yaml"
736:     - mercuryCommon:
737:         tenantNameFilter: true
738:     - global:
739:         mercury:
740:           enabled: true
741:           cpev2: true
742:         serviceAccount: ## Required by kweet - Service account names and whether to create them
743:           create: false
744:           name: mercurygeneric
745:         vault:
746:           endPoint: "https://vault.query.prod.consul:8200"
747:           path: "k8s-yul-dataplane"
748:         consul:
749:           region:
750:             key: yul
751:         webhook:
752:           host: engagement-mercury-psr.prod.channels.ocs.oraclecloud.com
753:         service:
754:           port: 8080
755:           targetPort: 8080
756:         image:
757:           repository: iad.ocir.io/osvcstage
758:           initImage: init-container/prod:1.0.0.18
759:           secret: osvcstage-ocirsecret
760:         ingress:
761:           class: public
762:           annotations:
763:             ingress.kubernetes.io/ssl-redirect: "true"
764:           hosts:
765:             - engagement-psr.us1.channels.ocs.oraclecloud.com
766:           tls:
767:             - secretName: engagement-psr-tls-cert
768:               hosts:
769:                 - engagement-psr.us1.channels.ocs.oraclecloud.com
770:         kafka:
771:           bootstrapServers: "mercury-psr-kafka-bootstrap-mercury-psr.service.yul-dataplane.prod.consul:443"
772:           tmsBootstrapServers: "shared-kafka-bootstrap-kafka.service.yul-dataplane.prod.consul:443"
773:         schemaRegistry:
774:           url: "https://schemaregistry-proxy-kafka.service.yul-dataplane.prod.consul"
775:         podAnnotations:
776:           vault.security.banzaicloud.io/log-level: "warn"
777:           vault.security.banzaicloud.io/vault-addr: "https://vault.query.prod.consul:8200"
778:           vault.security.banzaicloud.io/vault-role: "mercury-psr"
779:           vault.security.banzaicloud.io/vault-path: "k8s-yul-dataplane"
780:           vault.security.banzaicloud.io/vault-skip-verify: "true"
781:           vault.security.banzaicloud.io/vault-env-daemon: "true"
782:           vault.security.banzaicloud.io/vault-agent-configmap: "mercury-psr-va-configmap"
783:         keystore:
784:           tokenPath: /vault/.vault-token
785:           location: /vault/secrets/
786:         extraEnvVars:
787:           - name: AUTO_REFRESH_CONSUL_TOKEN
788:             value: "vault:cpe_consul/creds/mercury-psr#token"
789:             # These commands would have to be executed before anything else that tries to use these files
790:             # in an init container.  If not using an init container, none of this is necessary.
791:             # $ echo $KAFKA_TEMPLATED_PKI | jq .certificate -r > /tmp/certificate.crt
792:             # $ echo $KAFKA_TEMPLATED_PKI | jq .issuing_ca -r > /tmp/issuing_ca.crt
793:             # $ echo $KAFKA_TEMPLATED_PKI | jq .private_key -r > /tmp/private.key
794:           - name: KAFKA_TEMPLATED_PKI
795:             value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate" .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw", "ttl": "720h"}'
796:   # If region is marked as deploy_region, append deployment OKE type
797:   # ingress region will be us-phoenix-1 for preprod and the region everywhere else
798:   # for the mapping of region to ingress, see
799:   # https://confluence.oraclecorp.com/confluence/display/CPE/Ingress%2C+DNS%2C+and+Certificates
800:
801: - name: mercury
802:   createNamespace: false
803:   <<: *mercury
804:   kubeContext: "prod_ap-hyderabad-1_dataplane"
805:   labels:
806:     region: ap-hyderabad-1
807:     app: mercury-psr
808:   missingFileHandler: Info
809:   values:
810:     - "../../vars/charts/mercury/deployment.yaml"
811:     - "../../vars/charts/mercury/prod/deployment.yaml"
812:     - "../../vars/charts/mercury/prod/mercury-psr/deployment.yaml"
813:     - "../../vars/charts/mercury/prod/mercury-psr/ap-hyderabad-1_deployment.yaml"
814:     - mercuryCommon:
815:         tenantNameFilter: true
816:     - global:
817:         mercury:
818:           enabled: true
819:           cpev2: true
820:         serviceAccount: ## Required by kweet - Service account names and whether to create them
821:           create: false
822:           name: mercurygeneric
823:         vault:
824:           endPoint: "https://vault.query.prod.consul:8200"
825:           path: "k8s-hyd-dataplane"
826:         consul:
827:           region:
828:             key: hyd
829:         webhook:
830:           host: engagement-mercury-psr.prod.channels.ocs.oraclecloud.com
831:         service:
832:           port: 8080
833:           targetPort: 8080
834:         image:
835:           repository: iad.ocir.io/osvcstage
836:           initImage: init-container/prod:1.0.0.18
837:           secret: osvcstage-ocirsecret
838:         ingress:
839:           class: public
840:           annotations:
841:             ingress.kubernetes.io/ssl-redirect: "true"
842:           hosts:
843:             - engagement-psr.us1.channels.ocs.oraclecloud.com
844:           tls:
845:             - secretName: engagement-psr-tls-cert
846:               hosts:
847:                 - engagement-psr.us1.channels.ocs.oraclecloud.com
848:         kafka:
849:           bootstrapServers: "mercury-psr-kafka-bootstrap-mercury-psr.service.hyd-dataplane.prod.consul:443"
850:           tmsBootstrapServers: "shared-kafka-bootstrap-kafka.service.hyd-dataplane.prod.consul:443"
851:         schemaRegistry:
852:           url: "https://schemaregistry-proxy-kafka.service.hyd-dataplane.prod.consul"
853:         podAnnotations:
854:           vault.security.banzaicloud.io/log-level: "warn"
855:           vault.security.banzaicloud.io/vault-addr: "https://vault.query.prod.consul:8200"
856:           vault.security.banzaicloud.io/vault-role: "mercury-psr"
857:           vault.security.banzaicloud.io/vault-path: "k8s-hyd-dataplane"
858:           vault.security.banzaicloud.io/vault-skip-verify: "true"
859:           vault.security.banzaicloud.io/vault-env-daemon: "true"
860:           vault.security.banzaicloud.io/vault-agent-configmap: "mercury-psr-va-configmap"
861:         keystore:
862:           tokenPath: /vault/.vault-token
863:           location: /vault/secrets/
864:         extraEnvVars:
865:           - name: AUTO_REFRESH_CONSUL_TOKEN
866:             value: "vault:cpe_consul/creds/mercury-psr#token"
867:             # These commands would have to be executed before anything else that tries to use these files
868:             # in an init container.  If not using an init container, none of this is necessary.
869:             # $ echo $KAFKA_TEMPLATED_PKI | jq .certificate -r > /tmp/certificate.crt
870:             # $ echo $KAFKA_TEMPLATED_PKI | jq .issuing_ca -r > /tmp/issuing_ca.crt
871:             # $ echo $KAFKA_TEMPLATED_PKI | jq .private_key -r > /tmp/private.key
872:           - name: KAFKA_TEMPLATED_PKI
873:             value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate" .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw", "ttl": "720h"}'
874:   # If region is marked as deploy_region, append deployment OKE type
875:   # ingress region will be us-phoenix-1 for preprod and the region everywhere else
876:   # for the mapping of region to ingress, see
877:   # https://confluence.oraclecorp.com/confluence/display/CPE/Ingress%2C+DNS%2C+and+Certificates
878:
879: - name: mercury
880:   createNamespace: false
881:   <<: *mercury
882:   kubeContext: "prod_ap-mumbai-1_dataplane"
883:   labels:
884:     region: ap-mumbai-1
885:     app: mercury-psr
886:   missingFileHandler: Info
887:   values:
888:     - "../../vars/charts/mercury/deployment.yaml"
889:     - "../../vars/charts/mercury/prod/deployment.yaml"
890:     - "../../vars/charts/mercury/prod/mercury-psr/deployment.yaml"
891:     - "../../vars/charts/mercury/prod/mercury-psr/ap-mumbai-1_deployment.yaml"
892:     - mercuryCommon:
893:         tenantNameFilter: true
894:     - global:
895:         mercury:
896:           enabled: true
897:           cpev2: true
898:         serviceAccount: ## Required by kweet - Service account names and whether to create them
899:           create: false
900:           name: mercurygeneric
901:         vault:
902:           endPoint: "https://vault.query.prod.consul:8200"
903:           path: "k8s-bom-dataplane"
904:         consul:
905:           region:
906:             key: bom
907:         webhook:
908:           host: engagement-mercury-psr.prod.channels.ocs.oraclecloud.com
909:         service:
910:           port: 8080
911:           targetPort: 8080
912:         image:
913:           repository: iad.ocir.io/osvcstage
914:           initImage: init-container/prod:1.0.0.18
915:           secret: osvcstage-ocirsecret
916:         ingress:
917:           class: public
918:           annotations:
919:             ingress.kubernetes.io/ssl-redirect: "true"
920:           hosts:
921:             - engagement-psr.us1.channels.ocs.oraclecloud.com
922:           tls:
923:             - secretName: engagement-psr-tls-cert
924:               hosts:
925:                 - engagement-psr.us1.channels.ocs.oraclecloud.com
926:         kafka:
927:           bootstrapServers: "mercury-psr-kafka-bootstrap-mercury-psr.service.bom-dataplane.prod.consul:443"
928:           tmsBootstrapServers: "shared-kafka-bootstrap-kafka.service.bom-dataplane.prod.consul:443"
929:         schemaRegistry:
930:           url: "https://schemaregistry-proxy-kafka.service.bom-dataplane.prod.consul"
931:         podAnnotations:
932:           vault.security.banzaicloud.io/log-level: "warn"
933:           vault.security.banzaicloud.io/vault-addr: "https://vault.query.prod.consul:8200"
934:           vault.security.banzaicloud.io/vault-role: "mercury-psr"
935:           vault.security.banzaicloud.io/vault-path: "k8s-bom-dataplane"
936:           vault.security.banzaicloud.io/vault-skip-verify: "true"
937:           vault.security.banzaicloud.io/vault-env-daemon: "true"
938:           vault.security.banzaicloud.io/vault-agent-configmap: "mercury-psr-va-configmap"
939:         keystore:
940:           tokenPath: /vault/.vault-token
941:           location: /vault/secrets/
942:         extraEnvVars:
943:           - name: AUTO_REFRESH_CONSUL_TOKEN
944:             value: "vault:cpe_consul/creds/mercury-psr#token"
945:             # These commands would have to be executed before anything else that tries to use these files
946:             # in an init container.  If not using an init container, none of this is necessary.
947:             # $ echo $KAFKA_TEMPLATED_PKI | jq .certificate -r > /tmp/certificate.crt
948:             # $ echo $KAFKA_TEMPLATED_PKI | jq .issuing_ca -r > /tmp/issuing_ca.crt
949:             # $ echo $KAFKA_TEMPLATED_PKI | jq .private_key -r > /tmp/private.key
950:           - name: KAFKA_TEMPLATED_PKI
951:             value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate" .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw", "ttl": "720h"}'
952:   # If region is marked as deploy_region, append deployment OKE type
953:   # ingress region will be us-phoenix-1 for preprod and the region everywhere else
954:   # for the mapping of region to ingress, see
955:   # https://confluence.oraclecorp.com/confluence/display/CPE/Ingress%2C+DNS%2C+and+Certificates
956:
957: - name: mercury
958:   createNamespace: false
959:   <<: *mercury
960:   kubeContext: "prod_ap-osaka-1_dataplane"
961:   labels:
962:     region: ap-osaka-1
963:     app: mercury-psr
964:   missingFileHandler: Info
965:   values:
966:     - "../../vars/charts/mercury/deployment.yaml"
967:     - "../../vars/charts/mercury/prod/deployment.yaml"
968:     - "../../vars/charts/mercury/prod/mercury-psr/deployment.yaml"
969:     - "../../vars/charts/mercury/prod/mercury-psr/ap-osaka-1_deployment.yaml"
970:     - mercuryCommon:
971:         tenantNameFilter: true
972:     - global:
973:         mercury:
974:           enabled: true
975:           cpev2: true
976:         serviceAccount: ## Required by kweet - Service account names and whether to create them
977:           create: false
978:           name: mercurygeneric
979:         vault:
980:           endPoint: "https://vault.query.prod.consul:8200"
981:           path: "k8s-kix-dataplane"
982:         consul:
983:           region:
984:             key: kix
985:         webhook:
986:           host: engagement-mercury-psr.prod.channels.ocs.oraclecloud.com
987:         service:
988:           port: 8080
989:           targetPort: 8080
990:         image:
991:           repository: iad.ocir.io/osvcstage
992:           initImage: init-container/prod:1.0.0.18
993:           secret: osvcstage-ocirsecret
994:         ingress:
995:           class: public
996:           annotations:
997:             ingress.kubernetes.io/ssl-redirect: "true"
998:           hosts:
999:             - engagement-psr.us1.channels.ocs.oraclecloud.com
1000:           tls:
1001:             - secretName: engagement-psr-tls-cert
1002:               hosts:
1003:                 - engagement-psr.us1.channels.ocs.oraclecloud.com
1004:         kafka:
1005:           bootstrapServers: "mercury-psr-kafka-bootstrap-mercury-psr.service.kix-dataplane.prod.consul:443"
1006:           tmsBootstrapServers: "shared-kafka-bootstrap-kafka.service.kix-dataplane.prod.consul:443"
1007:         schemaRegistry:
1008:           url: "https://schemaregistry-proxy-kafka.service.kix-dataplane.prod.consul"
1009:         podAnnotations:
1010:           vault.security.banzaicloud.io/log-level: "warn"
1011:           vault.security.banzaicloud.io/vault-addr: "https://vault.query.prod.consul:8200"
1012:           vault.security.banzaicloud.io/vault-role: "mercury-psr"
1013:           vault.security.banzaicloud.io/vault-path: "k8s-kix-dataplane"
1014:           vault.security.banzaicloud.io/vault-skip-verify: "true"
1015:           vault.security.banzaicloud.io/vault-env-daemon: "true"
1016:           vault.security.banzaicloud.io/vault-agent-configmap: "mercury-psr-va-configmap"
1017:         keystore:
1018:           tokenPath: /vault/.vault-token
1019:           location: /vault/secrets/
1020:         extraEnvVars:
1021:           - name: AUTO_REFRESH_CONSUL_TOKEN
1022:             value: "vault:cpe_consul/creds/mercury-psr#token"
1023:             # These commands would have to be executed before anything else that tries to use these files
1024:             # in an init container.  If not using an init container, none of this is necessary.
1025:             # $ echo $KAFKA_TEMPLATED_PKI | jq .certificate -r > /tmp/certificate.crt
1026:             # $ echo $KAFKA_TEMPLATED_PKI | jq .issuing_ca -r > /tmp/issuing_ca.crt
1027:             # $ echo $KAFKA_TEMPLATED_PKI | jq .private_key -r > /tmp/private.key
1028:           - name: KAFKA_TEMPLATED_PKI
1029:             value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate" .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw", "ttl": "720h"}'
1030:   # If region is marked as deploy_region, append deployment OKE type
1031:   # ingress region will be us-phoenix-1 for preprod and the region everywhere else
1032:   # for the mapping of region to ingress, see
1033:   # https://confluence.oraclecorp.com/confluence/display/CPE/Ingress%2C+DNS%2C+and+Certificates
1034:
1035: - name: mercury
1036:   createNamespace: false
1037:   <<: *mercury
1038:   kubeContext: "prod_ap-tokyo-1_dataplane"
1039:   labels:
1040:     region: ap-tokyo-1
1041:     app: mercury-psr
1042:   missingFileHandler: Info
1043:   values:
1044:     - "../../vars/charts/mercury/deployment.yaml"
1045:     - "../../vars/charts/mercury/prod/deployment.yaml"
1046:     - "../../vars/charts/mercury/prod/mercury-psr/deployment.yaml"
1047:     - "../../vars/charts/mercury/prod/mercury-psr/ap-tokyo-1_deployment.yaml"
1048:     - mercuryCommon:
1049:         tenantNameFilter: true
1050:     - global:
1051:         mercury:
1052:           enabled: true
1053:           cpev2: true
1054:         serviceAccount: ## Required by kweet - Service account names and whether to create them
1055:           create: false
1056:           name: mercurygeneric
1057:         vault:
1058:           endPoint: "https://vault.query.prod.consul:8200"
1059:           path: "k8s-nrt-dataplane"
1060:         consul:
1061:           region:
1062:             key: nrt
1063:         webhook:
1064:           host: engagement-mercury-psr.prod.channels.ocs.oraclecloud.com
1065:         service:
1066:           port: 8080
1067:           targetPort: 8080
1068:         image:
1069:           repository: iad.ocir.io/osvcstage
1070:           initImage: init-container/prod:1.0.0.18
1071:           secret: osvcstage-ocirsecret
1072:         ingress:
1073:           class: public
1074:           annotations:
1075:             ingress.kubernetes.io/ssl-redirect: "true"
1076:           hosts:
1077:             - engagement-psr.us1.channels.ocs.oraclecloud.com
1078:           tls:
1079:             - secretName: engagement-psr-tls-cert
1080:               hosts:
1081:                 - engagement-psr.us1.channels.ocs.oraclecloud.com
1082:         kafka:
1083:           bootstrapServers: "mercury-psr-kafka-bootstrap-mercury-psr.service.nrt-dataplane.prod.consul:443"
1084:           tmsBootstrapServers: "shared-kafka-bootstrap-kafka.service.nrt-dataplane.prod.consul:443"
1085:         schemaRegistry:
1086:           url: "https://schemaregistry-proxy-kafka.service.nrt-dataplane.prod.consul"
1087:         podAnnotations:
1088:           vault.security.banzaicloud.io/log-level: "warn"
1089:           vault.security.banzaicloud.io/vault-addr: "https://vault.query.prod.consul:8200"
1090:           vault.security.banzaicloud.io/vault-role: "mercury-psr"
1091:           vault.security.banzaicloud.io/vault-path: "k8s-nrt-dataplane"
1092:           vault.security.banzaicloud.io/vault-skip-verify: "true"
1093:           vault.security.banzaicloud.io/vault-env-daemon: "true"
1094:           vault.security.banzaicloud.io/vault-agent-configmap: "mercury-psr-va-configmap"
1095:         keystore:
1096:           tokenPath: /vault/.vault-token
1097:           location: /vault/secrets/
1098:         extraEnvVars:
1099:           - name: AUTO_REFRESH_CONSUL_TOKEN
1100:             value: "vault:cpe_consul/creds/mercury-psr#token"
1101:             # These commands would have to be executed before anything else that tries to use these files
1102:             # in an init container.  If not using an init container, none of this is necessary.
1103:             # $ echo $KAFKA_TEMPLATED_PKI | jq .certificate -r > /tmp/certificate.crt
1104:             # $ echo $KAFKA_TEMPLATED_PKI | jq .issuing_ca -r > /tmp/issuing_ca.crt
1105:             # $ echo $KAFKA_TEMPLATED_PKI | jq .private_key -r > /tmp/private.key
1106:           - name: KAFKA_TEMPLATED_PKI
1107:             value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate" .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw", "ttl": "720h"}'
1108:   # If region is marked as deploy_region, append deployment OKE type
1109:   # ingress region will be us-phoenix-1 for preprod and the region everywhere else
1110:   # for the mapping of region to ingress, see
1111:   # https://confluence.oraclecorp.com/confluence/display/CPE/Ingress%2C+DNS%2C+and+Certificates
1112:
1113: - name: mercury
1114:   createNamespace: false
1115:   <<: *mercury
1116:   kubeContext: "prod_sa-saopaulo-1_dataplane"
1117:   labels:
1118:     region: sa-saopaulo-1
1119:     app: mercury-psr
1120:   missingFileHandler: Info
1121:   values:
1122:     - "../../vars/charts/mercury/deployment.yaml"
1123:     - "../../vars/charts/mercury/prod/deployment.yaml"
1124:     - "../../vars/charts/mercury/prod/mercury-psr/deployment.yaml"
1125:     - "../../vars/charts/mercury/prod/mercury-psr/sa-saopaulo-1_deployment.yaml"
1126:     - mercuryCommon:
1127:         tenantNameFilter: true
1128:     - global:
1129:         mercury:
1130:           enabled: true
1131:           cpev2: true
1132:         serviceAccount: ## Required by kweet - Service account names and whether to create them
1133:           create: false
1134:           name: mercurygeneric
1135:         vault:
1136:           endPoint: "https://vault.query.prod.consul:8200"
1137:           path: "k8s-gru-dataplane"
1138:         consul:
1139:           region:
1140:             key: gru
1141:         webhook:
1142:           host: engagement-mercury-psr.prod.channels.ocs.oraclecloud.com
1143:         service:
1144:           port: 8080
1145:           targetPort: 8080
1146:         image:
1147:           repository: iad.ocir.io/osvcstage
1148:           initImage: init-container/prod:1.0.0.18
1149:           secret: osvcstage-ocirsecret
1150:         ingress:
1151:           class: public
1152:           annotations:
1153:             ingress.kubernetes.io/ssl-redirect: "true"
1154:           hosts:
1155:             - engagement-psr.us1.channels.ocs.oraclecloud.com
1156:           tls:
1157:             - secretName: engagement-psr-tls-cert
1158:               hosts:
1159:                 - engagement-psr.us1.channels.ocs.oraclecloud.com
1160:         kafka:
1161:           bootstrapServers: "mercury-psr-kafka-bootstrap-mercury-psr.service.gru-dataplane.prod.consul:443"
1162:           tmsBootstrapServers: "shared-kafka-bootstrap-kafka.service.gru-dataplane.prod.consul:443"
1163:         schemaRegistry:
1164:           url: "https://schemaregistry-proxy-kafka.service.gru-dataplane.prod.consul"
1165:         podAnnotations:
1166:           vault.security.banzaicloud.io/log-level: "warn"
1167:           vault.security.banzaicloud.io/vault-addr: "https://vault.query.prod.consul:8200"
1168:           vault.security.banzaicloud.io/vault-role: "mercury-psr"
1169:           vault.security.banzaicloud.io/vault-path: "k8s-gru-dataplane"
1170:           vault.security.banzaicloud.io/vault-skip-verify: "true"
1171:           vault.security.banzaicloud.io/vault-env-daemon: "true"
1172:           vault.security.banzaicloud.io/vault-agent-configmap: "mercury-psr-va-configmap"
1173:         keystore:
1174:           tokenPath: /vault/.vault-token
1175:           location: /vault/secrets/
1176:         extraEnvVars:
1177:           - name: AUTO_REFRESH_CONSUL_TOKEN
1178:             value: "vault:cpe_consul/creds/mercury-psr#token"
1179:             # These commands would have to be executed before anything else that tries to use these files
1180:             # in an init container.  If not using an init container, none of this is necessary.
1181:             # $ echo $KAFKA_TEMPLATED_PKI | jq .certificate -r > /tmp/certificate.crt
1182:             # $ echo $KAFKA_TEMPLATED_PKI | jq .issuing_ca -r > /tmp/issuing_ca.crt
1183:             # $ echo $KAFKA_TEMPLATED_PKI | jq .private_key -r > /tmp/private.key
1184:           - name: KAFKA_TEMPLATED_PKI
1185:             value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate" .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw", "ttl": "720h"}'
1186:   # If region is marked as deploy_region, append deployment OKE type
1187:   # ingress region will be us-phoenix-1 for preprod and the region everywhere else
1188:   # for the mapping of region to ingress, see
1189:   # https://confluence.oraclecorp.com/confluence/display/CPE/Ingress%2C+DNS%2C+and+Certificates
1190:
1191: - name: mercury
1192:   createNamespace: false
1193:   <<: *mercury
1194:   kubeContext: "prod_sa-vinhedo-1_dataplane"
1195:   labels:
1196:     region: sa-vinhedo-1
1197:     app: mercury-psr
1198:   missingFileHandler: Info
1199:   values:
1200:     - "../../vars/charts/mercury/deployment.yaml"
1201:     - "../../vars/charts/mercury/prod/deployment.yaml"
1202:     - "../../vars/charts/mercury/prod/mercury-psr/deployment.yaml"
1203:     - "../../vars/charts/mercury/prod/mercury-psr/sa-vinhedo-1_deployment.yaml"
1204:     - mercuryCommon:
1205:         tenantNameFilter: true
1206:     - global:
1207:         mercury:
1208:           enabled: true
1209:           cpev2: true
1210:         serviceAccount: ## Required by kweet - Service account names and whether to create them
1211:           create: false
1212:           name: mercurygeneric
1213:         vault:
1214:           endPoint: "https://vault.query.prod.consul:8200"
1215:           path: "k8s-vcp-dataplane"
1216:         consul:
1217:           region:
1218:             key: vcp
1219:         webhook:
1220:           host: engagement-mercury-psr.prod.channels.ocs.oraclecloud.com
1221:         service:
1222:           port: 8080
1223:           targetPort: 8080
1224:         image:
1225:           repository: iad.ocir.io/osvcstage
1226:           initImage: init-container/prod:1.0.0.18
1227:           secret: osvcstage-ocirsecret
1228:         ingress:
1229:           class: public
1230:           annotations:
1231:             ingress.kubernetes.io/ssl-redirect: "true"
1232:           hosts:
1233:             - engagement-psr.us1.channels.ocs.oraclecloud.com
1234:           tls:
1235:             - secretName: engagement-psr-tls-cert
1236:               hosts:
1237:                 - engagement-psr.us1.channels.ocs.oraclecloud.com
1238:         kafka:
1239:           bootstrapServers: "mercury-psr-kafka-bootstrap-mercury-psr.service.vcp-dataplane.prod.consul:443"
1240:           tmsBootstrapServers: "shared-kafka-bootstrap-kafka.service.vcp-dataplane.prod.consul:443"
1241:         schemaRegistry:
1242:           url: "https://schemaregistry-proxy-kafka.service.vcp-dataplane.prod.consul"
1243:         podAnnotations:
1244:           vault.security.banzaicloud.io/log-level: "warn"
1245:           vault.security.banzaicloud.io/vault-addr: "https://vault.query.prod.consul:8200"
1246:           vault.security.banzaicloud.io/vault-role: "mercury-psr"
1247:           vault.security.banzaicloud.io/vault-path: "k8s-vcp-dataplane"
1248:           vault.security.banzaicloud.io/vault-skip-verify: "true"
1249:           vault.security.banzaicloud.io/vault-env-daemon: "true"
1250:           vault.security.banzaicloud.io/vault-agent-configmap: "mercury-psr-va-configmap"
1251:         keystore:
1252:           tokenPath: /vault/.vault-token
1253:           location: /vault/secrets/
1254:         extraEnvVars:
1255:           - name: AUTO_REFRESH_CONSUL_TOKEN
1256:             value: "vault:cpe_consul/creds/mercury-psr#token"
1257:             # These commands would have to be executed before anything else that tries to use these files
1258:             # in an init container.  If not using an init container, none of this is necessary.
1259:             # $ echo $KAFKA_TEMPLATED_PKI | jq .certificate -r > /tmp/certificate.crt
1260:             # $ echo $KAFKA_TEMPLATED_PKI | jq .issuing_ca -r > /tmp/issuing_ca.crt
1261:             # $ echo $KAFKA_TEMPLATED_PKI | jq .private_key -r > /tmp/private.key
1262:           - name: KAFKA_TEMPLATED_PKI
1263:             value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate" .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw", "ttl": "720h"}'
1264:   # If region is marked as deploy_region, append deployment OKE type
1265:   # ingress region will be us-phoenix-1 for preprod and the region everywhere else
1266:   # for the mapping of region to ingress, see
1267:   # https://confluence.oraclecorp.com/confluence/display/CPE/Ingress%2C+DNS%2C+and+Certificates
1268:
1269: - name: mercury
1270:   createNamespace: false
1271:   <<: *mercury
1272:   kubeContext: "prod_me-jeddah-1_dataplane"
1273:   labels:
1274:     region: me-jeddah-1
1275:     app: mercury-psr
1276:   missingFileHandler: Info
1277:   values:
1278:     - "../../vars/charts/mercury/deployment.yaml"
1279:     - "../../vars/charts/mercury/prod/deployment.yaml"
1280:     - "../../vars/charts/mercury/prod/mercury-psr/deployment.yaml"
1281:     - "../../vars/charts/mercury/prod/mercury-psr/me-jeddah-1_deployment.yaml"
1282:     - mercuryCommon:
1283:         tenantNameFilter: true
1284:     - global:
1285:         mercury:
1286:           enabled: true
1287:           cpev2: true
1288:         serviceAccount: ## Required by kweet - Service account names and whether to create them
1289:           create: false
1290:           name: mercurygeneric
1291:         vault:
1292:           endPoint: "https://vault.query.prod.consul:8200"
1293:           path: "k8s-jed-dataplane"
1294:         consul:
1295:           region:
1296:             key: jed
1297:         webhook:
1298:           host: engagement-mercury-psr.prod.channels.ocs.oraclecloud.com
1299:         service:
1300:           port: 8080
1301:           targetPort: 8080
1302:         image:
1303:           repository: iad.ocir.io/osvcstage
1304:           initImage: init-container/prod:1.0.0.18
1305:           secret: osvcstage-ocirsecret
1306:         ingress:
1307:           class: public
1308:           annotations:
1309:             ingress.kubernetes.io/ssl-redirect: "true"
1310:           hosts:
1311:             - engagement-psr.us1.channels.ocs.oraclecloud.com
1312:           tls:
1313:             - secretName: engagement-psr-tls-cert
1314:               hosts:
1315:                 - engagement-psr.us1.channels.ocs.oraclecloud.com
1316:         kafka:
1317:           bootstrapServers: "mercury-psr-kafka-bootstrap-mercury-psr.service.jed-dataplane.prod.consul:443"
1318:           tmsBootstrapServers: "shared-kafka-bootstrap-kafka.service.jed-dataplane.prod.consul:443"
1319:         schemaRegistry:
1320:           url: "https://schemaregistry-proxy-kafka.service.jed-dataplane.prod.consul"
1321:         podAnnotations:
1322:           vault.security.banzaicloud.io/log-level: "warn"
1323:           vault.security.banzaicloud.io/vault-addr: "https://vault.query.prod.consul:8200"
1324:           vault.security.banzaicloud.io/vault-role: "mercury-psr"
1325:           vault.security.banzaicloud.io/vault-path: "k8s-jed-dataplane"
1326:           vault.security.banzaicloud.io/vault-skip-verify: "true"
1327:           vault.security.banzaicloud.io/vault-env-daemon: "true"
1328:           vault.security.banzaicloud.io/vault-agent-configmap: "mercury-psr-va-configmap"
1329:         keystore:
1330:           tokenPath: /vault/.vault-token
1331:           location: /vault/secrets/
1332:         extraEnvVars:
1333:           - name: AUTO_REFRESH_CONSUL_TOKEN
1334:             value: "vault:cpe_consul/creds/mercury-psr#token"
1335:             # These commands would have to be executed before anything else that tries to use these files
1336:             # in an init container.  If not using an init container, none of this is necessary.
1337:             # $ echo $KAFKA_TEMPLATED_PKI | jq .certificate -r > /tmp/certificate.crt
1338:             # $ echo $KAFKA_TEMPLATED_PKI | jq .issuing_ca -r > /tmp/issuing_ca.crt
1339:             # $ echo $KAFKA_TEMPLATED_PKI | jq .private_key -r > /tmp/private.key
1340:           - name: KAFKA_TEMPLATED_PKI
1341:             value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate" .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw", "ttl": "720h"}'
1342:   # If region is marked as deploy_region, append deployment OKE type
1343:   # ingress region will be us-phoenix-1 for preprod and the region everywhere else
1344:   # for the mapping of region to ingress, see
1345:   # https://confluence.oraclecorp.com/confluence/display/CPE/Ingress%2C+DNS%2C+and+Certificates
1346:
1347: - name: mercury
1348:   createNamespace: false
1349:   <<: *mercury
1350:   kubeContext: "prod_me-dubai-1_dataplane"
1351:   labels:
1352:     region: me-dubai-1
1353:     app: mercury-psr
1354:   missingFileHandler: Info
1355:   values:
1356:     - "../../vars/charts/mercury/deployment.yaml"
1357:     - "../../vars/charts/mercury/prod/deployment.yaml"
1358:     - "../../vars/charts/mercury/prod/mercury-psr/deployment.yaml"
1359:     - "../../vars/charts/mercury/prod/mercury-psr/me-dubai-1_deployment.yaml"
1360:     - mercuryCommon:
1361:         tenantNameFilter: true
1362:     - global:
1363:         mercury:
1364:           enabled: true
1365:           cpev2: true
1366:         serviceAccount: ## Required by kweet - Service account names and whether to create them
1367:           create: false
1368:           name: mercurygeneric
1369:         vault:
1370:           endPoint: "https://vault.query.prod.consul:8200"
1371:           path: "k8s-dxb-dataplane"
1372:         consul:
1373:           region:
1374:             key: dxb
1375:         webhook:
1376:           host: engagement-mercury-psr.prod.channels.ocs.oraclecloud.com
1377:         service:
1378:           port: 8080
1379:           targetPort: 8080
1380:         image:
1381:           repository: iad.ocir.io/osvcstage
1382:           initImage: init-container/prod:1.0.0.18
1383:           secret: osvcstage-ocirsecret
1384:         ingress:
1385:           class: public
1386:           annotations:
1387:             ingress.kubernetes.io/ssl-redirect: "true"
1388:           hosts:
1389:             - engagement-psr.us1.channels.ocs.oraclecloud.com
1390:           tls:
1391:             - secretName: engagement-psr-tls-cert
1392:               hosts:
1393:                 - engagement-psr.us1.channels.ocs.oraclecloud.com
1394:         kafka:
1395:           bootstrapServers: "mercury-psr-kafka-bootstrap-mercury-psr.service.dxb-dataplane.prod.consul:443"
1396:           tmsBootstrapServers: "shared-kafka-bootstrap-kafka.service.dxb-dataplane.prod.consul:443"
1397:         schemaRegistry:
1398:           url: "https://schemaregistry-proxy-kafka.service.dxb-dataplane.prod.consul"
1399:         podAnnotations:
1400:           vault.security.banzaicloud.io/log-level: "warn"
1401:           vault.security.banzaicloud.io/vault-addr: "https://vault.query.prod.consul:8200"
1402:           vault.security.banzaicloud.io/vault-role: "mercury-psr"
1403:           vault.security.banzaicloud.io/vault-path: "k8s-dxb-dataplane"
1404:           vault.security.banzaicloud.io/vault-skip-verify: "true"
1405:           vault.security.banzaicloud.io/vault-env-daemon: "true"
1406:           vault.security.banzaicloud.io/vault-agent-configmap: "mercury-psr-va-configmap"
1407:         keystore:
1408:           tokenPath: /vault/.vault-token
1409:           location: /vault/secrets/
1410:         extraEnvVars:
1411:           - name: AUTO_REFRESH_CONSUL_TOKEN
1412:             value: "vault:cpe_consul/creds/mercury-psr#token"
1413:             # These commands would have to be executed before anything else that tries to use these files
1414:             # in an init container.  If not using an init container, none of this is necessary.
1415:             # $ echo $KAFKA_TEMPLATED_PKI | jq .certificate -r > /tmp/certificate.crt
1416:             # $ echo $KAFKA_TEMPLATED_PKI | jq .issuing_ca -r > /tmp/issuing_ca.crt
1417:             # $ echo $KAFKA_TEMPLATED_PKI | jq .private_key -r > /tmp/private.key
1418:           - name: KAFKA_TEMPLATED_PKI
1419:             value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate" .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw", "ttl": "720h"}'
1420:   # If region is marked as deploy_region, append deployment OKE type
1421:   # ingress region will be us-phoenix-1 for preprod and the region everywhere else
1422:   # for the mapping of region to ingress, see
1423:   # https://confluence.oraclecorp.com/confluence/display/CPE/Ingress%2C+DNS%2C+and+Certificates
1424:
1425: - name: mercury
1426:   createNamespace: false
1427:   <<: *mercury
1428:   kubeContext: "prod_eu-zurich-1_dataplane"
1429:   labels:
1430:     region: eu-zurich-1
1431:     app: mercury-psr
1432:   missingFileHandler: Info
1433:   values:
1434:     - "../../vars/charts/mercury/deployment.yaml"
1435:     - "../../vars/charts/mercury/prod/deployment.yaml"
1436:     - "../../vars/charts/mercury/prod/mercury-psr/deployment.yaml"
1437:     - "../../vars/charts/mercury/prod/mercury-psr/eu-zurich-1_deployment.yaml"
1438:     - mercuryCommon:
1439:         tenantNameFilter: true
1440:     - global:
1441:         mercury:
1442:           enabled: true
1443:           cpev2: true
1444:         serviceAccount: ## Required by kweet - Service account names and whether to create them
1445:           create: false
1446:           name: mercurygeneric
1447:         vault:
1448:           endPoint: "https://vault.query.prod.consul:8200"
1449:           path: "k8s-zrh-dataplane"
1450:         consul:
1451:           region:
1452:             key: zrh
1453:         webhook:
1454:           host: engagement-mercury-psr.prod.channels.ocs.oraclecloud.com
1455:         service:
1456:           port: 8080
1457:           targetPort: 8080
1458:         image:
1459:           repository: iad.ocir.io/osvcstage
1460:           initImage: init-container/prod:1.0.0.18
1461:           secret: osvcstage-ocirsecret
1462:         ingress:
1463:           class: public
1464:           annotations:
1465:             ingress.kubernetes.io/ssl-redirect: "true"
1466:           hosts:
1467:             - engagement-psr.us1.channels.ocs.oraclecloud.com
1468:           tls:
1469:             - secretName: engagement-psr-tls-cert
1470:               hosts:
1471:                 - engagement-psr.us1.channels.ocs.oraclecloud.com
1472:         kafka:
1473:           bootstrapServers: "mercury-psr-kafka-bootstrap-mercury-psr.service.zrh-dataplane.prod.consul:443"
1474:           tmsBootstrapServers: "shared-kafka-bootstrap-kafka.service.zrh-dataplane.prod.consul:443"
1475:         schemaRegistry:
1476:           url: "https://schemaregistry-proxy-kafka.service.zrh-dataplane.prod.consul"
1477:         podAnnotations:
1478:           vault.security.banzaicloud.io/log-level: "warn"
1479:           vault.security.banzaicloud.io/vault-addr: "https://vault.query.prod.consul:8200"
1480:           vault.security.banzaicloud.io/vault-role: "mercury-psr"
1481:           vault.security.banzaicloud.io/vault-path: "k8s-zrh-dataplane"
1482:           vault.security.banzaicloud.io/vault-skip-verify: "true"
1483:           vault.security.banzaicloud.io/vault-env-daemon: "true"
1484:           vault.security.banzaicloud.io/vault-agent-configmap: "mercury-psr-va-configmap"
1485:         keystore:
1486:           tokenPath: /vault/.vault-token
1487:           location: /vault/secrets/
1488:         extraEnvVars:
1489:           - name: AUTO_REFRESH_CONSUL_TOKEN
1490:             value: "vault:cpe_consul/creds/mercury-psr#token"
1491:             # These commands would have to be executed before anything else that tries to use these files
1492:             # in an init container.  If not using an init container, none of this is necessary.
1493:             # $ echo $KAFKA_TEMPLATED_PKI | jq .certificate -r > /tmp/certificate.crt
1494:             # $ echo $KAFKA_TEMPLATED_PKI | jq .issuing_ca -r > /tmp/issuing_ca.crt
1495:             # $ echo $KAFKA_TEMPLATED_PKI | jq .private_key -r > /tmp/private.key
1496:           - name: KAFKA_TEMPLATED_PKI
1497:             value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate" .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw", "ttl": "720h"}'
1498:   # If region is marked as deploy_region, append deployment OKE type
1499:   # ingress region will be us-phoenix-1 for preprod and the region everywhere else
1500:   # for the mapping of region to ingress, see
1501:   # https://confluence.oraclecorp.com/confluence/display/CPE/Ingress%2C+DNS%2C+and+Certificates
1502:
1503: - name: mercury
1504:   createNamespace: false
1505:   <<: *mercury
1506:   kubeContext: "prod_sa-santiago-1_dataplane"
1507:   labels:
1508:     region: sa-santiago-1
1509:     app: mercury-psr
1510:   missingFileHandler: Info
1511:   values:
1512:     - "../../vars/charts/mercury/deployment.yaml"
1513:     - "../../vars/charts/mercury/prod/deployment.yaml"
1514:     - "../../vars/charts/mercury/prod/mercury-psr/deployment.yaml"
1515:     - "../../vars/charts/mercury/prod/mercury-psr/sa-santiago-1_deployment.yaml"
1516:     - mercuryCommon:
1517:         tenantNameFilter: true
1518:     - global:
1519:         mercury:
1520:           enabled: true
1521:           cpev2: true
1522:         serviceAccount: ## Required by kweet - Service account names and whether to create them
1523:           create: false
1524:           name: mercurygeneric
1525:         vault:
1526:           endPoint: "https://vault.query.prod.consul:8200"
1527:           path: "k8s-scl-dataplane"
1528:         consul:
1529:           region:
1530:             key: scl
1531:         webhook:
1532:           host: engagement-mercury-psr.prod.channels.ocs.oraclecloud.com
1533:         service:
1534:           port: 8080
1535:           targetPort: 8080
1536:         image:
1537:           repository: iad.ocir.io/osvcstage
1538:           initImage: init-container/prod:1.0.0.18
1539:           secret: osvcstage-ocirsecret
1540:         ingress:
1541:           class: public
1542:           annotations:
1543:             ingress.kubernetes.io/ssl-redirect: "true"
1544:           hosts:
1545:             - engagement-psr.us1.channels.ocs.oraclecloud.com
1546:           tls:
1547:             - secretName: engagement-psr-tls-cert
1548:               hosts:
1549:                 - engagement-psr.us1.channels.ocs.oraclecloud.com
1550:         kafka:
1551:           bootstrapServers: "mercury-psr-kafka-bootstrap-mercury-psr.service.scl-dataplane.prod.consul:443"
1552:           tmsBootstrapServers: "shared-kafka-bootstrap-kafka.service.scl-dataplane.prod.consul:443"
1553:         schemaRegistry:
1554:           url: "https://schemaregistry-proxy-kafka.service.scl-dataplane.prod.consul"
1555:         podAnnotations:
1556:           vault.security.banzaicloud.io/log-level: "warn"
1557:           vault.security.banzaicloud.io/vault-addr: "https://vault.query.prod.consul:8200"
1558:           vault.security.banzaicloud.io/vault-role: "mercury-psr"
1559:           vault.security.banzaicloud.io/vault-path: "k8s-scl-dataplane"
1560:           vault.security.banzaicloud.io/vault-skip-verify: "true"
1561:           vault.security.banzaicloud.io/vault-env-daemon: "true"
1562:           vault.security.banzaicloud.io/vault-agent-configmap: "mercury-psr-va-configmap"
1563:         keystore:
1564:           tokenPath: /vault/.vault-token
1565:           location: /vault/secrets/
1566:         extraEnvVars:
1567:           - name: AUTO_REFRESH_CONSUL_TOKEN
1568:             value: "vault:cpe_consul/creds/mercury-psr#token"
1569:             # These commands would have to be executed before anything else that tries to use these files
1570:             # in an init container.  If not using an init container, none of this is necessary.
1571:             # $ echo $KAFKA_TEMPLATED_PKI | jq .certificate -r > /tmp/certificate.crt
1572:             # $ echo $KAFKA_TEMPLATED_PKI | jq .issuing_ca -r > /tmp/issuing_ca.crt
1573:             # $ echo $KAFKA_TEMPLATED_PKI | jq .private_key -r > /tmp/private.key
1574:           - name: KAFKA_TEMPLATED_PKI
1575:             value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate" .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw", "ttl": "720h"}'
1576:

merged environment: &{prod map[consul_dns_cluster_ip:10.97.10.10 dns_zone_name:ocs.oraclecloud.com oci_home_region:us-ashburn-1 oke_cluster_types:[dataplane] proxy:map[] realm:prod realmRegions:[map[abbreviation:us2 key:iad legacy_dns_servers:[10.134.0.21 10.134.0.63] name:us-ashburn-1 vault_sync:true] map[abbreviation:us1 deploy_region:true key:phx name:us-phoenix-1] map[abbreviation:eu2 key:ams name:eu-amsterdam-1] map[abbreviation:eu1 key:fra name:eu-frankfurt-1] map[abbreviation:uk2 key:cwl name:uk-cardiff-1] map[abbreviation:uk1 key:lhr name:uk-london-1] map[abbreviation:ap5 key:mel name:ap-melbourne-1] map[abbreviation:ap3 key:syd name:ap-sydney-1] map[abbreviation:ca1 key:yyz name:ca-toronto-1] map[abbreviation:ca2 key:yul name:ca-montreal-1] map[abbreviation:ap6 key:hyd name:ap-hyderabad-1] map[abbreviation:ap2 key:bom name:ap-mumbai-1] map[abbreviation:ap4 key:kix name:ap-osaka-1] map[abbreviation:ap1 key:nrt name:ap-tokyo-1] map[abbreviation:sa1 key:gru name:sa-saopaulo-1] map[abbreviation:sa3 key:vcp name:sa-vinhedo-1] map[abbreviation:me2 key:jed name:me-jeddah-1] map[abbreviation:me1 key:dxb name:me-dubai-1] map[abbreviation:eu3 key:zrh name:eu-zurich-1] map[abbreviation:sa2 key:scl name:sa-santiago-1]] schemaRegistryMasterRegion:us-ashburn-1 tenancy_name:osvcprod tenancy_ocid:ocid1.tenancy.oc1..aaaaaaaahqmm3ceg4kw7aqygde6vc66jlnypuxpgd6k623tzv2u6e43i7vkq] map[]}
helm:XVlBz> v3.6.1+g61d8e8c
1 release(s) matching region=us-phoenix-1 found in helmfile-mercury-psr-apps.yaml

Affected releases are:
  mercury (osvc-helm-virtual/mercury) UPDATED

processing 1 groups of releases in this order:
GROUP RELEASES
1     prod_us-phoenix-1_dataplane/mercury-psr/mercury

processing releases in group 1/1: prod_us-phoenix-1_dataplane/mercury-psr/mercury
Successfully generated the value file at ../../vars/charts/mercury/deployment.yaml. produced:
# These URLs can be passed as a command line parameter to a service by adding a property
# such as addPreferencesURL : true or addIntegrationURL : true
# to their values.yaml
global:
  preferencesURL: http://mercury-user-preference-service:8080
  sessionCommandURL: http://mercury-session-command-service:8080
  sessionQueryURL: http://mercury-session-query-service:8080
  integrationURL: http://mercury-integration-in-processor:8080
  osvcBridgeStateQueryURL: http://mercury-osvc-bridge-state-query-service:8080
  osvcBridgeApiServicesURL: http://mercury-osvc-bridge-api-services:8080

  ingress:
    apiVersion: networking.k8s.io/v1beta1
    # DO NOT CHANGE apiVersion until all ingress fields in the common chart are changed to their new accepted values.
    # See https://stackoverflow.com/questions/64125048/get-error-unknown-field-servicename-in-io-k8s-api-networking-v1-ingressbacken
    # apiVersion: networking.k8s.io/v1

kweet-facebook-client:
  projectname: mercury
  replicaCount: 1
  global:
    serviceAccount:
      create: false
      name: mercurygeneric
    mercury:
      enabled: true
    vault:
      token: vault:login

kweet-facebook-webhook:
  projectname: mercury
  replicaCount: 1
  global:
    serviceAccount:
      create: false
      name: mercurygeneric
    mercury:
      enabled: true
    vault:
      token: vault:login
  ingress:
    enabled: true
    hosts:
      - engagement-{{ $namespace }}-{{ $ingressRegion }}.{{ $realm }}.channels.{{ .Values.dns_zone_name }}
    tls:
      - secretName: star.channels.{{ .Values.dns_zone_name }}
        hosts:
          - engagement-{{ $namespace }}-{{ $ingressRegion }}.{{ $realm }}.channels.{{ .Values.dns_zone_name }}
    secretName: star.channels.{{ .Values.dns_zone_name }}

kweet-twiliosms-client:
  projectname: mercury
  replicaCount: 1
  global:
    serviceAccount:
      create: false
      name: mercurygeneric
    mercury:
      enabled: true
    vault:
      token: vault:login
  ingress:
    enabled: true
    hosts:
      - engagement-{{ $namespace }}-{{ $ingressRegion }}.{{ $realm }}.channels.{{ .Values.dns_zone_name }}
    tls:
      - secretName: star.channels.{{ .Values.dns_zone_name }}
        hosts:
          - engagement-{{ $namespace }}-{{ $ingressRegion }}.{{ $realm }}.channels.{{ .Values.dns_zone_name }}
    secretName: star.channels.{{ .Values.dns_zone_name }}

kweet-wechat-client:
  projectname: mercury
  replicaCount: 1
  global:
    serviceAccount:
      create: false
      name: mercurygeneric
    mercury:
      enabled: true
    vault:
      token: vault:login

kweet-wechat-webhook:
  projectname: mercury
  replicaCount: 1
  global:
    serviceAccount:
      create: false
      name: mercurygeneric
    mercury:
      enabled: true
    vault:
      token: vault:login
  ingress:
    enabled: true
    hosts:
      - engagement-{{ $namespace }}-{{ $ingressRegion }}.{{ $realm }}.channels.{{ .Values.dns_zone_name }}
    tls:
      - secretName: star.channels.{{ .Values.dns_zone_name }}
        hosts:
          - engagement-{{ $namespace }}-{{ $ingressRegion }}.{{ $realm }}.channels.{{ .Values.dns_zone_name }}
    secretName: star.channels.{{ .Values.dns_zone_name }}

kweet-userprofiles:
  projectname: mercury
  replicaCount: 1
  global:
    serviceAccount:
      create: false
      name: mercurygeneric
    mercury:
      enabled: true
    vault:
      token: vault:login

provisioning-processor:
  provisioning:
    syncUpstreamData: true

static-assets-service:
  # Switch off the initContainer
  initContainers:
    enabled: false
  ingress:
    paths:
      -
        scoped: true
        uri: (.*)(/Chat)/(.*)
  resources:
    limits:
      cpu: 200m
      ephemeral-storage: 1Gi
      memory: 60Mi
    requests:
      cpu: 50m
      ephemeral-storage: 1Gi
      memory: 30Mi

transcript-api:
  ingress:
    enabled: true
    annotations:
      nginx.ingress.kubernetes.io/rewrite-target: /engagement/api/transcript/$2/$3
      nginx.ingress.kubernetes.io/configuration-snippet: |
        more_set_input_headers "TenantID: $1";
    paths:
      - scoped: true
        uri: (.*)/engagement/api/transcript/(v1|v2)/(.*)

social-config:
  global:
    consul:
      enabled: false
Successfully generated the value file at ../../vars/charts/mercury/prod/deployment.yaml. produced:
global:
  octoEnv: prod
kweet-facebook-client:
  global:
    slack:
      api:
        enabled: true
        channel: "#om-alerts-prod"
kweet-facebook-webhook:
  global:
    slack:
      api:
        enabled: true
        channel: "#om-alerts-prod"
kweet-twiliosms-client:
  global:
    slack:
      api:
        enabled: true
        channel: "#om-alerts-prod"
kweet-userprofiles:
  global:
    slack:
      api:
        enabled: true
        channel: "#om-alerts-prod"
Successfully generated the value file at ../../vars/charts/mercury/prod/mercury-psr/deployment.yaml. produced:
# Mercury deployment - large footprint - prod mercury-psr

#By default services are allocated thusly:
#resources:
#  requests:
#    cpu: "100m"
#    memory: "2Gi"
#    ephemeral-storage: "2Gi"
#  limits:
#    cpu: "1"
#    memory: "6Gi"
#    ephemeral-storage: "10Gi"

global:
  tenantNameFilterValue: "^(etec)"

agent-command-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 4
      memory: 4Gi
      ephemeral-storage: 2Gi
    limits:
      cpu: 6
      memory: 10Gi
      ephemeral-storage: 50Gi
  addOsvcBridgeApiServicesURL: true
  ingress2:
    enabled: false
  ingress3:
    enabled: false
  ingress4:
    enabled: false

channel-api:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  # disable channel API until LX is officially supported
  replicaCount: 0

consumer-command-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 2
      memory: 3Gi
      ephemeral-storage: 2Gi
    limits:
      cpu: 4
      memory: 6Gi
      ephemeral-storage: 50Gi
  ingress2:
    enabled: false
  ingress3:
    enabled: false
  ingress4:
    enabled: false

custom-availability-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

data-mask-api:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    limits:
      cpu: 800m
      ephemeral-storage: 1Gi
      memory: 6Gi
    requests:
      cpu: 100m
      ephemeral-storage: 1Gi
      memory: 3Gi
  ingress:
    enabled: true
  ingress1:
    enabled: false

engagement-queue-api:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

enrichment-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  mercuryCommon:
    tenantNameFilter: true

event-sync-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  mercuryCommon:
    tenantNameFilter: true

integration-in-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 1500m
      memory: 4Gi
      ephemeral-storage: 10Gi

integration-out-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 1500m
      memory: 4Gi
      ephemeral-storage: 10Gi

kweet-facebook-client:
  replicaCount: 3
  resources:
    limits:
      cpu: "2"
      memory: 3Gi
    requests:
      cpu: 200m
      memory: 2Gi
  global:
    service:
      livenessProbe:
        initialDelaySeconds: 180
      readinessProbe:
        initialDelaySeconds: 180

kweet-facebook-webhook:
  replicaCount: 3
  resources:
    limits:
      cpu: "2"
      memory: 3Gi
    requests:
      cpu: 100m
      memory: 2Gi
  global:
    service:
      livenessProbe:
        initialDelaySeconds: 180
      readinessProbe:
        initialDelaySeconds: 180

kweet-twiliosms-client:
  replicaCount: 3
  global:
    service:
      livenessProbe:
        initialDelaySeconds: 180
      readinessProbe:
        initialDelaySeconds: 180

kweet-userprofiles:
  replicaCount: 3
  global:
    service:
      livenessProbe:
        initialDelaySeconds: 180
      readinessProbe:
        initialDelaySeconds: 180

kweet-wechat-client:
  # Until WeChat is supported, do not deploy the services in production
  replicaCount: 0
  resources:
    limits:
      cpu: "2"
      memory: 3Gi
    requests:
      cpu: 200m
      memory: 2Gi
  global:
    service:
      livenessProbe:
        initialDelaySeconds: 180
      readinessProbe:
        initialDelaySeconds: 180

kweet-wechat-webhook:
  # Until WeChat is supported, do not deploy the services in production
  replicaCount: 0
  resources:
    limits:
      cpu: "2"
      memory: 3Gi
    requests:
      cpu: 200m
      memory: 2Gi
  global:
    service:
      livenessProbe:
        initialDelaySeconds: 180
      readinessProbe:
        initialDelaySeconds: 180

mercury-ui:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  mercuryCommon:
    http:
      rest:
        security:
          headers:
            enabled: false
  singleSignOn:
    authentication: true

metric-aggregation-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    limits:
      cpu: 2
      ephemeral-storage: 5Gi
      memory: 6Gi
    requests:
      cpu: 1
      ephemeral-storage: 1Gi
      memory: 3Gi

metric-fusion-bridge:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

metric-generation-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    limits:
      cpu: 2
      ephemeral-storage: 10Gi
      memory: 6Gi
    requests:
      cpu: 500m
      ephemeral-storage: 2Gi
      memory: 3Gi

metric-internal-translation-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

metric-proxy-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold:
  replicaCount: 3
  resources:
    requests:
      cpu: 1
      memory: 3Gi
      ephemeral-storage: 2Gi
    limits:
      ephemeral-storage: 15Gi

omnichannel-assignment-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 1
      ephemeral-storage: 2Gi
    limits:
      cpu: 2
      ephemeral-storage: 10Gi

omnichannel-offer-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 1
      ephemeral-storage: 2Gi
    limits:
      cpu: 2
      ephemeral-storage: 10Gi

osvc-bridge-api-services:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

osvc-bridge-metrics-data-pipeline:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

osvc-bridge-osvc-data-extractor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  # To resolve the url while using umbrella chart.
  addOsvcBridgeStateQueryURL: true

osvc-bridge-provisioning-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 1

osvc-bridge-state-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

osvc-bridge-state-query-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

osvc-bridge-task-controller:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

provisioning-monitor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

provisioning-processor:
  javaopts: "-Dmercury.provisioning.max.time.mins=29"
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  provisioning:
    syncUpstreamData: true
    provisionPdb: false
    whiteListIp : false
    whiteListIpAddress : ""
  singleSignOn:
    samlURLs: true
  mercuryCommon:
    tenantNameFilter: true

queue-agent-info-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 1
      memory: 3Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 2
      memory: 6Gi
      ephemeral-storage: 10Gi

realtime-channel-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 800m
      memory: 3Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 2
      memory: 4Gi
      ephemeral-storage: 10Gi

resource-channel-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 2
      memory: 4Gi
      ephemeral-storage: 10Gi

resource-work-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 1
      memory: 2Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 2
      memory: 4Gi
      ephemeral-storage: 10Gi

resource-state-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

routing-processor-agent-assignment:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 500m
    limits:
      cpu: 2

routing-processor-agent-events-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold:
  replicaCount: 3
  resources:
    requests:
      cpu: 500m
    limits:
      cpu: 2

routing-processor-queue-assignment:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 500m

routing-processor-work-events-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 500m

session-housekeeping-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  javaopts: "-Dsession.housekeeping.watchdog.timer.ms=600000"

session-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

single-sign-on-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  singleSignOn:
    samlURLs: true

social-bridge:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

social-config:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 150m
      memory: 3Gi
    limits:
      cpu: 400m
      memory: 6Gi

static-assets-service:
  replicaCount: 3
  resources:
    requests:
      cpu: 100m
      ephemeral-storage: 2Gi
      memory: 1Gi
    limits:
      cpu: 500m
      ephemeral-storage: 10Gi
      memory: 2Gi

tenant-downtime-monitor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

transcript-api:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  ingress1:
    enabled: false
  ingress2:
    enabled: false
  ingress3:
    enabled: false

transcript-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 1
      memory: 6Gi
      ephemeral-storage: 50Gi

user-preference-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

work-api:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  ingress1:
    enabled: false
  ingress2:
    enabled: false
  ingress3:
    enabled: false
  ingress4:
    enabled: false

work-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 2
      memory: 3Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 3
      memory: 8Gi
      ephemeral-storage: 10Gi

Successfully generated the value file at ../../vars/charts/mercury/prod/mercury-psr/us-phoenix-1_deployment.yaml. produced:
# Mercury deployment - large footprint - prod, phoenix, mercury-psr

#By default services are allocated thusly:
#resources:
#  requests:
#    cpu: "100m"
#    memory: "2Gi"
#    ephemeral-storage: "2Gi"
#  limits:
#    cpu: "1"
#    memory: "6Gi"
#    ephemeral-storage: "10Gi"

global:
  tenantNameFilterValue: "^(evai)"

agent-command-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 4
      memory: 4Gi
      ephemeral-storage: 2Gi
    limits:
      cpu: 6
      memory: 10Gi
      ephemeral-storage: 50Gi
  addOsvcBridgeApiServicesURL: true
  ingress2:
    enabled: false
  ingress3:
    enabled: false
  ingress4:
    enabled: false

channel-api:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  # disable channel API until LX is officially supported
  replicaCount: 0

consumer-command-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 2
      memory: 3Gi
      ephemeral-storage: 2Gi
    limits:
      cpu: 4
      memory: 6Gi
      ephemeral-storage: 50Gi
  ingress2:
    enabled: false
  ingress3:
    enabled: false
  ingress4:
    enabled: false

custom-availability-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

data-mask-api:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    limits:
      cpu: 800m
      ephemeral-storage: 1Gi
      memory: 6Gi
    requests:
      cpu: 100m
      ephemeral-storage: 1Gi
      memory: 3Gi
  ingress:
    enabled: true
  ingress1:
    enabled: false

engagement-queue-api:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

enrichment-service:
  javaopts: "-XX:+UseG1GC -XX:MaxGCPauseMillis=250 -DwebHook.topic.name=helios-psr.priority.external.event -Dhelios.error.event=helios-psr.error.event"
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  mercuryCommon:
    tenantNameFilter: true
  global:
    webhookTopicName: "helios-psr.priority.external.event"
    webhookErrorTopicName: "helios-psr.error.event"

event-sync-service:
  javaopts: "-XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Dwebhook.topic.name=helios-psr.external.event -Dhelios.error.event=helios-psr.error.event"
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  mercuryCommon:
    tenantNameFilter: true
  global:
    webhookTopicName: "helios-psr.external.event"
    webhookErrorTopicName: "helios-psr.error.event"

integration-in-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 1500m
      memory: 4Gi
      ephemeral-storage: 10Gi

integration-out-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 1500m
      memory: 4Gi
      ephemeral-storage: 10Gi

kweet-facebook-client:
  replicaCount: 3
  resources:
    limits:
      cpu: "2"
      memory: 3Gi
    requests:
      cpu: 200m
      memory: 2Gi
  global:
    service:
      livenessProbe:
        initialDelaySeconds: 180
      readinessProbe:
        initialDelaySeconds: 180

kweet-facebook-webhook:
  replicaCount: 3
  resources:
    limits:
      cpu: "2"
      memory: 3Gi
    requests:
      cpu: 100m
      memory: 2Gi
  global:
    service:
      livenessProbe:
        initialDelaySeconds: 180
      readinessProbe:
        initialDelaySeconds: 180

kweet-twiliosms-client:
  replicaCount: 3
  global:
    service:
      livenessProbe:
        initialDelaySeconds: 180
      readinessProbe:
        initialDelaySeconds: 180

kweet-userprofiles:
  replicaCount: 3
  global:
    service:
      livenessProbe:
        initialDelaySeconds: 180
      readinessProbe:
        initialDelaySeconds: 180

kweet-wechat-client:
  # Until WeChat is supported, do not deploy the services in production
  replicaCount: 0
  resources:
    limits:
      cpu: "2"
      memory: 3Gi
    requests:
      cpu: 200m
      memory: 2Gi
  global:
    service:
      livenessProbe:
        initialDelaySeconds: 180
      readinessProbe:
        initialDelaySeconds: 180

kweet-wechat-webhook:
  # Until WeChat is supported, do not deploy the services in production
  replicaCount: 0
  resources:
    limits:
      cpu: "2"
      memory: 3Gi
    requests:
      cpu: 200m
      memory: 2Gi
  global:
    service:
      livenessProbe:
        initialDelaySeconds: 180
      readinessProbe:
        initialDelaySeconds: 180

mercury-ui:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  mercuryCommon:
    http:
      rest:
        security:
          headers:
            enabled: false
  singleSignOn:
    authentication: true

metric-aggregation-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    limits:
      cpu: 2
      ephemeral-storage: 5Gi
      memory: 6Gi
    requests:
      cpu: 1
      ephemeral-storage: 1Gi
      memory: 3Gi

metric-fusion-bridge:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

metric-generation-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    limits:
      cpu: 2
      ephemeral-storage: 10Gi
      memory: 6Gi
    requests:
      cpu: 500m
      ephemeral-storage: 2Gi
      memory: 3Gi

metric-internal-translation-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

metric-proxy-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold:
  replicaCount: 3
  resources:
    requests:
      cpu: 1
      memory: 3Gi
      ephemeral-storage: 2Gi
    limits:
      ephemeral-storage: 15Gi

omnichannel-assignment-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 1
      ephemeral-storage: 2Gi
    limits:
      cpu: 2
      ephemeral-storage: 10Gi

omnichannel-offer-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 1
      ephemeral-storage: 2Gi
    limits:
      cpu: 2
      ephemeral-storage: 10Gi

osvc-bridge-api-services:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

osvc-bridge-metrics-data-pipeline:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

osvc-bridge-osvc-data-extractor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  # To resolve the url while using umbrella chart.
  addOsvcBridgeStateQueryURL: true

osvc-bridge-provisioning-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 1

osvc-bridge-state-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

osvc-bridge-state-query-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

osvc-bridge-task-controller:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

provisioning-monitor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

provisioning-processor:
  javaopts: "-Dmercury.provisioning.max.time.mins=29 -Dwebhook.service.base.uri=http://authentication-service-helios.helios-psr.svc.cluster.local:80"
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  provisioning:
    syncUpstreamData: true
    provisionPdb: false
    whiteListIp : false
    whiteListIpAddress : ""
  singleSignOn:
    samlURLs: true
  mercuryCommon:
    tenantNameFilter: true
  global:
    webhookBaseUri: "http://authentication-service-helios.helios-psr.svc.cluster.local:80"

queue-agent-info-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 1
      memory: 3Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 2
      memory: 6Gi
      ephemeral-storage: 10Gi

realtime-channel-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 800m
      memory: 3Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 2
      memory: 4Gi
      ephemeral-storage: 10Gi

resource-channel-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 2
      memory: 4Gi
      ephemeral-storage: 10Gi

resource-work-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 1
      memory: 2Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 2
      memory: 4Gi
      ephemeral-storage: 10Gi

resource-state-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

routing-processor-agent-assignment:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 500m
    limits:
      cpu: 2

routing-processor-agent-events-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold:
  replicaCount: 3
  resources:
    requests:
      cpu: 500m
    limits:
      cpu: 2

routing-processor-queue-assignment:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 500m

routing-processor-work-events-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 500m

session-housekeeping-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  javaopts: "-Dsession.housekeeping.watchdog.timer.ms=600000"

session-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

single-sign-on-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  singleSignOn:
    samlURLs: true

social-bridge:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

social-config:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 150m
      memory: 3Gi
    limits:
      cpu: 400m
      memory: 6Gi

static-assets-service:
  replicaCount: 3
  resources:
    requests:
      cpu: 100m
      ephemeral-storage: 2Gi
      memory: 1Gi
    limits:
      cpu: 500m
      ephemeral-storage: 10Gi
      memory: 2Gi

tenant-downtime-monitor:
  javaopts: "-XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Dwebhook.death.topic.name=helios-psr.death.event -Dwebhook.base.url=http://authentication-service-helios.helios-psr.svc.cluster.local:80"
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  global:
    webhookDeathTopicName: "helios-psr.death.event"
    webhookBaseUri: "http://authentication-service-helios.helios-psr.svc.cluster.local:80"

transcript-api:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  ingress1:
    enabled: false
  ingress2:
    enabled: false
  ingress3:
    enabled: false

transcript-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 1
      memory: 6Gi
      ephemeral-storage: 50Gi

user-preference-service:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3

work-api:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  ingress1:
    enabled: false
  ingress2:
    enabled: false
  ingress3:
    enabled: false
  ingress4:
    enabled: false

work-processor:
  startupProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 120
  livenessProbe:
    httpGet:
      path: /health/checks
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ping
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 1
  replicaCount: 3
  resources:
    requests:
      cpu: 2
      memory: 3Gi
      ephemeral-storage: 5Gi
    limits:
      cpu: 3
      memory: 8Gi
      ephemeral-storage: 10Gi

Upgrading release=mercury, chart=osvc-helm-virtual/mercury
exec: helm upgrade --install --reset-values mercury osvc-helm-virtual/mercury --version 2201.18.1305 --wait --timeout 900s --kube-context prod_us-phoenix-1_dataplane --namespace mercury-psr --values /tmp/helmfile364217307/mercury-psr-mercury-values-c85994cc5 --values /tmp/helmfile682140030/mercury-psr-mercury-values-578596c79f --values /tmp/helmfile952633541/mercury-psr-mercury-values-6c6cbcdc6d --values /tmp/helmfile281924448/mercury-psr-mercury-values-69b49465f --values /tmp/helmfile552984383/mercury-psr-mercury-values-5dccb995c8 --values /tmp/helmfile276845458/mercury-psr-mercury-values-6b7b84b58 --history-max 4 --debug
helm:gbaiC> history.go:56: [debug] getting history for release mercury
helm:gbaiC> index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.02.24-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.02.24-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0001-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.04-b0001-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.04-b0002-SNAPSHOT" is invalid
helm:gbaiC> index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0001-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0001-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0003-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0003-SNAPSHOT" is invalid
helm:gbaiC> index.go:339: skipping loading invalid entry for chart "dcs-api" "version.0.1" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "version.0.1" is invalid
helm:gbaiC> index.go:339: skipping loading invalid entry for chart "escalation-aggregator-service" "1-21.03.04-b0019-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1-21.03.04-b0019-SNAPSHOT" is invalid
helm:gbaiC> index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.03-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.03.03-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.04-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.03.04-b0002-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0006-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.18-b0006-SNAPSHOT" is invalid
helm:gbaiC> index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0007-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.18-b0007-SNAPSHOT" is invalid
index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.19-b0005-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.19-b0005-SNAPSHOT" is invalid
helm:gbaiC> index.go:339: skipping loading invalid entry for chart "api-tracker" "version.0.1" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "version.0.1" is invalid
helm:gbaiC> index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.06-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.06-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.12-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.12-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.13-b0004" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.13-b0004" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.14-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.14-b0002" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.21-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.21-b0001" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.28-b0005" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.28-b0005" is invalid
helm:gbaiC> index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.03-b0001" is invalid
helm:gbaiC> index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0006" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.03-b0006" is invalid
helm:gbaiC> index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.06-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.06-b0002" is invalid
index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.10-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.10-b0002" is invalid
helm:gbaiC> index.go:339: skipping loading invalid entry for chart "kweet-config" "2111.08.1834-01" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "2111.08.1834-01" is invalid
helm:gbaiC> upgrade.go:123: [debug] preparing upgrade for mercury
helm:gbaiC> upgrade.go:437: [debug] resetting values to the chart's original version
helm:gbaiC> upgrade.go:131: [debug] performing update for mercury
helm:gbaiC> upgrade.go:303: [debug] creating upgraded release for mercury
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-agent-command-service-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-channel-api-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-consumer-command-service-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-custom-availability-service-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-data-mask-api-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-engagement-queue-api-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-enrichment-service-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-event-sync-service-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-integration-in-processor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-integration-out-processor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-kweet-facebook-client-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-kweet-facebook-webhook-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-kweet-twiliosms-client-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-kweet-userprofiles-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-kweet-wechat-client-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-kweet-wechat-webhook-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-mercury-ui-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-metric-aggregation-processor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-metric-fusion-bridge-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-metric-generation-processor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-metric-internal-translation-processor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-metric-proxy-service-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-omnichannel-assignment-processor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-omnichannel-offer-processor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-osvc-bridge-api-services-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-osvc-bridge-metrics-data-pipeline-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-osvc-bridge-osvc-data-extractor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-osvc-bridge-provisioning-processor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-osvc-bridge-state-processor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-osvc-bridge-state-query-service-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-osvc-bridge-task-controller-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-provisioning-monitor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-provisioning-processor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-queue-agent-info-processor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-realtime-channel-processor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-resource-channel-processor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-resource-state-processor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-resource-work-processor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-routing-processor-agent-assignment-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-routing-processor-agent-events-processor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-routing-processor-queue-assignment-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-routing-processor-work-events-processor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-session-housekeeping-processor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-session-processor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-single-sign-on-service-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-social-bridge-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-social-config-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-static-assets-service-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-tenant-downtime-monitor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-transcript-api-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-transcript-processor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-user-preference-service-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-work-api-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:284: [debug] Starting delete for "mercury-work-processor-keystore" Secret
helm:gbaiC> client.go:122: [debug] creating 1 resource(s)
helm:gbaiC> client.go:203: [debug] checking 254 resources for changes
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-agent-command-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-agent-command-service-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-agent-command-service-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-channel-api"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-channel-api-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-consumer-command-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-consumer-command-service-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-consumer-command-service-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-custom-availability-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-custom-availability-service-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-custom-availability-service-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-data-mask-api"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-data-mask-api-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-engagement-queue-api"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-engagement-queue-api-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-engagement-queue-api-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-enrichment-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-enrichment-service-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-enrichment-service-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-event-sync-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-event-sync-service-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-event-sync-service-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-integration-in-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-integration-in-processor-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-integration-in-processor-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-integration-out-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-mercury-ui"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-aggregation-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-aggregation-processor-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-fusion-bridge"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-fusion-bridge-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-fusion-bridge-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-generation-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-generation-processor-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-internal-translation-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-internal-translation-processor-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-proxy-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-proxy-service-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-assignment-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-assignment-processor-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-assignment-processor-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-offer-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-offer-processor-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-offer-processor-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-api-services"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-api-services-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-metrics-data-pipeline"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-metrics-data-pipeline-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-osvc-data-extractor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-osvc-data-extractor-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-osvc-data-extractor-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-provisioning-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-provisioning-processor-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-provisioning-processor-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-state-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-state-processor-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-state-query-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-state-query-service-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-task-controller"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-task-controller-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-monitor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-monitor-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-alertroute"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-processor-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-processor-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-queue-agent-info-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-queue-agent-info-processor-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-realtime-channel-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-realtime-channel-processor-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-channel-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-channel-processor-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-state-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-state-processor-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-work-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-work-processor-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-assignment"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-assignment-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-assignment-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-events-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-events-processor-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-events-processor-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-queue-assignment"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-queue-assignment-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-queue-assignment-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-work-events-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-work-events-processor-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-work-events-processor-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-session-housekeeping-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-session-housekeeping-processor-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-session-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-session-processor-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-single-sign-on-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-single-sign-on-service-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-single-sign-on-service-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-bridge"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-bridge-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-bridge-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-config"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-config-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-static-assets-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-static-assets-service-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-static-assets-service-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-tenant-downtime-monitor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-tenant-downtime-monitor-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-tenant-downtime-monitor-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-api"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-api-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-api-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-processor-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-user-preference-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-user-preference-service-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-user-preference-service-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-api"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-api-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-api-rule"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-processor-dashboard"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-agent-command-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-channel-api"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-consumer-command-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-custom-availability-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-data-mask-api"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-engagement-queue-api"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-enrichment-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-event-sync-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-integration-in-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-integration-out-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-facebook-client"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-facebook-webhook"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-twiliosms-client"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-userprofiles"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-wechat-client"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-wechat-webhook"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-mercury-ui"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-aggregation-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-fusion-bridge"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-generation-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-internal-translation-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-proxy-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-omnichannel-assignment-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-omnichannel-offer-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-api-services"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-metrics-data-pipeline"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-osvc-data-extractor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-provisioning-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-state-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-state-query-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-task-controller"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-provisioning-monitor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-provisioning-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-queue-agent-info-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-realtime-channel-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-resource-channel-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-resource-state-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-resource-work-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-routing-processor-agent-assignment"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-routing-processor-agent-events-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-routing-processor-queue-assignment"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-routing-processor-work-events-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-session-housekeeping-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-session-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-single-sign-on-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-social-bridge"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-social-config"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-static-assets-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-tenant-downtime-monitor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-transcript-api"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-transcript-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-user-preference-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-work-api"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Service "mercury-work-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-channel-api"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-custom-availability-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-data-mask-api"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-engagement-queue-api"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-enrichment-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-event-sync-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-integration-in-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-integration-out-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-mercury-ui"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-metric-fusion-bridge"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-metric-internal-translation-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-api-services"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-metrics-data-pipeline"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-osvc-data-extractor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-provisioning-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-state-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-state-query-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-task-controller"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-provisioning-monitor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-provisioning-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-resource-state-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-routing-processor-queue-assignment"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-routing-processor-work-events-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-session-housekeeping-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-session-processor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-single-sign-on-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-social-config"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-static-assets-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-tenant-downtime-monitor"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-transcript-api"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-user-preference-service"
helm:gbaiC> client.go:466: [debug] Looks like there are no changes for Deployment "mercury-work-api"
helm:gbaiC> W0121 10:51:40.239880    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:40.533663    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:40.830005    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:41.697556    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:41.989600    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:42.288329    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:43.157453    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:43.450691    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:43.763280    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:44.635243    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:44.927715    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:45.222225    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:46.091759    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:46.385450    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:46.681737    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:47.549066    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:47.845722    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:48.147751    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:49.019432    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:49.314524    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:49.611738    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:50.526155    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:50.820661    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:51.114858    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:51.982170    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:52.277857    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:52.577752    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:53.446482    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:53.739619    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:54.037634    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:54.907593    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:55.197971    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:55.493446    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:56.573806    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:56.883932    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:57.182998    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:58.051343    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:58.343117    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:58.643249    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:59.608480    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:51:59.902235    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:00.208615    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:01.052621    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:01.345719    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:01.648512    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:02.528982    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:02.822898    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:03.118135    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:03.987798    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:04.280972    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:04.582584    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:05.452578    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:05.745376    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:06.051534    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:06.921028    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:07.214711    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:07.513487    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:08.382327    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:08.673364    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:08.966560    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:09.833919    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:10.129278    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:10.426203    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:11.296021    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:11.592808    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:11.898253    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:12.765761    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:13.061668    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:13.362258    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:14.240508    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:14.534605    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:14.834025    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:15.702931    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:16.016598    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:16.314238    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:17.181962    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:17.474177    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> W0121 10:52:17.782223    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
helm:gbaiC> wait.go:47: [debug] beginning wait for 254 resources with timeout of 15m0s
helm:gbaiC> ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
helm:gbaiC> ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
helm:gbaiC> ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
helm:gbaiC> ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
helm:gbaiC> ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
helm:gbaiC> ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
helm:gbaiC> ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
helm:gbaiC> ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
helm:gbaiC> ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
helm:gbaiC> ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
helm:gbaiC> ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
helm:gbaiC> ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
helm:gbaiC> ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
helm:gbaiC> ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
helm:gbaiC> ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
helm:gbaiC> ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
helm:gbaiC> ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
helm:gbaiC> ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
helm:gbaiC> upgrade.go:369: [debug] warning: Upgrade "mercury" failed: Get "https://130.35.130.41:6443/apis/apps/v1/namespaces/mercury-psr/deployments/mercury-provisioning-monitor": context deadline exceeded
helm:gbaiC> Error: UPGRADE FAILED: Get "https://130.35.130.41:6443/apis/apps/v1/namespaces/mercury-psr/deployments/mercury-provisioning-monitor": context deadline exceeded
helm.go:81: [debug] Get "https://130.35.130.41:6443/apis/apps/v1/namespaces/mercury-psr/deployments/mercury-provisioning-monitor": context deadline exceeded
UPGRADE FAILED
main.newUpgradeCmd.func2
  helm.sh/helm/v3/cmd/helm/upgrade.go:157
github.com/spf13/cobra.(*Command).execute
  github.com/spf13/cobra@v1.1.3/command.go:852
github.com/spf13/cobra.(*Command).ExecuteC
  github.com/spf13/cobra@v1.1.3/command.go:960
github.com/spf13/cobra.(*Command).Execute
  github.com/spf13/cobra@v1.1.3/command.go:897
main.main
  helm.sh/helm/v3/cmd/helm/helm.go:80
runtime.main
  runtime/proc.go:225
runtime.goexit
  runtime/asm_amd64.s:1371
Removed /tmp/helmfile364217307/mercury-psr-mercury-values-c85994cc5
Removed /tmp/helmfile682140030/mercury-psr-mercury-values-578596c79f
Removed /tmp/helmfile952633541/mercury-psr-mercury-values-6c6cbcdc6d
Removed /tmp/helmfile281924448/mercury-psr-mercury-values-69b49465f
Removed /tmp/helmfile552984383/mercury-psr-mercury-values-5dccb995c8
Removed /tmp/helmfile276845458/mercury-psr-mercury-values-6b7b84b58

FAILED RELEASES:
NAME
mercury
err: release "mercury" in "helmfile-mercury-psr-apps.yaml" failed: failed processing release mercury: command "/usr/local/bin/helm" exited with non-zero status:

PATH:
  /usr/local/bin/helm

ARGS:
  0: helm (4 bytes)
  1: upgrade (7 bytes)
  2: --install (9 bytes)
  3: --reset-values (14 bytes)
  4: mercury (7 bytes)
  5: osvc-helm-virtual/mercury (25 bytes)
  6: --version (9 bytes)
  7: 2201.18.1305 (12 bytes)
  8: --wait (6 bytes)
  9: --timeout (9 bytes)
  10: 900s (4 bytes)
  11: --kube-context (14 bytes)
  12: prod_us-phoenix-1_dataplane (27 bytes)
  13: --namespace (11 bytes)
  14: mercury-psr (11 bytes)
  15: --values (8 bytes)
  16: /tmp/helmfile364217307/mercury-psr-mercury-values-c85994cc5 (59 bytes)
  17: --values (8 bytes)
  18: /tmp/helmfile682140030/mercury-psr-mercury-values-578596c79f (60 bytes)
  19: --values (8 bytes)
  20: /tmp/helmfile952633541/mercury-psr-mercury-values-6c6cbcdc6d (60 bytes)
  21: --values (8 bytes)
  22: /tmp/helmfile281924448/mercury-psr-mercury-values-69b49465f (59 bytes)
  23: --values (8 bytes)
  24: /tmp/helmfile552984383/mercury-psr-mercury-values-5dccb995c8 (60 bytes)
  25: --values (8 bytes)
  26: /tmp/helmfile276845458/mercury-psr-mercury-values-6b7b84b58 (59 bytes)
  27: --history-max (13 bytes)
  28: 4 (1 bytes)
  29: --debug (7 bytes)

ERROR:
  exit status 1

EXIT STATUS
  1

STDERR:
  history.go:56: [debug] getting history for release mercury
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.02.24-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.02.24-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0001-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.04-b0001-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.04-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0001-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0001-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0003-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0003-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "dcs-api" "version.0.1" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "version.0.1" is invalid
  index.go:339: skipping loading invalid entry for chart "escalation-aggregator-service" "1-21.03.04-b0019-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1-21.03.04-b0019-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.03-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.03.03-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.04-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.03.04-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0006-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.18-b0006-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0007-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.18-b0007-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.19-b0005-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.19-b0005-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "api-tracker" "version.0.1" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "version.0.1" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.06-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.06-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.12-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.12-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.13-b0004" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.13-b0004" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.14-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.14-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.21-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.21-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.28-b0005" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.28-b0005" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.03-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0006" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.03-b0006" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.06-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.06-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.10-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.10-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "kweet-config" "2111.08.1834-01" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "2111.08.1834-01" is invalid
  upgrade.go:123: [debug] preparing upgrade for mercury
  upgrade.go:437: [debug] resetting values to the chart's original version
  upgrade.go:131: [debug] performing update for mercury
  upgrade.go:303: [debug] creating upgraded release for mercury
  client.go:284: [debug] Starting delete for "mercury-agent-command-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-channel-api-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-consumer-command-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-custom-availability-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-data-mask-api-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-engagement-queue-api-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-enrichment-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-event-sync-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-integration-in-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-integration-out-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-facebook-client-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-facebook-webhook-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-twiliosms-client-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-userprofiles-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-wechat-client-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-wechat-webhook-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-mercury-ui-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-metric-aggregation-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-metric-fusion-bridge-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-metric-generation-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-metric-internal-translation-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-metric-proxy-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-omnichannel-assignment-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-omnichannel-offer-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-api-services-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-metrics-data-pipeline-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-osvc-data-extractor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-provisioning-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-state-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-state-query-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-task-controller-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-provisioning-monitor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-provisioning-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-queue-agent-info-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-realtime-channel-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-resource-channel-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-resource-state-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-resource-work-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-routing-processor-agent-assignment-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-routing-processor-agent-events-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-routing-processor-queue-assignment-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-routing-processor-work-events-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-session-housekeeping-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-session-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-single-sign-on-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-social-bridge-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-social-config-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-static-assets-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-tenant-downtime-monitor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-transcript-api-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-transcript-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-user-preference-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-work-api-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-work-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:203: [debug] checking 254 resources for changes
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-agent-command-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-agent-command-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-agent-command-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-channel-api"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-channel-api-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-consumer-command-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-consumer-command-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-consumer-command-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-custom-availability-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-custom-availability-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-custom-availability-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-data-mask-api"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-data-mask-api-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-engagement-queue-api"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-engagement-queue-api-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-engagement-queue-api-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-enrichment-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-enrichment-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-enrichment-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-event-sync-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-event-sync-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-event-sync-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-integration-in-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-integration-in-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-integration-in-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-integration-out-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-mercury-ui"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-aggregation-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-aggregation-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-fusion-bridge"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-fusion-bridge-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-fusion-bridge-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-generation-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-generation-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-internal-translation-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-internal-translation-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-proxy-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-proxy-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-assignment-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-assignment-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-assignment-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-offer-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-offer-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-offer-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-api-services"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-api-services-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-metrics-data-pipeline"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-metrics-data-pipeline-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-osvc-data-extractor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-osvc-data-extractor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-osvc-data-extractor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-provisioning-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-provisioning-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-state-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-state-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-state-query-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-state-query-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-task-controller"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-task-controller-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-monitor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-monitor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-alertroute"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-queue-agent-info-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-queue-agent-info-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-realtime-channel-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-realtime-channel-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-channel-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-channel-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-state-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-state-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-work-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-work-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-assignment"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-assignment-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-assignment-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-events-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-events-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-events-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-queue-assignment"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-queue-assignment-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-queue-assignment-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-work-events-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-work-events-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-work-events-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-session-housekeeping-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-session-housekeeping-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-session-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-session-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-single-sign-on-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-single-sign-on-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-single-sign-on-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-bridge"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-bridge-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-bridge-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-config"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-config-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-static-assets-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-static-assets-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-static-assets-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-tenant-downtime-monitor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-tenant-downtime-monitor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-tenant-downtime-monitor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-api"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-api-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-api-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-user-preference-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-user-preference-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-user-preference-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-api"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-api-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-api-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-agent-command-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-channel-api"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-consumer-command-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-custom-availability-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-data-mask-api"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-engagement-queue-api"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-enrichment-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-event-sync-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-integration-in-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-integration-out-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-facebook-client"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-facebook-webhook"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-twiliosms-client"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-userprofiles"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-wechat-client"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-wechat-webhook"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-mercury-ui"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-aggregation-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-fusion-bridge"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-generation-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-internal-translation-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-proxy-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-omnichannel-assignment-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-omnichannel-offer-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-api-services"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-metrics-data-pipeline"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-osvc-data-extractor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-state-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-state-query-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-task-controller"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-provisioning-monitor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-queue-agent-info-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-realtime-channel-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-resource-channel-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-resource-state-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-resource-work-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-routing-processor-agent-assignment"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-routing-processor-agent-events-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-routing-processor-queue-assignment"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-routing-processor-work-events-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-session-housekeeping-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-session-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-single-sign-on-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-social-bridge"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-social-config"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-static-assets-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-tenant-downtime-monitor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-transcript-api"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-transcript-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-user-preference-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-work-api"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-work-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-channel-api"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-custom-availability-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-data-mask-api"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-engagement-queue-api"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-enrichment-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-event-sync-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-integration-in-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-integration-out-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-mercury-ui"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-metric-fusion-bridge"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-metric-internal-translation-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-api-services"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-metrics-data-pipeline"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-osvc-data-extractor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-state-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-state-query-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-task-controller"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-provisioning-monitor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-resource-state-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-routing-processor-queue-assignment"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-routing-processor-work-events-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-session-housekeeping-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-session-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-single-sign-on-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-social-config"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-static-assets-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-tenant-downtime-monitor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-transcript-api"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-user-preference-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-work-api"
  W0121 10:51:40.239880    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:40.533663    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:40.830005    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:41.697556    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:41.989600    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:42.288329    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:43.157453    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:43.450691    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:43.763280    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:44.635243    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:44.927715    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:45.222225    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:46.091759    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:46.385450    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:46.681737    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:47.549066    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:47.845722    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:48.147751    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:49.019432    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:49.314524    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:49.611738    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:50.526155    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:50.820661    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:51.114858    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:51.982170    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:52.277857    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:52.577752    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:53.446482    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:53.739619    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:54.037634    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:54.907593    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:55.197971    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:55.493446    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:56.573806    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:56.883932    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:57.182998    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:58.051343    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:58.343117    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:58.643249    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:59.608480    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:59.902235    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:00.208615    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:01.052621    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:01.345719    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:01.648512    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:02.528982    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:02.822898    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:03.118135    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:03.987798    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:04.280972    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:04.582584    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:05.452578    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:05.745376    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:06.051534    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:06.921028    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:07.214711    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:07.513487    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:08.382327    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:08.673364    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:08.966560    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:09.833919    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:10.129278    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:10.426203    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:11.296021    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:11.592808    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:11.898253    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:12.765761    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:13.061668    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:13.362258    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:14.240508    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:14.534605    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:14.834025    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:15.702931    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:16.016598    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:16.314238    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:17.181962    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:17.474177    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:17.782223    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  wait.go:47: [debug] beginning wait for 254 resources with timeout of 15m0s
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  upgrade.go:369: [debug] warning: Upgrade "mercury" failed: Get "https://130.35.130.41:6443/apis/apps/v1/namespaces/mercury-psr/deployments/mercury-provisioning-monitor": context deadline exceeded
  Error: UPGRADE FAILED: Get "https://130.35.130.41:6443/apis/apps/v1/namespaces/mercury-psr/deployments/mercury-provisioning-monitor": context deadline exceeded
  helm.go:81: [debug] Get "https://130.35.130.41:6443/apis/apps/v1/namespaces/mercury-psr/deployments/mercury-provisioning-monitor": context deadline exceeded
  UPGRADE FAILED
  main.newUpgradeCmd.func2
    helm.sh/helm/v3/cmd/helm/upgrade.go:157
  github.com/spf13/cobra.(*Command).execute
    github.com/spf13/cobra@v1.1.3/command.go:852
  github.com/spf13/cobra.(*Command).ExecuteC
    github.com/spf13/cobra@v1.1.3/command.go:960
  github.com/spf13/cobra.(*Command).Execute
    github.com/spf13/cobra@v1.1.3/command.go:897
  main.main
    helm.sh/helm/v3/cmd/helm/helm.go:80
  runtime.main
    runtime/proc.go:225
  runtime.goexit
    runtime/asm_amd64.s:1371

COMBINED OUTPUT:
  history.go:56: [debug] getting history for release mercury
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.02.24-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.02.24-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0001-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.04-b0001-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.04-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0001-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0001-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0003-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0003-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "dcs-api" "version.0.1" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "version.0.1" is invalid
  index.go:339: skipping loading invalid entry for chart "escalation-aggregator-service" "1-21.03.04-b0019-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1-21.03.04-b0019-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.03-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.03.03-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.04-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.03.04-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0006-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.18-b0006-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0007-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.18-b0007-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.19-b0005-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.19-b0005-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "api-tracker" "version.0.1" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "version.0.1" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.06-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.06-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.12-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.12-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.13-b0004" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.13-b0004" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.14-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.14-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.21-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.21-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.28-b0005" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.28-b0005" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.03-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0006" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.03-b0006" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.06-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.06-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.10-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.10-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "kweet-config" "2111.08.1834-01" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "2111.08.1834-01" is invalid
  upgrade.go:123: [debug] preparing upgrade for mercury
  upgrade.go:437: [debug] resetting values to the chart's original version
  upgrade.go:131: [debug] performing update for mercury
  upgrade.go:303: [debug] creating upgraded release for mercury
  client.go:284: [debug] Starting delete for "mercury-agent-command-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-channel-api-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-consumer-command-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-custom-availability-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-data-mask-api-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-engagement-queue-api-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-enrichment-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-event-sync-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-integration-in-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-integration-out-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-facebook-client-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-facebook-webhook-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-twiliosms-client-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-userprofiles-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-wechat-client-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-wechat-webhook-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-mercury-ui-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-metric-aggregation-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-metric-fusion-bridge-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-metric-generation-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-metric-internal-translation-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-metric-proxy-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-omnichannel-assignment-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-omnichannel-offer-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-api-services-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-metrics-data-pipeline-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-osvc-data-extractor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-provisioning-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-state-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-state-query-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-task-controller-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-provisioning-monitor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-provisioning-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-queue-agent-info-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-realtime-channel-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-resource-channel-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-resource-state-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-resource-work-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-routing-processor-agent-assignment-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-routing-processor-agent-events-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-routing-processor-queue-assignment-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-routing-processor-work-events-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-session-housekeeping-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-session-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-single-sign-on-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-social-bridge-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-social-config-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-static-assets-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-tenant-downtime-monitor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-transcript-api-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-transcript-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-user-preference-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-work-api-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-work-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:203: [debug] checking 254 resources for changes
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-agent-command-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-agent-command-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-agent-command-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-channel-api"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-channel-api-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-consumer-command-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-consumer-command-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-consumer-command-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-custom-availability-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-custom-availability-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-custom-availability-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-data-mask-api"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-data-mask-api-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-engagement-queue-api"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-engagement-queue-api-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-engagement-queue-api-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-enrichment-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-enrichment-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-enrichment-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-event-sync-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-event-sync-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-event-sync-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-integration-in-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-integration-in-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-integration-in-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-integration-out-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-mercury-ui"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-aggregation-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-aggregation-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-fusion-bridge"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-fusion-bridge-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-fusion-bridge-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-generation-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-generation-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-internal-translation-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-internal-translation-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-proxy-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-proxy-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-assignment-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-assignment-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-assignment-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-offer-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-offer-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-offer-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-api-services"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-api-services-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-metrics-data-pipeline"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-metrics-data-pipeline-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-osvc-data-extractor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-osvc-data-extractor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-osvc-data-extractor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-provisioning-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-provisioning-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-state-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-state-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-state-query-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-state-query-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-task-controller"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-task-controller-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-monitor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-monitor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-alertroute"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-queue-agent-info-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-queue-agent-info-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-realtime-channel-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-realtime-channel-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-channel-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-channel-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-state-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-state-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-work-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-work-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-assignment"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-assignment-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-assignment-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-events-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-events-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-events-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-queue-assignment"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-queue-assignment-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-queue-assignment-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-work-events-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-work-events-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-work-events-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-session-housekeeping-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-session-housekeeping-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-session-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-session-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-single-sign-on-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-single-sign-on-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-single-sign-on-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-bridge"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-bridge-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-bridge-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-config"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-config-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-static-assets-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-static-assets-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-static-assets-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-tenant-downtime-monitor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-tenant-downtime-monitor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-tenant-downtime-monitor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-api"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-api-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-api-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-user-preference-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-user-preference-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-user-preference-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-api"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-api-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-api-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-agent-command-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-channel-api"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-consumer-command-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-custom-availability-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-data-mask-api"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-engagement-queue-api"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-enrichment-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-event-sync-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-integration-in-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-integration-out-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-facebook-client"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-facebook-webhook"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-twiliosms-client"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-userprofiles"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-wechat-client"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-wechat-webhook"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-mercury-ui"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-aggregation-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-fusion-bridge"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-generation-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-internal-translation-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-proxy-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-omnichannel-assignment-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-omnichannel-offer-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-api-services"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-metrics-data-pipeline"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-osvc-data-extractor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-state-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-state-query-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-task-controller"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-provisioning-monitor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-queue-agent-info-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-realtime-channel-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-resource-channel-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-resource-state-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-resource-work-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-routing-processor-agent-assignment"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-routing-processor-agent-events-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-routing-processor-queue-assignment"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-routing-processor-work-events-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-session-housekeeping-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-session-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-single-sign-on-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-social-bridge"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-social-config"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-static-assets-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-tenant-downtime-monitor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-transcript-api"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-transcript-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-user-preference-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-work-api"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-work-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-channel-api"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-custom-availability-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-data-mask-api"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-engagement-queue-api"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-enrichment-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-event-sync-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-integration-in-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-integration-out-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-mercury-ui"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-metric-fusion-bridge"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-metric-internal-translation-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-api-services"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-metrics-data-pipeline"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-osvc-data-extractor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-state-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-state-query-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-task-controller"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-provisioning-monitor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-resource-state-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-routing-processor-queue-assignment"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-routing-processor-work-events-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-session-housekeeping-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-session-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-single-sign-on-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-social-config"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-static-assets-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-tenant-downtime-monitor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-transcript-api"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-user-preference-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-work-api"
  W0121 10:51:40.239880    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:40.533663    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:40.830005    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:41.697556    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:41.989600    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:42.288329    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:43.157453    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:43.450691    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:43.763280    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:44.635243    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:44.927715    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:45.222225    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:46.091759    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:46.385450    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:46.681737    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:47.549066    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:47.845722    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:48.147751    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:49.019432    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:49.314524    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:49.611738    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:50.526155    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:50.820661    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:51.114858    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:51.982170    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:52.277857    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:52.577752    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:53.446482    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:53.739619    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:54.037634    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:54.907593    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:55.197971    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:55.493446    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:56.573806    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:56.883932    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:57.182998    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:58.051343    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:58.343117    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:58.643249    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:59.608480    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:59.902235    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:00.208615    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:01.052621    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:01.345719    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:01.648512    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:02.528982    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:02.822898    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:03.118135    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:03.987798    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:04.280972    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:04.582584    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:05.452578    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:05.745376    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:06.051534    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:06.921028    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:07.214711    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:07.513487    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:08.382327    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:08.673364    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:08.966560    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:09.833919    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:10.129278    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:10.426203    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:11.296021    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:11.592808    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:11.898253    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:12.765761    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:13.061668    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:13.362258    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:14.240508    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:14.534605    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:14.834025    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:15.702931    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:16.016598    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:16.314238    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:17.181962    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:17.474177    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:17.782223    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  wait.go:47: [debug] beginning wait for 254 resources with timeout of 15m0s
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  upgrade.go:369: [debug] warning: Upgrade "mercury" failed: Get "https://130.35.130.41:6443/apis/apps/v1/namespaces/mercury-psr/deployments/mercury-provisioning-monitor": context deadline exceeded
  Error: UPGRADE FAILED: Get "https://130.35.130.41:6443/apis/apps/v1/namespaces/mercury-psr/deployments/mercury-provisioning-monitor": context deadline exceeded
  helm.go:81: [debug] Get "https://130.35.130.41:6443/apis/apps/v1/namespaces/mercury-psr/deployments/mercury-provisioning-monitor": context deadline exceeded
  UPGRADE FAILED
  main.newUpgradeCmd.func2
    helm.sh/helm/v3/cmd/helm/upgrade.go:157
  github.com/spf13/cobra.(*Command).execute
    github.com/spf13/cobra@v1.1.3/command.go:852
  github.com/spf13/cobra.(*Command).ExecuteC
    github.com/spf13/cobra@v1.1.3/command.go:960
  github.com/spf13/cobra.(*Command).Execute
    github.com/spf13/cobra@v1.1.3/command.go:897
  main.main
    helm.sh/helm/v3/cmd/helm/helm.go:80
  runtime.main
    runtime/proc.go:225
  runtime.goexit
    runtime/asm_amd64.s:1371
changing working directory back to "/home/opc/galorndon/osvc-platform/helm/helmfile-releases"
in mercury-psr/helmfile-mercury-psr-apps.yaml: failed processing release mercury: command "/usr/local/bin/helm" exited with non-zero status:

PATH:
  /usr/local/bin/helm

ARGS:
  0: helm (4 bytes)
  1: upgrade (7 bytes)
  2: --install (9 bytes)
  3: --reset-values (14 bytes)
  4: mercury (7 bytes)
  5: osvc-helm-virtual/mercury (25 bytes)
  6: --version (9 bytes)
  7: 2201.18.1305 (12 bytes)
  8: --wait (6 bytes)
  9: --timeout (9 bytes)
  10: 900s (4 bytes)
  11: --kube-context (14 bytes)
  12: prod_us-phoenix-1_dataplane (27 bytes)
  13: --namespace (11 bytes)
  14: mercury-psr (11 bytes)
  15: --values (8 bytes)
  16: /tmp/helmfile364217307/mercury-psr-mercury-values-c85994cc5 (59 bytes)
  17: --values (8 bytes)
  18: /tmp/helmfile682140030/mercury-psr-mercury-values-578596c79f (60 bytes)
  19: --values (8 bytes)
  20: /tmp/helmfile952633541/mercury-psr-mercury-values-6c6cbcdc6d (60 bytes)
  21: --values (8 bytes)
  22: /tmp/helmfile281924448/mercury-psr-mercury-values-69b49465f (59 bytes)
  23: --values (8 bytes)
  24: /tmp/helmfile552984383/mercury-psr-mercury-values-5dccb995c8 (60 bytes)
  25: --values (8 bytes)
  26: /tmp/helmfile276845458/mercury-psr-mercury-values-6b7b84b58 (59 bytes)
  27: --history-max (13 bytes)
  28: 4 (1 bytes)
  29: --debug (7 bytes)

ERROR:
  exit status 1

EXIT STATUS
  1

STDERR:
  history.go:56: [debug] getting history for release mercury
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.02.24-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.02.24-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0001-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.04-b0001-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.04-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0001-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0001-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0003-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0003-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "dcs-api" "version.0.1" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "version.0.1" is invalid
  index.go:339: skipping loading invalid entry for chart "escalation-aggregator-service" "1-21.03.04-b0019-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1-21.03.04-b0019-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.03-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.03.03-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.04-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.03.04-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0006-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.18-b0006-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0007-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.18-b0007-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.19-b0005-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.19-b0005-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "api-tracker" "version.0.1" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "version.0.1" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.06-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.06-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.12-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.12-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.13-b0004" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.13-b0004" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.14-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.14-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.21-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.21-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.28-b0005" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.28-b0005" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.03-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0006" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.03-b0006" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.06-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.06-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.10-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.10-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "kweet-config" "2111.08.1834-01" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "2111.08.1834-01" is invalid
  upgrade.go:123: [debug] preparing upgrade for mercury
  upgrade.go:437: [debug] resetting values to the chart's original version
  upgrade.go:131: [debug] performing update for mercury
  upgrade.go:303: [debug] creating upgraded release for mercury
  client.go:284: [debug] Starting delete for "mercury-agent-command-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-channel-api-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-consumer-command-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-custom-availability-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-data-mask-api-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-engagement-queue-api-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-enrichment-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-event-sync-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-integration-in-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-integration-out-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-facebook-client-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-facebook-webhook-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-twiliosms-client-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-userprofiles-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-wechat-client-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-wechat-webhook-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-mercury-ui-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-metric-aggregation-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-metric-fusion-bridge-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-metric-generation-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-metric-internal-translation-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-metric-proxy-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-omnichannel-assignment-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-omnichannel-offer-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-api-services-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-metrics-data-pipeline-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-osvc-data-extractor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-provisioning-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-state-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-state-query-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-task-controller-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-provisioning-monitor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-provisioning-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-queue-agent-info-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-realtime-channel-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-resource-channel-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-resource-state-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-resource-work-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-routing-processor-agent-assignment-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-routing-processor-agent-events-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-routing-processor-queue-assignment-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-routing-processor-work-events-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-session-housekeeping-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-session-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-single-sign-on-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-social-bridge-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-social-config-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-static-assets-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-tenant-downtime-monitor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-transcript-api-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-transcript-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-user-preference-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-work-api-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-work-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:203: [debug] checking 254 resources for changes
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-agent-command-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-agent-command-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-agent-command-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-channel-api"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-channel-api-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-consumer-command-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-consumer-command-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-consumer-command-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-custom-availability-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-custom-availability-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-custom-availability-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-data-mask-api"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-data-mask-api-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-engagement-queue-api"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-engagement-queue-api-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-engagement-queue-api-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-enrichment-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-enrichment-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-enrichment-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-event-sync-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-event-sync-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-event-sync-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-integration-in-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-integration-in-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-integration-in-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-integration-out-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-mercury-ui"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-aggregation-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-aggregation-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-fusion-bridge"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-fusion-bridge-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-fusion-bridge-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-generation-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-generation-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-internal-translation-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-internal-translation-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-proxy-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-proxy-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-assignment-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-assignment-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-assignment-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-offer-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-offer-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-offer-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-api-services"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-api-services-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-metrics-data-pipeline"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-metrics-data-pipeline-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-osvc-data-extractor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-osvc-data-extractor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-osvc-data-extractor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-provisioning-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-provisioning-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-state-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-state-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-state-query-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-state-query-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-task-controller"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-task-controller-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-monitor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-monitor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-alertroute"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-queue-agent-info-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-queue-agent-info-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-realtime-channel-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-realtime-channel-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-channel-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-channel-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-state-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-state-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-work-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-work-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-assignment"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-assignment-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-assignment-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-events-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-events-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-events-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-queue-assignment"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-queue-assignment-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-queue-assignment-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-work-events-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-work-events-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-work-events-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-session-housekeeping-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-session-housekeeping-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-session-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-session-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-single-sign-on-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-single-sign-on-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-single-sign-on-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-bridge"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-bridge-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-bridge-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-config"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-config-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-static-assets-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-static-assets-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-static-assets-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-tenant-downtime-monitor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-tenant-downtime-monitor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-tenant-downtime-monitor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-api"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-api-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-api-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-user-preference-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-user-preference-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-user-preference-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-api"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-api-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-api-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-agent-command-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-channel-api"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-consumer-command-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-custom-availability-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-data-mask-api"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-engagement-queue-api"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-enrichment-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-event-sync-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-integration-in-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-integration-out-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-facebook-client"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-facebook-webhook"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-twiliosms-client"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-userprofiles"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-wechat-client"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-wechat-webhook"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-mercury-ui"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-aggregation-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-fusion-bridge"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-generation-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-internal-translation-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-proxy-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-omnichannel-assignment-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-omnichannel-offer-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-api-services"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-metrics-data-pipeline"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-osvc-data-extractor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-state-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-state-query-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-task-controller"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-provisioning-monitor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-queue-agent-info-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-realtime-channel-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-resource-channel-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-resource-state-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-resource-work-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-routing-processor-agent-assignment"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-routing-processor-agent-events-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-routing-processor-queue-assignment"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-routing-processor-work-events-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-session-housekeeping-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-session-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-single-sign-on-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-social-bridge"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-social-config"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-static-assets-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-tenant-downtime-monitor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-transcript-api"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-transcript-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-user-preference-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-work-api"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-work-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-channel-api"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-custom-availability-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-data-mask-api"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-engagement-queue-api"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-enrichment-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-event-sync-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-integration-in-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-integration-out-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-mercury-ui"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-metric-fusion-bridge"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-metric-internal-translation-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-api-services"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-metrics-data-pipeline"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-osvc-data-extractor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-state-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-state-query-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-task-controller"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-provisioning-monitor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-resource-state-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-routing-processor-queue-assignment"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-routing-processor-work-events-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-session-housekeeping-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-session-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-single-sign-on-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-social-config"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-static-assets-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-tenant-downtime-monitor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-transcript-api"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-user-preference-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-work-api"
  W0121 10:51:40.239880    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:40.533663    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:40.830005    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:41.697556    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:41.989600    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:42.288329    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:43.157453    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:43.450691    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:43.763280    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:44.635243    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:44.927715    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:45.222225    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:46.091759    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:46.385450    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:46.681737    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:47.549066    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:47.845722    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:48.147751    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:49.019432    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:49.314524    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:49.611738    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:50.526155    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:50.820661    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:51.114858    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:51.982170    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:52.277857    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:52.577752    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:53.446482    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:53.739619    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:54.037634    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:54.907593    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:55.197971    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:55.493446    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:56.573806    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:56.883932    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:57.182998    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:58.051343    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:58.343117    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:58.643249    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:59.608480    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:59.902235    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:00.208615    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:01.052621    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:01.345719    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:01.648512    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:02.528982    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:02.822898    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:03.118135    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:03.987798    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:04.280972    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:04.582584    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:05.452578    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:05.745376    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:06.051534    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:06.921028    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:07.214711    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:07.513487    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:08.382327    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:08.673364    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:08.966560    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:09.833919    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:10.129278    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:10.426203    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:11.296021    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:11.592808    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:11.898253    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:12.765761    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:13.061668    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:13.362258    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:14.240508    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:14.534605    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:14.834025    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:15.702931    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:16.016598    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:16.314238    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:17.181962    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:17.474177    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:17.782223    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  wait.go:47: [debug] beginning wait for 254 resources with timeout of 15m0s
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  upgrade.go:369: [debug] warning: Upgrade "mercury" failed: Get "https://130.35.130.41:6443/apis/apps/v1/namespaces/mercury-psr/deployments/mercury-provisioning-monitor": context deadline exceeded
  Error: UPGRADE FAILED: Get "https://130.35.130.41:6443/apis/apps/v1/namespaces/mercury-psr/deployments/mercury-provisioning-monitor": context deadline exceeded
  helm.go:81: [debug] Get "https://130.35.130.41:6443/apis/apps/v1/namespaces/mercury-psr/deployments/mercury-provisioning-monitor": context deadline exceeded
  UPGRADE FAILED
  main.newUpgradeCmd.func2
    helm.sh/helm/v3/cmd/helm/upgrade.go:157
  github.com/spf13/cobra.(*Command).execute
    github.com/spf13/cobra@v1.1.3/command.go:852
  github.com/spf13/cobra.(*Command).ExecuteC
    github.com/spf13/cobra@v1.1.3/command.go:960
  github.com/spf13/cobra.(*Command).Execute
    github.com/spf13/cobra@v1.1.3/command.go:897
  main.main
    helm.sh/helm/v3/cmd/helm/helm.go:80
  runtime.main
    runtime/proc.go:225
  runtime.goexit
    runtime/asm_amd64.s:1371

COMBINED OUTPUT:
  history.go:56: [debug] getting history for release mercury
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.02.24-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.02.24-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0001-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.04-b0001-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.04-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.04-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0001-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0001-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "helloworld" "3.0.0-20.03.05-b0003-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "3.0.0-20.03.05-b0003-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "dcs-api" "version.0.1" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "version.0.1" is invalid
  index.go:339: skipping loading invalid entry for chart "escalation-aggregator-service" "1-21.03.04-b0019-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1-21.03.04-b0019-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.03-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.03.03-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.03.04-b0002-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.03.04-b0002-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0006-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.18-b0006-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.18-b0007-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.18-b0007-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "site-data-collector" "1.0.0-21.05.19-b0005-SNAPSHOT" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.19-b0005-SNAPSHOT" is invalid
  index.go:339: skipping loading invalid entry for chart "api-tracker" "version.0.1" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "version.0.1" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.06-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.06-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.12-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.12-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.13-b0004" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.13-b0004" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.14-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.14-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.21-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.21-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.04.28-b0005" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.04.28-b0005" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0001" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.03-b0001" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.03-b0006" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.03-b0006" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.06-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.06-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "xo-service" "1.0.0-21.05.10-b0002" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "1.0.0-21.05.10-b0002" is invalid
  index.go:339: skipping loading invalid entry for chart "kweet-config" "2111.08.1834-01" from /home/opc/.cache/helm/repository/osvc-helm-virtual-index.yaml: validation: chart.metadata.version "2111.08.1834-01" is invalid
  upgrade.go:123: [debug] preparing upgrade for mercury
  upgrade.go:437: [debug] resetting values to the chart's original version
  upgrade.go:131: [debug] performing update for mercury
  upgrade.go:303: [debug] creating upgraded release for mercury
  client.go:284: [debug] Starting delete for "mercury-agent-command-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-channel-api-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-consumer-command-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-custom-availability-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-data-mask-api-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-engagement-queue-api-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-enrichment-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-event-sync-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-integration-in-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-integration-out-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-facebook-client-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-facebook-webhook-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-twiliosms-client-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-userprofiles-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-wechat-client-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-kweet-wechat-webhook-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-mercury-ui-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-metric-aggregation-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-metric-fusion-bridge-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-metric-generation-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-metric-internal-translation-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-metric-proxy-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-omnichannel-assignment-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-omnichannel-offer-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-api-services-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-metrics-data-pipeline-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-osvc-data-extractor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-provisioning-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-state-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-state-query-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-osvc-bridge-task-controller-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-provisioning-monitor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-provisioning-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-queue-agent-info-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-realtime-channel-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-resource-channel-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-resource-state-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-resource-work-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-routing-processor-agent-assignment-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-routing-processor-agent-events-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-routing-processor-queue-assignment-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-routing-processor-work-events-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-session-housekeeping-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-session-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-single-sign-on-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-social-bridge-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-social-config-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-static-assets-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-tenant-downtime-monitor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-transcript-api-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-transcript-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-user-preference-service-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-work-api-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:284: [debug] Starting delete for "mercury-work-processor-keystore" Secret
  client.go:122: [debug] creating 1 resource(s)
  client.go:203: [debug] checking 254 resources for changes
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-agent-command-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-agent-command-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-agent-command-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-channel-api"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-channel-api-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-consumer-command-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-consumer-command-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-consumer-command-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-custom-availability-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-custom-availability-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-custom-availability-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-data-mask-api"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-data-mask-api-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-engagement-queue-api"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-engagement-queue-api-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-engagement-queue-api-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-enrichment-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-enrichment-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-enrichment-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-event-sync-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-event-sync-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-event-sync-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-integration-in-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-integration-in-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-integration-in-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-integration-out-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-mercury-ui"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-aggregation-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-aggregation-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-fusion-bridge"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-fusion-bridge-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-fusion-bridge-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-generation-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-generation-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-internal-translation-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-internal-translation-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-proxy-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-metric-proxy-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-assignment-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-assignment-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-assignment-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-offer-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-offer-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-omnichannel-offer-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-api-services"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-api-services-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-metrics-data-pipeline"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-metrics-data-pipeline-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-osvc-data-extractor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-osvc-data-extractor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-osvc-data-extractor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-provisioning-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-provisioning-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-state-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-state-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-state-query-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-state-query-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-task-controller"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-osvc-bridge-task-controller-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-monitor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-monitor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-alertroute"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-provisioning-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-queue-agent-info-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-queue-agent-info-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-realtime-channel-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-realtime-channel-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-channel-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-channel-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-state-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-state-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-work-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-resource-work-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-assignment"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-assignment-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-assignment-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-events-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-events-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-agent-events-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-queue-assignment"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-queue-assignment-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-queue-assignment-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-work-events-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-work-events-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-routing-processor-work-events-processor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-session-housekeeping-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-session-housekeeping-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-session-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-session-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-single-sign-on-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-single-sign-on-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-single-sign-on-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-bridge"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-bridge-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-bridge-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-config"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-social-config-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-static-assets-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-static-assets-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-static-assets-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-tenant-downtime-monitor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-tenant-downtime-monitor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-tenant-downtime-monitor-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-api"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-api-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-api-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-transcript-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-user-preference-service"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-user-preference-service-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-user-preference-service-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-api"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-api-dashboard"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-api-rule"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-processor"
  client.go:466: [debug] Looks like there are no changes for ConfigMap "mercury-work-processor-dashboard"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-agent-command-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-channel-api"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-consumer-command-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-custom-availability-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-data-mask-api"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-engagement-queue-api"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-enrichment-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-event-sync-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-integration-in-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-integration-out-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-facebook-client"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-facebook-webhook"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-twiliosms-client"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-userprofiles"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-wechat-client"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-kweet-wechat-webhook"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-mercury-ui"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-aggregation-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-fusion-bridge"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-generation-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-internal-translation-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-metric-proxy-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-omnichannel-assignment-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-omnichannel-offer-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-api-services"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-metrics-data-pipeline"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-osvc-data-extractor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-state-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-state-query-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-osvc-bridge-task-controller"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-provisioning-monitor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-queue-agent-info-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-realtime-channel-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-resource-channel-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-resource-state-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-resource-work-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-routing-processor-agent-assignment"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-routing-processor-agent-events-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-routing-processor-queue-assignment"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-routing-processor-work-events-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-session-housekeeping-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-session-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-single-sign-on-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-social-bridge"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-social-config"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-static-assets-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-tenant-downtime-monitor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-transcript-api"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-transcript-processor"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-user-preference-service"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-work-api"
  client.go:466: [debug] Looks like there are no changes for Service "mercury-work-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-channel-api"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-custom-availability-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-data-mask-api"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-engagement-queue-api"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-enrichment-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-event-sync-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-integration-in-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-integration-out-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-mercury-ui"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-metric-fusion-bridge"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-metric-internal-translation-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-api-services"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-metrics-data-pipeline"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-osvc-data-extractor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-state-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-state-query-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-osvc-bridge-task-controller"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-provisioning-monitor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-provisioning-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-resource-state-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-routing-processor-queue-assignment"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-routing-processor-work-events-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-session-housekeeping-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-session-processor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-single-sign-on-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-social-config"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-static-assets-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-tenant-downtime-monitor"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-transcript-api"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-user-preference-service"
  client.go:466: [debug] Looks like there are no changes for Deployment "mercury-work-api"
  W0121 10:51:40.239880    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:40.533663    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:40.830005    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:41.697556    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:41.989600    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:42.288329    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:43.157453    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:43.450691    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:43.763280    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:44.635243    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:44.927715    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:45.222225    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:46.091759    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:46.385450    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:46.681737    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:47.549066    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:47.845722    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:48.147751    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:49.019432    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:49.314524    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:49.611738    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:50.526155    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:50.820661    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:51.114858    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:51.982170    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:52.277857    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:52.577752    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:53.446482    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:53.739619    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:54.037634    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:54.907593    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:55.197971    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:55.493446    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:56.573806    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:56.883932    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:57.182998    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:58.051343    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:58.343117    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:58.643249    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:59.608480    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:51:59.902235    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:00.208615    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:01.052621    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:01.345719    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:01.648512    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:02.528982    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:02.822898    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:03.118135    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:03.987798    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:04.280972    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:04.582584    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:05.452578    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:05.745376    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:06.051534    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:06.921028    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:07.214711    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:07.513487    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:08.382327    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:08.673364    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:08.966560    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:09.833919    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:10.129278    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:10.426203    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:11.296021    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:11.592808    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:11.898253    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:12.765761    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:13.061668    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:13.362258    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:14.240508    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:14.534605    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:14.834025    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:15.702931    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:16.016598    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:16.314238    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:17.181962    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:17.474177    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  W0121 10:52:17.782223    6428 warnings.go:70] networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
  wait.go:47: [debug] beginning wait for 254 resources with timeout of 15m0s
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  ready.go:277: [debug] Deployment is not ready: mercury-psr/mercury-tenant-downtime-monitor. 0 out of 3 expected pods are ready
  upgrade.go:369: [debug] warning: Upgrade "mercury" failed: Get "https://130.35.130.41:6443/apis/apps/v1/namespaces/mercury-psr/deployments/mercury-provisioning-monitor": context deadline exceeded
  Error: UPGRADE FAILED: Get "https://130.35.130.41:6443/apis/apps/v1/namespaces/mercury-psr/deployments/mercury-provisioning-monitor": context deadline exceeded
  helm.go:81: [debug] Get "https://130.35.130.41:6443/apis/apps/v1/namespaces/mercury-psr/deployments/mercury-provisioning-monitor": context deadline exceeded
  UPGRADE FAILED
  main.newUpgradeCmd.func2
    helm.sh/helm/v3/cmd/helm/upgrade.go:157
  github.com/spf13/cobra.(*Command).execute
    github.com/spf13/cobra@v1.1.3/command.go:852
  github.com/spf13/cobra.(*Command).ExecuteC
    github.com/spf13/cobra@v1.1.3/command.go:960
  github.com/spf13/cobra.(*Command).Execute
    github.com/spf13/cobra@v1.1.3/command.go:897
  main.main
    helm.sh/helm/v3/cmd/helm/helm.go:80
  runtime.main
    runtime/proc.go:225
  runtime.goexit
    runtime/asm_amd64.s:1371
[21:January:2022:11:07:21]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[21:January:2022:12:24:42]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ hln mercury-psr
NAME      NAMESPACE   REVISION  UPDATED                                 STATUS    CHART                 APP VERSION
app-kafka mercury-psr 1         2022-01-19 22:48:59.456337456 +0000 UTC deployed  cpe-kafka-0.1.0       0.1.0
appdata   mercury-psr 1         2022-01-19 22:49:17.75707579 +0000 UTC  deployed  kafka-data-0.1.0      null
mercury   mercury-psr 4         2022-01-21 10:41:51.3748752 +0000 UTC   failed    mercury-2201.18.1305  22.01.18-REL21.11-1305
[21:January:2022:13:06:42]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgpnk

kgc
NAME                                                             READY   STATUS    RESTARTS   AGE   IP             NODE          NOMINATED NODE   READINESS GATES
schemaregistry-proxy-7ff9d894ff-b2pjt                            2/2     Running   0          61d   10.245.1.24    10.20.1.38    <none>           <none>
shared-kafka-dr-kafka-0                                          2/2     Running   0          61d   10.245.2.64    10.20.1.35    <none>           <none>
shared-kafka-dr-kafka-1                                          2/2     Running   0          61d   10.245.1.65    10.20.1.38    <none>           <none>
shared-kafka-dr-kafka-2                                          2/2     Running   0          61d   10.245.6.6     10.20.1.44    <none>           <none>
shared-kafka-dr-kafka-exporter-648f4d4b7b-xg7pp                  1/1     Running   0          61d   10.245.4.145   10.20.1.227   <none>           <none>
shared-kafka-dr-zookeeper-0                                      1/1     Running   0          61d   10.245.3.190   10.20.0.250   <none>           <none>
shared-kafka-dr-zookeeper-1                                      1/1     Running   0          61d   10.245.1.85    10.20.1.38    <none>           <none>
shared-kafka-dr-zookeeper-2                                      1/1     Running   0          61d   10.245.4.60    10.20.1.27    <none>           <none>
shared-kafka-entity-operator-57f88d498c-dwntl                    3/3     Running   7          61d   10.245.4.146   10.20.1.227   <none>           <none>
shared-kafka-kafka-0                                             2/2     Running   0          61d   10.245.1.59    10.20.1.38    <none>           <none>
shared-kafka-kafka-1                                             2/2     Running   0          10d   10.245.3.238   10.20.0.250   <none>           <none>
shared-kafka-kafka-2                                             2/2     Running   0          61d   10.245.4.59    10.20.1.27    <none>           <none>
shared-kafka-kafka-exporter-7874cbb88f-pmpkz                     1/1     Running   0          61d   10.245.4.4     10.20.1.27    <none>           <none>
shared-kafka-mm2-dr-us-phoenix-1-mirrormaker2-5d69b58598-jjj56   1/1     Running   0          16d   10.245.6.156   10.20.1.239   <none>           <none>
shared-kafka-mm2-dr-us-phoenix-1-mirrormaker2-5d69b58598-t89nd   1/1     Running   0          16d   10.245.1.214   10.20.0.29    <none>           <none>
shared-kafka-mm2-us-phoenix-1-mirrormaker2-7cfd7d5494-jg9dm      1/1     Running   0          61d   10.245.0.142   10.20.0.244   <none>           <none>
shared-kafka-mm2-us-phoenix-1-mirrormaker2-7cfd7d5494-lnd58      1/1     Running   0          61d   10.245.3.9     10.20.0.7     <none>           <none>
shared-kafka-zookeeper-0                                         1/1     Running   0          61d   10.245.3.62    10.20.0.7     <none>           <none>
shared-kafka-zookeeper-1                                         1/1     Running   0          10d   10.245.8.32    10.20.0.134   <none>           <none>
shared-kafka-zookeeper-2                                         1/1     Running   0          61d   10.245.1.77    10.20.1.38    <none>           <none>
shared-schemaregistry-7b74b884c9-7hm44                           2/2     Running   0          16d   10.245.1.41    10.20.1.38    <none>           <none>
shared-schemaregistry-7b74b884c9-r8564                           2/2     Running   0          16d   10.245.3.3     10.20.0.7     <none>           <none>
strimzi-cluster-operator-7887b58b44-7lnv8                        1/1     Running   3          10d   10.245.6.165   10.20.1.239   <none>           <none>
[24:January:2022:06:25:51]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[24:January:2022:06:25:51]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgc
CURRENT   NAME                               CLUSTER               AUTHINFO           NAMESPACE
          prod_ap-hyderabad-1_controlplane   cluster-cbjr52qcfoa   user-cbjr52qcfoa
          prod_ap-hyderabad-1_dataplane      cluster-cpj2zxktnda   user-cpj2zxktnda
          prod_ap-melbourne-1_controlplane   cluster-czgcn3bmjtg   user-czgcn3bmjtg
          prod_ap-melbourne-1_dataplane      cluster-c4dazdggfst   user-c4dazdggfst
          prod_ap-mumbai-1_controlplane      cluster-cjiru3mifia   user-cjiru3mifia
          prod_ap-mumbai-1_dataplane         cluster-ci3ypr6tmga   user-ci3ypr6tmga
          prod_ap-osaka-1_controlplane       cluster-cp5327xtmfa   user-cp5327xtmfa
          prod_ap-osaka-1_dataplane          cluster-cqldck6xatq   user-cqldck6xatq
          prod_ap-sydney-1_controlplane      cluster-c4wembugqyt   user-c4wembugqyt
          prod_ap-sydney-1_dataplane         cluster-cswezbzgyyg   user-cswezbzgyyg
          prod_ap-tokyo-1_controlplane       cluster-cz54pc3z4wa   user-cz54pc3z4wa
          prod_ap-tokyo-1_dataplane          cluster-cyjtbtbbjfq   user-cyjtbtbbjfq
          prod_ca-montreal-1_controlplane    cluster-czdoolemnqw   user-czdoolemnqw
          prod_ca-montreal-1_dataplane       cluster-cqtkytbgq3d   user-cqtkytbgq3d
          prod_ca-toronto-1_controlplane     cluster-cygcmjxhfsd   user-cygcmjxhfsd
          prod_ca-toronto-1_dataplane        cluster-cywmodfmm3d   user-cywmodfmm3d
          prod_eu-amsterdam-1_controlplane   cluster-c4tkmjwgbrt   user-c4tkmjwgbrt
          prod_eu-amsterdam-1_dataplane      cluster-cstsy3bmjrd   user-cstsy3bmjrd
          prod_eu-frankfurt-1_controlplane   cluster-cydcytcg43d   user-cydcytcg43d
          prod_eu-frankfurt-1_dataplane      cluster-crdgzbrmjsd   user-crdgzbrmjsd
          prod_eu-zurich-1_controlplane      cluster-cwmoxn3bfwq   user-cwmoxn3bfwq
          prod_eu-zurich-1_dataplane         cluster-chtoe5bnsza   user-chtoe5bnsza
          prod_me-dubai-1_controlplane       cluster-ce5puidc4ya   user-ce5puidc4ya
          prod_me-dubai-1_dataplane          cluster-c77hcd3vizq   user-c77hcd3vizq
          prod_me-jeddah-1_controlplane      cluster-cpzagsoukeq   user-cpzagsoukeq
          prod_me-jeddah-1_dataplane         cluster-cf4f222mrfa   user-cf4f222mrfa
          prod_sa-santiago-1_controlplane    cluster-czxpimgibeq   user-czxpimgibeq
          prod_sa-santiago-1_dataplane       cluster-cuutxfvj2ea   user-cuutxfvj2ea
          prod_sa-saopaulo-1_controlplane    cluster-cc75n7hujpq   user-cc75n7hujpq
          prod_sa-saopaulo-1_dataplane       cluster-cpjqrjtbotq   user-cpjqrjtbotq
          prod_sa-vinhedo-1_controlplane     cluster-cihx6aprjua   user-cihx6aprjua
          prod_sa-vinhedo-1_dataplane        cluster-ciqmbmn6wta   user-ciqmbmn6wta
          prod_uk-cardiff-1_controlplane     cluster-ctggmtbmfrg   user-ctggmtbmfrg
          prod_uk-cardiff-1_dataplane        cluster-cqwmnrqmztd   user-cqwmnrqmztd
          prod_uk-london-1_controlplane      cluster-cywgnlcmqzd   user-cywgnlcmqzd
          prod_uk-london-1_dataplane         cluster-c3tazjsmjsd   user-c3tazjsmjsd
          prod_us-ashburn-1_controlplane     cluster-czwczjzgrsw   user-czwczjzgrsw
          prod_us-ashburn-1_dataplane        cluster-c3wkyjyguzt   user-c3wkyjyguzt
          prod_us-phoenix-1_controlplane     cluster-c3tkobugrst   user-c3tkobugrst
*         prod_us-phoenix-1_dataplane        cluster-csdqylfmi2g   user-csdqylfmi2g
          prod_us-phoenix-1_deployment       cluster-c4tgolgmjsd   user-c4tgolgmjsd
[24:January:2022:06:25:52]:(prod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ v2corp
[24:January:2022:07:35:22]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgc
CURRENT   NAME                             CLUSTER               AUTHINFO           NAMESPACE
          corp_us-ashburn-1_controlplane   cluster-c3wgzdgmfqt   user-c3wgzdgmfqt
*         corp_us-ashburn-1_dataplane      cluster-c2tqztfmy3t   user-c2tqztfmy3t
          corp_us-ashburn-1_deployment     cluster-crdcyjvmztg   user-crdcyjvmztg
[24:January:2022:07:35:23]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ rp
[24:January:2022:07:35:28]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgpnk

keitn sharNAME                                               READY   STATUS      RESTARTS   AGE     IP              NODE          NOMINATED NODE   READINESS GATES
azkclient-kafka-client                             1/1     Running     0          52d     10.245.24.119   10.11.9.127   <none>           <none>
cpe-cronjob-sr-backup-sr-backup-1643007300-tz5kp   0/1     Completed   0          40m     10.245.14.132   10.11.8.94    <none>           <none>
cpe-cronjob-sr-backup-sr-backup-1643008200-tchf8   0/1     Completed   0          25m     10.245.19.33    10.11.9.238   <none>           <none>
cpe-cronjob-sr-backup-sr-backup-1643009100-6zm2q   0/1     Completed   0          10m     10.245.18.115   10.11.9.160   <none>           <none>
drpod                                              1/1     Running     46         19d     10.245.21.55    10.11.9.182   <none>           <none>
karapace-schemaregistry-proxy-7d8f5776b5-6cwdf     2/2     Running     2          3d21h   10.245.35.137   10.11.9.177   <none>           <none>
karapace-shared-schemaregistry-7965cb76c9-4npq2    3/3     Running     1          19d     10.245.26.114   10.11.8.67    <none>           <none>
karapace-shared-schemaregistry-7965cb76c9-ss95j    3/3     Running     1          2d21h   10.245.17.11    10.11.9.124   <none>           <none>
kf-client-kafka-client                             1/1     Running     0          33d     10.245.1.51     10.11.9.135   <none>           <none>
schemaregistry-proxy-566f5f48c9-8s4gm              2/2     Running     0          19d     10.245.0.172    10.11.8.71    <none>           <none>
shared-kafka-entity-operator-74d4d899b4-dkfff      3/3     Running     1          3d22h   10.245.33.140   10.11.9.147   <none>           <none>
shared-kafka-kafka-0                               1/1     Running     0          5d12h   10.245.21.64    10.11.9.182   <none>           <none>
shared-kafka-kafka-1                               1/1     Running     0          5d12h   10.245.17.202   10.11.9.9     <none>           <none>
shared-kafka-kafka-2                               1/1     Running     0          5d12h   10.245.10.87    10.11.9.95    <none>           <none>
shared-kafka-kafka-3                               1/1     Running     0          5d12h   10.245.7.189    10.11.8.76    <none>           <none>
shared-kafka-kafka-4                               1/1     Running     0          5d12h   10.245.5.33     10.11.9.157   <none>           <none>
shared-kafka-kafka-5                               1/1     Running     0          5d12h   10.245.10.85    10.11.9.95    <none>           <none>
shared-kafka-kafka-6                               1/1     Running     0          3d10h   10.245.0.166    10.11.8.71    <none>           <none>
shared-kafka-kafka-7                               1/1     Running     0          5d12h   10.245.26.189   10.11.9.14    <none>           <none>
shared-kafka-kafka-8                               1/1     Running     0          5d12h   10.245.3.161    10.11.9.16    <none>           <none>
shared-kafka-kafka-exporter-6d6dd5f5c7-wmjdh       1/1     Running     0          5d12h   10.245.24.50    10.11.9.127   <none>           <none>
shared-kafka-zookeeper-0                           1/1     Running     0          5d13h   10.245.0.198    10.11.8.71    <none>           <none>
shared-kafka-zookeeper-1                           1/1     Running     0          5d13h   10.245.2.205    10.11.8.68    <none>           <none>
shared-kafka-zookeeper-2                           1/1     Running     0          2d18h   10.245.10.42    10.11.9.95    <none>           <none>
shared-schemaregistry-7b97454fcc-k4rb7             2/2     Running     0          19d     10.245.26.106   10.11.8.67    <none>           <none>
shared-schemaregistry-7b97454fcc-wbntg             2/2     Running     0          19d     10.245.9.72     10.11.9.136   <none>           <none>
strimzi-cluster-operator-7fbc69c4f4-9l7pz          1/1     Running     0          6d20h   10.245.11.242   10.11.9.208   <none>           <none>
[24:January:2022:07:35:41]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[24:January:2022:07:35:41]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ keitn kafka shared-kafka-zookeeper-0 -- bash

set -o vi
[kafka@shared-kafka-zookeeper-0 kafka]$
[kafka@shared-kafka-zookeeper-0 kafka]$ set -o vi
[kafka@shared-kafka-zookeeper-0 kafka]$
[kafka@shared-kafka-zookeeper-0 kafka]$ #./kafka-consumer-groups.sh --bootstrap-server shared-kafka-bootstrap-kafka.service.iad-dataplane.corp.consul:443 --command-config ../config/consumer-ssl.properties --list
[kafka@shared-kafka-zookeeper-0 kafka]$ ./bin/kafka-consumer-groups.sh --bootstrap-server shared-kafka-bootstrap-kafka.service.iad-dataplane.corp.consul:443 --command-config ../config/consumer-ssl.properties --list
Exception in thread "main" java.nio.file.NoSuchFileException: ../config/consumer-ssl.properties
  at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
  at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
  at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
  at java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:219)
  at java.base/java.nio.file.Files.newByteChannel(Files.java:371)
  at java.base/java.nio.file.Files.newByteChannel(Files.java:422)
  at java.base/java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:420)
  at java.base/java.nio.file.Files.newInputStream(Files.java:156)
  at org.apache.kafka.common.utils.Utils.loadProps(Utils.java:629)
  at kafka.admin.ConsumerGroupCommand$ConsumerGroupService.createAdminClient(ConsumerGroupCommand.scala:695)
  at kafka.admin.ConsumerGroupCommand$ConsumerGroupService.<init>(ConsumerGroupCommand.scala:177)
  at kafka.admin.ConsumerGroupCommand$.run(ConsumerGroupCommand.scala:68)
  at kafka.admin.ConsumerGroupCommand$.main(ConsumerGroupCommand.scala:60)
  at kafka.admin.ConsumerGroupCommand.main(ConsumerGroupCommand.scala)
[kafka@shared-kafka-zookeeper-0 kafka]$ ll ./config/consumer.properties
bash: ll: command not found
[kafka@shared-kafka-zookeeper-0 kafka]$ ls ./config/consumer.properties
./config/consumer.properties
[kafka@shared-kafka-zookeeper-0 kafka]$ ls config/
connect-console-sink.properties    connect-file-source.properties   consumer.properties  server.properties
connect-console-source.properties  connect-log4j.properties     kraft    tools-log4j.properties
connect-distributed.properties     connect-mirror-maker.properties  log4j.properties   trogdor.conf
connect-file-sink.properties     connect-standalone.properties    producer.properties  zookeeper.properties
[kafka@shared-kafka-zookeeper-0 kafka]$ ./bin/kafka-consumer-groups.sh --bootstrap-server shared-kafka-bootstrap-kafka.service.iad-dataplane.corp.consul:443 --command-config ./config/consumer.properties --list
[2022-01-24 07:40:51,530] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)

Error: Executing consumer group command failed due to org.apache.kafka.common.KafkaException: Failed to find brokers to send ListGroups
java.util.concurrent.ExecutionException: org.apache.kafka.common.KafkaException: Failed to find brokers to send ListGroups
  at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)
  at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)
  at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)
  at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)
  at kafka.admin.ConsumerGroupCommand$ConsumerGroupService.listConsumerGroups(ConsumerGroupCommand.scala:204)
  at kafka.admin.ConsumerGroupCommand$ConsumerGroupService.listGroups(ConsumerGroupCommand.scala:199)
  at kafka.admin.ConsumerGroupCommand$.run(ConsumerGroupCommand.scala:71)
  at kafka.admin.ConsumerGroupCommand$.main(ConsumerGroupCommand.scala:60)
  at kafka.admin.ConsumerGroupCommand.main(ConsumerGroupCommand.scala)
Caused by: org.apache.kafka.common.KafkaException: Failed to find brokers to send ListGroups
  at org.apache.kafka.clients.admin.KafkaAdminClient$24.handleFailure(KafkaAdminClient.java:3291)
  at org.apache.kafka.clients.admin.KafkaAdminClient$Call.failWithTimeout(KafkaAdminClient.java:818)
  at org.apache.kafka.clients.admin.KafkaAdminClient$Call.fail(KafkaAdminClient.java:789)
  at org.apache.kafka.clients.admin.KafkaAdminClient$TimeoutProcessor.handleTimeouts(KafkaAdminClient.java:912)
  at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1274)
  at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.kafka.common.errors.TimeoutException: Call(callName=findAllBrokers, deadlineMs=1643010056818, tries=1, nextAllowedTryMs=-9223372036854775709) timed out at 9223372036854775807 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: findAllBrokers
[2022-01-24 07:40:52,121] ERROR Uncaught exception in thread 'kafka-admin-client-thread | adminclient-1': (org.apache.kafka.common.utils.KafkaThread)
java.lang.OutOfMemoryError: Java heap space
  at java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)
  at java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)
  at org.apache.kafka.common.memory.MemoryPool$1.tryAllocate(MemoryPool.java:30)
  at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:113)
  at org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:452)
  at org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:402)
  at org.apache.kafka.common.network.Selector.attemptRead(Selector.java:674)
  at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:576)
  at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
  at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:561)
  at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1333)
  at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1264)
  at java.base/java.lang.Thread.run(Thread.java:829)
[kafka@shared-kafka-zookeeper-0 kafka]$ cat ./config/consumer.properties
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# see org.apache.kafka.clients.consumer.ConsumerConfig for more details

# list of brokers used for bootstrapping knowledge about the rest of the cluster
# format: host1:port1,host2:port2 ...
bootstrap.servers=localhost:9092

# consumer group id
group.id=test-consumer-group

# What to do when there is no initial offset in Kafka or if the current
# offset does not exist any more on the server: latest, earliest, none
#auto.offset.reset=
[kafka@shared-kafka-zookeeper-0 kafka]$
[kafka@shared-kafka-zookeeper-0 kafka]$ ./bin/kafka-consumer-groups.sh --bootstrap-server shared-kafka-bootstrap-kafka.service.iad-dataplane.corp.consul:443 --list

Error: Executing consumer group command failed due to org.apache.kafka.common.KafkaException: Failed to find brokers to send ListGroups
java.util.concurrent.ExecutionException: org.apache.kafka.common.KafkaException: Failed to find brokers to send ListGroups
  at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)
  at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)
  at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)
  at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)
  at kafka.admin.ConsumerGroupCommand$ConsumerGroupService.listConsumerGroups(ConsumerGroupCommand.scala:204)
  at kafka.admin.ConsumerGroupCommand$ConsumerGroupService.listGroups(ConsumerGroupCommand.scala:199)
  at kafka.admin.ConsumerGroupCommand$.run(ConsumerGroupCommand.scala:71)
  at kafka.admin.ConsumerGroupCommand$.main(ConsumerGroupCommand.scala:60)
  at kafka.admin.ConsumerGroupCommand.main(ConsumerGroupCommand.scala)
Caused by: org.apache.kafka.common.KafkaException: Failed to find brokers to send ListGroups
  at org.apache.kafka.clients.admin.KafkaAdminClient$24.handleFailure(KafkaAdminClient.java:3291)
  at org.apache.kafka.clients.admin.KafkaAdminClient$Call.failWithTimeout(KafkaAdminClient.java:818)
  at org.apache.kafka.clients.admin.KafkaAdminClient$Call.fail(KafkaAdminClient.java:789)
  at org.apache.kafka.clients.admin.KafkaAdminClient$TimeoutProcessor.handleTimeouts(KafkaAdminClient.java:912)
  at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1274)
  at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.kafka.common.errors.TimeoutException: Call(callName=findAllBrokers, deadlineMs=1643010161447, tries=1, nextAllowedTryMs=-9223372036854775709) timed out at 9223372036854775807 after 1 attempt(s)
Caused by: org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: findAllBrokers
[2022-01-24 07:42:36,819] ERROR Uncaught exception in thread 'kafka-admin-client-thread | adminclient-1': (org.apache.kafka.common.utils.KafkaThread)
java.lang.OutOfMemoryError: Java heap space
  at java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)
  at java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)
  at org.apache.kafka.common.memory.MemoryPool$1.tryAllocate(MemoryPool.java:30)
  at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:113)
  at org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:452)
  at org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:402)
  at org.apache.kafka.common.network.Selector.attemptRead(Selector.java:674)
  at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:576)
  at org.apache.kafka.common.network.Selector.poll(Selector.java:481)
  at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:561)
  at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1333)
  at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1264)
  at java.base/java.lang.Thread.run(Thread.java:829)
[kafka@shared-kafka-zookeeper-0 kafka]$ ./bin/kafka-acls.sh --list --authorizer-properties zookeeper.connect=localhost:2181
Error while executing ACL command: Timed out waiting for connection while in state: CONNECTING
kafka.zookeeper.ZooKeeperClientTimeoutException: Timed out waiting for connection while in state: CONNECTING
  at kafka.zookeeper.ZooKeeperClient.waitUntilConnected(ZooKeeperClient.scala:271)
  at kafka.zookeeper.ZooKeeperClient.<init>(ZooKeeperClient.scala:125)
  at kafka.zk.KafkaZkClient$.apply(KafkaZkClient.scala:1948)
  at kafka.security.authorizer.AclAuthorizer.configure(AclAuthorizer.scala:174)
  at kafka.admin.AclCommand$AuthorizerService.listAcls(AclCommand.scala:218)
  at kafka.admin.AclCommand$.main(AclCommand.scala:74)
  at kafka.admin.AclCommand.main(AclCommand.scala)

[kafka@shared-kafka-zookeeper-0 kafka]$ netstat -ntulp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:44495           0.0.0.0:*               LISTEN      67/java
tcp        0      0 10.245.0.198:3888       0.0.0.0:*               LISTEN      67/java
tcp        0      0 0.0.0.0:8080            0.0.0.0:*               LISTEN      67/java
tcp        0      0 127.0.0.1:12181         0.0.0.0:*               LISTEN      67/java
tcp        0      0 0.0.0.0:9404            0.0.0.0:*               LISTEN      67/java
tcp        0      0 0.0.0.0:2181            0.0.0.0:*               LISTEN      67/java
[kafka@shared-kafka-zookeeper-0 kafka]$ ss
bash: ss: command not found
[kafka@shared-kafka-zookeeper-0 kafka]$ netstat
Active Internet connections (w/o servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 localhost:59328         localhost:12181         TIME_WAIT
tcp        0      0 shared-kafka-z:eforward shared-kafka-kafk:57528 ESTABLISHED
tcp        0      0 shared-kafka-z:eforward 10-245-7-189.shar:42682 ESTABLISHED
tcp        0      0 shared-kafka-zooke:9404 10-245-36-168.pro:39496 ESTABLISHED
tcp        0      0 localhost:59238         localhost:12181         TIME_WAIT
tcp        0      0 shared-kaf:ciphire-serv shared-kafka-zook:51454 ESTABLISHED
tcp        0      0 shared-kafka-z:eforward 10-245-21-64.shar:57964 ESTABLISHED
tcp        0      0 shared-kafka-z:eforward shared-kafka-kafk:35422 ESTABLISHED
tcp        0      0 localhost:58894         localhost:12181         TIME_WAIT
tcp        0      0 shared-kafka-z:eforward shared-kafka-kafk:59950 ESTABLISHED
tcp        0      0 shared-kafka-z:eforward shared-kafka-kafk:59960 ESTABLISHED
tcp        0      0 localhost:58816         localhost:12181         TIME_WAIT
tcp        0      0 localhost:59898         localhost:12181         TIME_WAIT
tcp        0      0 localhost:59120         localhost:12181         TIME_WAIT
tcp        0      0 shared-kafka-zook:52862 shared-kafka:spcsdlobby ESTABLISHED
tcp        0      0 shared-kaf:ciphire-serv shared-kafka-zook:54812 ESTABLISHED
tcp        0      0 localhost:59464         localhost:12181         TIME_WAIT
tcp        0      0 localhost:58706         localhost:12181         TIME_WAIT
tcp        0      0 localhost:58628         localhost:12181         TIME_WAIT
tcp        0      0 shared-kafka-z:eforward shared-kafka-kafk:52170 ESTABLISHED
tcp        0      0 shared-kafka-z:eforward shared-kafka-kafk:57600 ESTABLISHED
tcp        0      0 shared-kafka-z:eforward 10-245-0-166.shar:55918 ESTABLISHED
tcp        0      0 localhost:59024         localhost:12181         TIME_WAIT
tcp        0      0 localhost:59774         localhost:12181         TIME_WAIT
tcp        0      0 shared-kafka-z:eforward 10-245-17-202.sha:33006 ESTABLISHED
tcp        0      0 shared-kafka-z:eforward 10.245.33.140:42030     ESTABLISHED
tcp        0      0 shared-kafka-z:eforward 10-245-21-64.shar:57982 ESTABLISHED
tcp        0      0 shared-kafka-z:eforward 10-245-26-189.sha:55438 ESTABLISHED
tcp        0      0 localhost:59550         localhost:12181         TIME_WAIT
tcp        0      0 localhost:59670         localhost:12181         TIME_WAIT
Active UNIX domain sockets (w/o servers)
Proto RefCnt Flags       Type       State         I-Node   Path
unix  2      [ ]         STREAM     CONNECTED     1876590244
unix  2      [ ]         STREAM     CONNECTED     1876590412
Active Bluetooth connections (w/o servers)
Proto  Destination       Source            State         PSM DCID   SCID      IMTU    OMTU Security
Proto  Destination       Source            State     Channel
[kafka@shared-kafka-zookeeper-0 kafka]$ tail -10 /etc/services
aigairserver    21221/tcp               # Services for Air Server
ka-kdp          31016/udp               # Kollective Agent Kollective Delivery
ka-sddp         31016/tcp               # Kollective Agent Secure Distributed Delivery
edi_service     34567/udp               # dhanalakshmi.org EDI Service
axio-disc       35100/tcp               # Axiomatic discovery protocol
axio-disc       35100/udp               # Axiomatic discovery protocol
pmwebapi        44323/tcp               # Performance Co-Pilot client HTTP API
cloudcheck-ping 45514/udp               # ASSIA CloudCheck WiFi Management keepalive
cloudcheck      45514/tcp               # ASSIA CloudCheck WiFi Management System
spremotetablet  46998/tcp               # Capture handwritten signatures
[kafka@shared-kafka-zookeeper-0 kafka]$ lsof -i -P -n | grep LISTEN
bash: lsof: command not found
[kafka@shared-kafka-zookeeper-0 kafka]$ nmap -sT -O localhost
bash: nmap: command not found
[kafka@shared-kafka-zookeeper-0 kafka]$ ./bin/kafka-topics.sh --zookeeper localhost:1281 -- list | grep -i kweet
Command must include exactly one action: --list, --describe, --create, --alter or --delete
Option                                   Description
------                                   -----------
--alter                                  Alter the number of partitions,
                                           replica assignment, and/or
                                           configuration for the topic.
--at-min-isr-partitions                  if set when describing topics, only
                                           show partitions whose isr count is
                                           equal to the configured minimum. Not
                                           supported with the --zookeeper
                                           option.
--bootstrap-server <String: server to    REQUIRED: The Kafka server to connect
  connect to>                              to. In case of providing this, a
                                           direct Zookeeper connection won't be
                                           required.
--command-config <String: command        Property file containing configs to be
  config property file>                    passed to Admin Client. This is used
                                           only with --bootstrap-server option
                                           for describing and altering broker
                                           configs.
--config <String: name=value>            A topic configuration override for the
                                           topic being created or altered. The
                                           following is a list of valid
                                           configurations:
                                          cleanup.policy
                                          compression.type
                                          delete.retention.ms
                                          file.delete.delay.ms
                                          flush.messages
                                          flush.ms
                                          follower.replication.throttled.
                                           replicas
                                          index.interval.bytes
                                          leader.replication.throttled.replicas
                                          max.compaction.lag.ms
                                          max.message.bytes
                                          message.downconversion.enable
                                          message.format.version
                                          message.timestamp.difference.max.ms
                                          message.timestamp.type
                                          min.cleanable.dirty.ratio
                                          min.compaction.lag.ms
                                          min.insync.replicas
                                          preallocate
                                          retention.bytes
                                          retention.ms
                                          segment.bytes
                                          segment.index.bytes
                                          segment.jitter.ms
                                          segment.ms
                                          unclean.leader.election.enable
                                         See the Kafka documentation for full
                                           details on the topic configs. It is
                                           supported only in combination with --
                                           create if --bootstrap-server option
                                           is used (the kafka-configs CLI
                                           supports altering topic configs with
                                           a --bootstrap-server option).
--create                                 Create a new topic.
--delete                                 Delete a topic
--delete-config <String: name>           A topic configuration override to be
                                           removed for an existing topic (see
                                           the list of configurations under the
                                           --config option). Not supported with
                                           the --bootstrap-server option.
--describe                               List details for the given topics.
--disable-rack-aware                     Disable rack aware replica assignment
--exclude-internal                       exclude internal topics when running
                                           list or describe command. The
                                           internal topics will be listed by
                                           default
--force                                  Suppress console prompts
--help                                   Print usage information.
--if-exists                              if set when altering or deleting or
                                           describing topics, the action will
                                           only execute if the topic exists.
--if-not-exists                          if set when creating topics, the
                                           action will only execute if the
                                           topic does not already exist.
--list                                   List all available topics.
--partitions <Integer: # of partitions>  The number of partitions for the topic
                                           being created or altered (WARNING:
                                           If partitions are increased for a
                                           topic that has a key, the partition
                                           logic or ordering of the messages
                                           will be affected). If not supplied
                                           for create, defaults to the cluster
                                           default.
--replica-assignment <String:            A list of manual partition-to-broker
  broker_id_for_part1_replica1 :           assignments for the topic being
  broker_id_for_part1_replica2 ,           created or altered.
  broker_id_for_part2_replica1 :
  broker_id_for_part2_replica2 , ...>
--replication-factor <Integer:           The replication factor for each
  replication factor>                      partition in the topic being
                                           created. If not supplied, defaults
                                           to the cluster default.
--topic <String: topic>                  The topic to create, alter, describe
                                           or delete. It also accepts a regular
                                           expression, except for --create
                                           option. Put topic name in double
                                           quotes and use the '\' prefix to
                                           escape regular expression symbols; e.
                                           g. "test\.topic".
--topics-with-overrides                  if set when describing topics, only
                                           show topics that have overridden
                                           configs
--unavailable-partitions                 if set when describing topics, only
                                           show partitions whose leader is not
                                           available
--under-min-isr-partitions               if set when describing topics, only
                                           show partitions whose isr count is
                                           less than the configured minimum.
                                           Not supported with the --zookeeper
                                           option.
--under-replicated-partitions            if set when describing topics, only
                                           show under replicated partitions
--version                                Display Kafka version.
--zookeeper <String: hosts>              DEPRECATED, The connection string for
                                           the zookeeper connection in the form
                                           host:port. Multiple hosts can be
                                           given to allow fail-over.
[kafka@shared-kafka-zookeeper-0 kafka]$ ./bin/kafka-topics.sh --zookeeper localhost:1281 --list | grep -i kweet
Exception in thread "main" kafka.zookeeper.ZooKeeperClientTimeoutException: Timed out waiting for connection while in state: CONNECTING
  at kafka.zookeeper.ZooKeeperClient.waitUntilConnected(ZooKeeperClient.scala:271)
  at kafka.zookeeper.ZooKeeperClient.<init>(ZooKeeperClient.scala:125)
  at kafka.zk.KafkaZkClient$.apply(KafkaZkClient.scala:1948)
  at kafka.admin.TopicCommand$ZookeeperTopicService$.apply(TopicCommand.scala:378)
  at kafka.admin.TopicCommand$.main(TopicCommand.scala:56)
  at kafka.admin.TopicCommand.main(TopicCommand.scala)
[kafka@shared-kafka-zookeeper-0 kafka]$ #./bin/kafka-topics.sh --zookeeper localhost:1281 --list | grep -i kweet
[kafka@shared-kafka-zookeeper-0 kafka]$ netstat -ntulp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:44495           0.0.0.0:*               LISTEN      67/java
tcp        0      0 10.245.0.198:3888       0.0.0.0:*               LISTEN      67/java
tcp        0      0 0.0.0.0:8080            0.0.0.0:*               LISTEN      67/java
tcp        0      0 127.0.0.1:12181         0.0.0.0:*               LISTEN      67/java
tcp        0      0 0.0.0.0:9404            0.0.0.0:*               LISTEN      67/java
tcp        0      0 0.0.0.0:2181            0.0.0.0:*               LISTEN      67/java
[kafka@shared-kafka-zookeeper-0 kafka]$ ./bin/kafka-topics.sh --zookeeper localhost:12181 --list | grep -i kweet
[kafka@shared-kafka-zookeeper-0 kafka]$ ./bin/kafka-topics.sh --zookeeper localhost:12181 --list
__consumer_offsets
__strimzi-topic-operator-kstreams-topic-store-changelog
__strimzi_store_topic
__transaction_state
_karapaceschemas
_karapacetest
_schemas
aggregated.escalation.multi
ai-apps.analytics.event
ai-apps.error.event
analytics-ci.audit
analytics-ci.connect.configs
analytics-ci.connect.offsets
analytics-ci.connect.status
analytics-ci.connector.template
analytics-ci.data
analytics-ci.definition
analytics-ci.message.publisher.run.queue
analytics-ci.message.publisher.runs
analytics-ci.message.publisher.specs
analytics-ci.queue
analytics-ci.queue.state
analytics-ci.report.classification
analytics-ci.schema
analytics-ci.schema.mapping
analytics-ci.tenant
analytics-ci.tenant.provision.info
analytics-ci.tenant.schema
analytics-ci.tenant.schema.deploy
analytics-ci.tenant.schema.deploy.response
analytics-ci.tenant.schema.mapping
analytics-ci.tenant.schema.metadata
analytics-e2e.audit
analytics-e2e.connect.configs
analytics-e2e.connect.offsets
analytics-e2e.connect.status
analytics-e2e.connector.template
analytics-e2e.data
analytics-e2e.definition
analytics-e2e.message.publisher.run.queue
analytics-e2e.message.publisher.runs
analytics-e2e.message.publisher.specs
analytics-e2e.queue
analytics-e2e.queue.state
analytics-e2e.report.classification
analytics-e2e.schema
analytics-e2e.schema.mapping
analytics-e2e.tenant
analytics-e2e.tenant.provision.info
analytics-e2e.tenant.schema
analytics-e2e.tenant.schema.deploy
analytics-e2e.tenant.schema.deploy.response
analytics-e2e.tenant.schema.mapping
analytics-e2e.tenant.schema.metadata
analytics-qa.audit
analytics-qa.connect.configs
analytics-qa.connect.offsets
analytics-qa.connect.status
analytics-qa.connector.template
analytics-qa.data
analytics-qa.definition
analytics-qa.message.publisher.run.queue
analytics-qa.message.publisher.runs
analytics-qa.message.publisher.specs
analytics-qa.queue
analytics-qa.queue.state
analytics-qa.report.classification
analytics-qa.schema
analytics-qa.schema.mapping
analytics-qa.tenant
analytics-qa.tenant.provision.info
analytics-qa.tenant.schema
analytics-qa.tenant.schema.deploy
analytics-qa.tenant.schema.deploy.response
analytics-qa.tenant.schema.mapping
analytics-qa.tenant.schema.metadata
analytics-sandbox1.audit
analytics-sandbox1.connect.configs
analytics-sandbox1.connect.offsets
analytics-sandbox1.connect.status
analytics-sandbox1.connector.template
analytics-sandbox1.data
analytics-sandbox1.definition
analytics-sandbox1.message.publisher.run.queue
analytics-sandbox1.message.publisher.runs
analytics-sandbox1.message.publisher.specs
analytics-sandbox1.queue
analytics-sandbox1.queue.state
analytics-sandbox1.report.classification
analytics-sandbox1.schema
analytics-sandbox1.schema.mapping
analytics-sandbox1.tenant
analytics-sandbox1.tenant.provision.info
analytics-sandbox1.tenant.schema
analytics-sandbox1.tenant.schema.deploy
analytics-sandbox1.tenant.schema.deploy.response
analytics-sandbox1.tenant.schema.mapping
analytics-sandbox1.tenant.schema.metadata
analytics-sandbox2.audit
analytics-sandbox2.connect.configs
analytics-sandbox2.connect.offsets
analytics-sandbox2.connect.status
analytics-sandbox2.connector.template
analytics-sandbox2.data
analytics-sandbox2.definition
analytics-sandbox2.message.publisher.run.queue
analytics-sandbox2.message.publisher.runs
analytics-sandbox2.message.publisher.specs
analytics-sandbox2.queue
analytics-sandbox2.queue.state
analytics-sandbox2.report.classification
analytics-sandbox2.schema
analytics-sandbox2.schema.mapping
analytics-sandbox2.tenant
analytics-sandbox2.tenant.provision.info
analytics-sandbox2.tenant.schema
analytics-sandbox2.tenant.schema.deploy
analytics-sandbox2.tenant.schema.deploy.response
analytics-sandbox2.tenant.schema.mapping
analytics-sandbox2.tenant.schema.metadata
analytics-tvm-sandbox1.audit
analytics-tvm-sandbox1.connect.configs
analytics-tvm-sandbox1.connect.offsets
analytics-tvm-sandbox1.connect.status
analytics-tvm-sandbox1.connector.template
analytics-tvm-sandbox1.data
analytics-tvm-sandbox1.definition
analytics-tvm-sandbox1.message.publisher.run.queue
analytics-tvm-sandbox1.message.publisher.runs
analytics-tvm-sandbox1.message.publisher.specs
analytics-tvm-sandbox1.queue
analytics-tvm-sandbox1.queue.state
analytics-tvm-sandbox1.report.classification
analytics-tvm-sandbox1.schema
analytics-tvm-sandbox1.schema.mapping
analytics-tvm-sandbox1.schema.ops
analytics-tvm-sandbox1.tenant
analytics-tvm-sandbox1.tenant.provision.info
analytics-tvm-sandbox1.tenant.schema
analytics-tvm-sandbox1.tenant.schema.deploy
analytics-tvm-sandbox1.tenant.schema.deploy.response
analytics-tvm-sandbox1.tenant.schema.mapping
analytics-tvm-sandbox2.audit
analytics-tvm-sandbox2.connect.configs
analytics-tvm-sandbox2.connect.offsets
analytics-tvm-sandbox2.connect.status
analytics-tvm-sandbox2.connector.template
analytics-tvm-sandbox2.data
analytics-tvm-sandbox2.definition
analytics-tvm-sandbox2.message.publisher.run.queue
analytics-tvm-sandbox2.message.publisher.runs
analytics-tvm-sandbox2.message.publisher.specs
analytics-tvm-sandbox2.queue
analytics-tvm-sandbox2.queue.state
analytics-tvm-sandbox2.report.classification
analytics-tvm-sandbox2.schema
analytics-tvm-sandbox2.schema.mapping
analytics-tvm-sandbox2.schema.ops
analytics-tvm-sandbox2.tenant
analytics-tvm-sandbox2.tenant.provision.info
analytics-tvm-sandbox2.tenant.schema
analytics-tvm-sandbox2.tenant.schema.deploy
analytics-tvm-sandbox2.tenant.schema.deploy.response
analytics-tvm-sandbox2.tenant.schema.mapping
analytics-tvm-sandbox3.audit
analytics-tvm-sandbox3.connect.configs
analytics-tvm-sandbox3.connect.offsets
analytics-tvm-sandbox3.connect.status
analytics-tvm-sandbox3.connector.template
analytics-tvm-sandbox3.data
analytics-tvm-sandbox3.definition
analytics-tvm-sandbox3.message.publisher.run.queue
analytics-tvm-sandbox3.message.publisher.runs
analytics-tvm-sandbox3.message.publisher.specs
analytics-tvm-sandbox3.queue
analytics-tvm-sandbox3.queue.state
analytics-tvm-sandbox3.report.classification
analytics-tvm-sandbox3.schema
analytics-tvm-sandbox3.schema.mapping
analytics-tvm-sandbox3.schema.ops
analytics-tvm-sandbox3.tenant
analytics-tvm-sandbox3.tenant.provision.info
analytics-tvm-sandbox3.tenant.schema
analytics-tvm-sandbox3.tenant.schema.deploy
analytics-tvm-sandbox3.tenant.schema.deploy.response
analytics-tvm-sandbox3.tenant.schema.mapping
analytics.audit
analytics.connect.configs
analytics.connect.offsets
analytics.connect.status
analytics.connector.template
analytics.data
analytics.definition
analytics.message.publisher.run.queue
analytics.message.publisher.runs
analytics.message.publisher.specs
analytics.queue
analytics.queue.state
analytics.report.classification
analytics.schema
analytics.schema.mapping
analytics.tenant
analytics.tenant.provision.info
analytics.tenant.schema
analytics.tenant.schema.deploy
analytics.tenant.schema.deploy.response
analytics.tenant.schema.mapping
analytics.tenant.schema.metadata
cns.events
com.oracle.osvc.analytics.bookmark
com.oracle.osvc.analytics.realtime.chat.stats
com.oracle.osvc.analytics.report.publish.job
com.oracle.osvc.analytics.report.publish.job.status
com.oracle.osvc.api.clusters
com.oracle.osvc.insights.metadata
cxu-integration-ci.config
cxu-integration-ci.event
cxu-integration-ci.grp-3001aa06-b5d2-45dc-a127-21a7d1557701-tmsStore-changelog
cxu-integration-ci.grp-3afbdb1c-2dc6-4dde-ac58-3f44288cacc9-tmsStore-changelog
cxu-integration-ci.grp-3c814e3d-049e-49dc-b0d0-f3f4b08ef0bb-tmsStore-changelog
cxu-integration-ci.grp-721499d3-5384-434b-bf2b-47772b268d0e-tmsStore-changelog
cxu-integration-ci.grp-FIXED-tmsStore-changelog
cxu-integration-ci.grp-a2380cdf-34b8-4264-be04-482fe11d78a4-tmsStore-changelog
cxu-integration-ci.grp-b1443b0f-ad04-4aeb-9326-0eafda54bffe-tmsStore-changelog
cxu-integration-ci.grp-bb02376d-8094-4bba-9cd9-e48064974432-tmsStore-changelog
cxu-integration-ci.grp-ed321435-bb1f-41c5-8edc-72f5a87189f1-tmsStore-changelog
cxu-integration-ci.grp-f1473df5-52a0-4ad6-a21d-213d289a1a90-tmsStore-changelog
cxu-integration-ci.mapping
cxu-integration-ci.status
cxu-integration-sandbox1.config
cxu-integration-sandbox1.event
cxu-integration-sandbox1.grp-11b2f5b5-079e-477d-8050-c57b17ae72df-tmsStore-changelog
cxu-integration-sandbox1.grp-14661f4e-c420-4a29-bf16-72b5b97a4297-tmsStore-changelog
cxu-integration-sandbox1.grp-756c521f-c73c-4e81-ba19-4ac48363545d-tmsStore-changelog
cxu-integration-sandbox1.grp-8ac7990b-423d-47e1-8db6-c143d01a05e2-tmsStore-changelog
cxu-integration-sandbox1.grp-FIXED-tmsStore-changelog
cxu-integration-sandbox1.grp-a2c97f85-1cc9-48a7-9289-989d17b8780d-tmsStore-changelog
cxu-integration-sandbox1.grp-b19c9e19-36c6-4ddd-bc58-f026ade09c13-tmsStore-changelog
cxu-integration-sandbox1.grp-fa7dd57e-262c-4c0d-8e4e-6f8d85accc9b-tmsStore-changelog
cxu-integration-sandbox1.mapping
cxu-integration-sandbox1.status
cxu-integration-sandbox2.config
cxu-integration-sandbox2.event
cxu-integration-sandbox2.grp-063c5683-3f29-4aa3-8772-16cc6d205029-tmsStore-changelog
cxu-integration-sandbox2.grp-0e7777db-d648-4dcd-97c3-91a46817da0c-tmsStore-changelog
cxu-integration-sandbox2.grp-1147bec5-7271-4e27-8c1a-24a3319bb0bb-tmsStore-changelog
cxu-integration-sandbox2.grp-1702f1aa-df0c-4593-a735-8c0f7b7b2f5b-tmsStore-changelog
cxu-integration-sandbox2.grp-1e580292-08fa-427c-8063-c16863539cfd-tmsStore-changelog
cxu-integration-sandbox2.grp-5ee1c606-d58c-4b16-ba4e-4536a88045a6-tmsStore-changelog
cxu-integration-sandbox2.grp-6b44d921-4763-4bd4-85c1-2b17d0564f34-tmsStore-changelog
cxu-integration-sandbox2.grp-7a807eae-7945-4e81-a691-9819f587254d-tmsStore-changelog
cxu-integration-sandbox2.grp-7dae3819-a3ba-4303-80e8-321e265c2984-tmsStore-changelog
cxu-integration-sandbox2.grp-8b0ee1f5-e43c-40dd-9e60-d8c3d54defde-tmsStore-changelog
cxu-integration-sandbox2.grp-9501b9cf-da67-434b-b72f-e2ec519086c2-tmsStore-changelog
cxu-integration-sandbox2.grp-FIXED-tmsStore-changelog
cxu-integration-sandbox2.grp-a0f16c14-75b8-4ca1-8cdb-04ac28c856c1-tmsStore-changelog
cxu-integration-sandbox2.grp-a51dd81b-6a06-48d7-942b-228d731e2332-tmsStore-changelog
cxu-integration-sandbox2.grp-ae7cfa22-7d04-4527-bf47-6dcada35194e-tmsStore-changelog
cxu-integration-sandbox2.grp-cc29dee7-2d88-467a-8aba-4d7e12e62f3f-tmsStore-changelog
cxu-integration-sandbox2.grp-ce250441-81de-4cad-bbd2-ff123dfc04ac-tmsStore-changelog
cxu-integration-sandbox2.grp-f6bfcc76-eeda-4bf4-8674-c0b9ae818a52-tmsStore-changelog
cxu-integration-sandbox2.mapping
cxu-integration-sandbox2.status
cxu-integration-sandbox3.config
cxu-integration-sandbox3.event
cxu-integration-sandbox3.grp-1b1d0b82-23ef-4116-9a1d-a25daf035618-tmsStore-changelog
cxu-integration-sandbox3.grp-4cdf3c7d-9323-4cce-b54e-7c5844e43f06-tmsStore-changelog
cxu-integration-sandbox3.grp-4fd122e0-00b0-4d6b-9b1f-e13b50fe517b-tmsStore-changelog
cxu-integration-sandbox3.grp-71a25b3b-37e5-4f70-ab03-09b958f7f95b-tmsStore-changelog
cxu-integration-sandbox3.grp-71fd48a5-02e9-40d3-a992-5054f73779a7-tmsStore-changelog
cxu-integration-sandbox3.grp-76df0dd7-d682-41a5-9e88-522fada8f488-tmsStore-changelog
cxu-integration-sandbox3.grp-8b2fe9e0-511a-4934-8651-d2c88c32065b-tmsStore-changelog
cxu-integration-sandbox3.grp-FIXED-tmsStore-changelog
cxu-integration-sandbox3.grp-FIXED-tmsStore2-changelog
cxu-integration-sandbox3.grp-FIXED2-tmsStore2-changelog
cxu-integration-sandbox3.grp-FIXED3-tmsStore3-changelog
cxu-integration-sandbox3.grp-FIXED4-tmsStore4-changelog
cxu-integration-sandbox3.grp-FIXED5-tmsStore5-changelog
cxu-integration-sandbox3.grp-FIXEDa-tmsStore-changelog
cxu-integration-sandbox3.grp-acfc59eb-1fe7-410f-bd74-27138e05641b-tmsStore-changelog
cxu-integration-sandbox3.grp-c885f5e1-5917-45cb-96fc-38b6e5930a63-tmsStore-changelog
cxu-integration-sandbox3.grp-d16e657c-b518-4e33-93b6-cf63b3822eca-tmsStore-changelog
cxu-integration-sandbox3.grp-e5ff2c3d-04c7-4456-9764-c2adb94fde3b-tmsStore-changelog
cxu-integration-sandbox3.grp-e7303566-9405-490d-8fb4-c56c5cd8fb5a-tmsStore-changelog
cxu-integration-sandbox3.grp-f9013146-7728-4d65-b559-b4471266a760-tmsStore-changelog
cxu-integration-sandbox3.mapping
cxu-integration-sandbox3.status
cxu-integration.config
cxu-integration.event
cxu-integration.grp-0a175b5a-d850-4283-988c-d0497bdf4ff0-tmsStore-changelog
cxu-integration.grp-431bc6c7-3317-4595-9293-73b838edbcc1-tmsStore-changelog
cxu-integration.grp-FIXED-tmsStore-changelog
cxu-integration.mapping
cxu-integration.status
data-pipeline-dev1.binlog.raw.event
data-pipeline-dev1.com.object.event
data-pipeline-dev1.com.schema.metadata
data-pipeline-dev1.com.schema.status
data-pipeline-dev1.com.schema.update
data-pipeline-dev1.dbdq62a-history
data-pipeline-dev1.dbdq62b-history
data-pipeline-dev1.delete.tenant
data-pipeline-dev1.event.trigger
data-pipeline-dev1.kafka-connect-configs
data-pipeline-dev1.kafka-connect-offsets
data-pipeline-dev1.kafka-connect-status
data-pipeline-dev1.mapping.metadata
data-pipeline-dev1.provisioned.site.binlog.record
data-pipeline-dev1.raw2com-PendingCoLookup-changelog
data-pipeline-dev1.raw2com-customFieldMenuLabelLookup-changelog
data-pipeline-dev1.raw2com-foldersLookup-changelog
data-pipeline-dev1.raw2com-hierMenusLookup-changelog
data-pipeline-dev1.raw2com-labelMenuLabelLookup-changelog
data-pipeline-dev1.raw2com-pendingCustomFieldMenuLabels-changelog
data-pipeline-dev1.raw2com-pendingFoldersLabelsLookup-changelog
data-pipeline-dev1.raw2com-pendingHierMenusLabelsLookup-changelog
data-pipeline-dev1.raw2com-pendingLabelMenuLabels-changelog
data-pipeline-dev1.raw2com-pendingStatusesLabelsLookup-changelog
data-pipeline-dev1.raw2com-statusesLookup-changelog
data-pipeline-dev1.raw2com.app-PendingCoLookup-changelog
data-pipeline-dev1.raw2com.app-customFieldMenuLabelLookup-changelog
data-pipeline-dev1.raw2com.app-foldersLookup-changelog
data-pipeline-dev1.raw2com.app-hierMenusLookup-changelog
data-pipeline-dev1.raw2com.app-labelMenuLabelLookup-changelog
data-pipeline-dev1.raw2com.app-pendingCustomFieldMenuLabels-changelog
data-pipeline-dev1.raw2com.app-pendingFoldersLabelsLookup-changelog
data-pipeline-dev1.raw2com.app-pendingHierMenusLabelsLookup-changelog
data-pipeline-dev1.raw2com.app-pendingLabelMenuLabels-changelog
data-pipeline-dev1.raw2com.app-pendingStatusesLabelsLookup-changelog
data-pipeline-dev1.raw2com.app-statusesLookup-changelog
data-pipeline-dev1.schema-service-DeletedObjectIdLookup-changelog
data-pipeline-dev1.schema-service-FieldMetadataLookup-changelog
data-pipeline-dev1.schema-service-KeyFieldLookup-changelog
data-pipeline-dev1.schema-service-KeyMetadataLookup-changelog
data-pipeline-dev1.schema-service-Object2SchemaMappingLookup-changelog
data-pipeline-dev1.schema-service-ObjectMetadataLookup-changelog
data-pipeline-dev1.schema-service-PackageDataLookup-changelog
data-pipeline-dev1.schema-service-RelationshipLookup-changelog
data-pipeline-dev1.schema-service-TableMappingLookup-changelog
data-pipeline-dev1.schema-service-encrypt1-DeletedObjectIdLookup-changelog
data-pipeline-dev1.schema-service-encrypt1-FieldMetadataLookup-changelog
data-pipeline-dev1.schema-service-encrypt1-KeyFieldLookup-changelog
data-pipeline-dev1.schema-service-encrypt1-KeyMetadataLookup-changelog
data-pipeline-dev1.schema-service-encrypt1-Object2SchemaMappingLookup-changelog
data-pipeline-dev1.schema-service-encrypt1-ObjectMetadataLookup-changelog
data-pipeline-dev1.schema-service-encrypt1-PackageDataLookup-changelog
data-pipeline-dev1.schema-service-encrypt1-RelationshipLookup-changelog
data-pipeline-dev1.schema-service-encrypt1-TableMappingLookup-changelog
data-pipeline-dev1.schema-service.1-ObjectMetadataLookup-changelog
data-pipeline-dev1.schema-service.1-PackageDataLookup-changelog
data-pipeline-dev1.schema-service.4-DeletedObjectIdLookup-changelog
data-pipeline-dev1.schema-service.4-FieldMetadataLookup-changelog
data-pipeline-dev1.schema-service.4-KeyFieldLookup-changelog
data-pipeline-dev1.schema-service.4-KeyMetadataLookup-changelog
data-pipeline-dev1.schema-service.4-Object2SchemaMappingLookup-changelog
data-pipeline-dev1.schema-service.4-ObjectMetadataLookup-changelog
data-pipeline-dev1.schema-service.4-PackageDataLookup-changelog
data-pipeline-dev1.schema-service.4-RelationshipLookup-changelog
data-pipeline-dev1.schema-service.4-TableMappingLookup-changelog
data-pipeline-dev1.schema.versions
data-pipeline-dev1.std.object.schema.versions
data-pipeline-dev1.tenant.product.version
data-pipeline-dev2.binlog.raw.event
data-pipeline-dev2.com.object.event
data-pipeline-dev2.com.schema.metadata
data-pipeline-dev2.com.schema.status
data-pipeline-dev2.com.schema.update
data-pipeline-dev2.dbdq62a-history
data-pipeline-dev2.dbdq62b-history
data-pipeline-dev2.delete.tenant
data-pipeline-dev2.event.trigger
data-pipeline-dev2.kafka-connect-configs
data-pipeline-dev2.kafka-connect-offsets
data-pipeline-dev2.kafka-connect-status
data-pipeline-dev2.mapping.metadata
data-pipeline-dev2.provisioned.site.binlog.record
data-pipeline-dev2.raw2com-PendingCoLookup-changelog
data-pipeline-dev2.raw2com-customFieldMenuLabelLookup-changelog
data-pipeline-dev2.raw2com-foldersLookup-changelog
data-pipeline-dev2.raw2com-hierMenusLookup-changelog
data-pipeline-dev2.raw2com-labelMenuLabelLookup-changelog
data-pipeline-dev2.raw2com-pendingCustomFieldMenuLabels-changelog
data-pipeline-dev2.raw2com-pendingFoldersLabelsLookup-changelog
data-pipeline-dev2.raw2com-pendingHierMenusLabelsLookup-changelog
data-pipeline-dev2.raw2com-pendingLabelMenuLabels-changelog
data-pipeline-dev2.raw2com-pendingStatusesLabelsLookup-changelog
data-pipeline-dev2.raw2com-statusesLookup-changelog
data-pipeline-dev2.raw2com.app-PendingCoLookup-changelog
data-pipeline-dev2.raw2com.app-customFieldMenuLabelLookup-changelog
data-pipeline-dev2.raw2com.app-foldersLookup-changelog
data-pipeline-dev2.raw2com.app-hierMenusLookup-changelog
data-pipeline-dev2.raw2com.app-labelMenuLabelLookup-changelog
data-pipeline-dev2.raw2com.app-pendingCustomFieldMenuLabels-changelog
data-pipeline-dev2.raw2com.app-pendingFoldersLabelsLookup-changelog
data-pipeline-dev2.raw2com.app-pendingHierMenusLabelsLookup-changelog
data-pipeline-dev2.raw2com.app-pendingLabelMenuLabels-changelog
data-pipeline-dev2.raw2com.app-pendingStatusesLabelsLookup-changelog
data-pipeline-dev2.raw2com.app-statusesLookup-changelog
data-pipeline-dev2.schema-service-DeletedObjectIdLookup-changelog
data-pipeline-dev2.schema-service-FieldMetadataLookup-changelog
data-pipeline-dev2.schema-service-KeyFieldLookup-changelog
data-pipeline-dev2.schema-service-KeyMetadataLookup-changelog
data-pipeline-dev2.schema-service-Object2SchemaMappingLookup-changelog
data-pipeline-dev2.schema-service-ObjectMetadataLookup-changelog
data-pipeline-dev2.schema-service-PackageDataLookup-changelog
data-pipeline-dev2.schema-service-RelationshipLookup-changelog
data-pipeline-dev2.schema-service-TableMappingLookup-changelog
data-pipeline-dev2.schema-service-encrypt1-DeletedObjectIdLookup-changelog
data-pipeline-dev2.schema-service-encrypt1-FieldMetadataLookup-changelog
data-pipeline-dev2.schema-service-encrypt1-KeyFieldLookup-changelog
data-pipeline-dev2.schema-service-encrypt1-KeyMetadataLookup-changelog
data-pipeline-dev2.schema-service-encrypt1-Object2SchemaMappingLookup-changelog
data-pipeline-dev2.schema-service-encrypt1-ObjectMetadataLookup-changelog
data-pipeline-dev2.schema-service-encrypt1-PackageDataLookup-changelog
data-pipeline-dev2.schema-service-encrypt1-RelationshipLookup-changelog
data-pipeline-dev2.schema-service-encrypt1-TableMappingLookup-changelog
data-pipeline-dev2.schema-service.4-DeletedObjectIdLookup-changelog
data-pipeline-dev2.schema-service.4-FieldMetadataLookup-changelog
data-pipeline-dev2.schema-service.4-KeyFieldLookup-changelog
data-pipeline-dev2.schema-service.4-KeyMetadataLookup-changelog
data-pipeline-dev2.schema-service.4-Object2SchemaMappingLookup-changelog
data-pipeline-dev2.schema-service.4-ObjectMetadataLookup-changelog
data-pipeline-dev2.schema-service.4-PackageDataLookup-changelog
data-pipeline-dev2.schema-service.4-RelationshipLookup-changelog
data-pipeline-dev2.schema-service.4-TableMappingLookup-changelog
data-pipeline-dev2.schema.versions
data-pipeline-dev2.std.object.schema.versions
data-pipeline-dev2.tenant.product.version
data-pipeline-dev3.binlog.raw.event
data-pipeline-dev3.com.object.event
data-pipeline-dev3.com.schema.metadata
data-pipeline-dev3.com.schema.status
data-pipeline-dev3.com.schema.update
data-pipeline-dev3.dbdq62a-history
data-pipeline-dev3.dbdq62b-history
data-pipeline-dev3.delete.tenant
data-pipeline-dev3.event.trigger
data-pipeline-dev3.kafka-connect-configs
data-pipeline-dev3.kafka-connect-offsets
data-pipeline-dev3.kafka-connect-status
data-pipeline-dev3.mapping.metadata
data-pipeline-dev3.provisioned.site.binlog.record
data-pipeline-dev3.raw2com-PendingCoLookup-changelog
data-pipeline-dev3.raw2com-customFieldMenuLabelLookup-changelog
data-pipeline-dev3.raw2com-foldersLookup-changelog
data-pipeline-dev3.raw2com-hierMenusLookup-changelog
data-pipeline-dev3.raw2com-labelMenuLabelLookup-changelog
data-pipeline-dev3.raw2com-pendingCustomFieldMenuLabels-changelog
data-pipeline-dev3.raw2com-pendingFoldersLabelsLookup-changelog
data-pipeline-dev3.raw2com-pendingHierMenusLabelsLookup-changelog
data-pipeline-dev3.raw2com-pendingLabelMenuLabels-changelog
data-pipeline-dev3.raw2com-pendingStatusesLabelsLookup-changelog
data-pipeline-dev3.raw2com-statusesLookup-changelog
data-pipeline-dev3.raw2com.app-customFieldMenuLabelLookup-changelog
data-pipeline-dev3.raw2com.app-foldersLookup-changelog
data-pipeline-dev3.raw2com.app-hierMenusLookup-changelog
data-pipeline-dev3.raw2com.app-labelMenuLabelLookup-changelog
data-pipeline-dev3.raw2com.app-pendingCustomFieldMenuLabels-changelog
data-pipeline-dev3.raw2com.app-pendingFoldersLabelsLookup-changelog
data-pipeline-dev3.raw2com.app-pendingHierMenusLabelsLookup-changelog
data-pipeline-dev3.raw2com.app-pendingLabelMenuLabels-changelog
data-pipeline-dev3.raw2com.app-pendingStatusesLabelsLookup-changelog
data-pipeline-dev3.raw2com.app-statusesLookup-changelog
data-pipeline-dev3.schema-service-DeletedObjectIdLookup-changelog
data-pipeline-dev3.schema-service-FieldMetadataLookup-changelog
data-pipeline-dev3.schema-service-KeyFieldLookup-changelog
data-pipeline-dev3.schema-service-KeyMetadataLookup-changelog
data-pipeline-dev3.schema-service-Object2SchemaMappingLookup-changelog
data-pipeline-dev3.schema-service-ObjectMetadataLookup-changelog
data-pipeline-dev3.schema-service-PackageDataLookup-changelog
data-pipeline-dev3.schema-service-RelationshipLookup-changelog
data-pipeline-dev3.schema-service-TableMappingLookup-changelog
data-pipeline-dev3.schema-service-encrypt1-DeletedObjectIdLookup-changelog
data-pipeline-dev3.schema-service-encrypt1-FieldMetadataLookup-changelog
data-pipeline-dev3.schema-service-encrypt1-KeyFieldLookup-changelog
data-pipeline-dev3.schema-service-encrypt1-KeyMetadataLookup-changelog
data-pipeline-dev3.schema-service-encrypt1-Object2SchemaMappingLookup-changelog
data-pipeline-dev3.schema-service-encrypt1-ObjectMetadataLookup-changelog
data-pipeline-dev3.schema-service-encrypt1-PackageDataLookup-changelog
data-pipeline-dev3.schema-service-encrypt1-RelationshipLookup-changelog
data-pipeline-dev3.schema-service-encrypt1-TableMappingLookup-changelog
data-pipeline-dev3.schema-service-encrypt2-DeletedObjectIdLookup-changelog
data-pipeline-dev3.schema-service-encrypt2-FieldMetadataLookup-changelog
data-pipeline-dev3.schema-service-encrypt2-KeyFieldLookup-changelog
data-pipeline-dev3.schema-service-encrypt2-KeyMetadataLookup-changelog
data-pipeline-dev3.schema-service-encrypt2-ObjectMetadataLookup-changelog
data-pipeline-dev3.schema-service-encrypt2-PackageDataLookup-changelog
data-pipeline-dev3.schema-service-encrypt2-RelationshipLookup-changelog
data-pipeline-dev3.schema-service-encrypt2-TableMappingLookup-changelog
data-pipeline-dev3.schema.versions
data-pipeline-dev3.std.object.schema.versions
data-pipeline-dev3.tenant.product.version
data-pipeline-dev4.binlog.raw.event
data-pipeline-dev4.com.object.event
data-pipeline-dev4.com.schema.metadata
data-pipeline-dev4.com.schema.status
data-pipeline-dev4.com.schema.update
data-pipeline-dev4.dbdq62a-history
data-pipeline-dev4.dbdq62b-history
data-pipeline-dev4.delete.tenant
data-pipeline-dev4.event.trigger
data-pipeline-dev4.kafka-connect-configs
data-pipeline-dev4.kafka-connect-offsets
data-pipeline-dev4.kafka-connect-status
data-pipeline-dev4.mapping.metadata
data-pipeline-dev4.provisioned.site.binlog.record
data-pipeline-dev4.raw2com-customFieldMenuLabelLookup-changelog
data-pipeline-dev4.raw2com-foldersLookup-changelog
data-pipeline-dev4.raw2com-hierMenusLookup-changelog
data-pipeline-dev4.raw2com-labelMenuLabelLookup-changelog
data-pipeline-dev4.raw2com-pendingCustomFieldMenuLabels-changelog
data-pipeline-dev4.raw2com-pendingFoldersLabelsLookup-changelog
data-pipeline-dev4.raw2com-pendingHierMenusLabelsLookup-changelog
data-pipeline-dev4.raw2com-pendingLabelMenuLabels-changelog
data-pipeline-dev4.raw2com-pendingStatusesLabelsLookup-changelog
data-pipeline-dev4.raw2com-statusesLookup-changelog
data-pipeline-dev4.raw2com.app-hierMenusLookup-changelog
data-pipeline-dev4.raw2com.app-labelMenuLabelLookup-changelog
data-pipeline-dev4.raw2com.app-pendingHierMenusLabelsLookup-changelog
data-pipeline-dev4.raw2com.app-pendingLabelMenuLabels-changelog
data-pipeline-dev4.raw2com.app-pendingStatusesLabelsLookup-changelog
data-pipeline-dev4.raw2com.app-statusesLookup-changelog
data-pipeline-dev4.raw2com.appid-labelMenuLabelLookup-changelog
data-pipeline-dev4.raw2com.appid-pendingLabelMenuLabels-changelog
data-pipeline-dev4.raw2com.appid-pendingStatusesLabelsLookup-changelog
data-pipeline-dev4.raw2com.appid-statusesLookup-changelog
data-pipeline-dev4.schema-service-5-ObjectMetadataLookup-changelog
data-pipeline-dev4.schema-service-5-PackageDataLookup-changelog
data-pipeline-dev4.schema-service-DeletedObjectIdLookup-changelog
data-pipeline-dev4.schema-service-FieldMetadataLookup-changelog
data-pipeline-dev4.schema-service-KeyFieldLookup-changelog
data-pipeline-dev4.schema-service-KeyMetadataLookup-changelog
data-pipeline-dev4.schema-service-Object2SchemaMappingLookup-changelog
data-pipeline-dev4.schema-service-ObjectMetadataLookup-changelog
data-pipeline-dev4.schema-service-PackageDataLookup-changelog
data-pipeline-dev4.schema-service-RelationshipLookup-changelog
data-pipeline-dev4.schema-service-TableMappingLookup-changelog
data-pipeline-dev4.schema-service-encrypt1-DeletedObjectIdLookup-changelog
data-pipeline-dev4.schema-service-encrypt1-FieldMetadataLookup-changelog
data-pipeline-dev4.schema-service-encrypt1-KeyFieldLookup-changelog
data-pipeline-dev4.schema-service-encrypt1-KeyMetadataLookup-changelog
data-pipeline-dev4.schema-service-encrypt1-Object2SchemaMappingLookup-changelog
data-pipeline-dev4.schema-service-encrypt1-ObjectMetadataLookup-changelog
data-pipeline-dev4.schema-service-encrypt1-PackageDataLookup-changelog
data-pipeline-dev4.schema-service-encrypt1-RelationshipLookup-changelog
data-pipeline-dev4.schema-service-encrypt1-TableMappingLookup-changelog
data-pipeline-dev4.schema-service.1-FieldMetadataLookup-changelog
data-pipeline-dev4.schema-service.1-KeyFieldLookup-changelog
data-pipeline-dev4.schema-service.1-KeyMetadataLookup-changelog
data-pipeline-dev4.schema-service.1-ObjectMetadataLookup-changelog
data-pipeline-dev4.schema-service.1-PackageDataLookup-changelog
data-pipeline-dev4.schema-service.2-FieldMetadataLookup-changelog
data-pipeline-dev4.schema-service.2-KeyFieldLookup-changelog
data-pipeline-dev4.schema-service.2-KeyMetadataLookup-changelog
data-pipeline-dev4.schema-service.2-ObjectMetadataLookup-changelog
data-pipeline-dev4.schema-service.2-PackageDataLookup-changelog
data-pipeline-dev4.schema-service.3-FieldMetadataLookup-changelog
data-pipeline-dev4.schema-service.3-KeyFieldLookup-changelog
data-pipeline-dev4.schema-service.3-KeyMetadataLookup-changelog
data-pipeline-dev4.schema-service.3-ObjectMetadataLookup-changelog
data-pipeline-dev4.schema-service.3-PackageDataLookup-changelog
data-pipeline-dev4.schema.versions
data-pipeline-dev4.std.object.schema.versions
data-pipeline-dev4.tenant.product.version
data-pipeline-e2e.binlog.raw.event
data-pipeline-e2e.com.object.event
data-pipeline-e2e.com.schema.metadata
data-pipeline-e2e.com.schema.status
data-pipeline-e2e.com.schema.update
data-pipeline-e2e.dbdq62a-history
data-pipeline-e2e.dbdq62b-history
data-pipeline-e2e.kafka-connect-configs
data-pipeline-e2e.kafka-connect-offsets
data-pipeline-e2e.kafka-connect-status
data-pipeline-e2e.raw2com-labelMenuLabelLookup-changelog
data-pipeline-e2e.raw2com-pendingLabelMenuLabels-changelog
data-pipeline-e2e.raw2com-pendingStatusesLabelsLookup-changelog
data-pipeline-e2e.raw2com-statusesLookup-changelog
data-pipeline-e2e.schema-service-ObjectMetadataLookup-changelog
data-pipeline-e2e.schema-service-PackageDataLookup-changelog
data-pipeline-kf1.schema.version
data-pipeline-kf1.schema.versions
data-pipeline-psr.binlog.raw.event
data-pipeline-psr.com.object.event
data-pipeline-psr.com.schema.metadata
data-pipeline-psr.com.schema.status
data-pipeline-psr.com.schema.update
data-pipeline-psr.dbdq62a-history
data-pipeline-psr.dbdq62b-history
data-pipeline-psr.delete.tenant
data-pipeline-psr.event.trigger
data-pipeline-psr.kafka-connect-configs
data-pipeline-psr.kafka-connect-offsets
data-pipeline-psr.kafka-connect-status
data-pipeline-psr.mapping.metadata
data-pipeline-psr.provisioned.site.binlog.record
data-pipeline-psr.schema.versions
data-pipeline-psr.std.object.schema.versions
data-pipeline-psr.tenant.product.version
data-pipeline-qa.binlog.raw.event
data-pipeline-qa.com.object.event
data-pipeline-qa.com.schema.metadata
data-pipeline-qa.com.schema.status
data-pipeline-qa.com.schema.update
data-pipeline-qa.dbdq62a-history
data-pipeline-qa.dbdq62b-history
data-pipeline-qa.delete.tenant
data-pipeline-qa.event.trigger
data-pipeline-qa.kafka-connect-configs
data-pipeline-qa.kafka-connect-offsets
data-pipeline-qa.kafka-connect-status
data-pipeline-qa.mapping.metadata
data-pipeline-qa.provisioned.site.binlog.record
data-pipeline-qa.raw2com-labelMenuLabelLookup-changelog
data-pipeline-qa.raw2com-pendingLabelMenuLabels-changelog
data-pipeline-qa.raw2com-pendingStatusesLabelsLookup-changelog
data-pipeline-qa.raw2com-statusesLookup-changelog
data-pipeline-qa.schema-service-ObjectMetadataLookup-changelog
data-pipeline-qa.schema-service-PackageDataLookup-changelog
data-pipeline-qa.schema.versions
data-pipeline-qa.std.object.schema.versions
data-pipeline-qa.tenant.product.version
data-pipeline.binlog.raw.event
data-pipeline.com.object.event
data-pipeline.com.schema.metadata
data-pipeline.com.schema.status
data-pipeline.com.schema.update
data-pipeline.dbdq62a-history
data-pipeline.dbdq62b-history
data-pipeline.delete.tenant
data-pipeline.event.trigger
data-pipeline.kafka-connect-configs
data-pipeline.kafka-connect-offsets
data-pipeline.kafka-connect-status
data-pipeline.mapping.metadata
data-pipeline.provisioned.site.binlog.record
data-pipeline.raw2com-hierMenusLookup-changelog
data-pipeline.raw2com-labelMenuLabelLookup-changelog
data-pipeline.raw2com-pendingHierMenusLabelsLookup-changelog
data-pipeline.raw2com-pendingLabelMenuLabels-changelog
data-pipeline.raw2com-pendingStatusesLabelsLookup-changelog
data-pipeline.raw2com-statusesLookup-changelog
data-pipeline.raw2com.app-PendingCoLookup-changelog
data-pipeline.raw2com.app-customFieldMenuLabelLookup-changelog
data-pipeline.raw2com.app-foldersLookup-changelog
data-pipeline.raw2com.app-hierMenusLookup-changelog
data-pipeline.raw2com.app-labelMenuLabelLookup-changelog
data-pipeline.raw2com.app-pendingCustomFieldMenuLabels-changelog
data-pipeline.raw2com.app-pendingFoldersLabelsLookup-changelog
data-pipeline.raw2com.app-pendingHierMenusLabelsLookup-changelog
data-pipeline.raw2com.app-pendingLabelMenuLabels-changelog
data-pipeline.raw2com.app-pendingStatusesLabelsLookup-changelog
data-pipeline.raw2com.app-statusesLookup-changelog
data-pipeline.schema-service-FieldMetadataLookup-changelog
data-pipeline.schema-service-KeyFieldLookup-changelog
data-pipeline.schema-service-KeyMetadataLookup-changelog
data-pipeline.schema-service-ObjectMetadataLookup-changelog
data-pipeline.schema-service-PackageDataLookup-changelog
data-pipeline.schema-service-RelationshipLookup-changelog
data-pipeline.schema-service.1-FieldMetadataLookup-changelog
data-pipeline.schema-service.1-KeyFieldLookup-changelog
data-pipeline.schema-service.1-KeyMetadataLookup-changelog
data-pipeline.schema-service.1-ObjectMetadataLookup-changelog
data-pipeline.schema-service.1-PackageDataLookup-changelog
data-pipeline.schema-service.2-FieldMetadataLookup-changelog
data-pipeline.schema-service.2-KeyFieldLookup-changelog
data-pipeline.schema-service.2-KeyMetadataLookup-changelog
data-pipeline.schema-service.2-ObjectMetadataLookup-changelog
data-pipeline.schema-service.2-PackageDataLookup-changelog
data-pipeline.schema-service.3-FieldMetadataLookup-changelog
data-pipeline.schema-service.3-KeyFieldLookup-changelog
data-pipeline.schema-service.3-KeyMetadataLookup-changelog
data-pipeline.schema-service.3-ObjectMetadataLookup-changelog
data-pipeline.schema-service.3-PackageDataLookup-changelog
data-pipeline.schema-service.4-DeletedObjectIdLookup-changelog
data-pipeline.schema-service.4-FieldMetadataLookup-changelog
data-pipeline.schema-service.4-KeyFieldLookup-changelog
data-pipeline.schema-service.4-KeyMetadataLookup-changelog
data-pipeline.schema-service.4-Object2SchemaMappingLookup-changelog
data-pipeline.schema-service.4-ObjectMetadataLookup-changelog
data-pipeline.schema-service.4-PackageDataLookup-changelog
data-pipeline.schema-service.4-RelationshipLookup-changelog
data-pipeline.schema-service.4-TableMappingLookup-changelog
data-pipeline.schema.versions
data-pipeline.std.object.schema.versions
data-pipeline.tenant.product.version
dms.analytics.tenant.request
dms.processor-MetadataStateStore-changelog
dms.tenant.request
dms.tenant.response
dmsci.analytics.tenant.request
dmsci.processor-MetadataStateStore-changelog
dmsci.tenant.request
dmsci.tenant.response
dmspsr.analytics.tenant.request
dmspsr.processor-MetadataStateStore-changelog
dmspsr.tenant.request
dmspsr.tenant.response
dmstest.analytics.tenant.request
dmstest.processor-MetadataStateStore-changelog
dmstest.processor-PdbMetadataStateStore-changelog
dmstest.tenant.request
dmstest.tenant.response
escalation-user-aggregator-20211001-aggregate.escalations.store-changelog
escalation-user-aggregator-20211001-expired-changelog
escalation-user-aggregator-20211001-schedule.escalations.store-changelog
escalation-user-aggregator-20211125-aggregate.escalations.store-changelog
escalation-user-aggregator-20211125-expired-changelog
escalation-user-aggregator-20211125-schedule.escalations.store-changelog
escalation-user-aggregator-2021112506f0e874-b893-4a52-9354-c37358bd7012-aggregate.escalations.store-changelog
escalation-user-aggregator-2021112506f0e874-b893-4a52-9354-c37358bd7012-expired-changelog
escalation-user-aggregator-2021112506f0e874-b893-4a52-9354-c37358bd7012-schedule.escalations.store-changelog
escalation-user-aggregator-2021112512862ea0-32fe-4a6a-9b34-d56e983cf583-aggregate.escalations.store-changelog
escalation-user-aggregator-2021112512862ea0-32fe-4a6a-9b34-d56e983cf583-expired-changelog
escalation-user-aggregator-2021112512862ea0-32fe-4a6a-9b34-d56e983cf583-schedule.escalations.store-changelog
escalation-user-aggregator-202111257560ccbd-d3a8-48aa-a642-373afeb0ba3d-aggregate.escalations.store-changelog
escalation-user-aggregator-202111257560ccbd-d3a8-48aa-a642-373afeb0ba3d-expired-changelog
escalation-user-aggregator-202111257560ccbd-d3a8-48aa-a642-373afeb0ba3d-schedule.escalations.store-changelog
escalation-user-aggregator-2021112583a59b07-af31-42da-adeb-0f21d4d529e6-aggregate.escalations.store-changelog
escalation-user-aggregator-2021112583a59b07-af31-42da-adeb-0f21d4d529e6-expired-changelog
escalation-user-aggregator-2021112583a59b07-af31-42da-adeb-0f21d4d529e6-schedule.escalations.store-changelog
escalation-user-aggregator-2021112589a49af6-9fe7-4f09-b5a7-8eeda5e9b056-aggregate.escalations.store-changelog
escalation-user-aggregator-2021112589a49af6-9fe7-4f09-b5a7-8eeda5e9b056-expired-changelog
escalation-user-aggregator-2021112589a49af6-9fe7-4f09-b5a7-8eeda5e9b056-schedule.escalations.store-changelog
escalation-user-aggregator-202111258baa394f-fa35-4234-bc5c-4258d38e1400-aggregate.escalations.store-changelog
escalation-user-aggregator-202111258baa394f-fa35-4234-bc5c-4258d38e1400-expired-changelog
escalation-user-aggregator-202111258baa394f-fa35-4234-bc5c-4258d38e1400-schedule.escalations.store-changelog
escalation-user-aggregator-20211125a012742a-4bfd-49be-90e5-060fda634e5e-aggregate.escalations.store-changelog
escalation-user-aggregator-20211125a012742a-4bfd-49be-90e5-060fda634e5e-expired-changelog
escalation-user-aggregator-20211125a012742a-4bfd-49be-90e5-060fda634e5e-schedule.escalations.store-changelog
escalation-user-aggregator-20211125a7c0d6d3-d67c-4897-9f6a-5c56e81339f7-aggregate.escalations.store-changelog
escalation-user-aggregator-20211125a7c0d6d3-d67c-4897-9f6a-5c56e81339f7-expired-changelog
escalation-user-aggregator-20211125a7c0d6d3-d67c-4897-9f6a-5c56e81339f7-schedule.escalations.store-changelog
escalation-user-aggregator-20211125b1a70cc6-e5d5-406b-b35d-7c70a121cd06-aggregate.escalations.store-changelog
escalation-user-aggregator-20211125b1a70cc6-e5d5-406b-b35d-7c70a121cd06-expired-changelog
escalation-user-aggregator-20211125b1a70cc6-e5d5-406b-b35d-7c70a121cd06-schedule.escalations.store-changelog
escalation-user-aggregator-20211125ca3205d2-d3f5-48e8-95ad-6ecd9783922a-aggregate.escalations.store-changelog
escalation-user-aggregator-20211125ca3205d2-d3f5-48e8-95ad-6ecd9783922a-expired-changelog
escalation-user-aggregator-20211125ca3205d2-d3f5-48e8-95ad-6ecd9783922a-schedule.escalations.store-changelog
escalation-user-aggregator-20211125defc2123-a657-4c52-85e8-59a9984255d1-aggregate.escalations.store-changelog
escalation-user-aggregator-20211125defc2123-a657-4c52-85e8-59a9984255d1-expired-changelog
escalation-user-aggregator-20211125defc2123-a657-4c52-85e8-59a9984255d1-schedule.escalations.store-changelog
escalation-user-aggregator-20211125e80f3f5f-cfc0-4c7b-a628-43e8a2b852d5-aggregate.escalations.store-changelog
escalation-user-aggregator-20211125e80f3f5f-cfc0-4c7b-a628-43e8a2b852d5-expired-changelog
escalation-user-aggregator-20211125e80f3f5f-cfc0-4c7b-a628-43e8a2b852d5-schedule.escalations.store-changelog
escalation-user-aggregator-20211126-aggregate.escalations.store-changelog
escalation-user-aggregator-20211126-expired-changelog
escalation-user-aggregator-20211126-schedule.escalations.store-changelog
escalation-user-aggregator-202111261cb0628f-0f0d-4f65-9a9c-945af7deb4ff-aggregate.escalations.store-changelog
escalation-user-aggregator-202111261cb0628f-0f0d-4f65-9a9c-945af7deb4ff-expired-changelog
escalation-user-aggregator-202111261cb0628f-0f0d-4f65-9a9c-945af7deb4ff-schedule.escalations.store-changelog
escalation-user-aggregator-2021112635834394-eb6e-4a82-ae37-d5434e07f2a3-aggregate.escalations.store-changelog
escalation-user-aggregator-2021112635834394-eb6e-4a82-ae37-d5434e07f2a3-expired-changelog
escalation-user-aggregator-2021112635834394-eb6e-4a82-ae37-d5434e07f2a3-schedule.escalations.store-changelog
escalation-user-aggregator-202111266081308b-f4fe-462e-8b0a-61e848b545f2-aggregate.escalations.store-changelog
escalation-user-aggregator-202111266081308b-f4fe-462e-8b0a-61e848b545f2-expired-changelog
escalation-user-aggregator-202111266081308b-f4fe-462e-8b0a-61e848b545f2-schedule.escalations.store-changelog
escalation-user-aggregator-2021112679281ecd-f1f5-431c-9fff-b1153ed1e6c4-aggregate.escalations.store-changelog
escalation-user-aggregator-2021112679281ecd-f1f5-431c-9fff-b1153ed1e6c4-expired-changelog
escalation-user-aggregator-2021112679281ecd-f1f5-431c-9fff-b1153ed1e6c4-schedule.escalations.store-changelog
escalation-user-aggregator-202111268afc704b-a57d-4597-9739-49025830f4d0-aggregate.escalations.store-changelog
escalation-user-aggregator-202111268afc704b-a57d-4597-9739-49025830f4d0-expired-changelog
escalation-user-aggregator-202111268afc704b-a57d-4597-9739-49025830f4d0-schedule.escalations.store-changelog
escalation-user-aggregator-20211126ca0868f4-7ec2-4c48-9ff9-0989752c86d9-aggregate.escalations.store-changelog
escalation-user-aggregator-20211126ca0868f4-7ec2-4c48-9ff9-0989752c86d9-expired-changelog
escalation-user-aggregator-20211126ca0868f4-7ec2-4c48-9ff9-0989752c86d9-schedule.escalations.store-changelog
escalation-user-aggregator-20211126dbe4b1b3-9e7e-43cb-8ba7-cfa2408b6c8d-aggregate.escalations.store-changelog
escalation-user-aggregator-20211126dbe4b1b3-9e7e-43cb-8ba7-cfa2408b6c8d-expired-changelog
escalation-user-aggregator-20211126dbe4b1b3-9e7e-43cb-8ba7-cfa2408b6c8d-schedule.escalations.store-changelog
escalation-user-aggregator-20211126fc081d37-1da6-494b-a528-7a29d596e69b-aggregate.escalations.store-changelog
escalation-user-aggregator-20211126fc081d37-1da6-494b-a528-7a29d596e69b-expired-changelog
escalation-user-aggregator-20211126fc081d37-1da6-494b-a528-7a29d596e69b-schedule.escalations.store-changelog
escalation-user-aggregator-20211127-aggregate.escalations.store-changelog
escalation-user-aggregator-20211127-expired-changelog
escalation-user-aggregator-20211127-schedule.escalations.store-changelog
escalation-user-aggregator-20211201-aggregate.escalations.store-changelog
escalation-user-aggregator-20211201-expired-changelog
escalation-user-aggregator-20211201-schedule.escalations.store-changelog
escalation-user-aggregator-aggregate.escalations.store-changelog
escalation-user-aggregator-expired-changelog
escalation-user-aggregator-schedule.escalations.store-changelog
escalation-user-aggregator7da5b78f-f19e-42a0-b69d-197abdb49809-aggregate.escalations.store-changelog
escalation-user-aggregator7da5b78f-f19e-42a0-b69d-197abdb49809-expired-changelog
escalation-user-aggregator7da5b78f-f19e-42a0-b69d-197abdb49809-schedule.escalations.store-changelog
escalation-user-aggregator83296ed8-2547-45a5-b808-2645166bab59-aggregate.escalations.store-changelog
escalation-user-aggregator83296ed8-2547-45a5-b808-2645166bab59-expired-changelog
escalation-user-aggregator83296ed8-2547-45a5-b808-2645166bab59-schedule.escalations.store-changelog
escalation-user-aggregator98efb817-c64a-4da8-a566-15a54c45f2d3-aggregate.escalations.store-changelog
escalation-user-aggregator98efb817-c64a-4da8-a566-15a54c45f2d3-expired-changelog
escalation-user-aggregator98efb817-c64a-4da8-a566-15a54c45f2d3-schedule.escalations.store-changelog
escalation-user-aggregator9974e0e8-5554-4070-add9-388197806ae5-aggregate.escalations.store-changelog
escalation-user-aggregator9974e0e8-5554-4070-add9-388197806ae5-expired-changelog
escalation-user-aggregator9974e0e8-5554-4070-add9-388197806ae5-schedule.escalations.store-changelog
escalation-user-aggregatorJan10-aggregate.escalations.store-changelog
escalation-user-aggregatorJan10-expired-changelog
escalation-user-aggregatorJan10-schedule.escalations.store-changelog
escalation-user-aggregatorJan100a3a0b76-247d-4ee1-890e-da6f4f5c7d2f-aggregate.escalations.store-changelog
escalation-user-aggregatorJan100a3a0b76-247d-4ee1-890e-da6f4f5c7d2f-expired-changelog
escalation-user-aggregatorJan100a3a0b76-247d-4ee1-890e-da6f4f5c7d2f-schedule.escalations.store-changelog
escalation-user-aggregatorJan108be80af4-9174-495d-8a80-26de47a4f224-aggregate.escalations.store-changelog
escalation-user-aggregatorJan108be80af4-9174-495d-8a80-26de47a4f224-expired-changelog
escalation-user-aggregatorJan108be80af4-9174-495d-8a80-26de47a4f224-schedule.escalations.store-changelog
escalation-user-aggregatorJan10b15596cf-c0f4-4f24-863b-e92e27583930-aggregate.escalations.store-changelog
escalation-user-aggregatorJan10b15596cf-c0f4-4f24-863b-e92e27583930-expired-changelog
escalation-user-aggregatorJan10b15596cf-c0f4-4f24-863b-e92e27583930-schedule.escalations.store-changelog
escalation-user-aggregatorJan11-5-aggregate.escalations.store-changelog
escalation-user-aggregatorJan11-5-expired-changelog
escalation-user-aggregatorJan11-5-schedule.escalations.store-changelog
escalation-user-aggregatorJan11-6-aggregate.escalations.store-changelog
escalation-user-aggregatorJan11-6-expired-changelog
escalation-user-aggregatorJan11-6-schedule.escalations.store-changelog
escalation-user-aggregatorJan11-aggregate.escalations.store-changelog
escalation-user-aggregatorJan11-expired-changelog
escalation-user-aggregatorJan11-schedule.escalations.store-changelog
escalation-user-aggregatorJan11.1-aggregate.escalations.store-changelog
escalation-user-aggregatorJan11.1-expired-changelog
escalation-user-aggregatorJan11.1-schedule.escalations.store-changelog
escalation-user-aggregatorJan11.2-aggregate.escalations.store-changelog
escalation-user-aggregatorJan11.2-expired-changelog
escalation-user-aggregatorJan11.2-schedule.escalations.store-changelog
escalation-user-aggregatorJan11.3-aggregate.escalations.store-changelog
escalation-user-aggregatorJan11.3-expired-changelog
escalation-user-aggregatorJan11.3-schedule.escalations.store-changelog
escalation-user-aggregatorJan11.4-aggregate.escalations.store-changelog
escalation-user-aggregatorJan11.4-expired-changelog
escalation-user-aggregatorJan11.4-schedule.escalations.store-changelog
escalation-user-aggregatorJan12-aggregate.escalations.store-changelog
escalation-user-aggregatorJan12-expired-changelog
escalation-user-aggregatorJan12-schedule.escalations.store-changelog
escalation-user-aggregatorJan12a-aggregate.escalations.store-changelog
escalation-user-aggregatorJan12a-expired-changelog
escalation-user-aggregatorJan12a-schedule.escalations.store-changelog
escalation-user-aggregatorJan12b-aggregate.escalations.store-changelog
escalation-user-aggregatorJan12b-expired-changelog
escalation-user-aggregatorJan12b-schedule.escalations.store-changelog
escalation-user-aggregatorJan12c-aggregate.escalations.store-changelog
escalation-user-aggregatorJan12c-expired-changelog
escalation-user-aggregatorJan12c-schedule.escalations.store-changelog
escalation-user-aggregatorJan12d-aggregate.escalations.store-changelog
escalation-user-aggregatorJan12d-expired-changelog
escalation-user-aggregatorJan12d-schedule.escalations.store-changelog
escalation-user-aggregatora5563e2a-1e12-4ccb-af1f-fbbfdee2c6cf-aggregate.escalations.store-changelog
escalation-user-aggregatora5563e2a-1e12-4ccb-af1f-fbbfdee2c6cf-expired-changelog
escalation-user-aggregatora5563e2a-1e12-4ccb-af1f-fbbfdee2c6cf-schedule.escalations.store-changelog
escalation-user-aggregatorad7f32c2-6cc1-4b93-8879-dd314369b579-aggregate.escalations.store-changelog
escalation-user-aggregatorad7f32c2-6cc1-4b93-8879-dd314369b579-expired-changelog
escalation-user-aggregatorad7f32c2-6cc1-4b93-8879-dd314369b579-schedule.escalations.store-changelog
escalation-user-aggregatorc74c3c60-2392-43a3-9e69-07132dba4f34-aggregate.escalations.store-changelog
escalation-user-aggregatorc74c3c60-2392-43a3-9e69-07132dba4f34-expired-changelog
escalation-user-aggregatorc74c3c60-2392-43a3-9e69-07132dba4f34-schedule.escalations.store-changelog
escalation-user-aggregatoreb6cb7d2-8beb-4f5a-b815-931478caee41-aggregate.escalations.store-changelog
escalation-user-aggregatoreb6cb7d2-8beb-4f5a-b815-931478caee41-expired-changelog
escalation-user-aggregatoreb6cb7d2-8beb-4f5a-b815-931478caee41-schedule.escalations.store-changelog
escalation-user-aggregatorf06df7b4-ae4e-45db-bfae-f759e222a3e3-aggregate.escalations.store-changelog
escalation-user-aggregatorf06df7b4-ae4e-45db-bfae-f759e222a3e3-expired-changelog
escalation-user-aggregatorf06df7b4-ae4e-45db-bfae-f759e222a3e3-schedule.escalations.store-changelog
escalation-user-executor-Jan10-expired-changelog
expired.escalation.multi
functional-test
helios-ci.analytics.event
helios-ci.dead.event
helios-ci.death.event
helios-ci.error.event
helios-ci.event.data
helios-ci.event.rule.map
helios-ci.external.event
helios-ci.grp-helios-ci-data-processor-service-DeadEventMap-changelog
helios-ci.grp-helios-ci-data-processor-service-DeadEventMapCount-changelog
helios-ci.grp-helios-ci-data-processor-service-RPCStreams-helios-ci.event.data-store-changelog
helios-ci.grp-helios-ci-data-processor-service-RPCStreams-helios-ci.event.rule.map-store-changelog
helios-ci.grp-helios-ci-data-processor-service-RPCStreams-helios-ci.rule.data-store-changelog
helios-ci.grp-helios-ci-data-processor-service-RPCStreams-helios-ci.source.data-store-changelog
helios-ci.grp-helios-ci-data-processor-service-RPCStreams-helios-ci.source.event.map-store-changelog
helios-ci.grp-helios-ci-data-processor-service-RPCStreams-helios-ci.source.webhook.map-store-changelog
helios-ci.grp-helios-ci-data-processor-service-RPCStreams-helios-ci.webhook.data-store-changelog
helios-ci.grp-helios-ci-data-processor-service-RPCStreams-helios-ci.webhook.rule.map-store-changelog
helios-ci.grp-helios-ci-data-processor-service-WebhookState-changelog
helios-ci.grp-helios-ci-data-processor-service-requeue-DeadEventMap-changelog
helios-ci.grp-helios-ci-data-processor-service-requeue-DeadEventMapCount-changelog
helios-ci.grp-helios-ci-data-processor-service-requeue-WebhookState-changelog
helios-ci.grp-helios-ci-data-provider-service-RPCStreams-helios-ci.event.data-store-changelog
helios-ci.grp-helios-ci-data-provider-service-RPCStreams-helios-ci.event.rule.map-store-changelog
helios-ci.grp-helios-ci-data-provider-service-RPCStreams-helios-ci.rule.data-store-changelog
helios-ci.grp-helios-ci-data-provider-service-RPCStreams-helios-ci.source.data-store-changelog
helios-ci.grp-helios-ci-data-provider-service-RPCStreams-helios-ci.source.event.map-store-changelog
helios-ci.grp-helios-ci-data-provider-service-RPCStreams-helios-ci.source.webhook.map-store-changelog
helios-ci.grp-helios-ci-data-provider-service-RPCStreams-helios-ci.webhook.data-store-changelog
helios-ci.grp-helios-ci-data-provider-service-RPCStreams-helios-ci.webhook.rule.map-store-changelog
helios-ci.grp-helios-ci-dead-letter-processor-service-RPCStreams-helios-ci.dead.event-store-changelog
helios-ci.grp-helios-ci-dead-letter-processor-service-RPCStreams-helios-ci.event.data-store-changelog
helios-ci.grp-helios-ci-dead-letter-processor-service-RPCStreams-helios-ci.event.rule.map-store-changelog
helios-ci.grp-helios-ci-dead-letter-processor-service-RPCStreams-helios-ci.rule.data-store-changelog
helios-ci.grp-helios-ci-dead-letter-processor-service-RPCStreams-helios-ci.source.data-store-changelog
helios-ci.grp-helios-ci-dead-letter-processor-service-RPCStreams-helios-ci.source.event.map-store-changelog
helios-ci.grp-helios-ci-dead-letter-processor-service-RPCStreams-helios-ci.source.webhook.map-store-changelog
helios-ci.grp-helios-ci-dead-letter-processor-service-RPCStreams-helios-ci.webhook.data-store-changelog
helios-ci.grp-helios-ci-dead-letter-processor-service-RPCStreams-helios-ci.webhook.dead.event.map-store-changelog
helios-ci.grp-helios-ci-dead-letter-processor-service-RPCStreams-helios-ci.webhook.rule.map-store-changelog
helios-ci.grp-helios-ci-dead-letter-processor-service-requeue-DeadEventMap-changelog
helios-ci.grp-helios-ci-dead-letter-processor-service-requeue-DeadEventMapCount-changelog
helios-ci.grp-helios-ci-dead-letter-processor-service-requeue-WebhookState-changelog
helios-ci.grp-helios-ci-event-scheduler-processor-service-EventStore-changelog
helios-ci.grp-helios-ci-event-scheduler-processor-service-RPCStreams-helios-ci.webhook.data-store-changelog
helios-ci.grp-helios-ci-event-scheduler-processor-service-ReasonStore-changelog
helios-ci.grp-helios-ci-event-scheduler-processor-service-StatusStore-changelog
helios-ci.grp-helios-ci-event-scheduler-processor-service-TimingStore-changelog
helios-ci.grp-helios-ci-event-scheduler-processor-service-WebhookRetryCountStore-changelog
helios-ci.grp-helios-ci-external-event-processor-service-RPCStreams-helios-ci.event.rule.map-store-changelog
helios-ci.grp-helios-ci-external-event-processor-service-RPCStreams-helios-ci.rule.data-store-changelog
helios-ci.grp-helios-ci-external-event-processor-service-RPCStreams-helios-ci.webhook.data-store-changelog
helios-ci.grp-helios-ci-external-event-processor-service-RateLimitedWebhookStore-changelog
helios-ci.grp-helios-ci-provisioning-service-RPCStreams-helios-ci.source.data-store-changelog
helios-ci.grp-helios-ci-provisioning-service-data-PDBProvisioningResponseStore_-changelog
helios-ci.grp-helios-ci-provisioning-service-data-PDBProvisioningStatusStore_-changelog
helios-ci.grp-helios-ci-provisioning-service-data-ServiceProvisioningStatesStore_-changelog
helios-ci.grp-helios-ci-provisioning-service-data-SourceRegistrationStore_-changelog
helios-ci.grp-helios-ci-provisioning-service-dataTMS-TenantEventStore_-changelog
helios-ci.preprocessed.external.event
helios-ci.priority.external.event
helios-ci.provisioning.request.event
helios-ci.provisioning.response.event
helios-ci.publish.event
helios-ci.recovery.event
helios-ci.rule.data
helios-ci.send.event
helios-ci.source.data
helios-ci.source.event.map
helios-ci.source.webhook.map
helios-ci.status.event
helios-ci.webhook.data
helios-ci.webhook.dead.event.map
helios-ci.webhook.rule.map
helios-psr.analytics.event
helios-psr.dead.event
helios-psr.death.event
helios-psr.error.event
helios-psr.event.data
helios-psr.event.rule.map
helios-psr.external.event
helios-psr.grp-helios-psr-data-processor-service-DeadEventMap-changelog
helios-psr.grp-helios-psr-data-processor-service-DeadEventMapCount-changelog
helios-psr.grp-helios-psr-data-processor-service-RPCStreams-helios-psr.event.data-store-changelog
helios-psr.grp-helios-psr-data-processor-service-RPCStreams-helios-psr.event.rule.map-store-changelog
helios-psr.grp-helios-psr-data-processor-service-RPCStreams-helios-psr.rule.data-store-changelog
helios-psr.grp-helios-psr-data-processor-service-RPCStreams-helios-psr.source.data-store-changelog
helios-psr.grp-helios-psr-data-processor-service-RPCStreams-helios-psr.source.event.map-store-changelog
helios-psr.grp-helios-psr-data-processor-service-RPCStreams-helios-psr.source.webhook.map-store-changelog
helios-psr.grp-helios-psr-data-processor-service-RPCStreams-helios-psr.webhook.data-store-changelog
helios-psr.grp-helios-psr-data-processor-service-RPCStreams-helios-psr.webhook.rule.map-store-changelog
helios-psr.grp-helios-psr-data-processor-service-WebhookState-changelog
helios-psr.grp-helios-psr-data-provider-service-RPCStreams-helios-psr.event.data-store-changelog
helios-psr.grp-helios-psr-data-provider-service-RPCStreams-helios-psr.event.rule.map-store-changelog
helios-psr.grp-helios-psr-data-provider-service-RPCStreams-helios-psr.rule.data-store-changelog
helios-psr.grp-helios-psr-data-provider-service-RPCStreams-helios-psr.source.data-store-changelog
helios-psr.grp-helios-psr-data-provider-service-RPCStreams-helios-psr.source.event.map-store-changelog
helios-psr.grp-helios-psr-data-provider-service-RPCStreams-helios-psr.source.webhook.map-store-changelog
helios-psr.grp-helios-psr-data-provider-service-RPCStreams-helios-psr.webhook.data-store-changelog
helios-psr.grp-helios-psr-data-provider-service-RPCStreams-helios-psr.webhook.rule.map-store-changelog
helios-psr.grp-helios-psr-dead-letter-processor-service-RPCStreams-helios-psr.dead.event-store-changelog
helios-psr.grp-helios-psr-dead-letter-processor-service-RPCStreams-helios-psr.event.data-store-changelog
helios-psr.grp-helios-psr-dead-letter-processor-service-RPCStreams-helios-psr.event.rule.map-store-changelog
helios-psr.grp-helios-psr-dead-letter-processor-service-RPCStreams-helios-psr.rule.data-store-changelog
helios-psr.grp-helios-psr-dead-letter-processor-service-RPCStreams-helios-psr.source.data-store-changelog
helios-psr.grp-helios-psr-dead-letter-processor-service-RPCStreams-helios-psr.source.event.map-store-changelog
helios-psr.grp-helios-psr-dead-letter-processor-service-RPCStreams-helios-psr.source.webhook.map-store-changelog
helios-psr.grp-helios-psr-dead-letter-processor-service-RPCStreams-helios-psr.webhook.data-store-changelog
helios-psr.grp-helios-psr-dead-letter-processor-service-RPCStreams-helios-psr.webhook.dead.event.map-store-changelog
helios-psr.grp-helios-psr-dead-letter-processor-service-RPCStreams-helios-psr.webhook.rule.map-store-changelog
helios-psr.grp-helios-psr-dead-letter-processor-service-requeue-DeadEventMap-changelog
helios-psr.grp-helios-psr-dead-letter-processor-service-requeue-DeadEventMapCount-changelog
helios-psr.grp-helios-psr-dead-letter-processor-service-requeue-WebhookState-changelog
helios-psr.grp-helios-psr-event-scheduler-processor-service-EventStore-changelog
helios-psr.grp-helios-psr-event-scheduler-processor-service-RPCStreams-helios-psr.webhook.data-store-changelog
helios-psr.grp-helios-psr-event-scheduler-processor-service-ReasonStore-changelog
helios-psr.grp-helios-psr-event-scheduler-processor-service-StatusStore-changelog
helios-psr.grp-helios-psr-event-scheduler-processor-service-TimingStore-changelog
helios-psr.grp-helios-psr-event-scheduler-processor-service-WebhookRetryCountStore-changelog
helios-psr.grp-helios-psr-external-event-processor-service-RPCStreams-helios-psr.event.rule.map-store-changelog
helios-psr.grp-helios-psr-external-event-processor-service-RPCStreams-helios-psr.rule.data-store-changelog
helios-psr.grp-helios-psr-external-event-processor-service-RPCStreams-helios-psr.webhook.data-store-changelog
helios-psr.grp-helios-psr-external-event-processor-service-RateLimitedWebhookStore-changelog
helios-psr.grp-helios-psr-provisioning-service-RPCStreams-helios-psr.source.data-store-changelog
helios-psr.grp-helios-psr-provisioning-service-data-PDBProvisioningResponseStore_-changelog
helios-psr.grp-helios-psr-provisioning-service-data-PDBProvisioningStatusStore_-changelog
helios-psr.grp-helios-psr-provisioning-service-data-ServiceProvisioningStatesStore_-changelog
helios-psr.grp-helios-psr-provisioning-service-data-SourceRegistrationStore_-changelog
helios-psr.grp-helios-psr-provisioning-service-dataTMS-TenantEventStore_-changelog
helios-psr.preprocessed.external.event
helios-psr.priority.external.event
helios-psr.provisioning.request.event
helios-psr.provisioning.response.event
helios-psr.publish.event
helios-psr.recovery.event
helios-psr.rule.data
helios-psr.send.event
helios-psr.source.data
helios-psr.source.event.map
helios-psr.source.webhook.map
helios-psr.status.event
helios-psr.webhook.data
helios-psr.webhook.dead.event.map
helios-psr.webhook.rule.map
helios-qa.analytics.event
helios-qa.dead.event
helios-qa.death.event
helios-qa.error.event
helios-qa.event.data
helios-qa.event.rule.map
helios-qa.external.event
helios-qa.grp-helios-qa-data-processor-service-requeue-DeadEventMap-changelog
helios-qa.grp-helios-qa-data-processor-service-requeue-DeadEventMapCount-changelog
helios-qa.grp-helios-qa-data-processor-service-requeue-WebhookState-changelog
helios-qa.grp-helios-qa-data-provider-service-RPCStreams-helios-qa.event.data-store-changelog
helios-qa.grp-helios-qa-data-provider-service-RPCStreams-helios-qa.event.rule.map-store-changelog
helios-qa.grp-helios-qa-data-provider-service-RPCStreams-helios-qa.rule.data-store-changelog
helios-qa.grp-helios-qa-data-provider-service-RPCStreams-helios-qa.source.data-store-changelog
helios-qa.grp-helios-qa-data-provider-service-RPCStreams-helios-qa.source.event.map-store-changelog
helios-qa.grp-helios-qa-data-provider-service-RPCStreams-helios-qa.source.webhook.map-store-changelog
helios-qa.grp-helios-qa-data-provider-service-RPCStreams-helios-qa.webhook.data-store-changelog
helios-qa.grp-helios-qa-data-provider-service-RPCStreams-helios-qa.webhook.rule.map-store-changelog
helios-qa.grp-helios-qa-dead-letter-processor-service-requeue-DeadEventMap-changelog
helios-qa.grp-helios-qa-dead-letter-processor-service-requeue-DeadEventMapCount-changelog
helios-qa.grp-helios-qa-dead-letter-processor-service-requeue-WebhookState-changelog
helios-qa.grp-helios-qa-event-scheduler-processor-service-EventStore-changelog
helios-qa.grp-helios-qa-event-scheduler-processor-service-ReasonStore-changelog
helios-qa.grp-helios-qa-event-scheduler-processor-service-StatusStore-changelog
helios-qa.grp-helios-qa-event-scheduler-processor-service-TimingStore-changelog
helios-qa.grp-helios-qa-event-scheduler-processor-service-WebhookRetryCountStore-changelog
helios-qa.grp-helios-qa-external-event-processor-service-RPCStreams-helios-qa.event.rule.map-store-changelog
helios-qa.grp-helios-qa-external-event-processor-service-RPCStreams-helios-qa.rule.data-store-changelog
helios-qa.grp-helios-qa-external-event-processor-service-RPCStreams-helios-qa.webhook.data-store-changelog
helios-qa.grp-helios-qa-external-event-processor-service-RateLimitedWebhookStore-changelog
helios-qa.grp-helios-qa-provisioning-service-RPCStreams-helios-qa.source.data-store-changelog
helios-qa.grp-helios-qa-provisioning-service-data-PDBProvisioningResponseStore_-changelog
helios-qa.grp-helios-qa-provisioning-service-data-PDBProvisioningStatusStore_-changelog
helios-qa.grp-helios-qa-provisioning-service-data-ServiceProvisioningStatesStore_-changelog
helios-qa.grp-helios-qa-provisioning-service-data-SourceRegistrationStore_-changelog
helios-qa.grp-helios-qa-provisioning-service-dataTMS-TenantEventStore_-changelog
helios-qa.preprocessed.external.event
helios-qa.priority.external.event
helios-qa.provisioning.request.event
helios-qa.provisioning.response.event
helios-qa.publish.event
helios-qa.recovery.event
helios-qa.rule.data
helios-qa.send.event
helios-qa.source.data
helios-qa.source.event.map
helios-qa.source.webhook.map
helios-qa.status.event
helios-qa.webhook.data
helios-qa.webhook.dead.event.map
helios-qa.webhook.rule.map
helios-qa2.analytics.event
helios-qa2.dead.event
helios-qa2.death.event
helios-qa2.error.event
helios-qa2.event.data
helios-qa2.event.rule.map
helios-qa2.external.event
helios-qa2.preprocessed.external.event
helios-qa2.priority.external.event
helios-qa2.provisioning.request.event
helios-qa2.provisioning.response.event
helios-qa2.publish.event
helios-qa2.recovery.event
helios-qa2.rule.data
helios-qa2.send.event
helios-qa2.source.data
helios-qa2.source.event.map
helios-qa2.source.webhook.map
helios-qa2.status.event
helios-qa2.webhook.data
helios-qa2.webhook.dead.event.map
helios-qa2.webhook.rule.map
helios-sandbox1.analytics.event
helios-sandbox1.dead.event
helios-sandbox1.death.event
helios-sandbox1.error.event
helios-sandbox1.event.data
helios-sandbox1.event.rule.map
helios-sandbox1.external.event
helios-sandbox1.grp-helios-sandbox1-data-processor-service-DeadEventMap-changelog
helios-sandbox1.grp-helios-sandbox1-data-processor-service-DeadEventMapCount-changelog
helios-sandbox1.grp-helios-sandbox1-data-processor-service-RPCStreams-helios-sandbox1.event.data-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-processor-service-RPCStreams-helios-sandbox1.event.data-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-processor-service-RPCStreams-helios-sandbox1.event.rule.map-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-processor-service-RPCStreams-helios-sandbox1.event.rule.map-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-processor-service-RPCStreams-helios-sandbox1.rule.data-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-processor-service-RPCStreams-helios-sandbox1.rule.data-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-processor-service-RPCStreams-helios-sandbox1.source.data-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-processor-service-RPCStreams-helios-sandbox1.source.data-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-processor-service-RPCStreams-helios-sandbox1.source.event.map-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-processor-service-RPCStreams-helios-sandbox1.source.event.map-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-processor-service-RPCStreams-helios-sandbox1.source.webhook.map-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-processor-service-RPCStreams-helios-sandbox1.source.webhook.map-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-processor-service-RPCStreams-helios-sandbox1.webhook.data-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-processor-service-RPCStreams-helios-sandbox1.webhook.data-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-processor-service-RPCStreams-helios-sandbox1.webhook.rule.map-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-processor-service-RPCStreams-helios-sandbox1.webhook.rule.map-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-processor-service-WebhookState-changelog
helios-sandbox1.grp-helios-sandbox1-data-processor-service-requeue-DeadEventMap-changelog
helios-sandbox1.grp-helios-sandbox1-data-processor-service-requeue-DeadEventMapCount-changelog
helios-sandbox1.grp-helios-sandbox1-data-processor-service-requeue-WebhookState-changelog
helios-sandbox1.grp-helios-sandbox1-data-provider-service-RPCStreams-helios-sandbox1.event.data-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-provider-service-RPCStreams-helios-sandbox1.event.data-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-provider-service-RPCStreams-helios-sandbox1.event.rule.map-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-provider-service-RPCStreams-helios-sandbox1.event.rule.map-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-provider-service-RPCStreams-helios-sandbox1.rule.data-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-provider-service-RPCStreams-helios-sandbox1.rule.data-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-provider-service-RPCStreams-helios-sandbox1.source.data-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-provider-service-RPCStreams-helios-sandbox1.source.data-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-provider-service-RPCStreams-helios-sandbox1.source.event.map-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-provider-service-RPCStreams-helios-sandbox1.source.event.map-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-provider-service-RPCStreams-helios-sandbox1.source.webhook.map-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-provider-service-RPCStreams-helios-sandbox1.source.webhook.map-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-provider-service-RPCStreams-helios-sandbox1.webhook.data-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-provider-service-RPCStreams-helios-sandbox1.webhook.data-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-provider-service-RPCStreams-helios-sandbox1.webhook.rule.map-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-data-provider-service-RPCStreams-helios-sandbox1.webhook.rule.map-store-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-RPCStreams-helios-sandbox1.dead.event-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-RPCStreams-helios-sandbox1.dead.event-store-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-RPCStreams-helios-sandbox1.event.data-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-RPCStreams-helios-sandbox1.event.data-store-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-RPCStreams-helios-sandbox1.event.rule.map-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-RPCStreams-helios-sandbox1.event.rule.map-store-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-RPCStreams-helios-sandbox1.rule.data-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-RPCStreams-helios-sandbox1.rule.data-store-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-RPCStreams-helios-sandbox1.source.data-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-RPCStreams-helios-sandbox1.source.data-store-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-RPCStreams-helios-sandbox1.source.event.map-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-RPCStreams-helios-sandbox1.source.event.map-store-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-RPCStreams-helios-sandbox1.source.webhook.map-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-RPCStreams-helios-sandbox1.source.webhook.map-store-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-RPCStreams-helios-sandbox1.webhook.data-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-RPCStreams-helios-sandbox1.webhook.data-store-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-RPCStreams-helios-sandbox1.webhook.dead.event.map-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-RPCStreams-helios-sandbox1.webhook.dead.event.map-store-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-RPCStreams-helios-sandbox1.webhook.rule.map-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-RPCStreams-helios-sandbox1.webhook.rule.map-store-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-requeue-DeadEventMap-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-requeue-DeadEventMapCount-changelog
helios-sandbox1.grp-helios-sandbox1-dead-letter-processor-service-requeue-WebhookState-changelog
helios-sandbox1.grp-helios-sandbox1-event-scheduler-processor-service-EventStore-changelog
helios-sandbox1.grp-helios-sandbox1-event-scheduler-processor-service-RPCStreams-helios-sandbox1.webhook.data-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-event-scheduler-processor-service-RPCStreams-helios-sandbox1.webhook.data-store-changelog
helios-sandbox1.grp-helios-sandbox1-event-scheduler-processor-service-ReasonStore-changelog
helios-sandbox1.grp-helios-sandbox1-event-scheduler-processor-service-StatusStore-changelog
helios-sandbox1.grp-helios-sandbox1-event-scheduler-processor-service-TimingStore-changelog
helios-sandbox1.grp-helios-sandbox1-event-scheduler-processor-service-WebhookRetryCountStore-changelog
helios-sandbox1.grp-helios-sandbox1-external-event-processor-service-RPCStreams-helios-sandbox1.event.rule.map-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-external-event-processor-service-RPCStreams-helios-sandbox1.event.rule.map-store-changelog
helios-sandbox1.grp-helios-sandbox1-external-event-processor-service-RPCStreams-helios-sandbox1.recovery.event-store-changelog
helios-sandbox1.grp-helios-sandbox1-external-event-processor-service-RPCStreams-helios-sandbox1.rule.data-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-external-event-processor-service-RPCStreams-helios-sandbox1.rule.data-store-changelog
helios-sandbox1.grp-helios-sandbox1-external-event-processor-service-RPCStreams-helios-sandbox1.webhook.data-state-store-changelog
helios-sandbox1.grp-helios-sandbox1-external-event-processor-service-RPCStreams-helios-sandbox1.webhook.data-store-changelog
helios-sandbox1.grp-helios-sandbox1-external-event-processor-service-RateLimitedWebhookStore-changelog
helios-sandbox1.grp-helios-sandbox1-provisioning-service-RPCStreams-helios-sandbox1.source.data-store-changelog
helios-sandbox1.grp-helios-sandbox1-provisioning-service-data-PDBProvisioningResponseStore-changelog
helios-sandbox1.grp-helios-sandbox1-provisioning-service-data-PDBProvisioningResponseStore_-changelog
helios-sandbox1.grp-helios-sandbox1-provisioning-service-data-PDBProvisioningStatusStore-changelog
helios-sandbox1.grp-helios-sandbox1-provisioning-service-data-PDBProvisioningStatusStore_-changelog
helios-sandbox1.grp-helios-sandbox1-provisioning-service-data-ServiceProvisioningStatesStore-changelog
helios-sandbox1.grp-helios-sandbox1-provisioning-service-data-ServiceProvisioningStatesStore_-changelog
helios-sandbox1.grp-helios-sandbox1-provisioning-service-data-SourceRegistrationStore_-changelog
helios-sandbox1.grp-helios-sandbox1-provisioning-service-dataTMS-TenantEventStore_-changelog
helios-sandbox1.preprocessed.external.event
helios-sandbox1.priority.external.event
helios-sandbox1.provisioning.request.event
helios-sandbox1.provisioning.response.event
helios-sandbox1.publish.event
helios-sandbox1.recovery.event
helios-sandbox1.rule.data
helios-sandbox1.send.event
helios-sandbox1.source.data
helios-sandbox1.source.event.map
helios-sandbox1.source.webhook.map
helios-sandbox1.status.event
helios-sandbox1.webhook.data
helios-sandbox1.webhook.dead.event.map
helios-sandbox1.webhook.rule.map
helios-sandbox2.analytics.event
helios-sandbox2.dead.event
helios-sandbox2.death.event
helios-sandbox2.error.event
helios-sandbox2.event.data
helios-sandbox2.event.rule.map
helios-sandbox2.external.event
helios-sandbox2.grp-helios-sandbox2-data-processor-service-DeadEventMap-changelog
helios-sandbox2.grp-helios-sandbox2-data-processor-service-DeadEventMapCount-changelog
helios-sandbox2.grp-helios-sandbox2-data-processor-service-RPCStreams-helios-sandbox2.event.data-store-changelog
helios-sandbox2.grp-helios-sandbox2-data-processor-service-RPCStreams-helios-sandbox2.event.rule.map-store-changelog
helios-sandbox2.grp-helios-sandbox2-data-processor-service-RPCStreams-helios-sandbox2.rule.data-store-changelog
helios-sandbox2.grp-helios-sandbox2-data-processor-service-RPCStreams-helios-sandbox2.source.data-store-changelog
helios-sandbox2.grp-helios-sandbox2-data-processor-service-RPCStreams-helios-sandbox2.source.event.map-store-changelog
helios-sandbox2.grp-helios-sandbox2-data-processor-service-RPCStreams-helios-sandbox2.source.webhook.map-store-changelog
helios-sandbox2.grp-helios-sandbox2-data-processor-service-RPCStreams-helios-sandbox2.webhook.data-store-changelog
helios-sandbox2.grp-helios-sandbox2-data-processor-service-RPCStreams-helios-sandbox2.webhook.rule.map-store-changelog
helios-sandbox2.grp-helios-sandbox2-data-processor-service-WebhookState-changelog
helios-sandbox2.grp-helios-sandbox2-data-processor-service-data-DeletedSourceStore-changelog
helios-sandbox2.grp-helios-sandbox2-data-processor-service-requeue-DeadEventMap-changelog
helios-sandbox2.grp-helios-sandbox2-data-processor-service-requeue-DeadEventMapCount-changelog
helios-sandbox2.grp-helios-sandbox2-data-processor-service-requeue-WebhookState-changelog
helios-sandbox2.grp-helios-sandbox2-data-provider-service-RPCStreams-helios-sandbox2.event.data-store-changelog
helios-sandbox2.grp-helios-sandbox2-data-provider-service-RPCStreams-helios-sandbox2.event.rule.map-store-changelog
helios-sandbox2.grp-helios-sandbox2-data-provider-service-RPCStreams-helios-sandbox2.rule.data-store-changelog
helios-sandbox2.grp-helios-sandbox2-data-provider-service-RPCStreams-helios-sandbox2.source.data-store-changelog
helios-sandbox2.grp-helios-sandbox2-data-provider-service-RPCStreams-helios-sandbox2.source.event.map-store-changelog
helios-sandbox2.grp-helios-sandbox2-data-provider-service-RPCStreams-helios-sandbox2.source.webhook.map-store-changelog
helios-sandbox2.grp-helios-sandbox2-data-provider-service-RPCStreams-helios-sandbox2.webhook.data-store-changelog
helios-sandbox2.grp-helios-sandbox2-data-provider-service-RPCStreams-helios-sandbox2.webhook.rule.map-store-changelog
helios-sandbox2.grp-helios-sandbox2-dead-letter-processor-service-RPCStreams-helios-sandbox2.dead.event-store-changelog
helios-sandbox2.grp-helios-sandbox2-dead-letter-processor-service-RPCStreams-helios-sandbox2.event.data-store-changelog
helios-sandbox2.grp-helios-sandbox2-dead-letter-processor-service-RPCStreams-helios-sandbox2.event.rule.map-store-changelog
helios-sandbox2.grp-helios-sandbox2-dead-letter-processor-service-RPCStreams-helios-sandbox2.rule.data-store-changelog
helios-sandbox2.grp-helios-sandbox2-dead-letter-processor-service-RPCStreams-helios-sandbox2.source.data-store-changelog
helios-sandbox2.grp-helios-sandbox2-dead-letter-processor-service-RPCStreams-helios-sandbox2.source.event.map-store-changelog
helios-sandbox2.grp-helios-sandbox2-dead-letter-processor-service-RPCStreams-helios-sandbox2.source.webhook.map-store-changelog
helios-sandbox2.grp-helios-sandbox2-dead-letter-processor-service-RPCStreams-helios-sandbox2.webhook.data-store-changelog
helios-sandbox2.grp-helios-sandbox2-dead-letter-processor-service-RPCStreams-helios-sandbox2.webhook.dead.event.map-store-changelog
helios-sandbox2.grp-helios-sandbox2-dead-letter-processor-service-RPCStreams-helios-sandbox2.webhook.rule.map-store-changelog
helios-sandbox2.grp-helios-sandbox2-dead-letter-processor-service-requeue-DeadEventMap-changelog
helios-sandbox2.grp-helios-sandbox2-dead-letter-processor-service-requeue-DeadEventMapCount-changelog
helios-sandbox2.grp-helios-sandbox2-dead-letter-processor-service-requeue-WebhookState-changelog
helios-sandbox2.grp-helios-sandbox2-event-scheduler-processor-service-EventStore-changelog
helios-sandbox2.grp-helios-sandbox2-event-scheduler-processor-service-RPCStreams-helios-sandbox2.webhook.data-store-changelog
helios-sandbox2.grp-helios-sandbox2-event-scheduler-processor-service-ReasonStore-changelog
helios-sandbox2.grp-helios-sandbox2-event-scheduler-processor-service-StatusStore-changelog
helios-sandbox2.grp-helios-sandbox2-event-scheduler-processor-service-TimingStore-changelog
helios-sandbox2.grp-helios-sandbox2-event-scheduler-processor-service-WebhookRetryCountStore-changelog
helios-sandbox2.grp-helios-sandbox2-external-event-processor-service-RPCStreams-helios-sandbox2.event.rule.map-store-changelog
helios-sandbox2.grp-helios-sandbox2-external-event-processor-service-RPCStreams-helios-sandbox2.rule.data-store-changelog
helios-sandbox2.grp-helios-sandbox2-external-event-processor-service-RPCStreams-helios-sandbox2.webhook.data-store-changelog
helios-sandbox2.grp-helios-sandbox2-external-event-processor-service-RateLimitedWebhookStore-changelog
helios-sandbox2.grp-helios-sandbox2-provisioning-service-RPCStreams-helios-sandbox2.source.data-store-changelog
helios-sandbox2.grp-helios-sandbox2-provisioning-service-TenantEventStore-changelog
helios-sandbox2.grp-helios-sandbox2-provisioning-service-data-PDBProvisioningResponseStore-changelog
helios-sandbox2.grp-helios-sandbox2-provisioning-service-data-PDBProvisioningResponseStore_-changelog
helios-sandbox2.grp-helios-sandbox2-provisioning-service-data-PDBProvisioningStatusStore-changelog
helios-sandbox2.grp-helios-sandbox2-provisioning-service-data-PDBProvisioningStatusStore_-changelog
helios-sandbox2.grp-helios-sandbox2-provisioning-service-data-ServiceProvisioningStatesStore-changelog
helios-sandbox2.grp-helios-sandbox2-provisioning-service-data-ServiceProvisioningStatesStore_-changelog
helios-sandbox2.grp-helios-sandbox2-provisioning-service-data-SourceRegistrationStore-changelog
helios-sandbox2.grp-helios-sandbox2-provisioning-service-data-SourceRegistrationStore_-changelog
helios-sandbox2.grp-helios-sandbox2-provisioning-service-dataTMS-TenantEventStore-changelog
helios-sandbox2.grp-helios-sandbox2-provisioning-service-dataTMS-TenantEventStore_-changelog
helios-sandbox2.grp-helios-sandbox2-provisioning-service-sourceRegistrationStore-changelog
helios-sandbox2.grp-helios-sandbox2-provisioning-service-tms-TenantEventStore-changelog
helios-sandbox2.preprocessed.external.event
helios-sandbox2.priority.external.event
helios-sandbox2.provisioning.request.event
helios-sandbox2.provisioning.response.event
helios-sandbox2.publish.event
helios-sandbox2.recovery.event
helios-sandbox2.rule.data
helios-sandbox2.send.event
helios-sandbox2.source.data
helios-sandbox2.source.event.map
helios-sandbox2.source.webhook.map
helios-sandbox2.status.event
helios-sandbox2.webhook.data
helios-sandbox2.webhook.dead.event.map
helios-sandbox2.webhook.rule.map
helios-sandbox3.analytics.event
helios-sandbox3.dead.event
helios-sandbox3.death.event
helios-sandbox3.error.event
helios-sandbox3.event.data
helios-sandbox3.event.rule.map
helios-sandbox3.external.event
helios-sandbox3.grp-helios-sandbox3-data-processor-service-DeadEventMap-changelog
helios-sandbox3.grp-helios-sandbox3-data-processor-service-DeadEventMapCount-changelog
helios-sandbox3.grp-helios-sandbox3-data-processor-service-RPCStreams-helios-sandbox3.event.data-store-changelog
helios-sandbox3.grp-helios-sandbox3-data-processor-service-RPCStreams-helios-sandbox3.event.rule.map-store-changelog
helios-sandbox3.grp-helios-sandbox3-data-processor-service-RPCStreams-helios-sandbox3.rule.data-store-changelog
helios-sandbox3.grp-helios-sandbox3-data-processor-service-RPCStreams-helios-sandbox3.source.data-store-changelog
helios-sandbox3.grp-helios-sandbox3-data-processor-service-RPCStreams-helios-sandbox3.source.event.map-store-changelog
helios-sandbox3.grp-helios-sandbox3-data-processor-service-RPCStreams-helios-sandbox3.source.webhook.map-store-changelog
helios-sandbox3.grp-helios-sandbox3-data-processor-service-RPCStreams-helios-sandbox3.webhook.data-store-changelog
helios-sandbox3.grp-helios-sandbox3-data-processor-service-RPCStreams-helios-sandbox3.webhook.rule.map-store-changelog
helios-sandbox3.grp-helios-sandbox3-data-processor-service-WebhookState-changelog
helios-sandbox3.grp-helios-sandbox3-data-processor-service-requeue-DeadEventMap-changelog
helios-sandbox3.grp-helios-sandbox3-data-processor-service-requeue-DeadEventMapCount-changelog
helios-sandbox3.grp-helios-sandbox3-data-processor-service-requeue-WebhookState-changelog
helios-sandbox3.grp-helios-sandbox3-data-provider-service-RPCStreams-helios-sandbox3.event.data-store-changelog
helios-sandbox3.grp-helios-sandbox3-data-provider-service-RPCStreams-helios-sandbox3.event.rule.map-store-changelog
helios-sandbox3.grp-helios-sandbox3-data-provider-service-RPCStreams-helios-sandbox3.rule.data-store-changelog
helios-sandbox3.grp-helios-sandbox3-data-provider-service-RPCStreams-helios-sandbox3.source.data-store-changelog
helios-sandbox3.grp-helios-sandbox3-data-provider-service-RPCStreams-helios-sandbox3.source.event.map-store-changelog
helios-sandbox3.grp-helios-sandbox3-data-provider-service-RPCStreams-helios-sandbox3.source.webhook.map-store-changelog
helios-sandbox3.grp-helios-sandbox3-data-provider-service-RPCStreams-helios-sandbox3.webhook.data-store-changelog
helios-sandbox3.grp-helios-sandbox3-data-provider-service-RPCStreams-helios-sandbox3.webhook.rule.map-store-changelog
helios-sandbox3.grp-helios-sandbox3-dead-letter-processor-service-RPCStreams-helios-sandbox3.dead.event-store-changelog
helios-sandbox3.grp-helios-sandbox3-dead-letter-processor-service-RPCStreams-helios-sandbox3.event.data-store-changelog
helios-sandbox3.grp-helios-sandbox3-dead-letter-processor-service-RPCStreams-helios-sandbox3.event.rule.map-store-changelog
helios-sandbox3.grp-helios-sandbox3-dead-letter-processor-service-RPCStreams-helios-sandbox3.rule.data-store-changelog
helios-sandbox3.grp-helios-sandbox3-dead-letter-processor-service-RPCStreams-helios-sandbox3.source.data-store-changelog
helios-sandbox3.grp-helios-sandbox3-dead-letter-processor-service-RPCStreams-helios-sandbox3.source.event.map-store-changelog
helios-sandbox3.grp-helios-sandbox3-dead-letter-processor-service-RPCStreams-helios-sandbox3.source.webhook.map-store-changelog
helios-sandbox3.grp-helios-sandbox3-dead-letter-processor-service-RPCStreams-helios-sandbox3.webhook.data-store-changelog
helios-sandbox3.grp-helios-sandbox3-dead-letter-processor-service-RPCStreams-helios-sandbox3.webhook.dead.event.map-store-changelog
helios-sandbox3.grp-helios-sandbox3-dead-letter-processor-service-RPCStreams-helios-sandbox3.webhook.rule.map-store-changelog
helios-sandbox3.grp-helios-sandbox3-dead-letter-processor-service-requeue-DeadEventMap-changelog
helios-sandbox3.grp-helios-sandbox3-dead-letter-processor-service-requeue-DeadEventMapCount-changelog
helios-sandbox3.grp-helios-sandbox3-dead-letter-processor-service-requeue-WebhookState-changelog
helios-sandbox3.grp-helios-sandbox3-event-scheduler-processor-service-EventStore-changelog
helios-sandbox3.grp-helios-sandbox3-event-scheduler-processor-service-RPCStreams-helios-sandbox3.webhook.data-store-changelog
helios-sandbox3.grp-helios-sandbox3-event-scheduler-processor-service-ReasonStore-changelog
helios-sandbox3.grp-helios-sandbox3-event-scheduler-processor-service-StatusStore-changelog
helios-sandbox3.grp-helios-sandbox3-event-scheduler-processor-service-TimingStore-changelog
helios-sandbox3.grp-helios-sandbox3-event-scheduler-processor-service-WebhookRetryCountStore-changelog
helios-sandbox3.grp-helios-sandbox3-external-event-processor-service-RPCStreams-helios-sandbox3.event.rule.map-store-changelog
helios-sandbox3.grp-helios-sandbox3-external-event-processor-service-RPCStreams-helios-sandbox3.rule.data-store-changelog
helios-sandbox3.grp-helios-sandbox3-external-event-processor-service-RPCStreams-helios-sandbox3.webhook.data-store-changelog
helios-sandbox3.grp-helios-sandbox3-external-event-processor-service-RateLimitedWebhookStore-changelog
helios-sandbox3.grp-helios-sandbox3-provisioning-service-RPCStreams-helios-sandbox3.source.data-store-changelog
helios-sandbox3.grp-helios-sandbox3-provisioning-service-data-PDBProvisioningResponseStore_-changelog
helios-sandbox3.grp-helios-sandbox3-provisioning-service-data-PDBProvisioningStatusStore_-changelog
helios-sandbox3.grp-helios-sandbox3-provisioning-service-data-ServiceProvisioningStatesStore_-changelog
helios-sandbox3.grp-helios-sandbox3-provisioning-service-data-SourceRegistrationStore_-changelog
helios-sandbox3.grp-helios-sandbox3-provisioning-service-dataTMS-TenantEventStore_-changelog
helios-sandbox3.preprocessed.external.event
helios-sandbox3.priority.external.event
helios-sandbox3.provisioning.request.event
helios-sandbox3.provisioning.response.event
helios-sandbox3.publish.event
helios-sandbox3.recovery.event
helios-sandbox3.rule.data
helios-sandbox3.send.event
helios-sandbox3.source.data
helios-sandbox3.source.event.map
helios-sandbox3.source.webhook.map
helios-sandbox3.status.event
helios-sandbox3.webhook.data
helios-sandbox3.webhook.dead.event.map
helios-sandbox3.webhook.rule.map
helios-sandbox4.analytics.event
helios-sandbox4.dead.event
helios-sandbox4.death.event
helios-sandbox4.error.event
helios-sandbox4.event.data
helios-sandbox4.event.rule.map
helios-sandbox4.external.event
helios-sandbox4.grp-helios-sandbox4-data-processor-service-RPCStreams-helios-sandbox4.event.data-store-changelog
helios-sandbox4.grp-helios-sandbox4-data-processor-service-RPCStreams-helios-sandbox4.event.rule.map-store-changelog
helios-sandbox4.grp-helios-sandbox4-data-processor-service-RPCStreams-helios-sandbox4.rule.data-store-changelog
helios-sandbox4.grp-helios-sandbox4-data-processor-service-RPCStreams-helios-sandbox4.source.data-store-changelog
helios-sandbox4.grp-helios-sandbox4-data-processor-service-RPCStreams-helios-sandbox4.source.event.map-store-changelog
helios-sandbox4.grp-helios-sandbox4-data-processor-service-RPCStreams-helios-sandbox4.source.webhook.map-store-changelog
helios-sandbox4.grp-helios-sandbox4-data-processor-service-RPCStreams-helios-sandbox4.webhook.data-store-changelog
helios-sandbox4.grp-helios-sandbox4-data-processor-service-RPCStreams-helios-sandbox4.webhook.rule.map-store-changelog
helios-sandbox4.grp-helios-sandbox4-data-provider-service-RPCStreams-helios-sandbox4.event.data-store-changelog
helios-sandbox4.grp-helios-sandbox4-data-provider-service-RPCStreams-helios-sandbox4.event.rule.map-store-changelog
helios-sandbox4.grp-helios-sandbox4-data-provider-service-RPCStreams-helios-sandbox4.rule.data-store-changelog
helios-sandbox4.grp-helios-sandbox4-data-provider-service-RPCStreams-helios-sandbox4.source.data-store-changelog
helios-sandbox4.grp-helios-sandbox4-data-provider-service-RPCStreams-helios-sandbox4.source.event.map-store-changelog
helios-sandbox4.grp-helios-sandbox4-data-provider-service-RPCStreams-helios-sandbox4.source.webhook.map-store-changelog
helios-sandbox4.grp-helios-sandbox4-data-provider-service-RPCStreams-helios-sandbox4.webhook.data-store-changelog
helios-sandbox4.grp-helios-sandbox4-data-provider-service-RPCStreams-helios-sandbox4.webhook.rule.map-store-changelog
helios-sandbox4.grp-helios-sandbox4-dead-letter-processor-service-RPCStreams-helios-sandbox4.dead.event-store-changelog
helios-sandbox4.grp-helios-sandbox4-dead-letter-processor-service-RPCStreams-helios-sandbox4.event.data-store-changelog
helios-sandbox4.grp-helios-sandbox4-dead-letter-processor-service-RPCStreams-helios-sandbox4.event.rule.map-store-changelog
helios-sandbox4.grp-helios-sandbox4-dead-letter-processor-service-RPCStreams-helios-sandbox4.rule.data-store-changelog
helios-sandbox4.grp-helios-sandbox4-dead-letter-processor-service-RPCStreams-helios-sandbox4.source.data-store-changelog
helios-sandbox4.grp-helios-sandbox4-dead-letter-processor-service-RPCStreams-helios-sandbox4.source.event.map-store-changelog
helios-sandbox4.grp-helios-sandbox4-dead-letter-processor-service-RPCStreams-helios-sandbox4.source.webhook.map-store-changelog
helios-sandbox4.grp-helios-sandbox4-dead-letter-processor-service-RPCStreams-helios-sandbox4.webhook.data-store-changelog
helios-sandbox4.grp-helios-sandbox4-dead-letter-processor-service-RPCStreams-helios-sandbox4.webhook.dead.event.map-store-changelog
helios-sandbox4.grp-helios-sandbox4-dead-letter-processor-service-RPCStreams-helios-sandbox4.webhook.rule.map-store-changelog
helios-sandbox4.grp-helios-sandbox4-dead-letter-processor-service-requeue-DeadEventMap-changelog
helios-sandbox4.grp-helios-sandbox4-dead-letter-processor-service-requeue-DeadEventMapCount-changelog
helios-sandbox4.grp-helios-sandbox4-dead-letter-processor-service-requeue-WebhookState-changelog
helios-sandbox4.grp-helios-sandbox4-event-scheduler-processor-service-EventStore-changelog
helios-sandbox4.grp-helios-sandbox4-event-scheduler-processor-service-RPCStreams-helios-sandbox4.webhook.data-store-changelog
helios-sandbox4.grp-helios-sandbox4-event-scheduler-processor-service-ReasonStore-changelog
helios-sandbox4.grp-helios-sandbox4-event-scheduler-processor-service-StatusStore-changelog
helios-sandbox4.grp-helios-sandbox4-event-scheduler-processor-service-TimingStore-changelog
helios-sandbox4.grp-helios-sandbox4-event-scheduler-processor-service-WebhookRetryCountStore-changelog
helios-sandbox4.grp-helios-sandbox4-external-event-processor-service-RPCStreams-helios-sandbox4.event.rule.map-store-changelog
helios-sandbox4.grp-helios-sandbox4-external-event-processor-service-RPCStreams-helios-sandbox4.rule.data-store-changelog
helios-sandbox4.grp-helios-sandbox4-external-event-processor-service-RPCStreams-helios-sandbox4.webhook.data-store-changelog
helios-sandbox4.grp-helios-sandbox4-external-event-processor-service-RateLimitedWebhookStore-changelog
helios-sandbox4.priority.external.event
helios-sandbox4.publish.event
helios-sandbox4.rule.data
helios-sandbox4.send.event
helios-sandbox4.source.data
helios-sandbox4.source.event.map
helios-sandbox4.source.webhook.map
helios-sandbox4.status.event
helios-sandbox4.webhook.data
helios-sandbox4.webhook.dead.event.map
helios-sandbox4.webhook.rule.map
helios-sandbox5.analytics.event
helios-sandbox5.dead.event
helios-sandbox5.death.event
helios-sandbox5.error.event
helios-sandbox5.event.data
helios-sandbox5.event.rule.map
helios-sandbox5.external.event
helios-sandbox5.grp-helios-sandbox5-analytics-service-AnalyticsDataStore-changelog
helios-sandbox5.grp-helios-sandbox5-analytics-service-AnalyticsKeyStore-changelog
helios-sandbox5.grp-helios-sandbox5-data-processor-service-RPCStreams-helios-sandbox5.event.data-store-changelog
helios-sandbox5.grp-helios-sandbox5-data-processor-service-RPCStreams-helios-sandbox5.event.rule.map-store-changelog
helios-sandbox5.grp-helios-sandbox5-data-processor-service-RPCStreams-helios-sandbox5.rule.data-store-changelog
helios-sandbox5.grp-helios-sandbox5-data-processor-service-RPCStreams-helios-sandbox5.source.data-store-changelog
helios-sandbox5.grp-helios-sandbox5-data-processor-service-RPCStreams-helios-sandbox5.source.event.map-store-changelog
helios-sandbox5.grp-helios-sandbox5-data-processor-service-RPCStreams-helios-sandbox5.source.webhook.map-store-changelog
helios-sandbox5.grp-helios-sandbox5-data-processor-service-RPCStreams-helios-sandbox5.webhook.data-store-changelog
helios-sandbox5.grp-helios-sandbox5-data-processor-service-RPCStreams-helios-sandbox5.webhook.rule.map-store-changelog
helios-sandbox5.grp-helios-sandbox5-data-provider-service-RPCStreams-helios-sandbox5.event.data-store-changelog
helios-sandbox5.grp-helios-sandbox5-data-provider-service-RPCStreams-helios-sandbox5.event.rule.map-store-changelog
helios-sandbox5.grp-helios-sandbox5-data-provider-service-RPCStreams-helios-sandbox5.rule.data-store-changelog
helios-sandbox5.grp-helios-sandbox5-data-provider-service-RPCStreams-helios-sandbox5.source.data-store-changelog
helios-sandbox5.grp-helios-sandbox5-data-provider-service-RPCStreams-helios-sandbox5.source.event.map-store-changelog
helios-sandbox5.grp-helios-sandbox5-data-provider-service-RPCStreams-helios-sandbox5.source.webhook.map-store-changelog
helios-sandbox5.grp-helios-sandbox5-data-provider-service-RPCStreams-helios-sandbox5.webhook.data-store-changelog
helios-sandbox5.grp-helios-sandbox5-data-provider-service-RPCStreams-helios-sandbox5.webhook.rule.map-store-changelog
helios-sandbox5.grp-helios-sandbox5-dead-letter-processor-service-RPCStreams-helios-sandbox5.dead.event-store-changelog
helios-sandbox5.grp-helios-sandbox5-dead-letter-processor-service-RPCStreams-helios-sandbox5.event.data-store-changelog
helios-sandbox5.grp-helios-sandbox5-dead-letter-processor-service-RPCStreams-helios-sandbox5.event.rule.map-store-changelog
helios-sandbox5.grp-helios-sandbox5-dead-letter-processor-service-RPCStreams-helios-sandbox5.rule.data-store-changelog
helios-sandbox5.grp-helios-sandbox5-dead-letter-processor-service-RPCStreams-helios-sandbox5.source.data-store-changelog
helios-sandbox5.grp-helios-sandbox5-dead-letter-processor-service-RPCStreams-helios-sandbox5.source.event.map-store-changelog
helios-sandbox5.grp-helios-sandbox5-dead-letter-processor-service-RPCStreams-helios-sandbox5.source.webhook.map-store-changelog
helios-sandbox5.grp-helios-sandbox5-dead-letter-processor-service-RPCStreams-helios-sandbox5.webhook.data-store-changelog
helios-sandbox5.grp-helios-sandbox5-dead-letter-processor-service-RPCStreams-helios-sandbox5.webhook.dead.event.map-store-changelog
helios-sandbox5.grp-helios-sandbox5-dead-letter-processor-service-RPCStreams-helios-sandbox5.webhook.rule.map-store-changelog
helios-sandbox5.grp-helios-sandbox5-dead-letter-processor-service-requeue-DeadEventMap-changelog
helios-sandbox5.grp-helios-sandbox5-dead-letter-processor-service-requeue-DeadEventMapCount-changelog
helios-sandbox5.grp-helios-sandbox5-dead-letter-processor-service-requeue-WebhookState-changelog
helios-sandbox5.grp-helios-sandbox5-event-scheduler-processor-service-EventStore-changelog
helios-sandbox5.grp-helios-sandbox5-event-scheduler-processor-service-RPCStreams-helios-sandbox5.webhook.data-store-changelog
helios-sandbox5.grp-helios-sandbox5-event-scheduler-processor-service-ReasonStore-changelog
helios-sandbox5.grp-helios-sandbox5-event-scheduler-processor-service-StatusStore-changelog
helios-sandbox5.grp-helios-sandbox5-event-scheduler-processor-service-TimingStore-changelog
helios-sandbox5.grp-helios-sandbox5-event-scheduler-processor-service-WebhookRetryCountStore-changelog
helios-sandbox5.grp-helios-sandbox5-external-event-processor-service-RPCStreams-helios-sandbox5.event.rule.map-store-changelog
helios-sandbox5.grp-helios-sandbox5-external-event-processor-service-RPCStreams-helios-sandbox5.rule.data-store-changelog
helios-sandbox5.grp-helios-sandbox5-external-event-processor-service-RPCStreams-helios-sandbox5.webhook.data-store-changelog
helios-sandbox5.grp-helios-sandbox5-external-event-processor-service-RateLimitedWebhookStore-changelog
helios-sandbox5.grp-helios-sandbox5-provisioning-service-RPCStreams-helios-sandbox5.source.data-store-changelog
helios-sandbox5.priority.external.event
helios-sandbox5.publish.event
helios-sandbox5.rule.data
helios-sandbox5.send.event
helios-sandbox5.source.data
helios-sandbox5.source.event.map
helios-sandbox5.source.webhook.map
helios-sandbox5.status.event
helios-sandbox5.webhook.data
helios-sandbox5.webhook.dead.event.map
helios-sandbox5.webhook.rule.map
helios-sandbox6.analytics.event
helios-sandbox6.dead.event
helios-sandbox6.death.event
helios-sandbox6.error.event
helios-sandbox6.event.data
helios-sandbox6.event.rule.map
helios-sandbox6.external.event
helios-sandbox6.priority.external.event
helios-sandbox6.publish.event
helios-sandbox6.rule.data
helios-sandbox6.send.event
helios-sandbox6.source.data
helios-sandbox6.source.event.map
helios-sandbox6.source.webhook.map
helios-sandbox6.status.event
helios-sandbox6.webhook.data
helios-sandbox6.webhook.dead.event.map
helios-sandbox6.webhook.rule.map
helios.analytics.event
helios.dead.event
helios.death.event
helios.error.event
helios.event.data
helios.event.rule.map
helios.external.event
helios.grp-helios-data-processor-service-DeadEventMap-changelog
helios.grp-helios-data-processor-service-DeadEventMapCount-changelog
helios.grp-helios-data-processor-service-RPCStreams-helios.event.data-store-changelog
helios.grp-helios-data-processor-service-RPCStreams-helios.event.rule.map-store-changelog
helios.grp-helios-data-processor-service-RPCStreams-helios.rule.data-store-changelog
helios.grp-helios-data-processor-service-RPCStreams-helios.source.data-store-changelog
helios.grp-helios-data-processor-service-RPCStreams-helios.source.event.map-store-changelog
helios.grp-helios-data-processor-service-RPCStreams-helios.source.webhook.map-store-changelog
helios.grp-helios-data-processor-service-RPCStreams-helios.webhook.data-store-changelog
helios.grp-helios-data-processor-service-RPCStreams-helios.webhook.rule.map-store-changelog
helios.grp-helios-data-processor-service-WebhookState-changelog
helios.grp-helios-data-processor-service-requeue-DeadEventMap-changelog
helios.grp-helios-data-processor-service-requeue-DeadEventMapCount-changelog
helios.grp-helios-data-processor-service-requeue-WebhookState-changelog
helios.grp-helios-data-provider-service-RPCStreams-helios.event.data-store-changelog
helios.grp-helios-data-provider-service-RPCStreams-helios.event.rule.map-store-changelog
helios.grp-helios-data-provider-service-RPCStreams-helios.rule.data-store-changelog
helios.grp-helios-data-provider-service-RPCStreams-helios.source.data-store-changelog
helios.grp-helios-data-provider-service-RPCStreams-helios.source.event.map-store-changelog
helios.grp-helios-data-provider-service-RPCStreams-helios.source.webhook.map-store-changelog
helios.grp-helios-data-provider-service-RPCStreams-helios.webhook.data-store-changelog
helios.grp-helios-data-provider-service-RPCStreams-helios.webhook.rule.map-store-changelog
helios.grp-helios-dead-letter-processor-service-RPCStreams-helios.dead.event-store-changelog
helios.grp-helios-dead-letter-processor-service-RPCStreams-helios.event.data-store-changelog
helios.grp-helios-dead-letter-processor-service-RPCStreams-helios.event.rule.map-store-changelog
helios.grp-helios-dead-letter-processor-service-RPCStreams-helios.rule.data-store-changelog
helios.grp-helios-dead-letter-processor-service-RPCStreams-helios.source.data-store-changelog
helios.grp-helios-dead-letter-processor-service-RPCStreams-helios.source.event.map-store-changelog
helios.grp-helios-dead-letter-processor-service-RPCStreams-helios.source.webhook.map-store-changelog
helios.grp-helios-dead-letter-processor-service-RPCStreams-helios.webhook.data-store-changelog
helios.grp-helios-dead-letter-processor-service-RPCStreams-helios.webhook.dead.event.map-store-changelog
helios.grp-helios-dead-letter-processor-service-RPCStreams-helios.webhook.rule.map-store-changelog
helios.grp-helios-dead-letter-processor-service-requeue-DeadEventMap-changelog
helios.grp-helios-dead-letter-processor-service-requeue-DeadEventMapCount-changelog
helios.grp-helios-dead-letter-processor-service-requeue-WebhookState-changelog
helios.grp-helios-event-scheduler-processor-service-BlacklistStore-changelog
helios.grp-helios-event-scheduler-processor-service-EventStore-changelog
helios.grp-helios-event-scheduler-processor-service-RPCStreams-helios.webhook.data-store-changelog
helios.grp-helios-event-scheduler-processor-service-ReasonStore-changelog
helios.grp-helios-event-scheduler-processor-service-StatusStore-changelog
helios.grp-helios-event-scheduler-processor-service-TimingStore-changelog
helios.grp-helios-event-scheduler-processor-service-WebhookRetryCountStore-changelog
helios.grp-helios-external-event-processor-service-RPCStreams-helios.event.rule.map-store-changelog
helios.grp-helios-external-event-processor-service-RPCStreams-helios.rule.data-store-changelog
helios.grp-helios-external-event-processor-service-RPCStreams-helios.webhook.data-store-changelog
helios.grp-helios-external-event-processor-service-RateLimitedWebhookStore-changelog
helios.grp-helios-provisioning-service-RPCStreams-helios.source.data-store-changelog
helios.grp-helios-provisioning-service-data-PDBProvisioningResponseStore_-changelog
helios.grp-helios-provisioning-service-data-PDBProvisioningStatusStore_-changelog
helios.grp-helios-provisioning-service-data-ServiceProvisioningStatesStore_-changelog
helios.grp-helios-provisioning-service-data-SourceRegistrationStore_-changelog
helios.grp-helios-provisioning-service-dataTMS-TenantEventStore_-changelog
helios.preprocessed.external.event
helios.priority.external.event
helios.provisioning.request.event
helios.provisioning.response.event
helios.publish.event
helios.recovery.event
helios.rule.data
helios.send.event
helios.source.data
helios.source.event.map
helios.source.webhook.map
helios.status.event
helios.webhook.data
helios.webhook.dead.event.map
helios.webhook.rule.map
kftest-kf
mercurygrp-mercury-cert-death-event-app-death.event.store-changelog
mercurygrp-mercury-chate2e-death-event-app-death.event.store-changelog
mercurygrp-mercury-death-event-app-death.event.store-changelog
mercurygrp-mercury-dev-death-event-app-death.event.store-changelog
mercurygrp-mercury-psr-death-event-app-death.event.store-changelog
pdb.processor-PdbMetadataStateStore-changelog
pdb.provision.request
pdb.provision.response
pdbtest.processor-MetadataStateStore-changelog
pdbtest.processor-PdbMetadataStateStore-changelog
pdbtest.provision.request
pdbtest.provision.response
sdc.requestqueue
sdc.responsequeue
search.search-metadata
tms.application.map
tms.operation.request
tms.operation.request.tracker
tms.operation.response
tms.processor-tenant-tms.internal.state-changelog
tms.processor-tenantOperation-tms.tenant.operation-store-changelog
tms.processor-tenantOperation-tms.tenant.operation-store-repartition
tms.tenant.lookup
tms.tenant.operation
tms.tenant.state
[kafka@shared-kafka-zookeeper-0 kafka]$
[kafka@shared-kafka-zookeeper-0 kafka]$ ./bin/kafka-topics.sh --zookeeper localhost:12181 --describe --topic functional-test
Topic: functional-test  TopicId: SlUINH03QuaKBUdEOzzgVg PartitionCount: 3 ReplicationFactor: 3  Configs:
  Topic: functional-test  Partition: 0  Leader: 0 Replicas: 0,1,2 Isr: 0,1,2
  Topic: functional-test  Partition: 1  Leader: 1 Replicas: 1,2,0 Isr: 0,1,2
  Topic: functional-test  Partition: 2  Leader: 2 Replicas: 2,0,1 Isr: 0,1,2
[kafka@shared-kafka-zookeeper-0 kafka]$ ./bin/kafka-
kafka-acls.sh                        kafka-dump-log.sh                    kafka-run-class.sh
kafka-broker-api-versions.sh         kafka-features.sh                    kafka-server-start.sh
kafka-cluster.sh                     kafka-leader-election.sh             kafka-server-stop.sh
kafka-configs.sh                     kafka-log-dirs.sh                    kafka-storage.sh
kafka-console-consumer.sh            kafka-metadata-shell.sh              kafka-streams-application-reset.sh
kafka-console-producer.sh            kafka-mirror-maker.sh                kafka-topics.sh
kafka-consumer-groups.sh             kafka-preferred-replica-election.sh  kafka-verifiable-consumer.sh
kafka-consumer-perf-test.sh          kafka-producer-perf-test.sh          kafka-verifiable-producer.sh
kafka-delegation-tokens.sh           kafka-reassign-partitions.sh
kafka-delete-records.sh              kafka-replica-verification.sh
[kafka@shared-kafka-zookeeper-0 kafka]$ ./bin/kafka-acls.sh
This tool helps to manage acls on kafka.
Option                                   Description
------                                   -----------
--add                                    Indicates you are trying to add ACLs.
--allow-host <String: allow-host>        Host from which principals listed in --
                                           allow-principal will have access. If
                                           you have specified --allow-principal
                                           then the default for this option
                                           will be set to * which allows access
                                           from all hosts.
--allow-principal <String: allow-        principal is in principalType:name
  principal>                               format. Note that principalType must
                                           be supported by the Authorizer being
                                           used. For example, User:* is the
                                           wild card indicating all users.
--authorizer <String: authorizer>        Fully qualified class name of the
                                           authorizer, defaults to kafka.
                                           security.authorizer.AclAuthorizer.
--authorizer-properties <String:         REQUIRED: properties required to
  authorizer-properties>                   configure an instance of Authorizer.
                                           These are key=val pairs. For the
                                           default authorizer the example
                                           values are: zookeeper.
                                           connect=localhost:2181
--bootstrap-server <String: server to    A list of host/port pairs to use for
  connect to>                              establishing the connection to the
                                           Kafka cluster. This list should be
                                           in the form host1:port1,host2:
                                           port2,... This config is required
                                           for acl management using admin
                                           client API.
--cluster                                Add/Remove cluster ACLs.
--command-config [String: command-       A property file containing configs to
  config]                                  be passed to Admin Client.
--consumer                               Convenience option to add/remove ACLs
                                           for consumer role. This will
                                           generate ACLs that allows READ,
                                           DESCRIBE on topic and READ on group.
--delegation-token <String: delegation-  Delegation token to which ACLs should
  token>                                   be added or removed. A value of *
                                           indicates ACL should apply to all
                                           tokens.
--deny-host <String: deny-host>          Host from which principals listed in --
                                           deny-principal will be denied
                                           access. If you have specified --deny-
                                           principal then the default for this
                                           option will be set to * which denies
                                           access from all hosts.
--deny-principal <String: deny-          principal is in principalType:name
  principal>                               format. By default anyone not added
                                           through --allow-principal is denied
                                           access. You only need to use this
                                           option as negation to already
                                           allowed set. Note that principalType
                                           must be supported by the Authorizer
                                           being used. For example if you
                                           wanted to allow access to all users
                                           in the system but not test-user you
                                           can define an ACL that allows access
                                           to User:* and specify --deny-
                                           principal=User:test@EXAMPLE.COM. AND
                                           PLEASE REMEMBER DENY RULES TAKES
                                           PRECEDENCE OVER ALLOW RULES.
--force                                  Assume Yes to all queries and do not
                                           prompt.
--group <String: group>                  Consumer Group to which the ACLs
                                           should be added or removed. A value
                                           of * indicates the ACLs should apply
                                           to all groups.
--help                                   Print usage information.
--idempotent                             Enable idempotence for the producer.
                                           This should be used in combination
                                           with the --producer option. Note
                                           that idempotence is enabled
                                           automatically if the producer is
                                           authorized to a particular
                                           transactional-id.
--list                                   List ACLs for the specified resource,
                                           use --topic <topic> or --group
                                           <group> or --cluster to specify a
                                           resource.
--operation <String>                     Operation that is being allowed or
                                           denied. Valid operation names are:
                                          Describe
                                          DescribeConfigs
                                          Alter
                                          IdempotentWrite
                                          Read
                                          Delete
                                          Create
                                          ClusterAction
                                          All
                                          Write
                                          AlterConfigs
                                          (default: All)
--principal [String: principal]          List ACLs for the specified principal.
                                           principal is in principalType:name
                                           format. Note that principalType must
                                           be supported by the Authorizer being
                                           used. Multiple --principal option
                                           can be passed.
--producer                               Convenience option to add/remove ACLs
                                           for producer role. This will
                                           generate ACLs that allows WRITE,
                                           DESCRIBE and CREATE on topic.
--remove                                 Indicates you are trying to remove
                                           ACLs.
--resource-pattern-type                  The type of the resource pattern or
  <ANY|MATCH|LITERAL|PREFIXED>             pattern filter. When adding acls,
                                           this should be a specific pattern
                                           type, e.g. 'literal' or 'prefixed'.
                                           When listing or removing acls, a
                                           specific pattern type can be used to
                                           list or remove acls from specific
                                           resource patterns, or use the filter
                                           values of 'any' or 'match', where
                                           'any' will match any pattern type,
                                           but will match the resource name
                                           exactly, where as 'match' will
                                           perform pattern matching to list or
                                           remove all acls that affect the
                                           supplied resource(s). WARNING:
                                           'match', when used in combination
                                           with the '--remove' switch, should
                                           be used with care. (default: LITERAL)
--topic <String: topic>                  topic to which ACLs should be added or
                                           removed. A value of * indicates ACL
                                           should apply to all topics.
--transactional-id <String:              The transactionalId to which ACLs
  transactional-id>                        should be added or removed. A value
                                           of * indicates the ACLs should apply
                                           to all transactionalIds.
--version                                Display Kafka version.
--zk-tls-config-file <String:            Identifies the file where ZooKeeper
  Authorizer ZooKeeper TLS                 client TLS connectivity properties
  configuration>                           for the authorizer are defined.  Any
                                           properties other than the following
                                           (with or without an "authorizer."
                                           prefix) are ignored: zookeeper.
                                           clientCnxnSocket, zookeeper.ssl.
                                           cipher.suites, zookeeper.ssl.client.
                                           enable, zookeeper.ssl.crl.enable,
                                           zookeeper.ssl.enabled.protocols,
                                           zookeeper.ssl.endpoint.
                                           identification.algorithm, zookeeper.
                                           ssl.keystore.location, zookeeper.ssl.
                                           keystore.password, zookeeper.ssl.
                                           keystore.type, zookeeper.ssl.ocsp.
                                           enable, zookeeper.ssl.protocol,
                                           zookeeper.ssl.truststore.location,
                                           zookeeper.ssl.truststore.password,
                                           zookeeper.ssl.truststore.type. Note
                                           that if SASL is not configured and
                                           zookeeper.set.acl is supposed to be
                                           true due to mutual certificate
                                           authentication being used then it is
                                           necessary to explicitly specify --
                                           authorizer-properties zookeeper.set.
                                           acl=true
[kafka@shared-kafka-zookeeper-0 kafka]$ ./bin/kafka-acls.sh  --authorizer-properties
Exception in thread "main" joptsimple.OptionMissingRequiredArgumentException: Option authorizer-properties requires an argument
  at joptsimple.RequiredArgumentOptionSpec.detectOptionArgument(RequiredArgumentOptionSpec.java:48)
  at joptsimple.ArgumentAcceptingOptionSpec.handleOption(ArgumentAcceptingOptionSpec.java:257)
  at joptsimple.OptionParser.handleLongOptionToken(OptionParser.java:513)
  at joptsimple.OptionParserState$2.handleArgument(OptionParserState.java:56)
  at joptsimple.OptionParser.parse(OptionParser.java:396)
  at kafka.admin.AclCommand$AclCommandOptions.<init>(AclCommand.scala:614)
  at kafka.admin.AclCommand$.main(AclCommand.scala:49)
  at kafka.admin.AclCommand.main(AclCommand.scala)
[kafka@shared-kafka-zookeeper-0 kafka]$ ^C
[kafka@shared-kafka-zookeeper-0 kafka]$ [24:January:2022:09:44:20]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[25:January:2022:04:43:50]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ v2corp
[25:January:2022:04:43:54]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgc
CURRENT   NAME                             CLUSTER               AUTHINFO           NAMESPACE
          corp_us-ashburn-1_controlplane   cluster-c3wgzdgmfqt   user-c3wgzdgmfqt
*         corp_us-ashburn-1_dataplane      cluster-c2tqztfmy3t   user-c2tqztfmy3t
          corp_us-ashburn-1_deployment     cluster-crdcyjvmztg   user-crdcyjvmztg
[25:January:2022:04:43:55]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ v3preprod
bash: v3preprod: command not found
[25:January:2022:04:43:58]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ v2preprod
[25:January:2022:04:44:01]:(preprod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgc
CURRENT   NAME                                CLUSTER               AUTHINFO           NAMESPACE
          preprod_us-ashburn-1_controlplane   cluster-cqwizjrme3w   user-cqwizjrme3w
          preprod_us-ashburn-1_dataplane      cluster-c4wmnjqg5rg   user-c4wmnjqg5rg
          preprod_us-phoenix-1_controlplane   cluster-c3teodfgftd   user-c3teodfgftd
*         preprod_us-phoenix-1_dataplane      cluster-c3dmzrygazg   user-c3dmzrygazg
          preprod_us-phoenix-1_deployment     cluster-c3wcmbqgi4d   user-c3wcmbqgi4d
[25:January:2022:04:44:02]:(preprod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ rp
[25:January:2022:04:44:03]:(preprod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kuc preprod_us-phoenix-1_deployment
Switched to context "preprod_us-phoenix-1_deployment".
[25:January:2022:04:44:10]:(preprod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ rp
[25:January:2022:04:44:11]:(preprod_us-phoenix-1_deployment):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgn infra-monitoring ingress
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME                                      CLASS    HOSTS                                                                ADDRESS          PORTS     AGE
grafana                                   <none>   infra-grafana-preprod.channels.ocs.oc-test.com                       147.154.103.59   80, 443   299d
prometheus-oper-phx-deploy-alertmanager   <none>   infra-alertmanager-preprod-phx-deployment.channels.ocs.oc-test.com   147.154.103.59   80, 443   483d
prometheus-oper-phx-deploy-prometheus     <none>   infra-prometheus-preprod-phx-deployment.channels.ocs.oc-test.com     147.154.103.59   80, 443   483d
thanos-query                              <none>   infra-thanos-preprod.channels.ocs.oc-test.com                        147.154.103.59   80, 443   217d
thanos-query-frontend                     <none>   infra-thanos-frontend-preprod.channels.ocs.oc-test.com               147.154.103.59   80, 443   217d
thanos-ruler                              <none>   infra-thanos-ruler-preprod.channels.ocs.oc-test.com                  147.154.103.59   80, 443   217d
[25:January:2022:04:44:43]:(preprod_us-phoenix-1_deployment):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ #for contexts in `kgc --output=name | grep dataplane`; do echo kuc $contexts; echo "--------Running in $contexts -------------";  for kafkapod in $(kgpnk | grep "kafka-[0-2]" | awk '{print $1}'); do echo "Check Disk Usage on $kafkapod" ;keitn kafka $kafkapod -- df -kh | grep data; echo "++++++++++++++++++++++++"; done; echo "--------------------"; done
[25:January:2022:04:45:52]:(preprod_us-phoenix-1_deployment):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgn frontend services
NAME                                        TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)                      AGE
ingress-ocna-phx-de-controller              LoadBalancer   10.97.146.220   147.154.103.59   80:30134/TCP,443:31248/TCP   493d
ingress-ocna-phx-de-controller-metrics      ClusterIP      10.97.216.12    <none>           10254/TCP                    493d
ingress-private-phx-de-controller           LoadBalancer   10.97.195.121   10.10.16.20      80:30858/TCP,443:31542/TCP   493d
ingress-private-phx-de-controller-metrics   ClusterIP      10.97.97.168    <none>           10254/TCP                    493d
[25:January:2022:04:46:04]:(preprod_us-phoenix-1_deployment):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgn frontend ingress
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
No resources found in frontend namespace.
[25:January:2022:04:46:22]:(preprod_us-phoenix-1_deployment):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ for contexts in `kgc --output=name | grep dataplane`; do echo kuc $contexts; echo "--------Running in $contexts -------------"; kgn infra-monitoring ingress ; done
kuc preprod_us-ashburn-1_dataplane
--------Running in preprod_us-ashburn-1_dataplane -------------
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME                                      CLASS    HOSTS                                                                ADDRESS          PORTS     AGE
grafana                                   <none>   infra-grafana-preprod.channels.ocs.oc-test.com                       147.154.103.59   80, 443   299d
prometheus-oper-phx-deploy-alertmanager   <none>   infra-alertmanager-preprod-phx-deployment.channels.ocs.oc-test.com   147.154.103.59   80, 443   483d
prometheus-oper-phx-deploy-prometheus     <none>   infra-prometheus-preprod-phx-deployment.channels.ocs.oc-test.com     147.154.103.59   80, 443   483d
thanos-query                              <none>   infra-thanos-preprod.channels.ocs.oc-test.com                        147.154.103.59   80, 443   217d
thanos-query-frontend                     <none>   infra-thanos-frontend-preprod.channels.ocs.oc-test.com               147.154.103.59   80, 443   217d
thanos-ruler                              <none>   infra-thanos-ruler-preprod.channels.ocs.oc-test.com                  147.154.103.59   80, 443   217d
kuc preprod_us-phoenix-1_dataplane
--------Running in preprod_us-phoenix-1_dataplane -------------
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME                                      CLASS    HOSTS                                                                ADDRESS          PORTS     AGE
grafana                                   <none>   infra-grafana-preprod.channels.ocs.oc-test.com                       147.154.103.59   80, 443   299d
prometheus-oper-phx-deploy-alertmanager   <none>   infra-alertmanager-preprod-phx-deployment.channels.ocs.oc-test.com   147.154.103.59   80, 443   483d
prometheus-oper-phx-deploy-prometheus     <none>   infra-prometheus-preprod-phx-deployment.channels.ocs.oc-test.com     147.154.103.59   80, 443   483d
thanos-query                              <none>   infra-thanos-preprod.channels.ocs.oc-test.com                        147.154.103.59   80, 443   217d
thanos-query-frontend                     <none>   infra-thanos-frontend-preprod.channels.ocs.oc-test.com               147.154.103.59   80, 443   217d
thanos-ruler                              <none>   infra-thanos-ruler-preprod.channels.ocs.oc-test.com                  147.154.103.59   80, 443   217d
[25:January:2022:04:47:07]:(preprod_us-phoenix-1_deployment):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ for contexts in `kgc --output=name`; do echo kuc $contexts; echo "--------Running in $contexts -------------"; kgn infra-monitoring ingress ; done
kuc preprod_us-ashburn-1_controlplane
--------Running in preprod_us-ashburn-1_controlplane -------------
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME                                      CLASS    HOSTS                                                                ADDRESS          PORTS     AGE
grafana                                   <none>   infra-grafana-preprod.channels.ocs.oc-test.com                       147.154.103.59   80, 443   299d
prometheus-oper-phx-deploy-alertmanager   <none>   infra-alertmanager-preprod-phx-deployment.channels.ocs.oc-test.com   147.154.103.59   80, 443   483d
prometheus-oper-phx-deploy-prometheus     <none>   infra-prometheus-preprod-phx-deployment.channels.ocs.oc-test.com     147.154.103.59   80, 443   483d
thanos-query                              <none>   infra-thanos-preprod.channels.ocs.oc-test.com                        147.154.103.59   80, 443   217d
thanos-query-frontend                     <none>   infra-thanos-frontend-preprod.channels.ocs.oc-test.com               147.154.103.59   80, 443   217d
thanos-ruler                              <none>   infra-thanos-ruler-preprod.channels.ocs.oc-test.com                  147.154.103.59   80, 443   217d
kuc preprod_us-ashburn-1_dataplane
--------Running in preprod_us-ashburn-1_dataplane -------------
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME                                      CLASS    HOSTS                                                                ADDRESS          PORTS     AGE
grafana                                   <none>   infra-grafana-preprod.channels.ocs.oc-test.com                       147.154.103.59   80, 443   299d
prometheus-oper-phx-deploy-alertmanager   <none>   infra-alertmanager-preprod-phx-deployment.channels.ocs.oc-test.com   147.154.103.59   80, 443   483d
prometheus-oper-phx-deploy-prometheus     <none>   infra-prometheus-preprod-phx-deployment.channels.ocs.oc-test.com     147.154.103.59   80, 443   483d
thanos-query                              <none>   infra-thanos-preprod.channels.ocs.oc-test.com                        147.154.103.59   80, 443   217d
thanos-query-frontend                     <none>   infra-thanos-frontend-preprod.channels.ocs.oc-test.com               147.154.103.59   80, 443   217d
thanos-ruler                              <none>   infra-thanos-ruler-preprod.channels.ocs.oc-test.com                  147.154.103.59   80, 443   217d
kuc preprod_us-phoenix-1_controlplane
--------Running in preprod_us-phoenix-1_controlplane -------------
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME                                      CLASS    HOSTS                                                                ADDRESS          PORTS     AGE
grafana                                   <none>   infra-grafana-preprod.channels.ocs.oc-test.com                       147.154.103.59   80, 443   299d
prometheus-oper-phx-deploy-alertmanager   <none>   infra-alertmanager-preprod-phx-deployment.channels.ocs.oc-test.com   147.154.103.59   80, 443   483d
prometheus-oper-phx-deploy-prometheus     <none>   infra-prometheus-preprod-phx-deployment.channels.ocs.oc-test.com     147.154.103.59   80, 443   483d
thanos-query                              <none>   infra-thanos-preprod.channels.ocs.oc-test.com                        147.154.103.59   80, 443   217d
thanos-query-frontend                     <none>   infra-thanos-frontend-preprod.channels.ocs.oc-test.com               147.154.103.59   80, 443   217d
thanos-ruler                              <none>   infra-thanos-ruler-preprod.channels.ocs.oc-test.com                  147.154.103.59   80, 443   217d
kuc preprod_us-phoenix-1_dataplane
--------Running in preprod_us-phoenix-1_dataplane -------------
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME                                      CLASS    HOSTS                                                                ADDRESS          PORTS     AGE
grafana                                   <none>   infra-grafana-preprod.channels.ocs.oc-test.com                       147.154.103.59   80, 443   299d
prometheus-oper-phx-deploy-alertmanager   <none>   infra-alertmanager-preprod-phx-deployment.channels.ocs.oc-test.com   147.154.103.59   80, 443   483d
prometheus-oper-phx-deploy-prometheus     <none>   infra-prometheus-preprod-phx-deployment.channels.ocs.oc-test.com     147.154.103.59   80, 443   483d
thanos-query                              <none>   infra-thanos-preprod.channels.ocs.oc-test.com                        147.154.103.59   80, 443   217d
thanos-query-frontend                     <none>   infra-thanos-frontend-preprod.channels.ocs.oc-test.com               147.154.103.59   80, 443   217d
thanos-ruler                              <none>   infra-thanos-ruler-preprod.channels.ocs.oc-test.com                  147.154.103.59   80, 443   217d
kuc preprod_us-phoenix-1_deployment
--------Running in preprod_us-phoenix-1_deployment -------------
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME                                      CLASS    HOSTS                                                                ADDRESS          PORTS     AGE
grafana                                   <none>   infra-grafana-preprod.channels.ocs.oc-test.com                       147.154.103.59   80, 443   299d
prometheus-oper-phx-deploy-alertmanager   <none>   infra-alertmanager-preprod-phx-deployment.channels.ocs.oc-test.com   147.154.103.59   80, 443   483d
prometheus-oper-phx-deploy-prometheus     <none>   infra-prometheus-preprod-phx-deployment.channels.ocs.oc-test.com     147.154.103.59   80, 443   483d
thanos-query                              <none>   infra-thanos-preprod.channels.ocs.oc-test.com                        147.154.103.59   80, 443   217d
thanos-query-frontend                     <none>   infra-thanos-frontend-preprod.channels.ocs.oc-test.com               147.154.103.59   80, 443   217d
thanos-ruler                              <none>   infra-thanos-ruler-preprod.channels.ocs.oc-test.com                  147.154.103.59   80, 443   217d
[25:January:2022:04:47:48]:(preprod_us-phoenix-1_deployment):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ for contexts in `kgc --output=name`; do echo kuc $contexts; echo "--------Running in $contexts -------------"; kgn infra-monitoring ingress > ~/galorndon/ctemp/preprod_infra-monitoring-ingress.yaml ; done
kuc preprod_us-ashburn-1_controlplane
--------Running in preprod_us-ashburn-1_controlplane -------------
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
kuc preprod_us-ashburn-1_dataplane
--------Running in preprod_us-ashburn-1_dataplane -------------
^C
[25:January:2022:04:48:26]:(preprod_us-phoenix-1_deployment):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ for contexts in `kgc --output=name`; do echo kuc $contexts; echo "--------Running in $contexts -------------"; kgn infra-monitoring ingress -oyaml > ~/galorndon/ctemp/preprod_infra-monitoring-ingress.yaml ; done
kuc preprod_us-ashburn-1_controlplane
--------Running in preprod_us-ashburn-1_controlplane -------------
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
kuc preprod_us-ashburn-1_dataplane
--------Running in preprod_us-ashburn-1_dataplane -------------
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
kuc preprod_us-phoenix-1_controlplane
--------Running in preprod_us-phoenix-1_controlplane -------------
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
kuc preprod_us-phoenix-1_dataplane
--------Running in preprod_us-phoenix-1_dataplane -------------
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
kuc preprod_us-phoenix-1_deployment
--------Running in preprod_us-phoenix-1_deployment -------------
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
[25:January:2022:04:48:51]:(preprod_us-phoenix-1_deployment):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgn traefik-frontend services
NAME                             TYPE           CLUSTER-IP      EXTERNAL-IP       PORT(S)                                     AGE
ingress-traefik-ocna-phx-de      LoadBalancer   10.97.68.200    147.154.100.205   9100:30978/TCP,80:30645/TCP,443:32214/TCP   28d
ingress-traefik-private-phx-de   LoadBalancer   10.97.170.231   10.10.16.82       9100:32123/TCP,80:31206/TCP,443:31007/TCP   28d
[25:January:2022:04:50:06]:(preprod_us-phoenix-1_deployment):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgpn traefik-frontend
NAME                                              READY   STATUS    RESTARTS   AGE   IP             NODE           NOMINATED NODE   READINESS GATES
ingress-traefik-ocna-phx-de-746657c695-f4lcp      1/1     Running   0          28d   10.245.5.23    10.10.16.33    <none>           <none>
ingress-traefik-ocna-phx-de-746657c695-x9lkp      1/1     Running   0          28d   10.245.5.148   10.10.16.189   <none>           <none>
ingress-traefik-ocna-phx-de-746657c695-zqgd6      1/1     Running   0          28d   10.245.6.158   10.10.16.167   <none>           <none>
ingress-traefik-private-phx-de-7d8cb64bbf-424wg   1/1     Running   0          28d   10.245.7.86    10.10.16.39    <none>           <none>
ingress-traefik-private-phx-de-7d8cb64bbf-4nj6x   1/1     Running   0          28d   10.245.0.18    10.10.16.171   <none>           <none>
ingress-traefik-private-phx-de-7d8cb64bbf-dplxw   1/1     Running   0          28d   10.245.4.15    10.10.16.120   <none>           <none>
[25:January:2022:04:50:26]:(preprod_us-phoenix-1_deployment):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgdpn traefik-frontend
NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
ingress-traefik-ocna-phx-de      3/3     3            3           28d
ingress-traefik-private-phx-de   3/3     3            3           28d
[25:January:2022:04:50:37]:(preprod_us-phoenix-1_deployment):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgdpn traefik-frontend ingress-traefik-ocna-phx-de -oyaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
    meta.helm.sh/release-name: ingress-traefik-ocna
    meta.helm.sh/release-namespace: traefik-frontend
  creationTimestamp: "2021-12-27T08:16:46Z"
  generation: 2
  labels:
    app.kubernetes.io/instance: ingress-traefik-ocna
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: traefik
    helm.sh/chart: traefik-10.3.6
  name: ingress-traefik-ocna-phx-de
  namespace: traefik-frontend
  resourceVersion: "242300566"
  selfLink: /apis/apps/v1/namespaces/traefik-frontend/deployments/ingress-traefik-ocna-phx-de
  uid: 449ec4d4-c54c-4d95-b228-ccc593b066c1
spec:
  progressDeadlineSeconds: 600
  replicas: 3
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/instance: ingress-traefik-ocna
      app.kubernetes.io/name: traefik
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "9100"
        prometheus.io/scrape: "true"
      creationTimestamp: null
      labels:
        app.kubernetes.io/instance: ingress-traefik-ocna
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: traefik
        helm.sh/chart: traefik-10.3.6
        stage: deployment
    spec:
      containers:
      - args:
        - --entryPoints.metrics.address=:9100/tcp
        - --entryPoints.traefik.address=:9000/tcp
        - --entryPoints.web.address=:8000/tcp
        - --entryPoints.websecure.address=:8443/tcp
        - --api.dashboard=true
        - --ping=true
        - --metrics.prometheus=true
        - --metrics.prometheus.entrypoint=metrics
        - --providers.kubernetescrd
        - --providers.kubernetesingress
        - --providers.kubernetesingress.ingressendpoint.publishedservice=traefik-frontend/ingress-traefik-ocna-phx-de
        - --entrypoints.web.http.redirections.entryPoint.to=:443
        - --entrypoints.web.http.redirections.entryPoint.scheme=https
        - --entrypoints.websecure.http.tls=true
        - --accesslog=true
        - --accesslog.fields.defaultmode=keep
        - --accesslog.fields.headers.defaultmode=drop
        - --providers.kubernetesingress.ingressclass=traefik-ocna
        - --log.level=DEBUG
        image: docker-master.cdaas.oraclecloud.com/docker-osvc-local/traefik:2.5.1
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /ping
            port: 9000
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 2
        name: ingress-traefik-ocna-phx-de
        ports:
        - containerPort: 9100
          name: metrics
          protocol: TCP
        - containerPort: 9000
          name: traefik
          protocol: TCP
        - containerPort: 8000
          name: web
          protocol: TCP
        - containerPort: 8443
          name: websecure
          protocol: TCP
        readinessProbe:
          failureThreshold: 1
          httpGet:
            path: /ping
            port: 9000
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 2
        resources:
          limits:
            cpu: 1200m
            memory: 1Gi
          requests:
            cpu: 700m
            memory: 512Mi
        securityContext:
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
          runAsGroup: 65532
          runAsNonRoot: true
          runAsUser: 65532
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /data
          name: data
        - mountPath: /tmp
          name: tmp
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext:
        fsGroup: 65532
      serviceAccount: ingress-traefik-ocna-phx-de
      serviceAccountName: ingress-traefik-ocna-phx-de
      terminationGracePeriodSeconds: 60
      volumes:
      - emptyDir: {}
        name: data
      - emptyDir: {}
        name: tmp
status:
  availableReplicas: 3
  conditions:
  - lastTransitionTime: "2021-12-27T08:16:46Z"
    lastUpdateTime: "2021-12-27T08:16:58Z"
    message: ReplicaSet "ingress-traefik-ocna-phx-de-746657c695" has successfully
      progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  - lastTransitionTime: "2021-12-27T08:17:21Z"
    lastUpdateTime: "2021-12-27T08:17:21Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  observedGeneration: 2
  readyReplicas: 3
  replicas: 3
  updatedReplicas: 3
[25:January:2022:04:50:54]:(preprod_us-phoenix-1_deployment):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[25:January:2022:06:52:23]:(preprod_us-phoenix-1_deployment):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgc
CURRENT   NAME                                CLUSTER               AUTHINFO           NAMESPACE
          preprod_us-ashburn-1_controlplane   cluster-cqwizjrme3w   user-cqwizjrme3w
          preprod_us-ashburn-1_dataplane      cluster-c4wmnjqg5rg   user-c4wmnjqg5rg
          preprod_us-phoenix-1_controlplane   cluster-c3teodfgftd   user-c3teodfgftd
          preprod_us-phoenix-1_dataplane      cluster-c3dmzrygazg   user-c3dmzrygazg
*         preprod_us-phoenix-1_deployment     cluster-c3wcmbqgi4d   user-c3wcmbqgi4d
[25:January:2022:06:52:27]:(preprod_us-phoenix-1_deployment):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kuc preprod_us-phoenix-1_controlplane
Switched to context "preprod_us-phoenix-1_controlplane".
[25:January:2022:06:52:30]:(preprod_us-phoenix-1_deployment):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ rp
[25:January:2022:06:52:31]:(preprod_us-phoenix-1_controlplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgn tms ingress
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME                            CLASS    HOSTS                                          ADDRESS        PORTS     AGE
tms-rest-proxy-tms-rest-proxy   <none>   tms-tms-phx.preprod.channels.ocs.oc-test.com   138.1.118.22   80, 443   164d
[25:January:2022:06:52:49]:(preprod_us-phoenix-1_controlplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgn tms ingress -oyaml
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    annotations:
      ingress.kubernetes.io/ssl-redirect: "true"
      ingressWatcher/external: "true"
      ingressWatcher/port: "443"
      ingressWatcher/service: tms-rest-proxy-tms-rest-proxy
      kubernetes.io/ingress.class: public
      kubernetes.io/tls-acme: "true"
      meta.helm.sh/release-name: tms-rest-proxy
      meta.helm.sh/release-namespace: tms
    creationTimestamp: "2021-08-13T16:59:42Z"
    generation: 1
    labels:
      app: tms-rest-proxy
      app.kubernetes.io/managed-by: Helm
      project: tms
      release: tms-rest-proxy
      version: "0"
    name: tms-rest-proxy-tms-rest-proxy
    namespace: tms
    resourceVersion: "172106010"
    selfLink: /apis/extensions/v1beta1/namespaces/tms/ingresses/tms-rest-proxy-tms-rest-proxy
    uid: a1f9d457-2c72-48f4-8570-0f5f38eed543
  spec:
    rules:
    - host: tms-tms-phx.preprod.channels.ocs.oc-test.com
      http:
        paths:
        - backend:
            serviceName: tms-rest-proxy-tms-rest-proxy
            servicePort: 8080
          path: /
          pathType: ImplementationSpecific
    tls:
    - hosts:
      - tms-tms-phx.preprod.channels.ocs.oc-test.com
      secretName: star.channels.ocs.oc-test.com
  status:
    loadBalancer:
      ingress:
      - ip: 138.1.118.22
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
[25:January:2022:06:53:06]:(preprod_us-phoenix-1_controlplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $
[25:January:2022:07:36:58]:(preprod_us-phoenix-1_controlplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgc
CURRENT   NAME                                CLUSTER               AUTHINFO           NAMESPACE
          preprod_us-ashburn-1_controlplane   cluster-cqwizjrme3w   user-cqwizjrme3w
          preprod_us-ashburn-1_dataplane      cluster-c4wmnjqg5rg   user-c4wmnjqg5rg
*         preprod_us-phoenix-1_controlplane   cluster-c3teodfgftd   user-c3teodfgftd
          preprod_us-phoenix-1_dataplane      cluster-c3dmzrygazg   user-c3dmzrygazg
          preprod_us-phoenix-1_deployment     cluster-c3wcmbqgi4d   user-c3wcmbqgi4d
[25:January:2022:07:37:04]:(preprod_us-phoenix-1_controlplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kuc preprod_us-phoenix-1_dataplane
Switched to context "preprod_us-phoenix-1_dataplane".
[25:January:2022:07:37:08]:(preprod_us-phoenix-1_controlplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ rp
[25:January:2022:07:37:09]:(preprod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ helm ls -n kafka
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_preprod.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_preprod.config
NAME                          NAMESPACE REVISION  UPDATED                                   STATUS    CHART                           APP VERSION
kafka-data-acs2               kafka     17        2021-08-16 22:12:21.082792707 +0000 UTC   deployed  kafka-data-0.1.0
kafka-data-admin-tools        kafka     1         2021-12-08 05:53:36.193144255 +0000 UTC   deployed  kafka-data-0.1.0
kafka-data-agora              kafka     1         2021-10-08 08:32:11.560119435 +0000 UTC   deployed  kafka-data-0.1.0
kafka-data-analytics          kafka     29        2021-12-01 23:57:54.646172739 +0000 UTC   deployed  kafka-data-0.1.0
kafka-data-bui                kafka     6         2021-08-16 22:12:21.219007735 +0000 UTC   deployed  kafka-data-0.1.0
kafka-data-cxu-integration    kafka     1         2022-01-21 21:33:31.287612484 +0000 UTC   deployed  kafka-data-0.1.0
kafka-data-data-pipeline      kafka     26        2021-12-17 12:37:09.995582644 +0000 UTC   deployed  kafka-data-0.1.0
kafka-data-db-provisioner     kafka     2         2021-02-17 07:59:26.899721559 -0700 -0700 deployed  kafka-data-0.1.0
kafka-data-exacs-provisioning kafka     1         2021-01-29 19:18:07.168631493 -0700 -0700 deployed  kafka-data-0.1.0
kafka-data-helios             kafka     28        2021-08-16 22:12:21.11540339 +0000 UTC    deployed  kafka-data-0.1.0
kafka-data-helios-psr         kafka     20        2021-12-10 18:15:11.047304824 +0000 UTC   deployed  kafka-data-0.1.0
kafka-data-kweet-dev          kafka     1         2021-10-27 14:02:32.000464262 +0000 UTC   deployed  kafka-data-0.1.0
kafka-data-mercury            kafka     19        2021-08-16 22:12:38.760306638 +0000 UTC   deployed  kafka-data-0.1.0
kafka-data-opaec              kafka     19        2021-08-16 22:12:39.396979531 +0000 UTC   deployed  kafka-data-0.1.0
kafka-data-opaec-rc           kafka     11        2021-08-16 22:12:39.576016573 +0000 UTC   deployed  kafka-data-0.1.0
kafka-data-pdb-provisioning   kafka     25        2021-08-16 22:12:39.526852754 +0000 UTC   deployed  kafka-data-0.1.0
kafka-data-sdc                kafka     18        2021-08-16 22:12:46.982135644 +0000 UTC   deployed  kafka-data-0.1.0
kafka-data-sitedashboard      kafka     18        2021-08-16 22:12:51.387462127 +0000 UTC   deployed  kafka-data-0.1.0
kafka-data-tms                kafka     22        2021-08-16 22:12:49.696842096 +0000 UTC   deployed  kafka-data-0.1.0
kafka-data-visitorservice     kafka     19        2021-08-16 22:12:50.285750736 +0000 UTC   deployed  kafka-data-0.1.0
kafka-data-visitorservice-psr kafka     1         2022-01-19 09:11:37.835698567 +0000 UTC   deployed  kafka-data-0.1.0
kafka-data-xo                 kafka     9         2021-08-16 22:12:53.028533514 +0000 UTC   deployed  kafka-data-0.1.0
kafka-schema-registry         kafka     42        2022-01-06 21:30:55.4325873 +0000 UTC     deployed  osvc-schema-registry-0.3.0      1.0
kafka-shared                  kafka     35        2021-10-08 04:45:52.798317151 +0000 UTC   deployed  cpe-kafka-0.1.0                 0.1.0
kafka-shared-mirrormaker      kafka     52        2022-01-06 21:30:22.6506845 +0000 UTC     deployed  cpe-kafka-mm2-0.1.0             0.1.0
karapace-schema-registry      kafka     1         2021-09-22 23:06:29.3644705 +0000 UTC     deployed  karapace-schema-registry-0.1.0  1.0
kc                            kafka     1         2021-10-18 17:32:31.326917016 +0000 UTC   deployed  kafka-client-0.2.0
kcpa                          kafka     2         2021-10-18 16:51:02.456754566 +0000 UTC   deployed  kafka-client-0.2.0
kctms                         kafka     1         2021-10-18 18:10:28.567012645 +0000 UTC   deployed  kafka-client-0.2.0
nginx                         kafka     41        2022-01-06 21:30:55.5686456 +0000 UTC     deployed  nginx-5.1.13                    1.17.9
nginx-karapace                kafka     1         2021-09-22 23:06:25.6612247 +0000 UTC     deployed  nginx-5.1.13                    1.17.9
samplenamespace-phx-dataplane kafka     3         2020-09-23 09:53:02.601105 -0500 -0500    deployed  oracle-namespace-0.0.9          2.8.1
strmzi                        kafka     36        2021-10-08 07:44:02.396346173 +0000 UTC   deployed  strimzi-kafka-operator-0.19.0   0.19.0
[25:January:2022:07:37:24]:(preprod_us-phoenix-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ v2corp
[25:January:2022:07:40:09]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgc
CURRENT   NAME                             CLUSTER               AUTHINFO           NAMESPACE
          corp_us-ashburn-1_controlplane   cluster-c3wgzdgmfqt   user-c3wgzdgmfqt
*         corp_us-ashburn-1_dataplane      cluster-c2tqztfmy3t   user-c2tqztfmy3t
          corp_us-ashburn-1_deployment     cluster-crdcyjvmztg   user-crdcyjvmztg
[25:January:2022:07:40:11]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kuc corp_us-ashburn-1_controlplane
error: open /home/opc/.kube/pv2_corp.config: permission denied
[25:January:2022:07:40:17]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ chmod 755 /home/opc/.kube/pv2_corp.config
[25:January:2022:07:40:29]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kuc corp_us-ashburn-1_controlplane
Switched to context "corp_us-ashburn-1_controlplane".
[25:January:2022:07:40:31]:(corp_us-ashburn-1_dataplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ rp
[25:January:2022:07:40:33]:(corp_us-ashburn-1_controlplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgn tms ingress
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME                            CLASS    HOSTS                                       ADDRESS       PORTS     AGE
tms-rest-proxy-tms-rest-proxy   <none>   tms-tms-iad.corp.channels.ocs.oc-test.com   138.3.79.14   80, 443   192d
[25:January:2022:07:40:54]:(corp_us-ashburn-1_controlplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgn traefik-frontend services
NAME                              TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)                                     AGE
ingress-traefik-internet-iad-co   LoadBalancer   10.97.101.158   138.3.75.137    9100:31298/TCP,80:30423/TCP,443:30049/TCP   34d
ingress-traefik-kafka-iad-co      LoadBalancer   10.97.28.203    10.11.10.141    9100:32197/TCP,80:32451/TCP,443:30144/TCP   34d
ingress-traefik-ocna-iad-co       LoadBalancer   10.97.239.218   144.25.97.211   9100:30233/TCP,80:32661/TCP,443:31156/TCP   34d
ingress-traefik-private-iad-co    LoadBalancer   10.97.129.182   10.11.10.149    9100:32173/TCP,80:31725/TCP,443:31581/TCP   34d
[25:January:2022:07:41:09]:(corp_us-ashburn-1_controlplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgn frontend services
NAME                                         TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)                                     AGE
ingress-internet-iad-co-controller           LoadBalancer   10.97.22.214   138.3.79.14      80:32516/TCP,443:30823/TCP                  196d
ingress-internet-iad-co-controller-metrics   ClusterIP      10.97.58.46    <none>           10254/TCP                                   196d
ingress-kafka-iad-co-controller              LoadBalancer   10.97.7.94     10.11.10.152     80:30853/TCP,443:32208/TCP,9000:31566/TCP   452d
ingress-kafka-iad-co-controller-metrics      ClusterIP      10.97.108.45   <none>           10254/TCP                                   452d
ingress-ocna-iad-co-controller               LoadBalancer   10.97.18.253   144.25.106.189   80:30461/TCP,443:30015/TCP                  452d
ingress-ocna-iad-co-controller-metrics       ClusterIP      10.97.34.168   <none>           10254/TCP                                   452d
ingress-private-iad-co-controller            LoadBalancer   10.97.79.128   10.11.10.155     80:31954/TCP,443:31260/TCP                  452d
ingress-private-iad-co-controller-metrics    ClusterIP      10.97.234.44   <none>           10254/TCP                                   452d
[25:January:2022:07:41:22]:(corp_us-ashburn-1_controlplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ kgn tms ingress tms-rest-proxy-tms-rest-proxy -oyaml
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    ingress.kubernetes.io/ssl-redirect: "true"
    ingressWatcher/external: "true"
    ingressWatcher/port: "443"
    ingressWatcher/service: tms-rest-proxy-tms-rest-proxy
    kubernetes.io/ingress.class: public
    kubernetes.io/tls-acme: "true"
    meta.helm.sh/release-name: tms-rest-proxy
    meta.helm.sh/release-namespace: tms
  creationTimestamp: "2021-07-16T10:15:01Z"
  generation: 4
  labels:
    app: tms-rest-proxy
    app.kubernetes.io/managed-by: Helm
    project: tms
    release: tms-rest-proxy
    version: "0"
  name: tms-rest-proxy-tms-rest-proxy
  namespace: tms
  resourceVersion: "228124587"
  selfLink: /apis/extensions/v1beta1/namespaces/tms/ingresses/tms-rest-proxy-tms-rest-proxy
  uid: 033bb7d7-5940-4e79-9db3-12e42b19471f
spec:
  rules:
  - host: tms-tms-iad.corp.channels.ocs.oc-test.com
    http:
      paths:
      - backend:
          serviceName: tms-rest-proxy-tms-rest-proxy
          servicePort: 8080
        path: /
        pathType: ImplementationSpecific
  tls:
  - hosts:
    - tms-tms-iad.corp.channels.ocs.oc-test.com
    secretName: star.channels.ocs.oc-test.com
status:
  loadBalancer:
    ingress:
    - ip: 138.3.79.14
[25:January:2022:07:41:33]:(corp_us-ashburn-1_controlplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ helm ls -n tms
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
NAME                    NAMESPACE REVISION  UPDATED                                 STATUS    CHART                                         APP VERSION
tms-idcs-manager        tms       2         2021-12-16 06:52:38.247049054 +0000 UTC deployed  tms-template-service-1.0.0-21-12-15-b0007     1.0.0-21-12-15-b0007
tms-rest-proxy          tms       7         2021-12-16 06:35:37.014345771 +0000 UTC deployed  tms-rest-proxy-1.0.0-21-12-16-b0003           1.0.0-21-12-16-b0003
tms-template-service    tms       2         2021-10-26 14:48:02.9510027 +0000 UTC   deployed  tms-template-service-1.0.0-21-05-10-b0001     1.0.0-21-05-10-b0001
tms-tenant-processor    tms       7         2021-12-15 12:01:08.989289025 +0000 UTC deployed  tms-tenant-processor-1.0.0-21-12-15-b0005     1.0.0-21-12-15-b0005
tms-test-meluha-service tms       4         2021-12-16 06:41:07.019563509 +0000 UTC deployed  tms-test-meluha-service-1.0.0-21-12-15-b0005  1.0.0-21-12-15-b0005
[25:January:2022:07:41:53]:(corp_us-ashburn-1_controlplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ strace
bash: strace: command not found
[26:January:2022:01:40:43]:(corp_us-ashburn-1_controlplane):~/galorndon/osvc-platform/helm/helmfile-releases
○ (rthazhek_lv14844) $ cd ~/galorndon/ctemp/
[27:January:2022:07:00:18]:(corp_us-ashburn-1_controlplane):~/galorndon/ctemp
○ $ ls -lrt
total 553080
-rw-r--r--  1 opc opc        61 Mar 25  2021 aztestns.yml
-rw-r--r--  1 opc opc        63 Mar 25  2021 azinfrans.yml
-rw-r--r--  1 opc opc      9915 Mar 26  2021 dev_fra_dep_thanos-storegateway-0.2021_03_26_11_00_AM.log
-rw-r--r--  1 opc opc       424 Apr  7  2021 test_pdb-provisioning-servicemonitor.yaml
-rw-r--r--  1 opc opc      2439 Apr  7  2021 pdb-provisioning-servicemonitor.sm2.yaml
-rw-r--r--  1 opc opc       650 Apr  7  2021 test_pdb-provisioning-servicemonitor2.yaml
-rw-r--r--  1 opc opc      8031 Apr 20  2021 shared-kafka-kafka-1.desc3
-rw-r--r--  1 opc opc    126850 Apr 22  2021 dev_eu-frankfurt-1_deployment_helmfile-infra-monitoring.diff
-rw-r--r--  1 opc opc    254034 May  3  2021 crd-alertmanager.yaml
-rw-r--r--  1 opc opc    126411 May  4  2021 alertmanagerconfigs.monitoring.coreos.com.crd_04may2021.desc
-rw-r--r--  1 opc opc      3801 May 15  2021 wp-config.php
-rw-r--r--  1 opc opc      4016 May 15  2021 wp-config (1).php
-rw-r--r--  1 opc opc       522 May 15  2021 htaccess_tlk
-rw-r--r--  1 opc opc      3763 May 15  2021 wp-config_tlk.php
-rw-r--r--  1 opc opc      2635 May 18  2021 csi-e71f6a5c-0a13-475d-9af1-4daaca78e591.pv.yaml
-rw-r--r--  1 opc opc      2635 May 18  2021 csi-54db31eb-5bea-411b-a4a3-06e112b9c844.pv.yaml
-rw-r--r--  1 opc opc      2635 May 18  2021 csi-0ad02183-2779-42c3-815e-ef1e746951ea.pv.yaml
-rw-r--r--  1 opc opc       970 May 18  2021 csi-e71f6a5c-0a13-475d-9af1-4daaca78e591-restored-210518.pv.yaml
-rw-r--r--  1 opc opc       970 May 18  2021 csi-54db31eb-5bea-411b-a4a3-06e112b9c844-restored-210518.pv.yaml
-rw-r--r--  1 opc opc       970 May 18  2021 csi-0ad02183-2779-42c3-815e-ef1e746951ea-restored-210518.pv.yaml
-rw-r--r--  1 opc opc      2635 May 18  2021 csi-f8e97b7d-d149-4b88-be77-f68b63e60eff.ams.pv.yaml
-rw-r--r--  1 opc opc      2635 May 18  2021 csi-20757826-0bf6-4f6f-bef8-a4da04dbe59f.ams.pv.yaml
-rw-r--r--  1 opc opc      2635 May 18  2021 csi-c8958e00-a3b6-44ba-bc1f-211db0f7092b.ams.pv.yaml
-rw-r--r--  1 opc opc       970 May 18  2021 csi-f8e97b7d-d149-4b88-be77-f68b63e60eff-restored-210518.ams.pv.yaml
-rw-r--r--  1 opc opc       970 May 18  2021 csi-20757826-0bf6-4f6f-bef8-a4da04dbe59f-restored-210518.ams.pv.yaml
-rw-r--r--  1 opc opc       970 May 18  2021 csi-c8958e00-a3b6-44ba-bc1f-211db0f7092b-restored-210518.ams.pv.yaml
-rw-r--r--  1 opc opc     12314 May 18  2021 shared-kafka-zookeeper.ss.prod_eu-amsterdam-1_dataplane.yaml
-rw-r--r--  1 opc opc      3947 May 18  2021 shared-kafka-zookeeper.ss.prod_eu-amsterdam-1_dataplane.desc
-rw-r--r--  1 opc opc     12314 May 18  2021 shared-kafka-zookeeper.ss.prod_eu-frankfurt-1_dataplane.yaml
-rw-r--r--  1 opc opc      3947 May 18  2021 shared-kafka-zookeeper.ss.prod_eu-frankfurt-1_dataplane.desc
drwxr-xr-x  5 opc opc       160 May 18  2021 prodfra
drwxr-xr-x  5 opc opc       160 May 18  2021 prodams
drwxr-xr-x  6 opc opc       192 May 18  2021 prodcardiff
drwxr-xr-x 11 opc opc       352 May 24  2021 schemakafka_mel
-rw-r--r--  1 opc opc   3298967 May 26  2021 JQ_TESTDATA_corp_ociinventory.list
-rw-r--r--  1 opc opc      3036 May 29  2021 corpiad_datapipeline.consumerconfig
-rw-r--r--  1 opc opc      2976 May 29  2021 prodphx_datapipeline.consumerconfig
-rw-r--r--  1 opc opc      3052 May 29  2021 prodphx_kafkaconnect.consumerconfig
-rw-r--r--  1 opc opc      2987 May 29  2021 preprodphx_datapipeline.consumerconfig
-rw-r--r--  1 opc opc     10372 May 31  2021 datapipeline-provisioner.deployments.phx.yaml
-rw-r--r--  1 opc opc     27739 Jun  2  2021 kafka.object_dev_eu-frankfurt-1_dataplane.yaml
-rw-r--r--  1 opc opc    564575 Jun  2  2021 10.245.0.156.metrics_r2
-rw-r--r--  1 opc opc       954 Jun  8  2021 tmp-cdaas_cpe-cronjob.json
drwxr-xr-x  3 opc opc        96 Jun  8  2021 Archive
-rw-r--r--  1 opc opc    148420 Jun  8  2021 tmp-cdaas_repo.list
-rw-r--r--  1 opc opc      3947 Jun  9  2021 pvc-watcher-sa-token-fb6b4.corp.secrets.yaml
-rw-r--r--  1 opc opc      1449 Jun  9  2021 pvc-watcher-sa.corp.serviceaccount.yaml
-rw-r--r--  1 opc opc     54598 Jun 15  2021 yum_galorndon-infra.list
-rw-r--r--  1 opc opc       548 Jun 15  2021 helm_presync_hook.sh
-rw-r--r--  1 opc opc       398 Jun 16  2021 test_pod.yaml
-rw-r--r--  1 opc opc       563 Jun 17  2021 test_pod_while.yaml
drwxr-xr-x 34 opc opc      1088 Jul  1  2021 helios-qa_logs_2107011019
drwxr-xr-x 34 opc opc      1088 Jul  2  2021 helios-qa_logs_2107020338
drwxr-xr-x  2 opc opc        64 Jul  2  2021 helios_logs_2107020922
drwxr-xr-x  3 opc opc        96 Jul  2  2021 dead-letter-processor-798945c6f-kq4c5_logs_2107020925
drwxr-xr-x  8 opc opc       256 Jul  2  2021 PROD_V1_HELIOS
drwxr-xr-x 24 opc opc       768 Jul  2  2021 helios_logs_2107021018
drwxr-xr-x 21 opc opc       672 Jul  5  2021 prodphx_helios
drwxr-xr-x 18 opc opc       576 Jul  5  2021 helios_logs_2107051234
drwxr-xr-x  4 opc opc       128 Aug 16 05:26 authentication-5859f57c86-bb4tt_logs_2108160525
drwxr-xr-x  4 opc opc       128 Aug 16 05:27 authentication-5859f57c86-rfg8p_logs_2108160526
drwxr-xr-x  4 opc opc       128 Aug 16 05:46 sender-85d8dd4996-gnh8l_logs_2108160546
drwxr-xr-x  4 opc opc       128 Aug 16 07:27 authentication-f74d65c6c-b78t8_logs_2108160726
drwxr-xr-x  4 opc opc       128 Aug 16 07:29 authentication-f74d65c6c-wfrsf_logs_2108160728
drwxr-xr-x  4 opc opc       128 Aug 16 07:30 authentication-f74d65c6c-xz4vk_logs_2108160729
drwxr-xr-x  4 opc opc       128 Aug 16 07:42 authentication-f74d65c6c-778ws_logs_2108160742
-rwxr-xr-x  1 opc opc     10896 Aug 19 06:50 setup_kubeconfig.sh
-rw-r--r--  1 opc opc   6600342 Sep 14 07:13 prod_eu_amsterdam_1_dataplane_kafkatopics
-rw-r--r--  1 opc opc   4032978 Sep 14 10:28 prod_eu_amsterdam_1_dataplane_kafkatopics_0914
-rw-r--r--  1 opc opc   3566446 Sep 15 05:25 prod_ca_montreal_1_dataplane_kafkatopics_0915
-rw-r--r--  1 opc opc   3959055 Sep 15 05:29 prod_ca_toronto_1_dataplane_kafkatopics_0915
-rw-r--r--  1 opc opc   3742022 Sep 15 05:32 prod_eu_amsterdam_1_dataplane_kafkatopics_0915
drwxr-xr-x  5 opc opc       160 Sep 20 06:51 oci_prod_config
-rw-r--r--  1 opc opc     48906 Sep 23 06:44 pv2_prod_23Sept2021.config
-rw-r--r--  1 opc opc      6699 Sep 23 09:45 strimzi-cluster-operator_prodfra.deployment.yaml
-rw-r--r--  1 opc opc   4956070 Nov 18 12:55 traefik-kafka-controller-58458f4cf-gh2rb2021_11_18_01_01_AM.log
-rw-r--r--  1 opc opc      3178 Nov 23 03:18 traefik-telemetry-alert-rules.yaml
-rw-r--r--  1 opc opc      2629 Nov 23 03:22 traefik-telemetry-alert-routes.yaml
-rw-r--r--  1 opc opc     39028 Nov 24 00:47 traefik-dashboard.yaml
drwxr-xr-x  4 opc opc       128 Nov 24 14:05 traefik_code
-rw-------  1 opc opc     76737 Nov 24 22:21 kube_config_all.prod
-rw-r--r--  1 opc opc    336833 Nov 30 01:30 shared-schemaregistry-546cb97944-4f4zm2021_11_24_01_48_PM.log
-rw-r--r--  1 opc opc    344596 Nov 30 03:34 shared-schemaregistry-546cb97944-wn5lz_1_2021_11_24_01_48_PM.log
-rw-r--r--  1 opc opc     68551 Nov 30 03:37 kafkaconnect-cluster-connect-5b4bf448ff-95rw22021_11_24_01_48_PM.log
drwxr-xr-x 14 opc opc       448 Nov 30 22:35 traefik-concept
-rw-r--r--  1 opc opc       487 Dec  1 22:34 schemaregistry_ingressroutetcp.yaml
-rw-r--r--  1 opc opc       485 Dec  1 23:08 shared-kafka-broker-0-kafka_ingressroutetcp.yaml
-rw-r--r--  1 opc opc       518 Dec  2 02:36 karapace-schemaregistry-proxy-kafka.service_ingressroutetcp.yaml
-rw-r--r--  1 opc opc       523 Dec  2 02:42 karapace-shared-schemaregistry.service_ingressroutetcp.yaml
-rw-r--r--  1 opc opc    685314 Dec  2 22:10 shared-kafka-kafka-02021_11_29_09_00_PM.log
-rw-r--r--  1 opc opc   1216506 Dec  2 22:10 shared-kafka-kafka-22021_11_29_09_00_PM.log
-rw-r--r--  1 opc opc   6038207 Dec  2 22:10 shared-kafka-kafka-12021_11_29_09_00_PM.log
-rw-r--r--  1 opc opc   7966451 Dec  2 22:28 strimzi-cluster-operator-7547df86cc-htnh7_2021_12_02_10_27_PM.log
-rw-r--r--  1 opc opc  15304668 Dec  2 22:41 strimzi-cluster-operator-b8bd4ff6d-m5wdc_2021_12_02_10_40_PM.log
-rw-r--r--  1 opc opc     23460 Dec  7 17:11 dev_lhr_kafka_ingress.yaml
-rw-------  1 opc opc     95279 Jan  3 15:16 config.prod
-rw-r--r--  1 opc opc      8146 Jan  5 16:10 strimzi-kafka-cluster.yaml
-r--------  1 opc opc      2358 Jan 10 05:02 kubeconfig_context-c5ppxaonnda
-rw-r--r--  1 opc opc         0 Jan 11 09:02 nginxdeployment.yaml
-rw-r--r--  1 opc opc       198 Jan 11 09:06 busybox_deployment.yaml
-rw-r--r--  1 opc opc      8138 Jan 11 09:14 kafkageorgecluster_kind.yaml
-rw-r--r--  1 opc opc      1434 Jan 11 09:25 kafkak8scluster_kind.yaml
-rw-r--r--  1 opc opc       986 Jan 11 09:30 test_kafkak8scluster_kind.yaml
-rw-r--r--  1 opc opc         0 Jan 11 11:20 kf-kafka-entity-operator-8585d869c4-b4995.log
-rw-r--r--  1 opc opc       189 Jan 11 11:20 testtopic.yaml
-rw-r--r--  1 opc opc    207526 Jan 11 11:21 kf-kafka-entity-operator-8585d869c4-b4995_topicoperator.log
drwxr-xr-x  6 opc opc       192 Jan 17 08:03 IngressTraefikL
-rw-r--r--  1 opc opc      5518 Jan 17 23:53 kind-testk8s-kubeconfig
-rw-r--r--  1 opc opc    507680 Jan 18 05:58 console4.log
-rw-r--r--  1 opc opc    474101 Jan 18 05:58 console3.log
-rw-r--r--  1 opc opc    507680 Jan 18 05:59 console2.log
-rw-r--r--  1 opc opc    401069 Jan 18 05:59 console1.log
drwxr-xr-x 22 opc opc       704 Jan 18 07:22 devlhrtraefik
-rwxr-xr-x  1 opc opc 492170292 Jan 20 09:51 sysint-fluentbit.log
-rw-r--r--  1 opc opc         0 Jan 21 07:00 data-processor-85dbcc656c-pxtlm2022_01_21_06_55_AM.log
-rw-r--r--  1 opc opc    724405 Jan 21 07:01 data-processor-85dbcc656c-pxtlm_app2022_01_21_06_55_AM.log
-rw-r--r--  1 opc opc   1991822 Jan 21 10:33 PHX-helmfile-mercury-psr-apps-helmfile.template
-rw-r--r--  1 opc opc    137234 Jan 21 10:59 PHX-mercury-tenant-downtime-monitor-b58fc56f8-tbdvj_tenant-downtime-monitor.log
-rw-r--r--  1 opc opc    372802 Jan 24 07:20 kafkatopic.object
-rw-r--r--  1 opc opc      8569 Jan 25 04:48 preprod_infra-monitoring-ingress.yaml
-rw-r--r--  1 opc opc      5517 Jan 25 16:20 KEVIN_kf-kafka-client.pod.yaml
-rw-r--r--  1 opc opc      5625 Jan 25 16:21 AZHER_azdevlhrclient-kafka-client.pod.yaml
drwxr-xr-x 46 opc opc      1472 Jan 27 01:53 traefik-test
[27:January:2022:07:00:20]:(corp_us-ashburn-1_controlplane):~/galorndon/ctemp
○ $ ls -ltr
total 553080
-rw-r--r--  1 opc opc        61 Mar 25  2021 aztestns.yml
-rw-r--r--  1 opc opc        63 Mar 25  2021 azinfrans.yml
-rw-r--r--  1 opc opc      9915 Mar 26  2021 dev_fra_dep_thanos-storegateway-0.2021_03_26_11_00_AM.log
-rw-r--r--  1 opc opc       424 Apr  7  2021 test_pdb-provisioning-servicemonitor.yaml
-rw-r--r--  1 opc opc      2439 Apr  7  2021 pdb-provisioning-servicemonitor.sm2.yaml
-rw-r--r--  1 opc opc       650 Apr  7  2021 test_pdb-provisioning-servicemonitor2.yaml
-rw-r--r--  1 opc opc      8031 Apr 20  2021 shared-kafka-kafka-1.desc3
-rw-r--r--  1 opc opc    126850 Apr 22  2021 dev_eu-frankfurt-1_deployment_helmfile-infra-monitoring.diff
-rw-r--r--  1 opc opc    254034 May  3  2021 crd-alertmanager.yaml
-rw-r--r--  1 opc opc    126411 May  4  2021 alertmanagerconfigs.monitoring.coreos.com.crd_04may2021.desc
-rw-r--r--  1 opc opc      3801 May 15  2021 wp-config.php
-rw-r--r--  1 opc opc      4016 May 15  2021 wp-config (1).php
-rw-r--r--  1 opc opc       522 May 15  2021 htaccess_tlk
-rw-r--r--  1 opc opc      3763 May 15  2021 wp-config_tlk.php
-rw-r--r--  1 opc opc      2635 May 18  2021 csi-e71f6a5c-0a13-475d-9af1-4daaca78e591.pv.yaml
-rw-r--r--  1 opc opc      2635 May 18  2021 csi-54db31eb-5bea-411b-a4a3-06e112b9c844.pv.yaml
-rw-r--r--  1 opc opc      2635 May 18  2021 csi-0ad02183-2779-42c3-815e-ef1e746951ea.pv.yaml
-rw-r--r--  1 opc opc       970 May 18  2021 csi-e71f6a5c-0a13-475d-9af1-4daaca78e591-restored-210518.pv.yaml
-rw-r--r--  1 opc opc       970 May 18  2021 csi-54db31eb-5bea-411b-a4a3-06e112b9c844-restored-210518.pv.yaml
-rw-r--r--  1 opc opc       970 May 18  2021 csi-0ad02183-2779-42c3-815e-ef1e746951ea-restored-210518.pv.yaml
-rw-r--r--  1 opc opc      2635 May 18  2021 csi-f8e97b7d-d149-4b88-be77-f68b63e60eff.ams.pv.yaml
-rw-r--r--  1 opc opc      2635 May 18  2021 csi-20757826-0bf6-4f6f-bef8-a4da04dbe59f.ams.pv.yaml
-rw-r--r--  1 opc opc      2635 May 18  2021 csi-c8958e00-a3b6-44ba-bc1f-211db0f7092b.ams.pv.yaml
-rw-r--r--  1 opc opc       970 May 18  2021 csi-f8e97b7d-d149-4b88-be77-f68b63e60eff-restored-210518.ams.pv.yaml
-rw-r--r--  1 opc opc       970 May 18  2021 csi-20757826-0bf6-4f6f-bef8-a4da04dbe59f-restored-210518.ams.pv.yaml
-rw-r--r--  1 opc opc       970 May 18  2021 csi-c8958e00-a3b6-44ba-bc1f-211db0f7092b-restored-210518.ams.pv.yaml
-rw-r--r--  1 opc opc     12314 May 18  2021 shared-kafka-zookeeper.ss.prod_eu-amsterdam-1_dataplane.yaml
-rw-r--r--  1 opc opc      3947 May 18  2021 shared-kafka-zookeeper.ss.prod_eu-amsterdam-1_dataplane.desc
-rw-r--r--  1 opc opc     12314 May 18  2021 shared-kafka-zookeeper.ss.prod_eu-frankfurt-1_dataplane.yaml
-rw-r--r--  1 opc opc      3947 May 18  2021 shared-kafka-zookeeper.ss.prod_eu-frankfurt-1_dataplane.desc
drwxr-xr-x  5 opc opc       160 May 18  2021 prodfra
drwxr-xr-x  5 opc opc       160 May 18  2021 prodams
drwxr-xr-x  6 opc opc       192 May 18  2021 prodcardiff
drwxr-xr-x 11 opc opc       352 May 24  2021 schemakafka_mel
-rw-r--r--  1 opc opc   3298967 May 26  2021 JQ_TESTDATA_corp_ociinventory.list
-rw-r--r--  1 opc opc      3036 May 29  2021 corpiad_datapipeline.consumerconfig
-rw-r--r--  1 opc opc      2976 May 29  2021 prodphx_datapipeline.consumerconfig
-rw-r--r--  1 opc opc      3052 May 29  2021 prodphx_kafkaconnect.consumerconfig
-rw-r--r--  1 opc opc      2987 May 29  2021 preprodphx_datapipeline.consumerconfig
-rw-r--r--  1 opc opc     10372 May 31  2021 datapipeline-provisioner.deployments.phx.yaml
-rw-r--r--  1 opc opc     27739 Jun  2  2021 kafka.object_dev_eu-frankfurt-1_dataplane.yaml
-rw-r--r--  1 opc opc    564575 Jun  2  2021 10.245.0.156.metrics_r2
-rw-r--r--  1 opc opc       954 Jun  8  2021 tmp-cdaas_cpe-cronjob.json
drwxr-xr-x  3 opc opc        96 Jun  8  2021 Archive
-rw-r--r--  1 opc opc    148420 Jun  8  2021 tmp-cdaas_repo.list
-rw-r--r--  1 opc opc      3947 Jun  9  2021 pvc-watcher-sa-token-fb6b4.corp.secrets.yaml
-rw-r--r--  1 opc opc      1449 Jun  9  2021 pvc-watcher-sa.corp.serviceaccount.yaml
-rw-r--r--  1 opc opc     54598 Jun 15  2021 yum_galorndon-infra.list
-rw-r--r--  1 opc opc       548 Jun 15  2021 helm_presync_hook.sh
-rw-r--r--  1 opc opc       398 Jun 16  2021 test_pod.yaml
-rw-r--r--  1 opc opc       563 Jun 17  2021 test_pod_while.yaml
drwxr-xr-x 34 opc opc      1088 Jul  1  2021 helios-qa_logs_2107011019
drwxr-xr-x 34 opc opc      1088 Jul  2  2021 helios-qa_logs_2107020338
drwxr-xr-x  2 opc opc        64 Jul  2  2021 helios_logs_2107020922
drwxr-xr-x  3 opc opc        96 Jul  2  2021 dead-letter-processor-798945c6f-kq4c5_logs_2107020925
drwxr-xr-x  8 opc opc       256 Jul  2  2021 PROD_V1_HELIOS
drwxr-xr-x 24 opc opc       768 Jul  2  2021 helios_logs_2107021018
drwxr-xr-x 21 opc opc       672 Jul  5  2021 prodphx_helios
drwxr-xr-x 18 opc opc       576 Jul  5  2021 helios_logs_2107051234
drwxr-xr-x  4 opc opc       128 Aug 16 05:26 authentication-5859f57c86-bb4tt_logs_2108160525
drwxr-xr-x  4 opc opc       128 Aug 16 05:27 authentication-5859f57c86-rfg8p_logs_2108160526
drwxr-xr-x  4 opc opc       128 Aug 16 05:46 sender-85d8dd4996-gnh8l_logs_2108160546
drwxr-xr-x  4 opc opc       128 Aug 16 07:27 authentication-f74d65c6c-b78t8_logs_2108160726
drwxr-xr-x  4 opc opc       128 Aug 16 07:29 authentication-f74d65c6c-wfrsf_logs_2108160728
drwxr-xr-x  4 opc opc       128 Aug 16 07:30 authentication-f74d65c6c-xz4vk_logs_2108160729
drwxr-xr-x  4 opc opc       128 Aug 16 07:42 authentication-f74d65c6c-778ws_logs_2108160742
-rwxr-xr-x  1 opc opc     10896 Aug 19 06:50 setup_kubeconfig.sh
-rw-r--r--  1 opc opc   6600342 Sep 14 07:13 prod_eu_amsterdam_1_dataplane_kafkatopics
-rw-r--r--  1 opc opc   4032978 Sep 14 10:28 prod_eu_amsterdam_1_dataplane_kafkatopics_0914
-rw-r--r--  1 opc opc   3566446 Sep 15 05:25 prod_ca_montreal_1_dataplane_kafkatopics_0915
-rw-r--r--  1 opc opc   3959055 Sep 15 05:29 prod_ca_toronto_1_dataplane_kafkatopics_0915
-rw-r--r--  1 opc opc   3742022 Sep 15 05:32 prod_eu_amsterdam_1_dataplane_kafkatopics_0915
drwxr-xr-x  5 opc opc       160 Sep 20 06:51 oci_prod_config
-rw-r--r--  1 opc opc     48906 Sep 23 06:44 pv2_prod_23Sept2021.config
-rw-r--r--  1 opc opc      6699 Sep 23 09:45 strimzi-cluster-operator_prodfra.deployment.yaml
-rw-r--r--  1 opc opc   4956070 Nov 18 12:55 traefik-kafka-controller-58458f4cf-gh2rb2021_11_18_01_01_AM.log
-rw-r--r--  1 opc opc      3178 Nov 23 03:18 traefik-telemetry-alert-rules.yaml
-rw-r--r--  1 opc opc      2629 Nov 23 03:22 traefik-telemetry-alert-routes.yaml
-rw-r--r--  1 opc opc     39028 Nov 24 00:47 traefik-dashboard.yaml
drwxr-xr-x  4 opc opc       128 Nov 24 14:05 traefik_code
-rw-------  1 opc opc     76737 Nov 24 22:21 kube_config_all.prod
-rw-r--r--  1 opc opc    336833 Nov 30 01:30 shared-schemaregistry-546cb97944-4f4zm2021_11_24_01_48_PM.log
-rw-r--r--  1 opc opc    344596 Nov 30 03:34 shared-schemaregistry-546cb97944-wn5lz_1_2021_11_24_01_48_PM.log
-rw-r--r--  1 opc opc     68551 Nov 30 03:37 kafkaconnect-cluster-connect-5b4bf448ff-95rw22021_11_24_01_48_PM.log
drwxr-xr-x 14 opc opc       448 Nov 30 22:35 traefik-concept
-rw-r--r--  1 opc opc       487 Dec  1 22:34 schemaregistry_ingressroutetcp.yaml
-rw-r--r--  1 opc opc       485 Dec  1 23:08 shared-kafka-broker-0-kafka_ingressroutetcp.yaml
-rw-r--r--  1 opc opc       518 Dec  2 02:36 karapace-schemaregistry-proxy-kafka.service_ingressroutetcp.yaml
-rw-r--r--  1 opc opc       523 Dec  2 02:42 karapace-shared-schemaregistry.service_ingressroutetcp.yaml
-rw-r--r--  1 opc opc    685314 Dec  2 22:10 shared-kafka-kafka-02021_11_29_09_00_PM.log
-rw-r--r--  1 opc opc   1216506 Dec  2 22:10 shared-kafka-kafka-22021_11_29_09_00_PM.log
-rw-r--r--  1 opc opc   6038207 Dec  2 22:10 shared-kafka-kafka-12021_11_29_09_00_PM.log
-rw-r--r--  1 opc opc   7966451 Dec  2 22:28 strimzi-cluster-operator-7547df86cc-htnh7_2021_12_02_10_27_PM.log
-rw-r--r--  1 opc opc  15304668 Dec  2 22:41 strimzi-cluster-operator-b8bd4ff6d-m5wdc_2021_12_02_10_40_PM.log
-rw-r--r--  1 opc opc     23460 Dec  7 17:11 dev_lhr_kafka_ingress.yaml
-rw-------  1 opc opc     95279 Jan  3 15:16 config.prod
-rw-r--r--  1 opc opc      8146 Jan  5 16:10 strimzi-kafka-cluster.yaml
-r--------  1 opc opc      2358 Jan 10 05:02 kubeconfig_context-c5ppxaonnda
-rw-r--r--  1 opc opc         0 Jan 11 09:02 nginxdeployment.yaml
-rw-r--r--  1 opc opc       198 Jan 11 09:06 busybox_deployment.yaml
-rw-r--r--  1 opc opc      8138 Jan 11 09:14 kafkageorgecluster_kind.yaml
-rw-r--r--  1 opc opc      1434 Jan 11 09:25 kafkak8scluster_kind.yaml
-rw-r--r--  1 opc opc       986 Jan 11 09:30 test_kafkak8scluster_kind.yaml
-rw-r--r--  1 opc opc         0 Jan 11 11:20 kf-kafka-entity-operator-8585d869c4-b4995.log
-rw-r--r--  1 opc opc       189 Jan 11 11:20 testtopic.yaml
-rw-r--r--  1 opc opc    207526 Jan 11 11:21 kf-kafka-entity-operator-8585d869c4-b4995_topicoperator.log
drwxr-xr-x  6 opc opc       192 Jan 17 08:03 IngressTraefikL
-rw-r--r--  1 opc opc      5518 Jan 17 23:53 kind-testk8s-kubeconfig
-rw-r--r--  1 opc opc    507680 Jan 18 05:58 console4.log
-rw-r--r--  1 opc opc    474101 Jan 18 05:58 console3.log
-rw-r--r--  1 opc opc    507680 Jan 18 05:59 console2.log
-rw-r--r--  1 opc opc    401069 Jan 18 05:59 console1.log
drwxr-xr-x 22 opc opc       704 Jan 18 07:22 devlhrtraefik
-rwxr-xr-x  1 opc opc 492170292 Jan 20 09:51 sysint-fluentbit.log
-rw-r--r--  1 opc opc         0 Jan 21 07:00 data-processor-85dbcc656c-pxtlm2022_01_21_06_55_AM.log
-rw-r--r--  1 opc opc    724405 Jan 21 07:01 data-processor-85dbcc656c-pxtlm_app2022_01_21_06_55_AM.log
-rw-r--r--  1 opc opc   1991822 Jan 21 10:33 PHX-helmfile-mercury-psr-apps-helmfile.template
-rw-r--r--  1 opc opc    137234 Jan 21 10:59 PHX-mercury-tenant-downtime-monitor-b58fc56f8-tbdvj_tenant-downtime-monitor.log
-rw-r--r--  1 opc opc    372802 Jan 24 07:20 kafkatopic.object
-rw-r--r--  1 opc opc      8569 Jan 25 04:48 preprod_infra-monitoring-ingress.yaml
-rw-r--r--  1 opc opc      5517 Jan 25 16:20 KEVIN_kf-kafka-client.pod.yaml
-rw-r--r--  1 opc opc      5625 Jan 25 16:21 AZHER_azdevlhrclient-kafka-client.pod.yaml
drwxr-xr-x 46 opc opc      1472 Jan 27 01:53 traefik-test
[27:January:2022:07:00:24]:(corp_us-ashburn-1_controlplane):~/galorndon/ctemp
○ $ ls -ltr IngressTraefikL
total 20
-rw-r--r-- 1 opc opc  606 Jan 13 16:22 nginx_ss.yaml
-rw------- 1 opc opc 5518 Jan 13 16:35 kind-testk8s_kubeconfig
-rw-r--r-- 1 opc opc  644 Jan 14 07:30 bear_deployment.yaml
-rw-r--r-- 1 opc opc  493 Jan 17 08:06 bear_ingress.yaml
[27:January:2022:07:00:46]:(corp_us-ashburn-1_controlplane):~/galorndon/ctemp
○ $ ls -ltr traefik-test
total 34412
-r-------- 1 opc opc     2358 Jan 10 05:02 kubeconfig_context-c5ppxaonnda
-rw-r--r-- 1 opc opc     4455 Jan 13 00:06 traefik-6ff7ff8ffd-cxgdn2022_01_12_11_33_PM.log
-rw-r--r-- 1 opc opc     1006 Jan 13 00:11 traefik.service
-rw-r--r-- 1 opc opc     7670 Jan 13 00:13 kf-kafka.kafka.yaml
-rw-r--r-- 1 opc opc    12389 Jan 13 00:15 corp.shared-kafka.kafka.yaml
-rw-r--r-- 1 opc opc      420 Jan 13 04:44 kf-kafka-kafka-0-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc      452 Jan 13 04:45 kf-kafka-kafka-bootstrap-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc      420 Jan 13 04:45 kf-kafka-kafka-1-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc      420 Jan 13 04:45 kf-kafka-kafka-2-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc     3186 Jan 13 05:05 kf-kafka.kafka_updated.yaml
-rw-r--r-- 1 opc opc     1358 Jan 13 08:12 kf-kafka-kafka-tlsexternal-1.ingress.yaml
-rw-r--r-- 1 opc opc     1331 Jan 13 08:24 kf-kafka-kafka-tlsexternal-0.ingress.yaml
-rw-r--r-- 1 opc opc     3782 Jan 13 08:34 kf-kafka-kafka_updated2.yaml
-rw-r--r-- 1 opc opc     2197 Jan 13 11:55 ca.crt
-rw-r--r-- 1 opc opc     2897 Jan 13 13:31 traefik-test_values.yaml
-rw-r--r-- 1 opc opc     1334 Jan 13 14:26 kf-kafka-kafka-0.ingress.1.yaml
-rw-r--r-- 1 opc opc     1334 Jan 13 14:26 kf-kafka-kafka-1.ingress.1.yaml
-rw-r--r-- 1 opc opc     1334 Jan 13 14:26 kf-kafka-kafka-2.ingress.1.yaml
-rw-r--r-- 1 opc opc     1383 Jan 13 14:26 kf-kafka-kafka-bootstrap.ingress.1.yaml
-rw-r--r-- 1 opc opc     3800 Jan 14 04:15 kf-kafka.cluster.kind.14jan22.yaml
-rw-r--r-- 1 opc opc     8904 Jan 14 04:42 kf-kafka.kafka.WORKING.14Jan22.yaml
-rw-r--r-- 1 opc opc     1379 Jan 14 04:53 kf-kafka-kafka-0.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     1379 Jan 14 04:53 kf-kafka-kafka-1.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     1379 Jan 14 04:53 kf-kafka-kafka-2.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     1428 Jan 14 04:53 kf-kafka-kafka-bootstrap.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     8904 Jan 17 04:23 kf-kafka.kafka.WORKING.17Jan22.yaml
-rw-r--r-- 1 opc opc  8656381 Jan 17 11:10 traefik-kafka-controller-5f9f755d5-j6nzw2022_01_12_11_33_PM.log
-rw-r--r-- 1 opc opc      238 Jan 17 16:55 traefik-web-ui_svc.yaml
-rw-r--r-- 1 opc opc      269 Jan 17 16:57 traefik-web-ui_ingress.yaml
-rw------- 1 opc opc     5884 Jan 18 00:08 kubeconfig_minikube_18jan2022
-rw-r--r-- 1 opc opc   211050 Jan 18 01:18 Traefik_dashboard.png
-rw-r--r-- 1 opc opc   106471 Jan 18 01:18 Traefik_serviceAdminPort.png
drwxr-xr-x 9 opc opc      288 Jan 18 01:20 traefik-setup
-rw-r--r-- 1 opc opc     8684 Jan 19 07:15 kf-kafka.kafka.19jan2022.yaml
-rw-r--r-- 1 opc opc    12498 Jan 19 07:16 kgn.kafka.services.19jan2022.yaml
-rw-r--r-- 1 opc opc     4534 Jan 19 07:17 kgn.kafka.ingressroutetcp.19jan2022.yaml
-rw-r--r-- 1 opc opc     5857 Jan 19 07:19 kgn.kafka.ingress.19jan2022.yaml
-rw-r--r-- 1 opc opc 14267697 Jan 20 07:26 traefik-kafka-controller.log
-rw-r--r-- 1 opc opc        0 Jan 20 07:27 kf-kafka-entity-operator-6dfd8c8649-95dc72022_01_19_07_25_AM.log
-rw-r--r-- 1 opc opc  5581954 Jan 20 07:29 kf-kafka-entity-operator-6dfd8c8649-95dc7_topicoperator2022_01_19_07_25_AM.log
-rw-r--r-- 1 opc opc    67865 Jan 20 07:31 strimzi-cluster-operator-6fb59d59cb-zdd6k2022_01_19_07_25_AM.log
-rw-r--r-- 1 opc opc  3450078 Jan 27 01:08 ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log
-rw-r--r-- 1 opc opc     9746 Jan 27 01:49 kafka.shared-kafka-dr.kafka-dev-lhr.yaml
-rw-r--r-- 1 opc opc     9229 Jan 27 01:53 shared-kafka-dr.kafka-dev-lhr-Updated.yaml
[27:January:2022:07:01:23]:(corp_us-ashburn-1_controlplane):~/galorndon/ctemp
○ $ v2dev
[27:January:2022:07:13:22]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp
○ $ ls -ltr
total 553080
-rw-r--r--  1 opc opc        61 Mar 25  2021 aztestns.yml
-rw-r--r--  1 opc opc        63 Mar 25  2021 azinfrans.yml
-rw-r--r--  1 opc opc      9915 Mar 26  2021 dev_fra_dep_thanos-storegateway-0.2021_03_26_11_00_AM.log
-rw-r--r--  1 opc opc       424 Apr  7  2021 test_pdb-provisioning-servicemonitor.yaml
-rw-r--r--  1 opc opc      2439 Apr  7  2021 pdb-provisioning-servicemonitor.sm2.yaml
-rw-r--r--  1 opc opc       650 Apr  7  2021 test_pdb-provisioning-servicemonitor2.yaml
-rw-r--r--  1 opc opc      8031 Apr 20  2021 shared-kafka-kafka-1.desc3
-rw-r--r--  1 opc opc    126850 Apr 22  2021 dev_eu-frankfurt-1_deployment_helmfile-infra-monitoring.diff
-rw-r--r--  1 opc opc    254034 May  3  2021 crd-alertmanager.yaml
-rw-r--r--  1 opc opc    126411 May  4  2021 alertmanagerconfigs.monitoring.coreos.com.crd_04may2021.desc
-rw-r--r--  1 opc opc      3801 May 15  2021 wp-config.php
-rw-r--r--  1 opc opc      4016 May 15  2021 wp-config (1).php
-rw-r--r--  1 opc opc       522 May 15  2021 htaccess_tlk
-rw-r--r--  1 opc opc      3763 May 15  2021 wp-config_tlk.php
-rw-r--r--  1 opc opc      2635 May 18  2021 csi-e71f6a5c-0a13-475d-9af1-4daaca78e591.pv.yaml
-rw-r--r--  1 opc opc      2635 May 18  2021 csi-54db31eb-5bea-411b-a4a3-06e112b9c844.pv.yaml
-rw-r--r--  1 opc opc      2635 May 18  2021 csi-0ad02183-2779-42c3-815e-ef1e746951ea.pv.yaml
-rw-r--r--  1 opc opc       970 May 18  2021 csi-e71f6a5c-0a13-475d-9af1-4daaca78e591-restored-210518.pv.yaml
-rw-r--r--  1 opc opc       970 May 18  2021 csi-54db31eb-5bea-411b-a4a3-06e112b9c844-restored-210518.pv.yaml
-rw-r--r--  1 opc opc       970 May 18  2021 csi-0ad02183-2779-42c3-815e-ef1e746951ea-restored-210518.pv.yaml
-rw-r--r--  1 opc opc      2635 May 18  2021 csi-f8e97b7d-d149-4b88-be77-f68b63e60eff.ams.pv.yaml
-rw-r--r--  1 opc opc      2635 May 18  2021 csi-20757826-0bf6-4f6f-bef8-a4da04dbe59f.ams.pv.yaml
-rw-r--r--  1 opc opc      2635 May 18  2021 csi-c8958e00-a3b6-44ba-bc1f-211db0f7092b.ams.pv.yaml
-rw-r--r--  1 opc opc       970 May 18  2021 csi-f8e97b7d-d149-4b88-be77-f68b63e60eff-restored-210518.ams.pv.yaml
-rw-r--r--  1 opc opc       970 May 18  2021 csi-20757826-0bf6-4f6f-bef8-a4da04dbe59f-restored-210518.ams.pv.yaml
-rw-r--r--  1 opc opc       970 May 18  2021 csi-c8958e00-a3b6-44ba-bc1f-211db0f7092b-restored-210518.ams.pv.yaml
-rw-r--r--  1 opc opc     12314 May 18  2021 shared-kafka-zookeeper.ss.prod_eu-amsterdam-1_dataplane.yaml
-rw-r--r--  1 opc opc      3947 May 18  2021 shared-kafka-zookeeper.ss.prod_eu-amsterdam-1_dataplane.desc
-rw-r--r--  1 opc opc     12314 May 18  2021 shared-kafka-zookeeper.ss.prod_eu-frankfurt-1_dataplane.yaml
-rw-r--r--  1 opc opc      3947 May 18  2021 shared-kafka-zookeeper.ss.prod_eu-frankfurt-1_dataplane.desc
drwxr-xr-x  5 opc opc       160 May 18  2021 prodfra
drwxr-xr-x  5 opc opc       160 May 18  2021 prodams
drwxr-xr-x  6 opc opc       192 May 18  2021 prodcardiff
drwxr-xr-x 11 opc opc       352 May 24  2021 schemakafka_mel
-rw-r--r--  1 opc opc   3298967 May 26  2021 JQ_TESTDATA_corp_ociinventory.list
-rw-r--r--  1 opc opc      3036 May 29  2021 corpiad_datapipeline.consumerconfig
-rw-r--r--  1 opc opc      2976 May 29  2021 prodphx_datapipeline.consumerconfig
-rw-r--r--  1 opc opc      3052 May 29  2021 prodphx_kafkaconnect.consumerconfig
-rw-r--r--  1 opc opc      2987 May 29  2021 preprodphx_datapipeline.consumerconfig
-rw-r--r--  1 opc opc     10372 May 31  2021 datapipeline-provisioner.deployments.phx.yaml
-rw-r--r--  1 opc opc     27739 Jun  2  2021 kafka.object_dev_eu-frankfurt-1_dataplane.yaml
-rw-r--r--  1 opc opc    564575 Jun  2  2021 10.245.0.156.metrics_r2
-rw-r--r--  1 opc opc       954 Jun  8  2021 tmp-cdaas_cpe-cronjob.json
drwxr-xr-x  3 opc opc        96 Jun  8  2021 Archive
-rw-r--r--  1 opc opc    148420 Jun  8  2021 tmp-cdaas_repo.list
-rw-r--r--  1 opc opc      3947 Jun  9  2021 pvc-watcher-sa-token-fb6b4.corp.secrets.yaml
-rw-r--r--  1 opc opc      1449 Jun  9  2021 pvc-watcher-sa.corp.serviceaccount.yaml
-rw-r--r--  1 opc opc     54598 Jun 15  2021 yum_galorndon-infra.list
-rw-r--r--  1 opc opc       548 Jun 15  2021 helm_presync_hook.sh
-rw-r--r--  1 opc opc       398 Jun 16  2021 test_pod.yaml
-rw-r--r--  1 opc opc       563 Jun 17  2021 test_pod_while.yaml
drwxr-xr-x 34 opc opc      1088 Jul  1  2021 helios-qa_logs_2107011019
drwxr-xr-x 34 opc opc      1088 Jul  2  2021 helios-qa_logs_2107020338
drwxr-xr-x  2 opc opc        64 Jul  2  2021 helios_logs_2107020922
drwxr-xr-x  3 opc opc        96 Jul  2  2021 dead-letter-processor-798945c6f-kq4c5_logs_2107020925
drwxr-xr-x  8 opc opc       256 Jul  2  2021 PROD_V1_HELIOS
drwxr-xr-x 24 opc opc       768 Jul  2  2021 helios_logs_2107021018
drwxr-xr-x 21 opc opc       672 Jul  5  2021 prodphx_helios
drwxr-xr-x 18 opc opc       576 Jul  5  2021 helios_logs_2107051234
drwxr-xr-x  4 opc opc       128 Aug 16 05:26 authentication-5859f57c86-bb4tt_logs_2108160525
drwxr-xr-x  4 opc opc       128 Aug 16 05:27 authentication-5859f57c86-rfg8p_logs_2108160526
drwxr-xr-x  4 opc opc       128 Aug 16 05:46 sender-85d8dd4996-gnh8l_logs_2108160546
drwxr-xr-x  4 opc opc       128 Aug 16 07:27 authentication-f74d65c6c-b78t8_logs_2108160726
drwxr-xr-x  4 opc opc       128 Aug 16 07:29 authentication-f74d65c6c-wfrsf_logs_2108160728
drwxr-xr-x  4 opc opc       128 Aug 16 07:30 authentication-f74d65c6c-xz4vk_logs_2108160729
drwxr-xr-x  4 opc opc       128 Aug 16 07:42 authentication-f74d65c6c-778ws_logs_2108160742
-rwxr-xr-x  1 opc opc     10896 Aug 19 06:50 setup_kubeconfig.sh
-rw-r--r--  1 opc opc   6600342 Sep 14 07:13 prod_eu_amsterdam_1_dataplane_kafkatopics
-rw-r--r--  1 opc opc   4032978 Sep 14 10:28 prod_eu_amsterdam_1_dataplane_kafkatopics_0914
-rw-r--r--  1 opc opc   3566446 Sep 15 05:25 prod_ca_montreal_1_dataplane_kafkatopics_0915
-rw-r--r--  1 opc opc   3959055 Sep 15 05:29 prod_ca_toronto_1_dataplane_kafkatopics_0915
-rw-r--r--  1 opc opc   3742022 Sep 15 05:32 prod_eu_amsterdam_1_dataplane_kafkatopics_0915
drwxr-xr-x  5 opc opc       160 Sep 20 06:51 oci_prod_config
-rw-r--r--  1 opc opc     48906 Sep 23 06:44 pv2_prod_23Sept2021.config
-rw-r--r--  1 opc opc      6699 Sep 23 09:45 strimzi-cluster-operator_prodfra.deployment.yaml
-rw-r--r--  1 opc opc   4956070 Nov 18 12:55 traefik-kafka-controller-58458f4cf-gh2rb2021_11_18_01_01_AM.log
-rw-r--r--  1 opc opc      3178 Nov 23 03:18 traefik-telemetry-alert-rules.yaml
-rw-r--r--  1 opc opc      2629 Nov 23 03:22 traefik-telemetry-alert-routes.yaml
-rw-r--r--  1 opc opc     39028 Nov 24 00:47 traefik-dashboard.yaml
drwxr-xr-x  4 opc opc       128 Nov 24 14:05 traefik_code
-rw-------  1 opc opc     76737 Nov 24 22:21 kube_config_all.prod
-rw-r--r--  1 opc opc    336833 Nov 30 01:30 shared-schemaregistry-546cb97944-4f4zm2021_11_24_01_48_PM.log
-rw-r--r--  1 opc opc    344596 Nov 30 03:34 shared-schemaregistry-546cb97944-wn5lz_1_2021_11_24_01_48_PM.log
-rw-r--r--  1 opc opc     68551 Nov 30 03:37 kafkaconnect-cluster-connect-5b4bf448ff-95rw22021_11_24_01_48_PM.log
drwxr-xr-x 14 opc opc       448 Nov 30 22:35 traefik-concept
-rw-r--r--  1 opc opc       487 Dec  1 22:34 schemaregistry_ingressroutetcp.yaml
-rw-r--r--  1 opc opc       485 Dec  1 23:08 shared-kafka-broker-0-kafka_ingressroutetcp.yaml
-rw-r--r--  1 opc opc       518 Dec  2 02:36 karapace-schemaregistry-proxy-kafka.service_ingressroutetcp.yaml
-rw-r--r--  1 opc opc       523 Dec  2 02:42 karapace-shared-schemaregistry.service_ingressroutetcp.yaml
-rw-r--r--  1 opc opc    685314 Dec  2 22:10 shared-kafka-kafka-02021_11_29_09_00_PM.log
-rw-r--r--  1 opc opc   1216506 Dec  2 22:10 shared-kafka-kafka-22021_11_29_09_00_PM.log
-rw-r--r--  1 opc opc   6038207 Dec  2 22:10 shared-kafka-kafka-12021_11_29_09_00_PM.log
-rw-r--r--  1 opc opc   7966451 Dec  2 22:28 strimzi-cluster-operator-7547df86cc-htnh7_2021_12_02_10_27_PM.log
-rw-r--r--  1 opc opc  15304668 Dec  2 22:41 strimzi-cluster-operator-b8bd4ff6d-m5wdc_2021_12_02_10_40_PM.log
-rw-r--r--  1 opc opc     23460 Dec  7 17:11 dev_lhr_kafka_ingress.yaml
-rw-------  1 opc opc     95279 Jan  3 15:16 config.prod
-rw-r--r--  1 opc opc      8146 Jan  5 16:10 strimzi-kafka-cluster.yaml
-r--------  1 opc opc      2358 Jan 10 05:02 kubeconfig_context-c5ppxaonnda
-rw-r--r--  1 opc opc         0 Jan 11 09:02 nginxdeployment.yaml
-rw-r--r--  1 opc opc       198 Jan 11 09:06 busybox_deployment.yaml
-rw-r--r--  1 opc opc      8138 Jan 11 09:14 kafkageorgecluster_kind.yaml
-rw-r--r--  1 opc opc      1434 Jan 11 09:25 kafkak8scluster_kind.yaml
-rw-r--r--  1 opc opc       986 Jan 11 09:30 test_kafkak8scluster_kind.yaml
-rw-r--r--  1 opc opc         0 Jan 11 11:20 kf-kafka-entity-operator-8585d869c4-b4995.log
-rw-r--r--  1 opc opc       189 Jan 11 11:20 testtopic.yaml
-rw-r--r--  1 opc opc    207526 Jan 11 11:21 kf-kafka-entity-operator-8585d869c4-b4995_topicoperator.log
drwxr-xr-x  6 opc opc       192 Jan 17 08:03 IngressTraefikL
-rw-r--r--  1 opc opc      5518 Jan 17 23:53 kind-testk8s-kubeconfig
-rw-r--r--  1 opc opc    507680 Jan 18 05:58 console4.log
-rw-r--r--  1 opc opc    474101 Jan 18 05:58 console3.log
-rw-r--r--  1 opc opc    507680 Jan 18 05:59 console2.log
-rw-r--r--  1 opc opc    401069 Jan 18 05:59 console1.log
drwxr-xr-x 22 opc opc       704 Jan 18 07:22 devlhrtraefik
-rwxr-xr-x  1 opc opc 492170292 Jan 20 09:51 sysint-fluentbit.log
-rw-r--r--  1 opc opc         0 Jan 21 07:00 data-processor-85dbcc656c-pxtlm2022_01_21_06_55_AM.log
-rw-r--r--  1 opc opc    724405 Jan 21 07:01 data-processor-85dbcc656c-pxtlm_app2022_01_21_06_55_AM.log
-rw-r--r--  1 opc opc   1991822 Jan 21 10:33 PHX-helmfile-mercury-psr-apps-helmfile.template
-rw-r--r--  1 opc opc    137234 Jan 21 10:59 PHX-mercury-tenant-downtime-monitor-b58fc56f8-tbdvj_tenant-downtime-monitor.log
-rw-r--r--  1 opc opc    372802 Jan 24 07:20 kafkatopic.object
-rw-r--r--  1 opc opc      8569 Jan 25 04:48 preprod_infra-monitoring-ingress.yaml
-rw-r--r--  1 opc opc      5517 Jan 25 16:20 KEVIN_kf-kafka-client.pod.yaml
-rw-r--r--  1 opc opc      5625 Jan 25 16:21 AZHER_azdevlhrclient-kafka-client.pod.yaml
drwxr-xr-x 51 opc opc      1632 Jan 27 07:07 traefik-test
[27:January:2022:07:13:32]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp
○ $ cd traefik-test
[27:January:2022:07:13:40]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ cl
total 34504
-r-------- 1 opc opc     2358 Jan 10 05:02 kubeconfig_context-c5ppxaonnda
-rw-r--r-- 1 opc opc     4455 Jan 13 00:06 traefik-6ff7ff8ffd-cxgdn2022_01_12_11_33_PM.log
-rw-r--r-- 1 opc opc     1006 Jan 13 00:11 traefik.service
-rw-r--r-- 1 opc opc     7670 Jan 13 00:13 kf-kafka.kafka.yaml
-rw-r--r-- 1 opc opc    12389 Jan 13 00:15 corp.shared-kafka.kafka.yaml
-rw-r--r-- 1 opc opc      420 Jan 13 04:44 kf-kafka-kafka-0-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc      452 Jan 13 04:45 kf-kafka-kafka-bootstrap-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc      420 Jan 13 04:45 kf-kafka-kafka-1-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc      420 Jan 13 04:45 kf-kafka-kafka-2-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc     3186 Jan 13 05:05 kf-kafka.kafka_updated.yaml
-rw-r--r-- 1 opc opc     1358 Jan 13 08:12 kf-kafka-kafka-tlsexternal-1.ingress.yaml
-rw-r--r-- 1 opc opc     1331 Jan 13 08:24 kf-kafka-kafka-tlsexternal-0.ingress.yaml
-rw-r--r-- 1 opc opc     3782 Jan 13 08:34 kf-kafka-kafka_updated2.yaml
-rw-r--r-- 1 opc opc     2197 Jan 13 11:55 ca.crt
-rw-r--r-- 1 opc opc     2897 Jan 13 13:31 traefik-test_values.yaml
-rw-r--r-- 1 opc opc     1334 Jan 13 14:26 kf-kafka-kafka-0.ingress.1.yaml
-rw-r--r-- 1 opc opc     1334 Jan 13 14:26 kf-kafka-kafka-1.ingress.1.yaml
-rw-r--r-- 1 opc opc     1334 Jan 13 14:26 kf-kafka-kafka-2.ingress.1.yaml
-rw-r--r-- 1 opc opc     1383 Jan 13 14:26 kf-kafka-kafka-bootstrap.ingress.1.yaml
-rw-r--r-- 1 opc opc     3800 Jan 14 04:15 kf-kafka.cluster.kind.14jan22.yaml
-rw-r--r-- 1 opc opc     8904 Jan 14 04:42 kf-kafka.kafka.WORKING.14Jan22.yaml
-rw-r--r-- 1 opc opc     1379 Jan 14 04:53 kf-kafka-kafka-0.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     1379 Jan 14 04:53 kf-kafka-kafka-1.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     1379 Jan 14 04:53 kf-kafka-kafka-2.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     1428 Jan 14 04:53 kf-kafka-kafka-bootstrap.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     8904 Jan 17 04:23 kf-kafka.kafka.WORKING.17Jan22.yaml
-rw-r--r-- 1 opc opc  8656381 Jan 17 11:10 traefik-kafka-controller-5f9f755d5-j6nzw2022_01_12_11_33_PM.log
-rw-r--r-- 1 opc opc      238 Jan 17 16:55 traefik-web-ui_svc.yaml
-rw-r--r-- 1 opc opc      269 Jan 17 16:57 traefik-web-ui_ingress.yaml
-rw------- 1 opc opc     5884 Jan 18 00:08 kubeconfig_minikube_18jan2022
-rw-r--r-- 1 opc opc   211050 Jan 18 01:18 Traefik_dashboard.png
-rw-r--r-- 1 opc opc   106471 Jan 18 01:18 Traefik_serviceAdminPort.png
drwxr-xr-x 9 opc opc      288 Jan 18 01:20 traefik-setup
-rw-r--r-- 1 opc opc     8684 Jan 19 07:15 kf-kafka.kafka.19jan2022.yaml
-rw-r--r-- 1 opc opc    12498 Jan 19 07:16 kgn.kafka.services.19jan2022.yaml
-rw-r--r-- 1 opc opc     4534 Jan 19 07:17 kgn.kafka.ingressroutetcp.19jan2022.yaml
-rw-r--r-- 1 opc opc     5857 Jan 19 07:19 kgn.kafka.ingress.19jan2022.yaml
-rw-r--r-- 1 opc opc 14267697 Jan 20 07:26 traefik-kafka-controller.log
-rw-r--r-- 1 opc opc        0 Jan 20 07:27 kf-kafka-entity-operator-6dfd8c8649-95dc72022_01_19_07_25_AM.log
-rw-r--r-- 1 opc opc  5581954 Jan 20 07:29 kf-kafka-entity-operator-6dfd8c8649-95dc7_topicoperator2022_01_19_07_25_AM.log
-rw-r--r-- 1 opc opc    67865 Jan 20 07:31 strimzi-cluster-operator-6fb59d59cb-zdd6k2022_01_19_07_25_AM.log
-rw-r--r-- 1 opc opc  3450078 Jan 27 01:08 ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log
-rw-r--r-- 1 opc opc     9746 Jan 27 01:49 kafka.shared-kafka-dr.kafka-dev-lhr.yaml
-rw-r--r-- 1 opc opc     9229 Jan 27 01:53 shared-kafka-dr.kafka-dev-lhr-Updated.yaml
-rw-r--r-- 1 opc opc    22895 Jan 27 07:03 shared-kafka-dev-lhr-ingress-nginx_27jan22.yaml
-rw-r--r-- 1 opc opc    32378 Jan 27 07:05 shared-kafka-dev-lhr-services_27jan22.yaml
-rw-r--r-- 1 opc opc     9915 Jan 27 07:05 shared-kafka-dev-lhr-kafkacluster_27jan22.yaml
-rw-r--r-- 1 opc opc     9229 Jan 27 07:05 shared-kafka-dr-dev-lhr-kafkacluster_27jan22.yaml
-rw-r--r-- 1 opc opc     9923 Jan 27 07:07 shared-kafka-dev-lhr-kafkacluster-classTraefik_27jan22.yaml
[27:January:2022:07:13:42]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ path kubeconfig_context-c5ppxaonnda
bash: path: command not found
[27:January:2022:07:13:54]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ basename kubeconfig_context-c5ppxaonnda
kubeconfig_context-c5ppxaonnda
[27:January:2022:07:13:57]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ rp
[27:January:2022:07:14:03]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ oong &
[1] 7565
[27:January:2022:07:14:08]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ {
  "data": "osvcstage"
}
env | grep -i kube
KUBECONFIG=/home/opc/.kube/pv2_dev.config
KUBE_EDITOR=vim
[1]+  Done                    oci os ns get
[27:January:2022:07:14:16]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ #export KUBECONFIG=~/galorndon/ctemp/traefik-test/kubeconfig_context-c5ppxaonnda
[27:January:2022:07:14:35]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ kgc
CURRENT   NAME                              CLUSTER               AUTHINFO           NAMESPACE
*         dev_eu-frankfurt-1_controlplane   cluster-c4dsmruga4w   user-c4dsmruga4w
          dev_eu-frankfurt-1_dataplane      cluster-cqtkntfgrtg   user-cqtkntfgrtg
          dev_eu-frankfurt-1_deployment     cluster-cqtqnbymy2t   user-cqtqnbymy2t
          dev_uk-london-1_controlplane      cluster-cydknrqgzrg   user-cydknrqgzrg
          dev_uk-london-1_dataplane         cluster-cqtozrugy3t   user-cqtozrugy3t
[27:January:2022:07:14:37]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ export KUBECONFIG=~/galorndon/ctemp/traefik-test/kubeconfig_context-c5ppxaonnda
[27:January:2022:07:14:38]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ kgc
CURRENT   NAME                  CLUSTER               AUTHINFO           NAMESPACE
*         context-c5ppxaonnda   cluster-c5ppxaonnda   user-c5ppxaonnda
[27:January:2022:07:14:40]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ kgns
NAME              STATUS   AGE
default           Active   99d
kafka             Active   78d
kafka-strimzi     Active   21d
kube-node-lease   Active   99d
kube-public       Active   99d
kube-system       Active   99d
traefik-apps      Active   13d
traefik-test      Active   21d
[27:January:2022:07:14:46]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ kgpn kafka &
[1] 7650
[27:January:2022:07:14:54]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ kgn kafka NAME                                        READY   STATUS    RESTARTS   AGE     IP             NODE          NOMINATED NODE   READINESS GATES
az-kc-kafka-client                          1/1     Running   0          15d     10.244.0.88    10.0.10.231   <none>           <none>
kf-kafka-entity-operator-6dfd8c8649-95dc7   3/3     Running   1          78d     10.244.0.199   10.0.10.97    <none>           <none>
kf-kafka-kafka-0                            1/1     Running   0          7d22h   10.244.1.202   10.0.10.12    <none>           <none>
kf-kafka-kafka-1                            1/1     Running   0          7d22h   10.244.0.207   10.0.10.97    <none>           <none>
kf-kafka-kafka-2                            1/1     Running   0          7d22h   10.244.0.90    10.0.10.231   <none>           <none>
kf-kafka-kafka-exporter-6dc96d4bc9-rz9jh    1/1     Running   2          78d     10.244.1.190   10.0.10.12    <none>           <none>
kf-kafka-zookeeper-0                        1/1     Running   0          78d     10.244.1.188   10.0.10.12    <none>           <none>
kf-kafka-zookeeper-1                        1/1     Running   1          78d     10.244.0.200   10.0.10.97    <none>           <none>
kf-kafka-zookeeper-2                        1/1     Running   0          78d     10.244.0.75    10.0.10.231   <none>           <none>
strimzi-cluster-operator-6fb59d59cb-zdd6k   1/1     Running   176        78d     10.244.0.72    10.0.10.231   <none>           <none>
ingress
No resources found in kafka namespace.
[1]+  Done                    kubectl get pods -o wide -n kafka
[27:January:2022:07:15:01]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $
[27:January:2022:07:15:07]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ kgn kafka ingress
No resources found in kafka namespace.
[27:January:2022:07:15:14]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ kgn kafka kafka
NAME       DESIRED KAFKA REPLICAS   DESIRED ZK REPLICAS   READY   WARNINGS
kf-kafka   3                        3                     True
[27:January:2022:07:15:25]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $
[27:January:2022:07:15:25]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ kgn kafka kafka kf-kafka -oyaml > kf-kafka-cluster-OKE-27jan22.yaml
[27:January:2022:07:15:53]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ ls -ltr | grep -i ingress
-rw-r--r-- 1 opc opc      420 Jan 13 04:44 kf-kafka-kafka-0-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc      452 Jan 13 04:45 kf-kafka-kafka-bootstrap-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc      420 Jan 13 04:45 kf-kafka-kafka-1-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc      420 Jan 13 04:45 kf-kafka-kafka-2-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc     1358 Jan 13 08:12 kf-kafka-kafka-tlsexternal-1.ingress.yaml
-rw-r--r-- 1 opc opc     1331 Jan 13 08:24 kf-kafka-kafka-tlsexternal-0.ingress.yaml
-rw-r--r-- 1 opc opc     1334 Jan 13 14:26 kf-kafka-kafka-0.ingress.1.yaml
-rw-r--r-- 1 opc opc     1334 Jan 13 14:26 kf-kafka-kafka-1.ingress.1.yaml
-rw-r--r-- 1 opc opc     1334 Jan 13 14:26 kf-kafka-kafka-2.ingress.1.yaml
-rw-r--r-- 1 opc opc     1383 Jan 13 14:26 kf-kafka-kafka-bootstrap.ingress.1.yaml
-rw-r--r-- 1 opc opc     1379 Jan 14 04:53 kf-kafka-kafka-0.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     1379 Jan 14 04:53 kf-kafka-kafka-1.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     1379 Jan 14 04:53 kf-kafka-kafka-2.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     1428 Jan 14 04:53 kf-kafka-kafka-bootstrap.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc      269 Jan 17 16:57 traefik-web-ui_ingress.yaml
-rw-r--r-- 1 opc opc     4534 Jan 19 07:17 kgn.kafka.ingressroutetcp.19jan2022.yaml
-rw-r--r-- 1 opc opc     5857 Jan 19 07:19 kgn.kafka.ingress.19jan2022.yaml
-rw-r--r-- 1 opc opc  3450078 Jan 27 01:08 ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log
-rw-r--r-- 1 opc opc    22895 Jan 27 07:03 shared-kafka-dev-lhr-ingress-nginx_27jan22.yaml
[27:January:2022:07:22:32]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ helm ls -n traefik-test

helm ls -n kafka
NAME                      NAMESPACE     REVISION  UPDATED                                 STATUS    CHART           APP VERSION
traefik-kafka-controller  traefik-test  2         2022-01-13 22:38:44.176951 +0530 +0530  deployed  traefik-10.3.6  2.5.3
[27:January:2022:08:07:04]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $
[27:January:2022:08:07:04]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ helm ls -n kafka
NAME    NAMESPACE REVISION  UPDATED                               STATUS    CHART                         APP VERSION
az-kc   kafka     1         2022-01-12 05:30:33.8039179 +0000 UTC deployed  kafka-client-0.2.0
strimzi kafka     2         2021-11-09 10:50:03.9044307 +0000 UTC deployed  strimzi-kafka-operator-0.25.0 0.25.0
[27:January:2022:08:07:09]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ cl
total 34544
-r-------- 1 opc opc     2358 Jan 10 05:02 kubeconfig_context-c5ppxaonnda
-rw-r--r-- 1 opc opc     4455 Jan 13 00:06 traefik-6ff7ff8ffd-cxgdn2022_01_12_11_33_PM.log
-rw-r--r-- 1 opc opc     1006 Jan 13 00:11 traefik.service
-rw-r--r-- 1 opc opc     7670 Jan 13 00:13 kf-kafka.kafka.yaml
-rw-r--r-- 1 opc opc    12389 Jan 13 00:15 corp.shared-kafka.kafka.yaml
-rw-r--r-- 1 opc opc      420 Jan 13 04:44 kf-kafka-kafka-0-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc      452 Jan 13 04:45 kf-kafka-kafka-bootstrap-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc      420 Jan 13 04:45 kf-kafka-kafka-1-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc      420 Jan 13 04:45 kf-kafka-kafka-2-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc     3186 Jan 13 05:05 kf-kafka.kafka_updated.yaml
-rw-r--r-- 1 opc opc     1358 Jan 13 08:12 kf-kafka-kafka-tlsexternal-1.ingress.yaml
-rw-r--r-- 1 opc opc     1331 Jan 13 08:24 kf-kafka-kafka-tlsexternal-0.ingress.yaml
-rw-r--r-- 1 opc opc     3782 Jan 13 08:34 kf-kafka-kafka_updated2.yaml
-rw-r--r-- 1 opc opc     2197 Jan 13 11:55 ca.crt
-rw-r--r-- 1 opc opc     2897 Jan 13 13:31 traefik-test_values.yaml
-rw-r--r-- 1 opc opc     1334 Jan 13 14:26 kf-kafka-kafka-0.ingress.1.yaml
-rw-r--r-- 1 opc opc     1334 Jan 13 14:26 kf-kafka-kafka-1.ingress.1.yaml
-rw-r--r-- 1 opc opc     1334 Jan 13 14:26 kf-kafka-kafka-2.ingress.1.yaml
-rw-r--r-- 1 opc opc     1383 Jan 13 14:26 kf-kafka-kafka-bootstrap.ingress.1.yaml
-rw-r--r-- 1 opc opc     3800 Jan 14 04:15 kf-kafka.cluster.kind.14jan22.yaml
-rw-r--r-- 1 opc opc     8904 Jan 14 04:42 kf-kafka.kafka.WORKING.14Jan22.yaml
-rw-r--r-- 1 opc opc     1379 Jan 14 04:53 kf-kafka-kafka-0.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     1379 Jan 14 04:53 kf-kafka-kafka-1.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     1379 Jan 14 04:53 kf-kafka-kafka-2.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     1428 Jan 14 04:53 kf-kafka-kafka-bootstrap.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     8904 Jan 17 04:23 kf-kafka.kafka.WORKING.17Jan22.yaml
-rw-r--r-- 1 opc opc  8656381 Jan 17 11:10 traefik-kafka-controller-5f9f755d5-j6nzw2022_01_12_11_33_PM.log
-rw-r--r-- 1 opc opc      238 Jan 17 16:55 traefik-web-ui_svc.yaml
-rw-r--r-- 1 opc opc      269 Jan 17 16:57 traefik-web-ui_ingress.yaml
-rw------- 1 opc opc     5884 Jan 18 00:08 kubeconfig_minikube_18jan2022
-rw-r--r-- 1 opc opc   211050 Jan 18 01:18 Traefik_dashboard.png
-rw-r--r-- 1 opc opc   106471 Jan 18 01:18 Traefik_serviceAdminPort.png
drwxr-xr-x 9 opc opc      288 Jan 18 01:20 traefik-setup
-rw-r--r-- 1 opc opc     8684 Jan 19 07:15 kf-kafka.kafka.19jan2022.yaml
-rw-r--r-- 1 opc opc    12498 Jan 19 07:16 kgn.kafka.services.19jan2022.yaml
-rw-r--r-- 1 opc opc     4534 Jan 19 07:17 kgn.kafka.ingressroutetcp.19jan2022.yaml
-rw-r--r-- 1 opc opc     5857 Jan 19 07:19 kgn.kafka.ingress.19jan2022.yaml
-rw-r--r-- 1 opc opc 14267697 Jan 20 07:26 traefik-kafka-controller.log
-rw-r--r-- 1 opc opc        0 Jan 20 07:27 kf-kafka-entity-operator-6dfd8c8649-95dc72022_01_19_07_25_AM.log
-rw-r--r-- 1 opc opc  5581954 Jan 20 07:29 kf-kafka-entity-operator-6dfd8c8649-95dc7_topicoperator2022_01_19_07_25_AM.log
-rw-r--r-- 1 opc opc    67865 Jan 20 07:31 strimzi-cluster-operator-6fb59d59cb-zdd6k2022_01_19_07_25_AM.log
-rw-r--r-- 1 opc opc  3450078 Jan 27 01:08 ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log
-rw-r--r-- 1 opc opc     9746 Jan 27 01:49 kafka.shared-kafka-dr.kafka-dev-lhr.yaml
-rw-r--r-- 1 opc opc     9229 Jan 27 01:53 shared-kafka-dr.kafka-dev-lhr-Updated.yaml
-rw-r--r-- 1 opc opc    22895 Jan 27 07:03 shared-kafka-dev-lhr-ingress-nginx_27jan22.yaml
-rw-r--r-- 1 opc opc    32378 Jan 27 07:05 shared-kafka-dev-lhr-services_27jan22.yaml
-rw-r--r-- 1 opc opc     9915 Jan 27 07:05 shared-kafka-dev-lhr-kafkacluster_27jan22.yaml
-rw-r--r-- 1 opc opc     9229 Jan 27 07:05 shared-kafka-dr-dev-lhr-kafkacluster_27jan22.yaml
-rw-r--r-- 1 opc opc     9923 Jan 27 07:07 shared-kafka-dev-lhr-kafkacluster-classTraefik_27jan22.yaml
-rw-r--r-- 1 opc opc     7672 Jan 27 07:15 kf-kafka-cluster-OKE-27jan22.yaml
-rw-r--r-- 1 opc opc    22783 Jan 27 07:53 shared-kafka-dev-lhr-ingress-TRAEFIK-KAFKA_27jan22.yaml
-rw-r--r-- 1 opc opc     4966 Jan 27 08:25 TRAEFIK-KAFKA_27jan22-shared-kafka-dev-lhr.yaml
[27:January:2022:08:27:57]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ cat shared-kafka-dr-dev-lhr-kafkacluster_27jan22.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  annotations:
    meta.helm.sh/release-name: kafka-shared
    meta.helm.sh/release-namespace: kafka
  creationTimestamp: "2021-12-03T23:52:15Z"
  generation: 6
  labels:
    app: cpekafka
    app.kubernetes.io/managed-by: Helm
    chart: cpe-kafka-0.2.6
    cluster_name: shared-kafka-dr-lhr
    heritage: Helm
    release: kafka-shared
  name: shared-kafka-dr
  namespace: kafka
  resourceVersion: "268844774"
  selfLink: /apis/kafka.strimzi.io/v1beta2/namespaces/kafka/kafkas/shared-kafka-dr
  uid: 220e0c59-daa8-4fcc-8532-b99a3f5661cb
spec:
  clientsCa:
    generateCertificateAuthority: false
  clusterCa:
    generateCertificateAuthority: false
  kafka:
    authorization:
      superUsers:
      - CN=mirrormaker
      - CN=mirrormaker-lhr
      - CN=mirrormaker-dr-lhr
      type: simple
    config:
      auto.create.topics.enable: false
      default.replication.factor: 3
      num.partitions: 3
      offsets.topic.replication.factor: 3
      transaction.state.log.min.isr: 2
      transaction.state.log.replication.factor: 3
    jvmOptions:
      -Xms: 8000m
      -Xmx: 8000m
    listeners:
    - name: plain
      port: 9092
      tls: false
      type: internal
    - authentication:
        type: tls
      name: tls
      port: 9093
      tls: true
      type: internal
    - authentication:
        type: tls
      configuration:
        bootstrap:
          host: shared-kafka-dr-bootstrap-kafka.service.lhr-dataplane.dev.consul
        brokers:
        - broker: 0
          host: shared-kafka-dr-broker-0-kafka.service.lhr-dataplane.dev.consul
        - broker: 1
          host: shared-kafka-dr-broker-1-kafka.service.lhr-dataplane.dev.consul
        - broker: 2
          host: shared-kafka-dr-broker-2-kafka.service.lhr-dataplane.dev.consul
        class: kafka
      name: external
      port: 9094
      tls: true
      type: ingress
    livenessProbe: {}
    metricsConfig:
      type: jmxPrometheusExporter
      valueFrom:
        configMapKeyRef:
          key: shared-kafka-kafka-metrics-config.yaml
          name: shared-kafka-kafka-metrics
    rack:
      topologyKey: failure-domain.beta.kubernetes.io/zone
    readinessProbe: {}
    replicas: 3
    resources:
      limits:
        cpu: "4"
        memory: 32Gi
      requests:
        cpu: "2"
        memory: 16Gi
    storage:
      type: jbod
      volumes:
      - class: oci-bv
        id: 0
        size: 100Gi
        type: persistent-claim
    template:
      pod:
        metadata:
          labels:
            stage: dataplane
    version: 2.8.0
  kafkaExporter:
    livenessProbe:
      initialDelaySeconds: 300
      periodSeconds: 60
      timeoutSeconds: 600
    readinessProbe:
      initialDelaySeconds: 300
      periodSeconds: 60
      timeoutSeconds: 600
    resources:
      limits:
        cpu: 3000m
        memory: 8192M
      requests:
        cpu: 500m
        memory: 750M
    template:
      pod:
        metadata:
          labels:
            stage: dataplane
  zookeeper:
    livenessProbe: {}
    metricsConfig:
      type: jmxPrometheusExporter
      valueFrom:
        configMapKeyRef:
          key: shared-kafka-zookeeper-metrics-config.yaml
          name: shared-kafka-zookeeper-metrics
    readinessProbe: {}
    replicas: 3
    resources:
      limits:
        cpu: 500m
        memory: 8Gi
      requests:
        cpu: 125m
        memory: 4Gi
    storage:
      class: oci-bv
      size: 50Gi
      type: persistent-claim
    template:
      pod:
        metadata:
          labels:
            stage: dataplane
status:
  clusterId: eG1QgnkISzqsiNYh8fxvqw
  conditions:
  - lastTransitionTime: "2022-01-27T01:52:26.621Z"
    status: "True"
    type: Ready
  listeners:
  - addresses:
    - host: shared-kafka-dr-kafka-bootstrap.kafka.svc
      port: 9092
    bootstrapServers: shared-kafka-dr-kafka-bootstrap.kafka.svc:9092
    type: plain
  - addresses:
    - host: shared-kafka-dr-kafka-bootstrap.kafka.svc
      port: 9093
    bootstrapServers: shared-kafka-dr-kafka-bootstrap.kafka.svc:9093
    certificates:
    - |
      -----BEGIN CERTIFICATE-----
      MIIDDDCCArKgAwIBAgIJAMa+4QeM8Ac9MAoGCCqGSM49BAMCMIG5MQswCQYDVQQG
      EwJVUzELMAkGA1UECBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xGjAYBgNV
      BAkTETEwMSBTZWNvbmQgU3RyZWV0MQ4wDAYDVQQREwU5NDEwNTEXMBUGA1UEChMO
      SGFzaGlDb3JwIEluYy4xQDA+BgNVBAMTN0NvbnN1bCBBZ2VudCBDQSAyMjc0MjA4
      ODE2Njk4ODgyOTczNTU2MjkwMTM2Mzk4MTc0Njk2MDAwHhcNMjAwODMxMjE1MTQ2
      WhcNMjUwODMwMjE1MTQ2WjBzMQswCQYDVQQGEwJVUzELMAkGA1UECAwCQ0ExFTAT
      BgNVBAcMDFJlZHdvb2QgQ2l0eTEPMA0GA1UECgwGT3JhY2xlMQ0wCwYDVQQLDARP
      U3ZDMSAwHgYDVQQDDBdpbnRlcm1lZGlhdGUuZGV2LmNvbnN1bDCCASIwDQYJKoZI
      hvcNAQEBBQADggEPADCCAQoCggEBAOSSwhmeN5oLjhHmx9TOmBOMjv9wEiRYosEh
      WzpOOc4h/AFiY9puMw2lLqQaB8dRVszWk7l3Y4+G9fBT0uGrsENuUDjmlFfv4G4I
      09DAbrplXCbUG9XTS71ul64HpRJ68nkMiHQoFL5CANFuXYdkQ66S3RfdsMYQzssn
      VI4ubgPMSWhSjQ/ielONXeGV0/vKjkYeTxXwwvAXQ5V72rstVl242Cu3xrj75OOl
      oqXyl4IG1sbdjdyfkdcq376Fq07V+GWexSWVmYnwYfeuf+FMYa8rLGc6PmpPiuRq
      GShqzZ9zsplx0CPoDbI9/4Bm19GUu/zOm+UMRtyE+E6mskmCsg0CAwEAAaMdMBsw
      DAYDVR0TBAUwAwEB/zALBgNVHQ8EBAMCAQYwCgYIKoZIzj0EAwIDSAAwRQIhAMgY
      e7UnxYAbgTbbLBCGb3CO+b8sXZIGHLn2JpZIER4QAiB85dl1JrjwEKi6ca6nMIgS
      RTO9M/Qj2L3EJlOysgiZXA==
      -----END CERTIFICATE-----
      -----BEGIN CERTIFICATE-----
      MIIC7jCCApSgAwIBAgIRAKsXqylIx9M/WTbMcgd+HqAwCgYIKoZIzj0EAwIwgbkx
      CzAJBgNVBAYTAlVTMQswCQYDVQQIEwJDQTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNj
      bzEaMBgGA1UECRMRMTAxIFNlY29uZCBTdHJlZXQxDjAMBgNVBBETBTk0MTA1MRcw
      FQYDVQQKEw5IYXNoaUNvcnAgSW5jLjFAMD4GA1UEAxM3Q29uc3VsIEFnZW50IENB
      IDIyNzQyMDg4MTY2OTg4ODI5NzM1NTYyOTAxMzYzOTgxNzQ2OTYwMDAeFw0yMDA4
      MjMyMzQ1MTFaFw0yNTA4MjIyMzQ1MTFaMIG5MQswCQYDVQQGEwJVUzELMAkGA1UE
      CBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xGjAYBgNVBAkTETEwMSBTZWNv
      bmQgU3RyZWV0MQ4wDAYDVQQREwU5NDEwNTEXMBUGA1UEChMOSGFzaGlDb3JwIElu
      Yy4xQDA+BgNVBAMTN0NvbnN1bCBBZ2VudCBDQSAyMjc0MjA4ODE2Njk4ODgyOTcz
      NTU2MjkwMTM2Mzk4MTc0Njk2MDAwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAASE
      aKwNx8k0+casMPt/qkEa4ajItFqtFL22llPlX5AgnXUxeJKC3fvusEHptrpWPXoo
      Ke5H5n8U6y4jmg7/O2Buo3sweTAOBgNVHQ8BAf8EBAMCAYYwDwYDVR0TAQH/BAUw
      AwEB/zApBgNVHQ4EIgQgFWS9tm0ScTJrTeowqm+JAtfink4/xvZn2brSMzZZEnsw
      KwYDVR0jBCQwIoAgFWS9tm0ScTJrTeowqm+JAtfink4/xvZn2brSMzZZEnswCgYI
      KoZIzj0EAwIDSAAwRQIhAM3dMq1X3vzKItfir+aXfmvk74BkLW3s7PRDfrSD0ljU
      AiB1jfrlC6JY/Y5rbP9gda0da7iOTp7OXc/pxWYVrerjRw==
      -----END CERTIFICATE-----
    type: tls
  - addresses:
    - host: shared-kafka-dr-bootstrap-kafka.service.lhr-dataplane.dev.consul
      port: 443
    bootstrapServers: shared-kafka-dr-bootstrap-kafka.service.lhr-dataplane.dev.consul:443
    certificates:
    - |
      -----BEGIN CERTIFICATE-----
      MIIDDDCCArKgAwIBAgIJAMa+4QeM8Ac9MAoGCCqGSM49BAMCMIG5MQswCQYDVQQG
      EwJVUzELMAkGA1UECBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xGjAYBgNV
      BAkTETEwMSBTZWNvbmQgU3RyZWV0MQ4wDAYDVQQREwU5NDEwNTEXMBUGA1UEChMO
      SGFzaGlDb3JwIEluYy4xQDA+BgNVBAMTN0NvbnN1bCBBZ2VudCBDQSAyMjc0MjA4
      ODE2Njk4ODgyOTczNTU2MjkwMTM2Mzk4MTc0Njk2MDAwHhcNMjAwODMxMjE1MTQ2
      WhcNMjUwODMwMjE1MTQ2WjBzMQswCQYDVQQGEwJVUzELMAkGA1UECAwCQ0ExFTAT
      BgNVBAcMDFJlZHdvb2QgQ2l0eTEPMA0GA1UECgwGT3JhY2xlMQ0wCwYDVQQLDARP
      U3ZDMSAwHgYDVQQDDBdpbnRlcm1lZGlhdGUuZGV2LmNvbnN1bDCCASIwDQYJKoZI
      hvcNAQEBBQADggEPADCCAQoCggEBAOSSwhmeN5oLjhHmx9TOmBOMjv9wEiRYosEh
      WzpOOc4h/AFiY9puMw2lLqQaB8dRVszWk7l3Y4+G9fBT0uGrsENuUDjmlFfv4G4I
      09DAbrplXCbUG9XTS71ul64HpRJ68nkMiHQoFL5CANFuXYdkQ66S3RfdsMYQzssn
      VI4ubgPMSWhSjQ/ielONXeGV0/vKjkYeTxXwwvAXQ5V72rstVl242Cu3xrj75OOl
      oqXyl4IG1sbdjdyfkdcq376Fq07V+GWexSWVmYnwYfeuf+FMYa8rLGc6PmpPiuRq
      GShqzZ9zsplx0CPoDbI9/4Bm19GUu/zOm+UMRtyE+E6mskmCsg0CAwEAAaMdMBsw
      DAYDVR0TBAUwAwEB/zALBgNVHQ8EBAMCAQYwCgYIKoZIzj0EAwIDSAAwRQIhAMgY
      e7UnxYAbgTbbLBCGb3CO+b8sXZIGHLn2JpZIER4QAiB85dl1JrjwEKi6ca6nMIgS
      RTO9M/Qj2L3EJlOysgiZXA==
      -----END CERTIFICATE-----
      -----BEGIN CERTIFICATE-----
      MIIC7jCCApSgAwIBAgIRAKsXqylIx9M/WTbMcgd+HqAwCgYIKoZIzj0EAwIwgbkx
      CzAJBgNVBAYTAlVTMQswCQYDVQQIEwJDQTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNj
      bzEaMBgGA1UECRMRMTAxIFNlY29uZCBTdHJlZXQxDjAMBgNVBBETBTk0MTA1MRcw
      FQYDVQQKEw5IYXNoaUNvcnAgSW5jLjFAMD4GA1UEAxM3Q29uc3VsIEFnZW50IENB
      IDIyNzQyMDg4MTY2OTg4ODI5NzM1NTYyOTAxMzYzOTgxNzQ2OTYwMDAeFw0yMDA4
      MjMyMzQ1MTFaFw0yNTA4MjIyMzQ1MTFaMIG5MQswCQYDVQQGEwJVUzELMAkGA1UE
      CBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xGjAYBgNVBAkTETEwMSBTZWNv
      bmQgU3RyZWV0MQ4wDAYDVQQREwU5NDEwNTEXMBUGA1UEChMOSGFzaGlDb3JwIElu
      Yy4xQDA+BgNVBAMTN0NvbnN1bCBBZ2VudCBDQSAyMjc0MjA4ODE2Njk4ODgyOTcz
      NTU2MjkwMTM2Mzk4MTc0Njk2MDAwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAASE
      aKwNx8k0+casMPt/qkEa4ajItFqtFL22llPlX5AgnXUxeJKC3fvusEHptrpWPXoo
      Ke5H5n8U6y4jmg7/O2Buo3sweTAOBgNVHQ8BAf8EBAMCAYYwDwYDVR0TAQH/BAUw
      AwEB/zApBgNVHQ4EIgQgFWS9tm0ScTJrTeowqm+JAtfink4/xvZn2brSMzZZEnsw
      KwYDVR0jBCQwIoAgFWS9tm0ScTJrTeowqm+JAtfink4/xvZn2brSMzZZEnswCgYI
      KoZIzj0EAwIDSAAwRQIhAM3dMq1X3vzKItfir+aXfmvk74BkLW3s7PRDfrSD0ljU
      AiB1jfrlC6JY/Y5rbP9gda0da7iOTp7OXc/pxWYVrerjRw==
      -----END CERTIFICATE-----
    type: external
  observedGeneration: 6
[27:January:2022:08:28:10]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ ls -ltr
total 34544
-r-------- 1 opc opc     2358 Jan 10 05:02 kubeconfig_context-c5ppxaonnda
-rw-r--r-- 1 opc opc     4455 Jan 13 00:06 traefik-6ff7ff8ffd-cxgdn2022_01_12_11_33_PM.log
-rw-r--r-- 1 opc opc     1006 Jan 13 00:11 traefik.service
-rw-r--r-- 1 opc opc     7670 Jan 13 00:13 kf-kafka.kafka.yaml
-rw-r--r-- 1 opc opc    12389 Jan 13 00:15 corp.shared-kafka.kafka.yaml
-rw-r--r-- 1 opc opc      420 Jan 13 04:44 kf-kafka-kafka-0-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc      452 Jan 13 04:45 kf-kafka-kafka-bootstrap-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc      420 Jan 13 04:45 kf-kafka-kafka-1-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc      420 Jan 13 04:45 kf-kafka-kafka-2-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc     3186 Jan 13 05:05 kf-kafka.kafka_updated.yaml
-rw-r--r-- 1 opc opc     1358 Jan 13 08:12 kf-kafka-kafka-tlsexternal-1.ingress.yaml
-rw-r--r-- 1 opc opc     1331 Jan 13 08:24 kf-kafka-kafka-tlsexternal-0.ingress.yaml
-rw-r--r-- 1 opc opc     3782 Jan 13 08:34 kf-kafka-kafka_updated2.yaml
-rw-r--r-- 1 opc opc     2197 Jan 13 11:55 ca.crt
-rw-r--r-- 1 opc opc     2897 Jan 13 13:31 traefik-test_values.yaml
-rw-r--r-- 1 opc opc     1334 Jan 13 14:26 kf-kafka-kafka-0.ingress.1.yaml
-rw-r--r-- 1 opc opc     1334 Jan 13 14:26 kf-kafka-kafka-1.ingress.1.yaml
-rw-r--r-- 1 opc opc     1334 Jan 13 14:26 kf-kafka-kafka-2.ingress.1.yaml
-rw-r--r-- 1 opc opc     1383 Jan 13 14:26 kf-kafka-kafka-bootstrap.ingress.1.yaml
-rw-r--r-- 1 opc opc     3800 Jan 14 04:15 kf-kafka.cluster.kind.14jan22.yaml
-rw-r--r-- 1 opc opc     8904 Jan 14 04:42 kf-kafka.kafka.WORKING.14Jan22.yaml
-rw-r--r-- 1 opc opc     1379 Jan 14 04:53 kf-kafka-kafka-0.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     1379 Jan 14 04:53 kf-kafka-kafka-1.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     1379 Jan 14 04:53 kf-kafka-kafka-2.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     1428 Jan 14 04:53 kf-kafka-kafka-bootstrap.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     8904 Jan 17 04:23 kf-kafka.kafka.WORKING.17Jan22.yaml
-rw-r--r-- 1 opc opc  8656381 Jan 17 11:10 traefik-kafka-controller-5f9f755d5-j6nzw2022_01_12_11_33_PM.log
-rw-r--r-- 1 opc opc      238 Jan 17 16:55 traefik-web-ui_svc.yaml
-rw-r--r-- 1 opc opc      269 Jan 17 16:57 traefik-web-ui_ingress.yaml
-rw------- 1 opc opc     5884 Jan 18 00:08 kubeconfig_minikube_18jan2022
-rw-r--r-- 1 opc opc   211050 Jan 18 01:18 Traefik_dashboard.png
-rw-r--r-- 1 opc opc   106471 Jan 18 01:18 Traefik_serviceAdminPort.png
drwxr-xr-x 9 opc opc      288 Jan 18 01:20 traefik-setup
-rw-r--r-- 1 opc opc     8684 Jan 19 07:15 kf-kafka.kafka.19jan2022.yaml
-rw-r--r-- 1 opc opc    12498 Jan 19 07:16 kgn.kafka.services.19jan2022.yaml
-rw-r--r-- 1 opc opc     4534 Jan 19 07:17 kgn.kafka.ingressroutetcp.19jan2022.yaml
-rw-r--r-- 1 opc opc     5857 Jan 19 07:19 kgn.kafka.ingress.19jan2022.yaml
-rw-r--r-- 1 opc opc 14267697 Jan 20 07:26 traefik-kafka-controller.log
-rw-r--r-- 1 opc opc        0 Jan 20 07:27 kf-kafka-entity-operator-6dfd8c8649-95dc72022_01_19_07_25_AM.log
-rw-r--r-- 1 opc opc  5581954 Jan 20 07:29 kf-kafka-entity-operator-6dfd8c8649-95dc7_topicoperator2022_01_19_07_25_AM.log
-rw-r--r-- 1 opc opc    67865 Jan 20 07:31 strimzi-cluster-operator-6fb59d59cb-zdd6k2022_01_19_07_25_AM.log
-rw-r--r-- 1 opc opc  3450078 Jan 27 01:08 ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log
-rw-r--r-- 1 opc opc     9746 Jan 27 01:49 kafka.shared-kafka-dr.kafka-dev-lhr.yaml
-rw-r--r-- 1 opc opc     9229 Jan 27 01:53 shared-kafka-dr.kafka-dev-lhr-Updated.yaml
-rw-r--r-- 1 opc opc    22895 Jan 27 07:03 shared-kafka-dev-lhr-ingress-nginx_27jan22.yaml
-rw-r--r-- 1 opc opc    32378 Jan 27 07:05 shared-kafka-dev-lhr-services_27jan22.yaml
-rw-r--r-- 1 opc opc     9915 Jan 27 07:05 shared-kafka-dev-lhr-kafkacluster_27jan22.yaml
-rw-r--r-- 1 opc opc     9229 Jan 27 07:05 shared-kafka-dr-dev-lhr-kafkacluster_27jan22.yaml
-rw-r--r-- 1 opc opc     9923 Jan 27 07:07 shared-kafka-dev-lhr-kafkacluster-classTraefik_27jan22.yaml
-rw-r--r-- 1 opc opc     7672 Jan 27 07:15 kf-kafka-cluster-OKE-27jan22.yaml
-rw-r--r-- 1 opc opc    22783 Jan 27 07:53 shared-kafka-dev-lhr-ingress-TRAEFIK-KAFKA_27jan22.yaml
-rw-r--r-- 1 opc opc     4966 Jan 27 08:25 TRAEFIK-KAFKA_27jan22-shared-kafka-dev-lhr.yaml
[27:January:2022:08:28:38]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ diff TRAEFIK-KAFKA_27jan22-shared-kafka-dev-lhr.yaml kafka.shared-kafka-dr.kafka-dev-lhr.yaml
8c8
<   generation: 7
---
>   generation: 5
13c13
<     cluster_name: shared-kafka-lhr
---
>     cluster_name: shared-kafka-dr-lhr
16c16
<   name: shared-kafka
---
>   name: shared-kafka-dr
18,20c18,20
<   resourceVersion: "268954478"
<   selfLink: /apis/kafka.strimzi.io/v1beta2/namespaces/kafka/kafkas/shared-kafka
<   uid: 6b3e23f6-d322-412c-b13e-90a944dd001d
---
>   resourceVersion: "264370106"
>   selfLink: /apis/kafka.strimzi.io/v1beta2/namespaces/kafka/kafkas/shared-kafka-dr
>   uid: 220e0c59-daa8-4fcc-8532-b99a3f5661cb
26,61d25
<   entityOperator:
<     template:
<       pod:
<         metadata:
<           labels:
<             stage: dataplane
<     tlsSidecar:
<       livenessProbe: {}
<       readinessProbe: {}
<       resources:
<         limits:
<           cpu: 500m
<           memory: 1500M
<         requests:
<           cpu: 100m
<           memory: 750M
<     topicOperator:
<       livenessProbe: {}
<       readinessProbe: {}
<       resources:
<         limits:
<           cpu: 1000m
<           memory: 1500M
<         requests:
<           cpu: 500m
<           memory: 750M
<     userOperator:
<       livenessProbe: {}
<       readinessProbe: {}
<       resources:
<         limits:
<           cpu: 1000m
<           memory: 1500M
<         requests:
<           cpu: 500m
<           memory: 750M
94c58,61
<           host: shared-kafka-bootstrap-kafka.service.lhr-dataplane.dev.consul
---
>           annotations:
>             ingressWatcher/port: "443"
>             ingressWatcher/service: shared-kafka-dr-bootstrap-kafka
>           host: shared-kafka-dr-bootstrap-kafka.service.lhr-dataplane.dev.consul
96,102c63,78
<         - broker: 0
<           host: shared-kafka-broker-0-kafka.service.lhr-dataplane.dev.consul
<         - broker: 1
<           host: shared-kafka-broker-1-kafka.service.lhr-dataplane.dev.consul
<         - broker: 2
<           host: shared-kafka-broker-2-kafka.service.lhr-dataplane.dev.consul
<         class: traefik-kafka
---
>         - annotations:
>             ingressWatcher/port: "443"
>             ingressWatcher/service: shared-kafka-dr-broker-0-kafka
>           broker: 0
>           host: shared-kafka-dr-broker-0-kafka.service.lhr-dataplane.dev.consul
>         - annotations:
>             ingressWatcher/port: "443"
>             ingressWatcher/service: shared-kafka-dr-broker-1-kafka
>           broker: 1
>           host: shared-kafka-dr-broker-1-kafka.service.lhr-dataplane.dev.consul
>         - annotations:
>             ingressWatcher/port: "443"
>             ingressWatcher/service: shared-kafka-dr-broker-2-kafka
>           broker: 2
>           host: shared-kafka-dr-broker-2-kafka.service.lhr-dataplane.dev.consul
>         class: kafka
149c125
<         cpu: 5000m
---
>         cpu: 3000m
152c128
<         cpu: 750m
---
>         cpu: 500m
171c147
<         cpu: 1000m
---
>         cpu: 500m
174c150
<         cpu: 500m
---
>         cpu: 125m
185a162
>   clusterId: eG1QgnkISzqsiNYh8fxvqw
187,190c164
<   - lastTransitionTime: "2022-01-27T07:23:51.220Z"
<     message: Exceeded timeout of 300000ms while waiting for Ingress resource shared-kafka-kafka-bootstrap
<       in namespace kafka to be addressable
<     reason: TimeoutException
---
>   - lastTransitionTime: "2022-01-17T17:58:55.052Z"
192c166
<     type: NotReady
---
>     type: Ready
195c169
<     - host: shared-kafka-kafka-bootstrap.kafka.svc
---
>     - host: shared-kafka-dr-kafka-bootstrap.kafka.svc
197c171
<     bootstrapServers: shared-kafka-kafka-bootstrap.kafka.svc:9092
---
>     bootstrapServers: shared-kafka-dr-kafka-bootstrap.kafka.svc:9092
200c174
<     - host: shared-kafka-kafka-bootstrap.kafka.svc
---
>     - host: shared-kafka-dr-kafka-bootstrap.kafka.svc
202c176,215
<     bootstrapServers: shared-kafka-kafka-bootstrap.kafka.svc:9093
---
>     bootstrapServers: shared-kafka-dr-kafka-bootstrap.kafka.svc:9093
>     certificates:
>     - |
>       -----BEGIN CERTIFICATE-----
>       MIIDDDCCArKgAwIBAgIJAMa+4QeM8Ac9MAoGCCqGSM49BAMCMIG5MQswCQYDVQQG
>       EwJVUzELMAkGA1UECBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xGjAYBgNV
>       BAkTETEwMSBTZWNvbmQgU3RyZWV0MQ4wDAYDVQQREwU5NDEwNTEXMBUGA1UEChMO
>       SGFzaGlDb3JwIEluYy4xQDA+BgNVBAMTN0NvbnN1bCBBZ2VudCBDQSAyMjc0MjA4
>       ODE2Njk4ODgyOTczNTU2MjkwMTM2Mzk4MTc0Njk2MDAwHhcNMjAwODMxMjE1MTQ2
>       WhcNMjUwODMwMjE1MTQ2WjBzMQswCQYDVQQGEwJVUzELMAkGA1UECAwCQ0ExFTAT
>       BgNVBAcMDFJlZHdvb2QgQ2l0eTEPMA0GA1UECgwGT3JhY2xlMQ0wCwYDVQQLDARP
>       U3ZDMSAwHgYDVQQDDBdpbnRlcm1lZGlhdGUuZGV2LmNvbnN1bDCCASIwDQYJKoZI
>       hvcNAQEBBQADggEPADCCAQoCggEBAOSSwhmeN5oLjhHmx9TOmBOMjv9wEiRYosEh
>       WzpOOc4h/AFiY9puMw2lLqQaB8dRVszWk7l3Y4+G9fBT0uGrsENuUDjmlFfv4G4I
>       09DAbrplXCbUG9XTS71ul64HpRJ68nkMiHQoFL5CANFuXYdkQ66S3RfdsMYQzssn
>       VI4ubgPMSWhSjQ/ielONXeGV0/vKjkYeTxXwwvAXQ5V72rstVl242Cu3xrj75OOl
>       oqXyl4IG1sbdjdyfkdcq376Fq07V+GWexSWVmYnwYfeuf+FMYa8rLGc6PmpPiuRq
>       GShqzZ9zsplx0CPoDbI9/4Bm19GUu/zOm+UMRtyE+E6mskmCsg0CAwEAAaMdMBsw
>       DAYDVR0TBAUwAwEB/zALBgNVHQ8EBAMCAQYwCgYIKoZIzj0EAwIDSAAwRQIhAMgY
>       e7UnxYAbgTbbLBCGb3CO+b8sXZIGHLn2JpZIER4QAiB85dl1JrjwEKi6ca6nMIgS
>       RTO9M/Qj2L3EJlOysgiZXA==
>       -----END CERTIFICATE-----
>       -----BEGIN CERTIFICATE-----
>       MIIC7jCCApSgAwIBAgIRAKsXqylIx9M/WTbMcgd+HqAwCgYIKoZIzj0EAwIwgbkx
>       CzAJBgNVBAYTAlVTMQswCQYDVQQIEwJDQTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNj
>       bzEaMBgGA1UECRMRMTAxIFNlY29uZCBTdHJlZXQxDjAMBgNVBBETBTk0MTA1MRcw
>       FQYDVQQKEw5IYXNoaUNvcnAgSW5jLjFAMD4GA1UEAxM3Q29uc3VsIEFnZW50IENB
>       IDIyNzQyMDg4MTY2OTg4ODI5NzM1NTYyOTAxMzYzOTgxNzQ2OTYwMDAeFw0yMDA4
>       MjMyMzQ1MTFaFw0yNTA4MjIyMzQ1MTFaMIG5MQswCQYDVQQGEwJVUzELMAkGA1UE
>       CBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xGjAYBgNVBAkTETEwMSBTZWNv
>       bmQgU3RyZWV0MQ4wDAYDVQQREwU5NDEwNTEXMBUGA1UEChMOSGFzaGlDb3JwIElu
>       Yy4xQDA+BgNVBAMTN0NvbnN1bCBBZ2VudCBDQSAyMjc0MjA4ODE2Njk4ODgyOTcz
>       NTU2MjkwMTM2Mzk4MTc0Njk2MDAwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAASE
>       aKwNx8k0+casMPt/qkEa4ajItFqtFL22llPlX5AgnXUxeJKC3fvusEHptrpWPXoo
>       Ke5H5n8U6y4jmg7/O2Buo3sweTAOBgNVHQ8BAf8EBAMCAYYwDwYDVR0TAQH/BAUw
>       AwEB/zApBgNVHQ4EIgQgFWS9tm0ScTJrTeowqm+JAtfink4/xvZn2brSMzZZEnsw
>       KwYDVR0jBCQwIoAgFWS9tm0ScTJrTeowqm+JAtfink4/xvZn2brSMzZZEnswCgYI
>       KoZIzj0EAwIDSAAwRQIhAM3dMq1X3vzKItfir+aXfmvk74BkLW3s7PRDfrSD0ljU
>       AiB1jfrlC6JY/Y5rbP9gda0da7iOTp7OXc/pxWYVrerjRw==
>       -----END CERTIFICATE-----
204c217,261
<   observedGeneration: 7
---
>   - addresses:
>     - host: shared-kafka-dr-bootstrap-kafka.service.lhr-dataplane.dev.consul
>       port: 443
>     bootstrapServers: shared-kafka-dr-bootstrap-kafka.service.lhr-dataplane.dev.consul:443
>     certificates:
>     - |
>       -----BEGIN CERTIFICATE-----
>       MIIDDDCCArKgAwIBAgIJAMa+4QeM8Ac9MAoGCCqGSM49BAMCMIG5MQswCQYDVQQG
>       EwJVUzELMAkGA1UECBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xGjAYBgNV
>       BAkTETEwMSBTZWNvbmQgU3RyZWV0MQ4wDAYDVQQREwU5NDEwNTEXMBUGA1UEChMO
>       SGFzaGlDb3JwIEluYy4xQDA+BgNVBAMTN0NvbnN1bCBBZ2VudCBDQSAyMjc0MjA4
>       ODE2Njk4ODgyOTczNTU2MjkwMTM2Mzk4MTc0Njk2MDAwHhcNMjAwODMxMjE1MTQ2
>       WhcNMjUwODMwMjE1MTQ2WjBzMQswCQYDVQQGEwJVUzELMAkGA1UECAwCQ0ExFTAT
>       BgNVBAcMDFJlZHdvb2QgQ2l0eTEPMA0GA1UECgwGT3JhY2xlMQ0wCwYDVQQLDARP
>       U3ZDMSAwHgYDVQQDDBdpbnRlcm1lZGlhdGUuZGV2LmNvbnN1bDCCASIwDQYJKoZI
>       hvcNAQEBBQADggEPADCCAQoCggEBAOSSwhmeN5oLjhHmx9TOmBOMjv9wEiRYosEh
>       WzpOOc4h/AFiY9puMw2lLqQaB8dRVszWk7l3Y4+G9fBT0uGrsENuUDjmlFfv4G4I
>       09DAbrplXCbUG9XTS71ul64HpRJ68nkMiHQoFL5CANFuXYdkQ66S3RfdsMYQzssn
>       VI4ubgPMSWhSjQ/ielONXeGV0/vKjkYeTxXwwvAXQ5V72rstVl242Cu3xrj75OOl
>       oqXyl4IG1sbdjdyfkdcq376Fq07V+GWexSWVmYnwYfeuf+FMYa8rLGc6PmpPiuRq
>       GShqzZ9zsplx0CPoDbI9/4Bm19GUu/zOm+UMRtyE+E6mskmCsg0CAwEAAaMdMBsw
>       DAYDVR0TBAUwAwEB/zALBgNVHQ8EBAMCAQYwCgYIKoZIzj0EAwIDSAAwRQIhAMgY
>       e7UnxYAbgTbbLBCGb3CO+b8sXZIGHLn2JpZIER4QAiB85dl1JrjwEKi6ca6nMIgS
>       RTO9M/Qj2L3EJlOysgiZXA==
>       -----END CERTIFICATE-----
>       -----BEGIN CERTIFICATE-----
>       MIIC7jCCApSgAwIBAgIRAKsXqylIx9M/WTbMcgd+HqAwCgYIKoZIzj0EAwIwgbkx
>       CzAJBgNVBAYTAlVTMQswCQYDVQQIEwJDQTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNj
>       bzEaMBgGA1UECRMRMTAxIFNlY29uZCBTdHJlZXQxDjAMBgNVBBETBTk0MTA1MRcw
>       FQYDVQQKEw5IYXNoaUNvcnAgSW5jLjFAMD4GA1UEAxM3Q29uc3VsIEFnZW50IENB
>       IDIyNzQyMDg4MTY2OTg4ODI5NzM1NTYyOTAxMzYzOTgxNzQ2OTYwMDAeFw0yMDA4
>       MjMyMzQ1MTFaFw0yNTA4MjIyMzQ1MTFaMIG5MQswCQYDVQQGEwJVUzELMAkGA1UE
>       CBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xGjAYBgNVBAkTETEwMSBTZWNv
>       bmQgU3RyZWV0MQ4wDAYDVQQREwU5NDEwNTEXMBUGA1UEChMOSGFzaGlDb3JwIElu
>       Yy4xQDA+BgNVBAMTN0NvbnN1bCBBZ2VudCBDQSAyMjc0MjA4ODE2Njk4ODgyOTcz
>       NTU2MjkwMTM2Mzk4MTc0Njk2MDAwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAASE
>       aKwNx8k0+casMPt/qkEa4ajItFqtFL22llPlX5AgnXUxeJKC3fvusEHptrpWPXoo
>       Ke5H5n8U6y4jmg7/O2Buo3sweTAOBgNVHQ8BAf8EBAMCAYYwDwYDVR0TAQH/BAUw
>       AwEB/zApBgNVHQ4EIgQgFWS9tm0ScTJrTeowqm+JAtfink4/xvZn2brSMzZZEnsw
>       KwYDVR0jBCQwIoAgFWS9tm0ScTJrTeowqm+JAtfink4/xvZn2brSMzZZEnswCgYI
>       KoZIzj0EAwIDSAAwRQIhAM3dMq1X3vzKItfir+aXfmvk74BkLW3s7PRDfrSD0ljU
>       AiB1jfrlC6JY/Y5rbP9gda0da7iOTp7OXc/pxWYVrerjRw==
>       -----END CERTIFICATE-----
>     type: external
>   observedGeneration: 5
[27:January:2022:08:28:54]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ ls -ltr
total 34556
-r-------- 1 opc opc     2358 Jan 10 05:02 kubeconfig_context-c5ppxaonnda
-rw-r--r-- 1 opc opc     4455 Jan 13 00:06 traefik-6ff7ff8ffd-cxgdn2022_01_12_11_33_PM.log
-rw-r--r-- 1 opc opc     1006 Jan 13 00:11 traefik.service
-rw-r--r-- 1 opc opc     7670 Jan 13 00:13 kf-kafka.kafka.yaml
-rw-r--r-- 1 opc opc    12389 Jan 13 00:15 corp.shared-kafka.kafka.yaml
-rw-r--r-- 1 opc opc      420 Jan 13 04:44 kf-kafka-kafka-0-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc      452 Jan 13 04:45 kf-kafka-kafka-bootstrap-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc      420 Jan 13 04:45 kf-kafka-kafka-1-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc      420 Jan 13 04:45 kf-kafka-kafka-2-ingressroutetcp.yaml
-rw-r--r-- 1 opc opc     3186 Jan 13 05:05 kf-kafka.kafka_updated.yaml
-rw-r--r-- 1 opc opc     1358 Jan 13 08:12 kf-kafka-kafka-tlsexternal-1.ingress.yaml
-rw-r--r-- 1 opc opc     1331 Jan 13 08:24 kf-kafka-kafka-tlsexternal-0.ingress.yaml
-rw-r--r-- 1 opc opc     3782 Jan 13 08:34 kf-kafka-kafka_updated2.yaml
-rw-r--r-- 1 opc opc     2197 Jan 13 11:55 ca.crt
-rw-r--r-- 1 opc opc     2897 Jan 13 13:31 traefik-test_values.yaml
-rw-r--r-- 1 opc opc     1334 Jan 13 14:26 kf-kafka-kafka-0.ingress.1.yaml
-rw-r--r-- 1 opc opc     1334 Jan 13 14:26 kf-kafka-kafka-1.ingress.1.yaml
-rw-r--r-- 1 opc opc     1334 Jan 13 14:26 kf-kafka-kafka-2.ingress.1.yaml
-rw-r--r-- 1 opc opc     1383 Jan 13 14:26 kf-kafka-kafka-bootstrap.ingress.1.yaml
-rw-r--r-- 1 opc opc     3800 Jan 14 04:15 kf-kafka.cluster.kind.14jan22.yaml
-rw-r--r-- 1 opc opc     8904 Jan 14 04:42 kf-kafka.kafka.WORKING.14Jan22.yaml
-rw-r--r-- 1 opc opc     1379 Jan 14 04:53 kf-kafka-kafka-0.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     1379 Jan 14 04:53 kf-kafka-kafka-1.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     1379 Jan 14 04:53 kf-kafka-kafka-2.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     1428 Jan 14 04:53 kf-kafka-kafka-bootstrap.ingress.LB-14jan22.yaml
-rw-r--r-- 1 opc opc     8904 Jan 17 04:23 kf-kafka.kafka.WORKING.17Jan22.yaml
-rw-r--r-- 1 opc opc  8656381 Jan 17 11:10 traefik-kafka-controller-5f9f755d5-j6nzw2022_01_12_11_33_PM.log
-rw-r--r-- 1 opc opc      238 Jan 17 16:55 traefik-web-ui_svc.yaml
-rw-r--r-- 1 opc opc      269 Jan 17 16:57 traefik-web-ui_ingress.yaml
-rw------- 1 opc opc     5884 Jan 18 00:08 kubeconfig_minikube_18jan2022
-rw-r--r-- 1 opc opc   211050 Jan 18 01:18 Traefik_dashboard.png
-rw-r--r-- 1 opc opc   106471 Jan 18 01:18 Traefik_serviceAdminPort.png
drwxr-xr-x 9 opc opc      288 Jan 18 01:20 traefik-setup
-rw-r--r-- 1 opc opc     8684 Jan 19 07:15 kf-kafka.kafka.19jan2022.yaml
-rw-r--r-- 1 opc opc    12498 Jan 19 07:16 kgn.kafka.services.19jan2022.yaml
-rw-r--r-- 1 opc opc     4534 Jan 19 07:17 kgn.kafka.ingressroutetcp.19jan2022.yaml
-rw-r--r-- 1 opc opc     5857 Jan 19 07:19 kgn.kafka.ingress.19jan2022.yaml
-rw-r--r-- 1 opc opc 14267697 Jan 20 07:26 traefik-kafka-controller.log
-rw-r--r-- 1 opc opc        0 Jan 20 07:27 kf-kafka-entity-operator-6dfd8c8649-95dc72022_01_19_07_25_AM.log
-rw-r--r-- 1 opc opc  5581954 Jan 20 07:29 kf-kafka-entity-operator-6dfd8c8649-95dc7_topicoperator2022_01_19_07_25_AM.log
-rw-r--r-- 1 opc opc    67865 Jan 20 07:31 strimzi-cluster-operator-6fb59d59cb-zdd6k2022_01_19_07_25_AM.log
-rw-r--r-- 1 opc opc  3450078 Jan 27 01:08 ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log
-rw-r--r-- 1 opc opc     9746 Jan 27 01:49 kafka.shared-kafka-dr.kafka-dev-lhr.yaml
-rw-r--r-- 1 opc opc     9229 Jan 27 01:53 shared-kafka-dr.kafka-dev-lhr-Updated.yaml
-rw-r--r-- 1 opc opc    22895 Jan 27 07:03 shared-kafka-dev-lhr-ingress-nginx_27jan22.yaml
-rw-r--r-- 1 opc opc    32378 Jan 27 07:05 shared-kafka-dev-lhr-services_27jan22.yaml
-rw-r--r-- 1 opc opc     9915 Jan 27 07:05 shared-kafka-dev-lhr-kafkacluster_27jan22.yaml
-rw-r--r-- 1 opc opc     9229 Jan 27 07:05 shared-kafka-dr-dev-lhr-kafkacluster_27jan22.yaml
-rw-r--r-- 1 opc opc     9923 Jan 27 07:07 shared-kafka-dev-lhr-kafkacluster-classTraefik_27jan22.yaml
-rw-r--r-- 1 opc opc     7672 Jan 27 07:15 kf-kafka-cluster-OKE-27jan22.yaml
-rw-r--r-- 1 opc opc    22783 Jan 27 07:53 shared-kafka-dev-lhr-ingress-TRAEFIK-KAFKA_27jan22.yaml
-rw-r--r-- 1 opc opc     4966 Jan 27 08:25 TRAEFIK-KAFKA_27jan22-shared-kafka-dev-lhr.yaml
-rw-r--r-- 1 opc opc     9915 Jan 27 08:31 kafka.shared-kafka.kafka-dev-lhr_27jan22.yaml
[27:January:2022:08:32:00]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ cat kafka.shared-kafka.kafka-dev-lhr_27jan22.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  annotations:
    meta.helm.sh/release-name: kafka-shared
    meta.helm.sh/release-namespace: kafka
  creationTimestamp: "2021-12-03T23:52:15Z"
  generation: 8
  labels:
    app: cpekafka
    app.kubernetes.io/managed-by: Helm
    chart: cpe-kafka-0.2.6
    cluster_name: shared-kafka-lhr
    heritage: Helm
    release: kafka-shared
  name: shared-kafka
  namespace: kafka
  resourceVersion: "268976098"
  selfLink: /apis/kafka.strimzi.io/v1beta2/namespaces/kafka/kafkas/shared-kafka
  uid: 6b3e23f6-d322-412c-b13e-90a944dd001d
spec:
  clientsCa:
    generateCertificateAuthority: false
  clusterCa:
    generateCertificateAuthority: false
  entityOperator:
    template:
      pod:
        metadata:
          labels:
            stage: dataplane
    tlsSidecar:
      livenessProbe: {}
      readinessProbe: {}
      resources:
        limits:
          cpu: 500m
          memory: 1500M
        requests:
          cpu: 100m
          memory: 750M
    topicOperator:
      livenessProbe: {}
      readinessProbe: {}
      resources:
        limits:
          cpu: 1000m
          memory: 1500M
        requests:
          cpu: 500m
          memory: 750M
    userOperator:
      livenessProbe: {}
      readinessProbe: {}
      resources:
        limits:
          cpu: 1000m
          memory: 1500M
        requests:
          cpu: 500m
          memory: 750M
  kafka:
    authorization:
      superUsers:
      - CN=mirrormaker
      - CN=mirrormaker-lhr
      - CN=mirrormaker-dr-lhr
      type: simple
    config:
      auto.create.topics.enable: false
      default.replication.factor: 3
      num.partitions: 3
      offsets.topic.replication.factor: 3
      transaction.state.log.min.isr: 2
      transaction.state.log.replication.factor: 3
    jvmOptions:
      -Xms: 8000m
      -Xmx: 8000m
    listeners:
    - name: plain
      port: 9092
      tls: false
      type: internal
    - authentication:
        type: tls
      name: tls
      port: 9093
      tls: true
      type: internal
    - authentication:
        type: tls
      configuration:
        bootstrap:
          host: shared-kafka-bootstrap-kafka.service.lhr-dataplane.dev.consul
        brokers:
        - broker: 0
          host: shared-kafka-broker-0-kafka.service.lhr-dataplane.dev.consul
        - broker: 1
          host: shared-kafka-broker-1-kafka.service.lhr-dataplane.dev.consul
        - broker: 2
          host: shared-kafka-broker-2-kafka.service.lhr-dataplane.dev.consul
        class: kafka
      name: external
      port: 9094
      tls: true
      type: ingress
    livenessProbe: {}
    metricsConfig:
      type: jmxPrometheusExporter
      valueFrom:
        configMapKeyRef:
          key: shared-kafka-kafka-metrics-config.yaml
          name: shared-kafka-kafka-metrics
    rack:
      topologyKey: failure-domain.beta.kubernetes.io/zone
    readinessProbe: {}
    replicas: 3
    resources:
      limits:
        cpu: "4"
        memory: 32Gi
      requests:
        cpu: "2"
        memory: 16Gi
    storage:
      type: jbod
      volumes:
      - class: oci-bv
        id: 0
        size: 100Gi
        type: persistent-claim
    template:
      pod:
        metadata:
          labels:
            stage: dataplane
    version: 2.8.0
  kafkaExporter:
    livenessProbe:
      initialDelaySeconds: 300
      periodSeconds: 60
      timeoutSeconds: 600
    readinessProbe:
      initialDelaySeconds: 300
      periodSeconds: 60
      timeoutSeconds: 600
    resources:
      limits:
        cpu: 5000m
        memory: 8192M
      requests:
        cpu: 750m
        memory: 750M
    template:
      pod:
        metadata:
          labels:
            stage: dataplane
  zookeeper:
    livenessProbe: {}
    metricsConfig:
      type: jmxPrometheusExporter
      valueFrom:
        configMapKeyRef:
          key: shared-kafka-zookeeper-metrics-config.yaml
          name: shared-kafka-zookeeper-metrics
    readinessProbe: {}
    replicas: 3
    resources:
      limits:
        cpu: 1000m
        memory: 8Gi
      requests:
        cpu: 500m
        memory: 4Gi
    storage:
      class: oci-bv
      size: 50Gi
      type: persistent-claim
    template:
      pod:
        metadata:
          labels:
            stage: dataplane
status:
  clusterId: aiQPACGBTSK_VbxruHNXNw
  conditions:
  - lastTransitionTime: "2022-01-27T08:28:55.185Z"
    status: "True"
    type: Ready
  listeners:
  - addresses:
    - host: shared-kafka-kafka-bootstrap.kafka.svc
      port: 9092
    bootstrapServers: shared-kafka-kafka-bootstrap.kafka.svc:9092
    type: plain
  - addresses:
    - host: shared-kafka-kafka-bootstrap.kafka.svc
      port: 9093
    bootstrapServers: shared-kafka-kafka-bootstrap.kafka.svc:9093
    certificates:
    - |
      -----BEGIN CERTIFICATE-----
      MIIDDDCCArKgAwIBAgIJAMa+4QeM8Ac9MAoGCCqGSM49BAMCMIG5MQswCQYDVQQG
      EwJVUzELMAkGA1UECBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xGjAYBgNV
      BAkTETEwMSBTZWNvbmQgU3RyZWV0MQ4wDAYDVQQREwU5NDEwNTEXMBUGA1UEChMO
      SGFzaGlDb3JwIEluYy4xQDA+BgNVBAMTN0NvbnN1bCBBZ2VudCBDQSAyMjc0MjA4
      ODE2Njk4ODgyOTczNTU2MjkwMTM2Mzk4MTc0Njk2MDAwHhcNMjAwODMxMjE1MTQ2
      WhcNMjUwODMwMjE1MTQ2WjBzMQswCQYDVQQGEwJVUzELMAkGA1UECAwCQ0ExFTAT
      BgNVBAcMDFJlZHdvb2QgQ2l0eTEPMA0GA1UECgwGT3JhY2xlMQ0wCwYDVQQLDARP
      U3ZDMSAwHgYDVQQDDBdpbnRlcm1lZGlhdGUuZGV2LmNvbnN1bDCCASIwDQYJKoZI
      hvcNAQEBBQADggEPADCCAQoCggEBAOSSwhmeN5oLjhHmx9TOmBOMjv9wEiRYosEh
      WzpOOc4h/AFiY9puMw2lLqQaB8dRVszWk7l3Y4+G9fBT0uGrsENuUDjmlFfv4G4I
      09DAbrplXCbUG9XTS71ul64HpRJ68nkMiHQoFL5CANFuXYdkQ66S3RfdsMYQzssn
      VI4ubgPMSWhSjQ/ielONXeGV0/vKjkYeTxXwwvAXQ5V72rstVl242Cu3xrj75OOl
      oqXyl4IG1sbdjdyfkdcq376Fq07V+GWexSWVmYnwYfeuf+FMYa8rLGc6PmpPiuRq
      GShqzZ9zsplx0CPoDbI9/4Bm19GUu/zOm+UMRtyE+E6mskmCsg0CAwEAAaMdMBsw
      DAYDVR0TBAUwAwEB/zALBgNVHQ8EBAMCAQYwCgYIKoZIzj0EAwIDSAAwRQIhAMgY
      e7UnxYAbgTbbLBCGb3CO+b8sXZIGHLn2JpZIER4QAiB85dl1JrjwEKi6ca6nMIgS
      RTO9M/Qj2L3EJlOysgiZXA==
      -----END CERTIFICATE-----
      -----BEGIN CERTIFICATE-----
      MIIC7jCCApSgAwIBAgIRAKsXqylIx9M/WTbMcgd+HqAwCgYIKoZIzj0EAwIwgbkx
      CzAJBgNVBAYTAlVTMQswCQYDVQQIEwJDQTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNj
      bzEaMBgGA1UECRMRMTAxIFNlY29uZCBTdHJlZXQxDjAMBgNVBBETBTk0MTA1MRcw
      FQYDVQQKEw5IYXNoaUNvcnAgSW5jLjFAMD4GA1UEAxM3Q29uc3VsIEFnZW50IENB
      IDIyNzQyMDg4MTY2OTg4ODI5NzM1NTYyOTAxMzYzOTgxNzQ2OTYwMDAeFw0yMDA4
      MjMyMzQ1MTFaFw0yNTA4MjIyMzQ1MTFaMIG5MQswCQYDVQQGEwJVUzELMAkGA1UE
      CBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xGjAYBgNVBAkTETEwMSBTZWNv
      bmQgU3RyZWV0MQ4wDAYDVQQREwU5NDEwNTEXMBUGA1UEChMOSGFzaGlDb3JwIElu
      Yy4xQDA+BgNVBAMTN0NvbnN1bCBBZ2VudCBDQSAyMjc0MjA4ODE2Njk4ODgyOTcz
      NTU2MjkwMTM2Mzk4MTc0Njk2MDAwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAASE
      aKwNx8k0+casMPt/qkEa4ajItFqtFL22llPlX5AgnXUxeJKC3fvusEHptrpWPXoo
      Ke5H5n8U6y4jmg7/O2Buo3sweTAOBgNVHQ8BAf8EBAMCAYYwDwYDVR0TAQH/BAUw
      AwEB/zApBgNVHQ4EIgQgFWS9tm0ScTJrTeowqm+JAtfink4/xvZn2brSMzZZEnsw
      KwYDVR0jBCQwIoAgFWS9tm0ScTJrTeowqm+JAtfink4/xvZn2brSMzZZEnswCgYI
      KoZIzj0EAwIDSAAwRQIhAM3dMq1X3vzKItfir+aXfmvk74BkLW3s7PRDfrSD0ljU
      AiB1jfrlC6JY/Y5rbP9gda0da7iOTp7OXc/pxWYVrerjRw==
      -----END CERTIFICATE-----
    type: tls
  - addresses:
    - host: shared-kafka-bootstrap-kafka.service.lhr-dataplane.dev.consul
      port: 443
    bootstrapServers: shared-kafka-bootstrap-kafka.service.lhr-dataplane.dev.consul:443
    certificates:
    - |
      -----BEGIN CERTIFICATE-----
      MIIDDDCCArKgAwIBAgIJAMa+4QeM8Ac9MAoGCCqGSM49BAMCMIG5MQswCQYDVQQG
      EwJVUzELMAkGA1UECBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xGjAYBgNV
      BAkTETEwMSBTZWNvbmQgU3RyZWV0MQ4wDAYDVQQREwU5NDEwNTEXMBUGA1UEChMO
      SGFzaGlDb3JwIEluYy4xQDA+BgNVBAMTN0NvbnN1bCBBZ2VudCBDQSAyMjc0MjA4
      ODE2Njk4ODgyOTczNTU2MjkwMTM2Mzk4MTc0Njk2MDAwHhcNMjAwODMxMjE1MTQ2
      WhcNMjUwODMwMjE1MTQ2WjBzMQswCQYDVQQGEwJVUzELMAkGA1UECAwCQ0ExFTAT
      BgNVBAcMDFJlZHdvb2QgQ2l0eTEPMA0GA1UECgwGT3JhY2xlMQ0wCwYDVQQLDARP
      U3ZDMSAwHgYDVQQDDBdpbnRlcm1lZGlhdGUuZGV2LmNvbnN1bDCCASIwDQYJKoZI
      hvcNAQEBBQADggEPADCCAQoCggEBAOSSwhmeN5oLjhHmx9TOmBOMjv9wEiRYosEh
      WzpOOc4h/AFiY9puMw2lLqQaB8dRVszWk7l3Y4+G9fBT0uGrsENuUDjmlFfv4G4I
      09DAbrplXCbUG9XTS71ul64HpRJ68nkMiHQoFL5CANFuXYdkQ66S3RfdsMYQzssn
      VI4ubgPMSWhSjQ/ielONXeGV0/vKjkYeTxXwwvAXQ5V72rstVl242Cu3xrj75OOl
      oqXyl4IG1sbdjdyfkdcq376Fq07V+GWexSWVmYnwYfeuf+FMYa8rLGc6PmpPiuRq
      GShqzZ9zsplx0CPoDbI9/4Bm19GUu/zOm+UMRtyE+E6mskmCsg0CAwEAAaMdMBsw
      DAYDVR0TBAUwAwEB/zALBgNVHQ8EBAMCAQYwCgYIKoZIzj0EAwIDSAAwRQIhAMgY
      e7UnxYAbgTbbLBCGb3CO+b8sXZIGHLn2JpZIER4QAiB85dl1JrjwEKi6ca6nMIgS
      RTO9M/Qj2L3EJlOysgiZXA==
      -----END CERTIFICATE-----
      -----BEGIN CERTIFICATE-----
      MIIC7jCCApSgAwIBAgIRAKsXqylIx9M/WTbMcgd+HqAwCgYIKoZIzj0EAwIwgbkx
      CzAJBgNVBAYTAlVTMQswCQYDVQQIEwJDQTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNj
      bzEaMBgGA1UECRMRMTAxIFNlY29uZCBTdHJlZXQxDjAMBgNVBBETBTk0MTA1MRcw
      FQYDVQQKEw5IYXNoaUNvcnAgSW5jLjFAMD4GA1UEAxM3Q29uc3VsIEFnZW50IENB
      IDIyNzQyMDg4MTY2OTg4ODI5NzM1NTYyOTAxMzYzOTgxNzQ2OTYwMDAeFw0yMDA4
      MjMyMzQ1MTFaFw0yNTA4MjIyMzQ1MTFaMIG5MQswCQYDVQQGEwJVUzELMAkGA1UE
      CBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xGjAYBgNVBAkTETEwMSBTZWNv
      bmQgU3RyZWV0MQ4wDAYDVQQREwU5NDEwNTEXMBUGA1UEChMOSGFzaGlDb3JwIElu
      Yy4xQDA+BgNVBAMTN0NvbnN1bCBBZ2VudCBDQSAyMjc0MjA4ODE2Njk4ODgyOTcz
      NTU2MjkwMTM2Mzk4MTc0Njk2MDAwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAASE
      aKwNx8k0+casMPt/qkEa4ajItFqtFL22llPlX5AgnXUxeJKC3fvusEHptrpWPXoo
      Ke5H5n8U6y4jmg7/O2Buo3sweTAOBgNVHQ8BAf8EBAMCAYYwDwYDVR0TAQH/BAUw
      AwEB/zApBgNVHQ4EIgQgFWS9tm0ScTJrTeowqm+JAtfink4/xvZn2brSMzZZEnsw
      KwYDVR0jBCQwIoAgFWS9tm0ScTJrTeowqm+JAtfink4/xvZn2brSMzZZEnswCgYI
      KoZIzj0EAwIDSAAwRQIhAM3dMq1X3vzKItfir+aXfmvk74BkLW3s7PRDfrSD0ljU
      AiB1jfrlC6JY/Y5rbP9gda0da7iOTp7OXc/pxWYVrerjRw==
      -----END CERTIFICATE-----
    type: external
  observedGeneration: 8
[27:January:2022:08:32:04]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $
[27:January:2022:10:50:52]:(dev_eu-frankfurt-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ v2corp
[27:January:2022:10:50:55]:(corp_us-ashburn-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ kgc
CURRENT   NAME                             CLUSTER               AUTHINFO           NAMESPACE
*         corp_us-ashburn-1_controlplane   cluster-c3wgzdgmfqt   user-c3wgzdgmfqt
          corp_us-ashburn-1_dataplane      cluster-c2tqztfmy3t   user-c2tqztfmy3t
          corp_us-ashburn-1_deployment     cluster-crdcyjvmztg   user-crdcyjvmztg
[27:January:2022:10:50:58]:(corp_us-ashburn-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ kuc corp_us-ashburn-1_dataplane
Switched to context "corp_us-ashburn-1_dataplane".
[27:January:2022:10:51:02]:(corp_us-ashburn-1_controlplane):~/galorndon/ctemp/traefik-test
○ $ rp
[27:January:2022:10:51:03]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ kgns
NAME                       STATUS   AGE
abs                        Active   198d
abs-ci                     Active   189d
abs-sandbox1               Active   189d
abs-sandbox2               Active   189d
abs-sandbox3               Active   189d
acs2                       Active   428d
acs2-ci                    Active   413d
acs2-dev1                  Active   408d
acs2-dev2                  Active   408d
acs2-dev3                  Active   408d
acs2-e2e                   Active   408d
acs2-e2e-ci                Active   189d
acs2-psr                   Active   408d
acs2-qa                    Active   408d
acs2-xo-dev1               Active   156d
acs2-xo-qa1                Active   155d
agora                      Active   407d
agora-ci                   Active   344d
agora-psr                  Active   111d
ai-apps                    Active   356d
ai-apps-dev                Active   356d
ai-apps-dev-mp             Active   51d
ai-apps-sandbox1           Active   42d
analytics                  Active   419d
analytics-ci               Active   412d
analytics-e2e              Active   412d
analytics-psr              Active   412d
analytics-qa               Active   246d
analytics-sandbox1         Active   412d
analytics-sandbox2         Active   412d
analytics-tvm-ci           Active   125d
analytics-tvm-sandbox1     Active   342d
analytics-tvm-sandbox2     Active   342d
analytics-tvm-sandbox3     Active   342d
as-accelxo                 Active   301d
as-accelxo1                Active   252d
as-accelxo2                Active   159d
as-aicxservice             Active   6d
as-analytics               Active   400d
as-analytics2              Active   314d
as-atv                     Active   317d
as-atv2                    Active   314d
as-bcoidc                  Active   317d
as-bcoidc2                 Active   317d
as-bcoidc3                 Active   265d
as-bcous                   Active   317d
as-bcous2                  Active   317d
as-bcous3                  Active   265d
as-buikf1                  Active   260d
as-buikf2                  Active   250d
as-buikf3                  Active   107d
as-buikf4                  Active   76d
as-businessrules           Active   296d
as-cccchatidc              Active   275d
as-cccchatidc2             Active   268d
as-ccp                     Active   400d
as-ccp2                    Active   323d
as-ccp3                    Active   13d
as-ccpidc                  Active   307d
as-ccpidc2                 Active   307d
as-ccptvm                  Active   325d
as-ccptvm1                 Active   294d
as-ccptvm2                 Active   293d
as-chat                    Active   400d
as-chat2                   Active   268d
as-chat3                   Active   267d
as-ciat                    Active   149d
as-coreserveridc           Active   260d
as-coreserveridc2          Active   232d
as-cxsets                  Active   272d
as-cxsets2                 Active   272d
as-dcs                     Active   293d
as-dock                    Active   414d
as-e2eqa                   Active   239d
as-ee                      Active   273d
as-jcs                     Active   58d
as-okcsapps                Active   268d
as-okcsapps2               Active   250d
as-petvm                   Active   295d
as-pit1                    Active   274d
as-pit2                    Active   274d
as-psr                     Active   40d
as-saas1                   Active   250d
as-saas2                   Active   224d
as-saas3                   Active   224d
as-seof                    Active   246d
as-seof2                   Active   310d
as-socialmonitor           Active   307d
as-stable                  Active   364d
as-workspaces              Active   400d
as-workspaces2             Active   323d
as-workspacestvm           Active   320d
as-workspacestvm2          Active   320d
as-xotvm1                  Active   295d
bui                        Active   442d
bui-psr                    Active   404d
casper                     Active   308d
casper-sandbox1            Active   282d
cns                        Active   136d
coe                        Active   295d
consul                     Active   452d
cxemailservice             Active   41d
cxservice                  Active   79d
cxu-integration            Active   246d
cxu-integration-ci         Active   246d
cxu-integration-sandbox1   Active   246d
cxu-integration-sandbox2   Active   246d
cxu-integration-sandbox3   Active   246d
data-pipeline              Active   401d
data-pipeline-ci           Active   401d
data-pipeline-dev1         Active   401d
data-pipeline-dev2         Active   401d
data-pipeline-dev3         Active   362d
data-pipeline-dev4         Active   362d
data-pipeline-dev5         Active   247d
data-pipeline-e2e          Active   401d
data-pipeline-precorp      Active   104d
data-pipeline-psr          Active   401d
data-pipeline-qa           Active   401d
dcs                        Active   348d
dcs-integration            Active   178d
default                    Active   463d
dev                        Active   315d
dlm                        Active   36d
dlm-ci                     Active   16d
dlm-sandbox1               Active   10d
dlm-sandbox2               Active   10d
escalation                 Active   323d
escalation-ci              Active   323d
escalation-sandbox1        Active   323d
example                    Active   246d
example-sandbox1           Active   294d
extensions                 Active   245d
extensions-qa              Active   245d
fa-service-psr             Active   63d
frontend                   Active   462d
gatekeeper                 Active   156d
helios                     Active   414d
helios-ci                  Active   338d
helios-psr                 Active   352d
helios-qa                  Active   245d
helios-qa2                 Active   209d
helios-sandbox1            Active   338d
helios-sandbox2            Active   338d
helios-sandbox3            Active   338d
helios-sandbox4            Active   187d
helios-sandbox5            Active   15d
helios-sandbox6            Active   9d
helmjobs                   Active   419d
hms-sync                   Active   335d
ia                         Active   301d
ia-ci                      Active   63d
ia-sandbox1                Active   63d
ia-sandbox2                Active   63d
ia-sandbox3                Active   59d
infra-monitoring           Active   452d
ingress-watcher            Active   462d
iris                       Active   36d
janus                      Active   176d
kafka                      Active   452d
kfkafka                    Active   10d
kube-node-lease            Active   463d
kube-public                Active   463d
kube-system                Active   463d
kweet-develop              Active   331d
lx-cib01                   Active   58d
ma-global                  Active   89d
mercury                    Active   406d
mercury-cert               Active   274d
mercury-chate2e            Active   338d
mercury-dev                Active   329d
mercury-pint               Active   11h
mercury-psr                Active   315d
monitoring-agent           Active   404d
notification-dev           Active   96d
opaec                      Active   428d
opaec-ci                   Active   252d
opaec-psr                  Active   335d
opaec-sandbox1             Active   196d
osvcprovisioner            Active   69d
pattest                    Active   209d
ph-namespace-test          Active   280d
psr                        Active   82d
pvc-watcher                Active   211d
redis                      Active   72d
s2s-idcs                   Active   98d
sdc                        Active   363d
sdc-dev1                   Active   280d
sdc-dev2                   Active   280d
sdc-dev3                   Active   280d
sdc-qa                     Active   280d
search                     Active   411d
search-ci                  Active   411d
search-dev1                Active   280d
search-dev2                Active   280d
search-dev3                Active   280d
search-test1               Active   189d
search-test2               Active   189d
search-test3               Active   191d
shared-kafka-pint          Active   56d
sitedashboard              Active   419d
sitekafka                  Active   411d
skylight                   Active   16d
sonarqube                  Active   40d
ssapp                      Active   338d
surveys                    Active   328d
surveys-ci                 Active   296d
surveys-sandbox1           Active   296d
telekafka                  Active   98d
testkafka                  Active   310d
testnsdash                 Active   404d
tms                        Active   405d
tms-dev                    Active   317d
traefik-apps               Active   105d
traefik-frontend           Active   36d
vault                      Active   462d
visitorservice             Active   414d
visitorservice-ci          Active   187d
visitorservice-psr         Active   352d
xo                         Active   419d
xo-ci                      Active   310d
xo-ci-branch               Active   258d
xo-dev1                    Active   310d
xo-dev2                    Active   160d
xo-e2e                     Active   310d
xo-psr                     Active   310d
xo-qa                      Active   313d
xo-qa1                     Active   236d
xo-qa2                     Active   159d
xo-sandbox1                Active   310d
xo-sandbox2                Active   258d
xo-tvm-sandbox1            Active   309d
[27:January:2022:10:51:14]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ kgpn cxemailservice
NAME                                READY   STATUS    RESTARTS   AGE     IP              NODE          NOMINATED NODE   READINESS GATES
emailapp-86c7f7bd7d-bt4qk           1/1     Running   0          2d19h   10.245.40.140   10.11.9.161   <none>           <none>
emailapp-prashit-6c66fb9c97-4t75q   1/1     Running   0          2d4h    10.245.45.153   10.11.9.214   <none>           <none>
emailapp-rambat-1-9bd4d58db-rbh4z   1/1     Running   0          4h40m   10.245.35.144   10.11.9.177   <none>           <none>
emailapp-rambat-58494b474b-ddt8x    1/1     Running   0          5h11m   10.245.12.45    10.11.8.226   <none>           <none>
emailapp-wentwei-777966bcc6-2hs2s   1/1     Running   0          7d1h    10.245.32.164   10.11.9.7     <none>           <none>
emailappabp-855bbdffb4-sd2lx        1/1     Running   0          53m     10.245.16.202   10.11.9.248   <none>           <none>
emailappashossin-584c579d4-vskwx    1/1     Running   0          3h30m   10.245.35.153   10.11.9.177   <none>           <none>
emailappprashija-699c5677b8-fd7wr   1/1     Running   0          2d22h   10.245.24.136   10.11.9.100   <none>           <none>
emailappsankanun-7f648b46bb-49wms   1/1     Running   0          2d22h   10.245.25.142   10.11.8.248   <none>           <none>
secrets-init                        1/1     Running   0          18h     10.245.16.210   10.11.9.248   <none>           <none>
[27:January:2022:10:51:32]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ kgpnk
kgn kafka NAME                                               READY   STATUS      RESTARTS   AGE     IP              NODE          NOMINATED NODE   READINESS GATES
cpe-cronjob-sr-backup-sr-backup-1643278200-rxnpv   0/1     Completed   0          41m     10.245.12.169   10.11.9.20    <none>           <none>
cpe-cronjob-sr-backup-sr-backup-1643279100-6l5nl   0/1     Completed   0          26m     10.245.41.199   10.11.9.106   <none>           <none>
cpe-cronjob-sr-backup-sr-backup-1643280000-j7j9n   0/1     Completed   0          11m     10.245.41.204   10.11.9.106   <none>           <none>
karapace-schemaregistry-proxy-7d8f5776b5-6cwdf     2/2     Running     2          7d      10.245.35.137   10.11.9.177   <none>           <none>
karapace-shared-schemaregistry-7965cb76c9-k4bsh    3/3     Running     0          2d22h   10.245.24.150   10.11.9.100   <none>           <none>
karapace-shared-schemaregistry-7965cb76c9-kmkz7    3/3     Running     1          2d5h    10.245.44.19    10.11.9.179   <none>           <none>
schemaregistry-proxy-566f5f48c9-hntbf              2/2     Running     0          3d1h    10.245.10.157   10.11.9.155   <none>           <none>
shared-kafka-entity-operator-5d445cf4f6-xtm7k      3/3     Running     0          40h     10.245.23.206   10.11.8.184   <none>           <none>
shared-kafka-kafka-0                               1/1     Running     0          40h     10.245.37.227   10.11.9.119   <none>           <none>
shared-kafka-kafka-1                               1/1     Running     0          40h     10.245.11.103   10.11.8.188   <none>           <none>
shared-kafka-kafka-2                               1/1     Running     0          40h     10.245.27.78    10.11.8.239   <none>           <none>
shared-kafka-kafka-3                               1/1     Running     0          40h     10.245.11.104   10.11.8.188   <none>           <none>
shared-kafka-kafka-4                               1/1     Running     0          40h     10.245.22.198   10.11.8.240   <none>           <none>
shared-kafka-kafka-5                               1/1     Running     0          40h     10.245.27.79    10.11.8.239   <none>           <none>
shared-kafka-kafka-6                               1/1     Running     0          40h     10.245.40.222   10.11.9.161   <none>           <none>
shared-kafka-kafka-7                               1/1     Running     0          40h     10.245.27.225   10.11.9.189   <none>           <none>
shared-kafka-kafka-8                               1/1     Running     0          40h     10.245.37.66    10.11.9.8     <none>           <none>
shared-kafka-kafka-exporter-7d5749b87f-xgddm       1/1     Running     0          40h     10.245.38.23    10.11.9.130   <none>           <none>
shared-kafka-zookeeper-0                           1/1     Running     0          40h     10.245.21.237   10.11.8.53    <none>           <none>
shared-kafka-zookeeper-1                           1/1     Running     0          40h     10.245.37.226   10.11.9.119   <none>           <none>
shared-kafka-zookeeper-2                           1/1     Running     0          40h     10.245.20.97    10.11.9.240   <none>           <none>
shared-schemaregistry-7b97454fcc-mjw7j             2/2     Running     0          3d      10.245.20.28    10.11.9.240   <none>           <none>
shared-schemaregistry-7b97454fcc-z8cm7             2/2     Running     0          2d22h   10.245.25.161   10.11.8.248   <none>           <none>
strimzi-cluster-operator-68b95b4449-v27vz          1/1     Running     0          41h     10.245.39.179   10.11.9.141   <none>           <none>
[27:January:2022:10:51:46]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ kgn kafka ingress
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME                             CLASS    HOSTS                                                                    ADDRESS       PORTS     AGE
karapace-schemaregistry-proxy    <none>   karapace-schemaregistry-proxy-kafka.service.iad-dataplane.corp.consul    10.11.8.216   80, 443   153d
karapace-shared-schemaregistry   <none>   karapace-shared-schemaregistry-kafka.service.iad-dataplane.corp.consul   10.11.8.216   80, 443   153d
schemaregistry-proxy             <none>   schemaregistry-proxy-kafka.service.iad-dataplane.corp.consul             10.11.8.216   80, 443   452d
shared-kafka-kafka-0             kafka    shared-kafka-broker-0-kafka.service.iad-dataplane.corp.consul            10.11.8.216   80, 443   322d
shared-kafka-kafka-1             kafka    shared-kafka-broker-1-kafka.service.iad-dataplane.corp.consul            10.11.8.216   80, 443   322d
shared-kafka-kafka-2             kafka    shared-kafka-broker-2-kafka.service.iad-dataplane.corp.consul            10.11.8.216   80, 443   322d
shared-kafka-kafka-3             kafka    shared-kafka-broker-3-kafka.service.iad-dataplane.corp.consul            10.11.8.216   80, 443   120d
shared-kafka-kafka-4             kafka    shared-kafka-broker-4-kafka.service.iad-dataplane.corp.consul            10.11.8.216   80, 443   120d
shared-kafka-kafka-5             kafka    shared-kafka-broker-5-kafka.service.iad-dataplane.corp.consul            10.11.8.216   80, 443   120d
shared-kafka-kafka-6             kafka    shared-kafka-broker-6-kafka.service.iad-dataplane.corp.consul            10.11.8.216   80, 443   120d
shared-kafka-kafka-7             kafka    shared-kafka-broker-7-kafka.service.iad-dataplane.corp.consul            10.11.8.216   80, 443   120d
shared-kafka-kafka-8             kafka    shared-kafka-broker-8-kafka.service.iad-dataplane.corp.consul            10.11.8.216   80, 443   120d
shared-kafka-kafka-bootstrap     kafka    shared-kafka-bootstrap-kafka.service.iad-dataplane.corp.consul           10.11.8.216   80, 443   322d
shared-schemaregistry            <none>   shared-schemaregistry-kafka.service.iad-dataplane.corp.consul            10.11.8.216   80, 443   452d
[27:January:2022:10:51:51]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $
[27:January:2022:10:52:30]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ kgpn mercury
NAME                                                              READY   STATUS    RESTARTS   AGE     IP              NODE          NOMINATED NODE   READINESS GATES
kpt-kafka-producer-tool-679d4db54f-v8hzg                          2/2     Running   0          2d3h    10.245.46.158   10.11.9.39    <none>           <none>
mercury-agent-command-service-ddfb7b9d8-6rzzl                     2/2     Running   0          6d14h   10.245.36.238   10.11.9.86    <none>           <none>
mercury-channel-api-574f8dc56f-nmdp7                              2/2     Running   0          3d1h    10.245.12.246   10.11.9.20    <none>           <none>
mercury-consumer-command-service-7d98599fcc-4mzvq                 2/2     Running   0          2d23h   10.245.22.36    10.11.8.159   <none>           <none>
mercury-custom-availability-service-665b645d4c-6nfx7              2/2     Running   0          6d14h   10.245.32.192   10.11.9.7     <none>           <none>
mercury-data-mask-api-7dfdbc799-dr7pj                             2/2     Running   6          5d20h   10.245.36.138   10.11.9.86    <none>           <none>
mercury-engagement-queue-api-84f5bd7cc4-x7zwc                     2/2     Running   0          2d5h    10.245.30.195   10.11.9.138   <none>           <none>
mercury-enrichment-service-68cbfd8797-grtrh                       2/2     Running   0          6d14h   10.245.33.224   10.11.9.147   <none>           <none>
mercury-event-sync-service-78fc54ff4f-5ppbt                       2/2     Running   0          6d14h   10.245.35.69    10.11.8.227   <none>           <none>
mercury-integration-in-processor-596899b74-kfdtw                  2/2     Running   0          6d14h   10.245.34.80    10.11.8.24    <none>           <none>
mercury-integration-out-processor-6fdbb5ff78-vw4nk                2/2     Running   0          5d21h   10.245.12.196   10.11.9.20    <none>           <none>
mercury-kafka-entity-operator-5c68647bd6-7shsv                    3/3     Running   0          24h     10.245.11.107   10.11.8.188   <none>           <none>
mercury-kafka-kafka-0                                             1/1     Running   0          24h     10.245.21.175   10.11.8.53    <none>           <none>
mercury-kafka-kafka-1                                             1/1     Running   0          24h     10.245.32.44    10.11.8.69    <none>           <none>
mercury-kafka-kafka-2                                             1/1     Running   0          24h     10.245.27.138   10.11.9.189   <none>           <none>
mercury-kafka-kafka-exporter-5f66bd6655-gn46f                     1/1     Running   0          24h     10.245.39.206   10.11.9.141   <none>           <none>
mercury-kafka-zookeeper-0                                         1/1     Running   0          25h     10.245.21.168   10.11.8.53    <none>           <none>
mercury-kafka-zookeeper-1                                         1/1     Running   0          25h     10.245.32.40    10.11.8.69    <none>           <none>
mercury-kafka-zookeeper-2                                         1/1     Running   0          24h     10.245.39.204   10.11.9.141   <none>           <none>
mercury-kweet-facebook-client-54df5fb994-rfjzs                    2/2     Running   0          6d14h   10.245.36.237   10.11.9.86    <none>           <none>
mercury-kweet-facebook-webhook-d87555b68-6k2hz                    2/2     Running   0          6d14h   10.245.32.58    10.11.8.69    <none>           <none>
mercury-kweet-twiliosms-client-86f68d98dc-xqwk4                   2/2     Running   6          5d20h   10.245.12.206   10.11.9.20    <none>           <none>
mercury-kweet-userprofiles-85fdccb8d9-ws659                       2/2     Running   0          6d14h   10.245.32.50    10.11.8.69    <none>           <none>
mercury-kweet-wechat-client-8cddc948c-cnb45                       2/2     Running   0          6d14h   10.245.32.181   10.11.9.7     <none>           <none>
mercury-kweet-wechat-webhook-79db5476cf-hx28r                     2/2     Running   0          6d14h   10.245.36.231   10.11.9.86    <none>           <none>
mercury-mercury-ui-7dd6b44c6c-mr2tg                               2/2     Running   1          6d14h   10.245.34.183   10.11.9.103   <none>           <none>
mercury-metric-aggregation-processor-6d8b6b5cdc-5drql             2/2     Running   0          6d14h   10.245.23.78    10.11.9.195   <none>           <none>
mercury-metric-fusion-bridge-694f9c9794-tcw8j                     2/2     Running   1          6d14h   10.245.34.184   10.11.9.103   <none>           <none>
mercury-metric-generation-processor-69cf885c9c-wrk7p              2/2     Running   0          6d14h   10.245.23.79    10.11.9.195   <none>           <none>
mercury-metric-internal-translation-processor-7ddfc87fd8-xvmkp    2/2     Running   0          6d14h   10.245.23.80    10.11.9.195   <none>           <none>
mercury-metric-proxy-service-d778b6d45-77rbz                      2/2     Running   0          6d14h   10.245.32.182   10.11.9.7     <none>           <none>
mercury-omnichannel-assignment-processor-7988887f59-f9vc8         2/2     Running   0          6d14h   10.245.36.69    10.11.9.165   <none>           <none>
mercury-omnichannel-offer-processor-788cb687df-4rzlf              2/2     Running   0          6d14h   10.245.31.177   10.11.8.27    <none>           <none>
mercury-osvc-bridge-api-services-88f7f4698-rb87b                  2/2     Running   0          6d14h   10.245.32.183   10.11.9.7     <none>           <none>
mercury-osvc-bridge-metrics-data-pipeline-567c5869df-9wfnv        2/2     Running   0          2d23h   10.245.23.161   10.11.8.184   <none>           <none>
mercury-osvc-bridge-osvc-data-extractor-5b47454567-4pc8r          2/2     Running   0          2d23h   10.245.23.146   10.11.8.184   <none>           <none>
mercury-osvc-bridge-provisioning-processor-c967dbd99-4p6t7        2/2     Running   0          6d14h   10.245.32.186   10.11.9.7     <none>           <none>
mercury-osvc-bridge-state-processor-77bf4b45c8-vm522              2/2     Running   0          6d14h   10.245.35.73    10.11.8.227   <none>           <none>
mercury-osvc-bridge-state-query-service-9b95975b4-kpb5d           2/2     Running   0          6d14h   10.245.32.188   10.11.9.7     <none>           <none>
mercury-osvc-bridge-task-controller-d8555b4bc-krbwh               2/2     Running   0          6d14h   10.245.32.189   10.11.9.7     <none>           <none>
mercury-provisioning-monitor-54789c89fc-5j67h                     2/2     Running   0          6d14h   10.245.33.78    10.11.9.198   <none>           <none>
mercury-provisioning-processor-6558f4cfb9-4dcxm                   2/2     Running   0          6d14h   10.245.33.82    10.11.9.198   <none>           <none>
mercury-queue-agent-info-processor-5db45d8bf7-vw68s               2/2     Running   0          6d14h   10.245.33.81    10.11.9.198   <none>           <none>
mercury-realtime-channel-processor-b974d7c69-jx8jz                2/2     Running   0          5d20h   10.245.7.85     10.11.9.48    <none>           <none>
mercury-resource-channel-processor-57f7877876-sx7dz               2/2     Running   1          5d20h   10.245.36.139   10.11.9.86    <none>           <none>
mercury-resource-state-processor-79fc45c6dc-wrdj2                 2/2     Running   0          6d14h   10.245.30.183   10.11.9.138   <none>           <none>
mercury-resource-work-processor-5d85dcdfb-p5mfj                   2/2     Running   0          6d14h   10.245.30.182   10.11.9.138   <none>           <none>
mercury-routing-processor-agent-assignment-8bb5bcb97-hkrfk        2/2     Running   0          6d14h   10.245.30.181   10.11.9.138   <none>           <none>
mercury-routing-processor-agent-events-processor-7b4797d8ftjnwc   2/2     Running   0          6d14h   10.245.33.230   10.11.9.147   <none>           <none>
mercury-routing-processor-queue-assignment-76844cb695-fxd72       2/2     Running   0          6d14h   10.245.34.76    10.11.8.24    <none>           <none>
mercury-routing-processor-work-events-processor-59944c86765v9gk   2/2     Running   0          6d14h   10.245.33.231   10.11.9.147   <none>           <none>
mercury-session-housekeeping-processor-76bb9b5b74-bjznb           2/2     Running   0          6d14h   10.245.34.73    10.11.8.24    <none>           <none>
mercury-session-processor-68855fc8f5-w5j7g                        2/2     Running   0          6d14h   10.245.34.72    10.11.8.24    <none>           <none>
mercury-single-sign-on-service-549884bc88-qcm9v                   2/2     Running   0          6d14h   10.245.34.74    10.11.8.24    <none>           <none>
mercury-social-bridge-fcf5667d-sm97v                              2/2     Running   0          6d14h   10.245.34.75    10.11.8.24    <none>           <none>
mercury-social-config-7b568c47bf-6h8hd                            2/2     Running   0          6d14h   10.245.34.77    10.11.8.24    <none>           <none>
mercury-static-assets-service-57f89dd8c7-qwb9v                    2/2     Running   0          6d14h   10.245.34.78    10.11.8.24    <none>           <none>
mercury-tenant-downtime-monitor-68996d78c4-dds8t                  2/2     Running   0          6d14h   10.245.33.232   10.11.9.147   <none>           <none>
mercury-transcript-api-55cfcff5b-gwldm                            2/2     Running   0          6d14h   10.245.36.70    10.11.9.165   <none>           <none>
mercury-transcript-processor-85d586644-brlh4                      2/2     Running   0          6d14h   10.245.34.81    10.11.8.24    <none>           <none>
mercury-user-preference-service-7d7bf85574-frrx9                  2/2     Running   1          6d14h   10.245.32.60    10.11.8.69    <none>           <none>
mercury-work-api-5bbf669944-qlr8x                                 2/2     Running   0          6d14h   10.245.31.55    10.11.9.6     <none>           <none>
mercury-work-processor-85c4bd6674-gz6bs                           2/2     Running   0          6d14h   10.245.32.61    10.11.8.69    <none>           <none>
[27:January:2022:10:53:41]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ kgpn mercury kpt-kafka-producer-tool-679d4db54f-v8hzg -oyaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubernetes.io/psp: psp-protect-docker
    vault.security.banzaicloud.io/log-level: warn
    vault.security.banzaicloud.io/vault-addr: https://vault.query.corp.consul:8200
    vault.security.banzaicloud.io/vault-agent-configmap: mercury-va-configmap
    vault.security.banzaicloud.io/vault-env-daemon: "true"
    vault.security.banzaicloud.io/vault-path: k8s-iad-dataplane
    vault.security.banzaicloud.io/vault-role: mercury
    vault.security.banzaicloud.io/vault-skip-verify: "true"
  creationTimestamp: "2022-01-25T07:31:52Z"
  generateName: kpt-kafka-producer-tool-679d4db54f-
  labels:
    app.kubernetes.io/instance: kpt
    app.kubernetes.io/name: kafka-producer-tool
    chronos.enabled: "True"
    chronos.maxPodLifetime: "1440"
    chronos.minAvailable: "0"
    pod-template-hash: 679d4db54f
  name: kpt-kafka-producer-tool-679d4db54f-v8hzg
  namespace: mercury
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: kpt-kafka-producer-tool-679d4db54f
    uid: 330355bf-8d38-4abb-8f91-68057eba06d0
  resourceVersion: "779624325"
  selfLink: /api/v1/namespaces/mercury/pods/kpt-kafka-producer-tool-679d4db54f-v8hzg
  uid: 16b7eeb3-74cb-4ebd-ad9c-d822925a2606
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - kafka-producer-tool
            - key: release
              operator: In
              values:
              - kpt
          topologyKey: failure-domain.beta.kubernetes.io/zone
        weight: 100
  containers:
  - args:
    - agent
    - -config
    - /vault/config/config.hcl
    env:
    - name: VAULT_ADDR
      value: https://vault.query.corp.consul:8200
    - name: VAULT_SKIP_VERIFY
      value: "true"
    image: docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5
    imagePullPolicy: IfNotPresent
    name: vault-agent
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 100m
        memory: 128Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        add:
        - IPC_LOCK
        - SYS_PTRACE
        drop:
        - NET_RAW
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /vault/
      name: vault-env
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: mercurygeneric-token-wxr4g
      readOnly: true
    - mountPath: /vault/secrets
      name: agent-secrets
    - mountPath: /vault/config/config.hcl
      name: agent-configmap
      readOnly: true
      subPath: config.hcl
  - args:
    - /kafka-producer-tool/bin/kafka-producer-tool
    command:
    - /vault/vault-env
    env:
    - name: profile
      value: oci-k8s
    - name: DATACENTER
      value: OCI
    - name: RELEASE_NAME
      value: kpt
    - name: POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.namespace
    - name: POD_IP
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.podIP
    - name: POD_SERVICE_ACCOUNT
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: spec.serviceAccountName
    - name: MERCURY_URL_PATTERN
      value: https://engagement-mercury-iad.corp.channels.ocs.oc-test.com/mercury
    - name: JAEGER_REMOTE_REPORTER
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.hostIP
    - name: VAULT_TOKEN
      value: vault:login
    - name: JAVA_OPTS
      value: '-Dvault.enabled=true -Dvault.namespace=mercury -Dvault.transit.kafkaMessageEncryption.path=transit_mercury
        -Dvault.base.address=https://vault.query.corp.consul:8200 -Dvault.shared.kv.engine=/kv
        -Dthidwick.vault.uri=https://vault.query.corp.consul:8200 -Dthidwick.vault.transit.cache-expiration=300
        -Dthidwick.vault.kv.cache-expiration=300 -Dthidwick.vault.serviceaccount=mercurygeneric
        -Dmercury.cpeVersion=v2 -Dfile.token.enabled=true -Dvault.authentication=TOKEN
        -Dvault.role=mercury -Dvault.path=k8s-iad-dataplane -Dkafka.log_level=WARN
        -Dthidwick-server.logback.level=INFO -Dthidwick-server.logback.service.log_level=DEBUG
        -Dthidwick.kafka.message.encryption.cipher=AES/GCM -Djwt.tenant.validation.enabled=false
        -Dthidwick.kafka.schema.registry.url=https://schemaregistry-proxy-kafka.service.iad-dataplane.corp.consul
        -Dthidwick.kafka.bootstrap.servers=mercury-kafka-bootstrap-mercury.service.iad-dataplane.corp.consul:443
        -Dthidwick.tms.base.tmsStateStream.bootstrap.servers=shared-kafka-bootstrap-kafka.service.iad-dataplane.corp.consul:443
        -Djavax.net.ssl.trustStore=/keystores/truststore.jks -Djavax.net.ssl.keyStore=/keystores/keystore.jks
        -Dhttp.security.headers.enabled=true -Dhttp.rest.security.headers.enabled=true
        -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -Xms1536m   -Xmx4096m   '
    - name: VAULT_ADDR
      value: https://vault.query.corp.consul:8200
    - name: VAULT_SKIP_VERIFY
      value: "true"
    - name: VAULT_AUTH_METHOD
      value: jwt
    - name: VAULT_PATH
      value: k8s-iad-dataplane
    - name: VAULT_ROLE
      value: mercury
    - name: VAULT_IGNORE_MISSING_SECRETS
      value: "false"
    - name: VAULT_ENV_PASSTHROUGH
    - name: VAULT_JSON_LOG
      value: "false"
    - name: VAULT_CLIENT_TIMEOUT
      value: 10s
    - name: VAULT_LOG_LEVEL
      value: warn
    - name: VAULT_ENV_DAEMON
      value: "true"
    image: iad.ocir.io/osvcstage/mercury/kafka-producer-tool:21.07.01-TRUNK-180
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 2
      httpGet:
        path: /health/checks
        port: 8080
        scheme: HTTP
      initialDelaySeconds: 120
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    name: kafka-producer-tool
    ports:
    - containerPort: 8080
      name: http
      protocol: TCP
    readinessProbe:
      failureThreshold: 2
      httpGet:
        path: /health/ping
        port: 8080
        scheme: HTTP
      initialDelaySeconds: 120
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    resources:
      limits:
        cpu: "1"
        ephemeral-storage: 10Gi
        memory: 6Gi
      requests:
        cpu: 100m
        ephemeral-storage: 2Gi
        memory: 2Gi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - NET_RAW
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /keystores
      name: keystore-volume
    - mountPath: /tmp/keystore-pass
      name: keystore-pass
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: mercurygeneric-token-wxr4g
      readOnly: true
    - mountPath: /vault/
      name: vault-env
    - mountPath: /vault/secrets
      name: agent-secrets
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  imagePullSecrets:
  - name: osvcstage-ocirsecret
  initContainers:
  - command:
    - sh
    - -c
    - cp /usr/local/bin/vault-env /vault/
    image: docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3
    imagePullPolicy: IfNotPresent
    name: copy-vault-env
    resources:
      limits:
        cpu: 250m
        memory: 64Mi
      requests:
        cpu: 50m
        memory: 64Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - NET_RAW
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /vault/
      name: vault-env
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: mercurygeneric-token-wxr4g
      readOnly: true
  - args:
    - /bin/bash
    - -c
    - /init/init-script.sh
    command:
    - /vault/vault-env
    env:
    - name: AUTO_REFRESH_CONSUL_TOKEN
      value: vault:cpe_consul/creds/mercury#token
    - name: KAFKA_TEMPLATED_PKI
      value: '>>vault:infra_pki/issue/kafka_client#${mustToRawJson (dict "certificate"
        .certificate "issuing_ca" .issuing_ca "private_key" .private_key)}#{"common_name":"mercury-mercurygeneric-rw",
        "ttl": "720h"}'
    - name: KEYSTORE_LOCATION
      value: /keystores
    - name: TRUSTSTORE_NAME
      value: truststore.jks
    - name: KEYSTORE_SECRETPATH
      value: /tmp/keystore-pass
    - name: KEYSTORE_TMPPASSFILENAME
      value: props/kafka.properties
    - name: KEYSTORE_NAME
      value: keystore.jks
    - name: SERVICEACCOUNT_SECRETPATH
      value: /var/run/secrets/kubernetes.io/serviceaccount
    - name: VAULT_ADDR
      value: https://vault.query.corp.consul:8200
    - name: VAULT_SECRETPATH
      value: /tmp/vaultca
    - name: PKI_VAULT_ENGINE_PATH
      value: v1/infra_pki/issue/kafka_client
    - name: CERT_CN
      value: mercury-mercurygeneric-rw
    - name: KAFKA_SECURITY_PROTOCOL
      value: SSL
    - name: SA_ROLE
      value: mercury_mercurygeneric
    - name: VAULT_SERVICEACCOUNT
      value: mercurygeneric
    - name: VAULT_ADDR
      value: https://vault.query.corp.consul:8200
    - name: VAULT_SKIP_VERIFY
      value: "true"
    - name: VAULT_AUTH_METHOD
      value: jwt
    - name: VAULT_PATH
      value: k8s-iad-dataplane
    - name: VAULT_ROLE
      value: mercury
    - name: VAULT_IGNORE_MISSING_SECRETS
      value: "false"
    - name: VAULT_ENV_PASSTHROUGH
    - name: VAULT_JSON_LOG
      value: "false"
    - name: VAULT_CLIENT_TIMEOUT
      value: 10s
    - name: VAULT_LOG_LEVEL
      value: warn
    - name: VAULT_ENV_DAEMON
      value: "true"
    image: iad.ocir.io/osvcstage/mercury/init-container/dev:1.0.0.18
    imagePullPolicy: IfNotPresent
    name: init
    resources:
      limits:
        cpu: "1"
        ephemeral-storage: 10Gi
        memory: 6Gi
      requests:
        cpu: 100m
        ephemeral-storage: 2Gi
        memory: 2Gi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - NET_RAW
      runAsUser: 0
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /keystores
      name: keystore-volume
    - mountPath: /tmp/vaultca
      name: vaultca
      readOnly: true
    - mountPath: /tmp/keystore-pass
      name: keystore-pass
      readOnly: true
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: mercurygeneric-token-wxr4g
      readOnly: true
    - mountPath: /vault/
      name: vault-env
  nodeName: 10.11.9.39
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: mercurygeneric
  serviceAccountName: mercurygeneric
  shareProcessNamespace: true
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: vaultca
    secret:
      defaultMode: 420
      secretName: vault-tls
  - name: keystore-pass
    secret:
      defaultMode: 420
      secretName: kpt-kafka-producer-tool-keystore
  - emptyDir: {}
    name: keystore-volume
  - name: mercurygeneric-token-wxr4g
    secret:
      defaultMode: 420
      secretName: mercurygeneric-token-wxr4g
  - emptyDir:
      medium: Memory
    name: vault-env
  - emptyDir:
      medium: Memory
    name: agent-secrets
  - configMap:
      defaultMode: 420
      items:
      - key: config.hcl
        path: config.hcl
      name: mercury-va-configmap
    name: agent-configmap
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2022-01-25T07:32:10Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2022-01-25T08:10:12Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2022-01-25T08:10:12Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2022-01-25T07:31:52Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: docker://a7fbf9a8f3481c159e004c035263520564fcca1dab8e3cf4f5d2fb73fbc25713
    image: iad.ocir.io/osvcstage/mercury/kafka-producer-tool:21.07.01-TRUNK-180
    imageID: docker-pullable://iad.ocir.io/osvcstage/mercury/kafka-producer-tool@sha256:35cdc7ce894da8f5b87b321e3e193942a9927d228f028f9ce4f516293d7aaa46
    lastState: {}
    name: kafka-producer-tool
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2022-01-25T08:08:03Z"
  - containerID: docker://ad9a13b3f8c5814fd4e245ea5aab716b82af556fc61c745a4730503f1db07d4a
    image: docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault:1.6.5
    imageID: docker-pullable://docker-master.cdaas.oraclecloud.com/docker-osvc-local/cpe-images/oracle-vault@sha256:57438e97b72c05368ca06f4d5f34667e965248635718001abdea8fa11f18adad
    lastState: {}
    name: vault-agent
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2022-01-25T07:32:10Z"
  hostIP: 10.11.9.39
  initContainerStatuses:
  - containerID: docker://c3cc4a4ab033b74dc5be5c1fffcb3400b277333a39e250af2f008e2866410b1a
    image: docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env:1.14.3
    imageID: docker-pullable://docker.cdaas.oraclecloud.com/docker-osvc/banzaicloud/vault-env@sha256:7435d1c641c0aa0fd4faaff32ab189b29d3d909b100d36d4d90df3b79cee8ce3
    lastState: {}
    name: copy-vault-env
    ready: true
    restartCount: 0
    state:
      terminated:
        containerID: docker://c3cc4a4ab033b74dc5be5c1fffcb3400b277333a39e250af2f008e2866410b1a
        exitCode: 0
        finishedAt: "2022-01-25T07:32:04Z"
        reason: Completed
        startedAt: "2022-01-25T07:32:04Z"
  - containerID: docker://a82ac16616cd303825674374129951f9e85667694986525e0f67379e9530fa76
    image: iad.ocir.io/osvcstage/mercury/init-container/dev:1.0.0.18
    imageID: docker-pullable://iad.ocir.io/osvcstage/mercury/init-container/dev@sha256:59dbfb749d6cd87a0c86711bcadf5f2328a2a611c1db4cbb31f117fe60e5d299
    lastState: {}
    name: init
    ready: true
    restartCount: 0
    state:
      terminated:
        containerID: docker://a82ac16616cd303825674374129951f9e85667694986525e0f67379e9530fa76
        exitCode: 0
        finishedAt: "2022-01-25T07:32:09Z"
        reason: Completed
        startedAt: "2022-01-25T07:32:05Z"
  phase: Running
  podIP: 10.245.46.158
  podIPs:
  - ip: 10.245.46.158
  qosClass: Burstable
  startTime: "2022-01-25T07:31:52Z"
[27:January:2022:10:54:13]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ kln mercury kpt-kafka-producer-tool-679d4db54f-v8hzg > ../kpt-kafka-producer-tool-679d4db54f-v8hzg$dnow
error: a container name must be specified for pod kpt-kafka-producer-tool-679d4db54f-v8hzg, choose one of: [vault-agent kafka-producer-tool] or one of the init containers: [copy-vault-env init]
[27:January:2022:10:56:03]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ kln mercury kpt-kafka-producer-tool-679d4db54f-v8hzg -c kafka-producer-tool > ../kpt-kafka-producer-tool-679d4db54f-v8hzg$dnow
[27:January:2022:10:56:17]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ rm ../kpt-kafka-producer-tool-679d4db54f-v8hzg2022_01_27_10_51_AM.log
[27:January:2022:10:59:02]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ kgdpn cxemailservice
NAME                READY   UP-TO-DATE   AVAILABLE   AGE
emailapp            1/1     1            1           3d2h
emailapp-prashit    1/1     1            1           8d
emailapp-rambat     1/1     1            1           5h19m
emailapp-rambat-1   1/1     1            1           4h48m
emailapp-wentwei    1/1     1            1           13d
emailappabp         1/1     1            1           162m
emailappashossin    1/1     1            1           3h39m
emailappprashija    1/1     1            1           8d
emailappsankanun    1/1     1            1           16d
[27:January:2022:11:00:02]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ kgpn cxemailservice
NAME                                READY   STATUS    RESTARTS   AGE     IP              NODE          NOMINATED NODE   READINESS GATES
emailapp-86c7f7bd7d-bt4qk           1/1     Running   0          2d19h   10.245.40.140   10.11.9.161   <none>           <none>
emailapp-prashit-6c66fb9c97-4t75q   1/1     Running   0          2d4h    10.245.45.153   10.11.9.214   <none>           <none>
emailapp-rambat-1-9bd4d58db-rbh4z   1/1     Running   0          4h48m   10.245.35.144   10.11.9.177   <none>           <none>
emailapp-rambat-58494b474b-ddt8x    1/1     Running   0          5h20m   10.245.12.45    10.11.8.226   <none>           <none>
emailapp-wentwei-777966bcc6-2hs2s   1/1     Running   0          7d2h    10.245.32.164   10.11.9.7     <none>           <none>
emailappabp-855bbdffb4-sd2lx        1/1     Running   0          61m     10.245.16.202   10.11.9.248   <none>           <none>
emailappashossin-584c579d4-vskwx    1/1     Running   0          3h39m   10.245.35.153   10.11.9.177   <none>           <none>
emailappprashija-699c5677b8-fd7wr   1/1     Running   0          2d23h   10.245.24.136   10.11.9.100   <none>           <none>
emailappsankanun-7f648b46bb-49wms   1/1     Running   0          2d22h   10.245.25.142   10.11.8.248   <none>           <none>
secrets-init                        1/1     Running   0          18h     10.245.16.210   10.11.9.248   <none>           <none>
[27:January:2022:11:00:18]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ kgpn cxemailservice -oyaml | grep -i bootstrap
[27:January:2022:11:01:24]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ helm ls -n cxemailservice
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/opc/.kube/pv2_corp.config
NAME              NAMESPACE       REVISION  UPDATED                                 STATUS    CHART                   APP VERSION
emailapp          cxemailservice  1         2022-01-07 17:31:49.199721 +0530 +0530  deployed  emailapp-0.1.0          1.0
emailapp-prashit  cxemailservice  1         2022-01-19 11:46:12.780589 +0530 +0530  deployed  emailapp-prashit-0.1.0  1.0
emailapp-rambat   cxemailservice  1         2022-01-27 11:09:51.375354 +0530 +0530  deployed  emailapp-rambat-0.1.0   1.0
emailapp-rambat-1 cxemailservice  1         2022-01-27 11:41:08.477923 +0530 +0530  deployed  emailapp-rambat-1-0.1.0 1.0
emailapp-wentwei  cxemailservice  1         2022-01-13 11:36:11.56592 -0800 -0800   deployed  emailapp-wentwei-0.1.0  1.0
emailappabp       cxemailservice  3         2022-01-27 15:28:09.053112 +0530 +0530  deployed  emailappabp-0.1.0       1.0
emailappashossin  cxemailservice  1         2022-01-27 12:50:46.132877 +0530 +0530  deployed  emailappashossin-0.1.0  1.0
emailappprashija  cxemailservice  1         2022-01-19 12:46:10.149514 +0530 +0530  deployed  emailappprashija-0.1.0  1.0
emailappsankanun  cxemailservice  1         2022-01-10 18:26:22.063495 +0530 +0530  deployed  emailappsankanun-0.1.0  1.0
[27:January:2022:11:01:41]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ date
Thu Jan 27 11:01:46 UTC 2022
[27:January:2022:11:01:46]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $
[27:January:2022:12:09:20]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ grep -ir ingresswatcher .
./shared-kafka-dev-lhr-ingress-TRAEFIK-KAFKA_27jan22.yaml:      ingressWatcher/port: "443"
./shared-kafka-dev-lhr-ingress-TRAEFIK-KAFKA_27jan22.yaml:      ingressWatcher/service: karapace-schemaregistry-proxy-kafka
./shared-kafka-dev-lhr-ingress-TRAEFIK-KAFKA_27jan22.yaml:      ingressWatcher/port: "443"
./shared-kafka-dev-lhr-ingress-TRAEFIK-KAFKA_27jan22.yaml:      ingressWatcher/service: karapace-shared-schemaregistry-kafka
./shared-kafka-dev-lhr-ingress-TRAEFIK-KAFKA_27jan22.yaml:      ingressWatcher/port: "443"
./shared-kafka-dev-lhr-ingress-TRAEFIK-KAFKA_27jan22.yaml:      ingressWatcher/service: schemaregistry-proxy-kafka
./shared-kafka-dev-lhr-ingress-TRAEFIK-KAFKA_27jan22.yaml:      ingressWatcher/port: "443"
./shared-kafka-dev-lhr-ingress-TRAEFIK-KAFKA_27jan22.yaml:      ingressWatcher/service: shared-schemaregistry-kafka
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 12:25:34,638: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 12:29:38,180: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 12:29:41,444: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 12:33:44,759: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 12:33:48,234: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 12:37:51,483: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 12:37:54,747: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 12:41:58,222: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 12:42:01,522: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 12:46:04,901: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 12:46:08,283: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 12:50:11,554: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 12:50:14,779: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 12:54:18,146: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 12:54:21,406: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 12:58:24,759: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 12:58:28,235: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:02:31,545: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:02:34,803: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:06:38,220: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:06:41,529: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:10:44,926: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:10:48,307: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:14:51,607: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:14:55,024: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:18:58,303: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:19:01,530: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:23:04,869: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:23:08,310: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:27:11,621: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:27:14,882: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:31:18,133: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:31:21,406: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:35:24,732: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:35:28,210: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:39:31,442: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:39:34,701: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:43:38,136: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:43:41,410: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:47:44,640: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:47:48,122: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:51:51,338: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:51:54,641: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:55:58,033: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 13:56:01,226: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:00:04,461: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:00:07,981: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:04:11,212: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:04:14,491: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:08:17,954: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:08:21,133: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:12:24,343: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:12:27,678: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:16:30,968: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:16:34,244: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:20:37,510: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:20:40,758: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:24:43,995: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:24:47,264: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:28:50,386: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:28:53,658: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:32:56,806: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:32:59,939: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:37:03,159: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:37:06,526: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:41:09,909: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:41:13,167: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:45:16,663: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:45:19,994: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:49:23,247: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:49:26,519: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:53:29,733: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:53:33,048: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:57:36,399: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 14:57:39,703: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:01:42,986: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:01:46,266: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:05:49,523: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:05:52,852: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:09:56,120: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:09:59,359: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:14:02,613: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:14:05,969: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:18:09,302: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:18:12,638: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:22:15,905: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:22:19,175: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:26:22,541: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:26:25,951: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:30:29,282: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:30:32,524: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:34:35,781: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:34:39,057: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:38:42,288: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:38:45,611: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:42:48,852: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:42:52,119: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:46:55,409: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:46:58,663: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:51:02,126: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:51:05,403: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:55:08,699: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:55:11,959: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:59:15,347: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 15:59:18,707: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:03:22,020: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:03:25,318: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:07:28,541: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:07:31,972: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:11:35,269: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:11:38,736: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:15:42,050: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:15:45,355: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:19:48,618: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:19:51,848: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:23:55,148: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:23:58,510: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:28:01,765: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:28:05,176: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:32:08,407: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:32:11,729: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:36:14,977: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:36:18,365: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:40:21,695: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:40:25,085: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:44:28,420: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:44:31,658: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:48:35,030: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:48:38,314: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:52:41,584: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:52:44,906: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:56:48,276: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 16:56:51,567: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:00:54,866: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:00:58,256: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:05:01,552: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:05:04,923: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:09:08,227: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:09:11,478: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:13:14,788: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:13:18,291: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:17:21,496: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:17:24,750: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:21:28,276: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:21:31,477: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:25:34,696: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:25:38,229: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:29:41,559: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:29:44,859: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:33:48,225: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:33:51,507: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:37:54,749: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:37:58,121: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:42:01,402: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:42:04,696: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:46:08,201: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:46:11,476: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:50:14,804: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:50:18,235: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:54:21,600: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:54:24,941: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:58:28,312: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 17:58:31,541: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:02:34,868: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:02:38,374: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:06:41,641: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:06:44,871: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:10:48,291: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:10:51,635: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:14:54,904: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:14:58,254: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:19:01,568: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:19:04,881: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:23:08,245: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:23:11,568: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:27:14,828: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:27:18,214: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:31:21,434: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:31:24,780: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:35:28,169: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:35:31,502: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:39:34,886: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:39:38,299: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:43:41,608: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:43:44,990: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:47:48,257: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:47:51,490: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:51:54,798: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:51:58,237: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:56:01,455: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 18:56:04,857: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:00:08,231: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:00:11,519: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:04:14,865: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:04:18,316: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:08:21,676: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:08:24,930: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:12:28,225: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:12:31,514: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:16:34,737: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:16:38,186: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:20:41,437: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:20:44,708: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:24:48,177: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:24:51,431: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:28:54,766: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:28:58,175: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:33:01,439: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:33:04,833: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:37:08,227: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:37:11,464: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:41:14,806: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:41:18,177: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:45:21,519: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:45:24,781: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:49:28,182: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:49:31,416: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:53:34,710: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:53:38,177: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:57:41,429: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 19:57:44,695: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:01:48,124: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:01:51,461: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:05:54,805: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:05:58,236: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:10:01,473: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:10:04,768: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:14:08,175: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:14:11,452: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:18:14,783: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:18:18,138: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:22:21,432: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:22:24,613: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:26:28,029: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:26:31,282: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:30:34,558: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:30:38,007: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:34:41,185: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:34:44,445: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:38:48,010: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:38:51,292: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:42:54,569: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:42:58,003: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:47:01,196: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:47:04,419: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:51:07,769: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:51:11,267: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:55:14,620: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:55:18,035: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:59:21,301: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 20:59:24,571: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:03:28,174: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:03:31,478: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:07:34,745: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:07:38,197: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:11:41,359: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:11:44,641: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:15:48,078: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:15:51,229: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:19:54,632: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:19:58,088: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:24:01,226: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:24:04,381: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:28:07,575: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:28:10,899: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:32:14,237: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:32:17,469: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:36:20,699: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:36:23,871: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:40:27,060: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:40:30,294: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:44:33,539: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:44:36,661: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:48:39,917: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:48:43,184: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:52:46,343: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:52:49,532: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:56:52,828: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 21:56:56,000: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:00:59,183: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:01:02,416: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:05:05,741: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:05:08,980: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:09:12,235: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:09:15,510: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:13:18,762: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:13:21,985: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:17:25,246: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:17:28,463: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:21:31,630: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:21:34,998: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:25:38,302: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:25:41,560: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:29:44,784: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:29:48,210: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:33:51,584: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:33:54,907: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:37:58,277: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:38:01,538: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:42:04,903: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:42:08,336: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:46:11,525: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:46:14,773: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:50:18,126: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:50:21,335: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:54:24,559: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:54:27,930: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:58:31,164: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 22:58:34,515: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:02:37,999: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:02:41,324: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:06:44,637: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:06:48,110: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:10:51,479: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:10:54,792: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:14:58,174: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:15:01,636: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:19:05,082: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:19:08,432: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:23:11,724: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:23:15,031: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:27:18,306: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:27:21,546: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:31:24,815: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:31:28,212: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:35:31,500: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:35:34,777: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:39:38,212: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:39:41,461: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:43:44,920: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:43:48,141: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:47:51,440: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:47:54,855: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:51:58,255: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:52:01,509: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:56:04,891: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-26 23:56:08,332: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:00:11,693: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:00:15,087: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:04:18,408: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:04:21,675: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:08:25,212: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:08:28,508: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:12:31,724: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:12:35,046: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:16:38,318: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:16:41,573: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:20:44,791: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:20:48,141: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:24:51,298: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:24:54,610: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:28:58,123: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:29:01,344: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:33:04,649: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:33:08,132: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:37:11,377: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:37:14,723: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:41:18,203: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:41:21,432: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:45:24,704: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:45:28,183: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:49:31,454: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:49:34,732: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:53:38,150: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:53:41,340: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:57:44,656: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 00:57:48,164: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 01:01:51,374: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 01:01:54,716: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 01:05:58,195: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./ingress-watcher-58f5cf6b6b-kzpc92022_01_27_01_07_AM.log:2022-01-27 01:06:01,421: INFO: Running: consul_tool.py --consul-host 10.10.8.33 list --output-json --with-tag ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t --with-servicemeta ingressWatcherOkeId:ocid1.cluster.oc1.uk-london-1.aaaaaaaaae4dkzrqgi4diolfgm3tqmdcmizdombrgi2wmmtbmcqtozrugy3t
./kafka.shared-kafka-dr.kafka-dev-lhr.yaml:            ingressWatcher/port: "443"
./kafka.shared-kafka-dr.kafka-dev-lhr.yaml:            ingressWatcher/service: shared-kafka-dr-bootstrap-kafka
./kafka.shared-kafka-dr.kafka-dev-lhr.yaml:            ingressWatcher/port: "443"
./kafka.shared-kafka-dr.kafka-dev-lhr.yaml:            ingressWatcher/service: shared-kafka-dr-broker-0-kafka
./kafka.shared-kafka-dr.kafka-dev-lhr.yaml:            ingressWatcher/port: "443"
./kafka.shared-kafka-dr.kafka-dev-lhr.yaml:            ingressWatcher/service: shared-kafka-dr-broker-1-kafka
./kafka.shared-kafka-dr.kafka-dev-lhr.yaml:            ingressWatcher/port: "443"
./kafka.shared-kafka-dr.kafka-dev-lhr.yaml:            ingressWatcher/service: shared-kafka-dr-broker-2-kafka
./shared-kafka-dev-lhr-ingress-nginx_27jan22.yaml:      ingressWatcher/port: "443"
./shared-kafka-dev-lhr-ingress-nginx_27jan22.yaml:      ingressWatcher/service: karapace-schemaregistry-proxy-kafka
./shared-kafka-dev-lhr-ingress-nginx_27jan22.yaml:      ingressWatcher/port: "443"
./shared-kafka-dev-lhr-ingress-nginx_27jan22.yaml:      ingressWatcher/service: karapace-shared-schemaregistry-kafka
./shared-kafka-dev-lhr-ingress-nginx_27jan22.yaml:      ingressWatcher/port: "443"
./shared-kafka-dev-lhr-ingress-nginx_27jan22.yaml:      ingressWatcher/service: schemaregistry-proxy-kafka
./shared-kafka-dev-lhr-ingress-nginx_27jan22.yaml:      ingressWatcher/port: "443"
./shared-kafka-dev-lhr-ingress-nginx_27jan22.yaml:      ingressWatcher/service: shared-schemaregistry-kafka
./corp.shared-kafka.kafka.yaml:            ingressWatcher/port: "443"
./corp.shared-kafka.kafka.yaml:            ingressWatcher/service: shared-kafka-bootstrap-kafka
./corp.shared-kafka.kafka.yaml:            ingressWatcher/port: "443"
./corp.shared-kafka.kafka.yaml:            ingressWatcher/service: shared-kafka-broker-0-kafka
./corp.shared-kafka.kafka.yaml:            ingressWatcher/port: "443"
./corp.shared-kafka.kafka.yaml:            ingressWatcher/service: shared-kafka-broker-1-kafka
./corp.shared-kafka.kafka.yaml:            ingressWatcher/port: "443"
./corp.shared-kafka.kafka.yaml:            ingressWatcher/service: shared-kafka-broker-2-kafka
./corp.shared-kafka.kafka.yaml:            ingressWatcher/port: "443"
./corp.shared-kafka.kafka.yaml:            ingressWatcher/service: shared-kafka-broker-3-kafka
./corp.shared-kafka.kafka.yaml:            ingressWatcher/port: "443"
./corp.shared-kafka.kafka.yaml:            ingressWatcher/service: shared-kafka-broker-4-kafka
./corp.shared-kafka.kafka.yaml:            ingressWatcher/port: "443"
./corp.shared-kafka.kafka.yaml:            ingressWatcher/service: shared-kafka-broker-5-kafka
./corp.shared-kafka.kafka.yaml:            ingressWatcher/port: "443"
./corp.shared-kafka.kafka.yaml:            ingressWatcher/service: shared-kafka-broker-6-kafka
./corp.shared-kafka.kafka.yaml:            ingressWatcher/port: "443"
./corp.shared-kafka.kafka.yaml:            ingressWatcher/service: shared-kafka-broker-7-kafka
./corp.shared-kafka.kafka.yaml:            ingressWatcher/port: "443"
./corp.shared-kafka.kafka.yaml:            ingressWatcher/service: shared-kafka-broker-8-kafka
^C
[28:January:2022:03:33:25]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ cat ./corp.shared-kafka.kafka.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  annotations:
    meta.helm.sh/release-name: kafka-shared
    meta.helm.sh/release-namespace: kafka
  creationTimestamp: "2021-03-10T18:46:50Z"
  generation: 21
  labels:
    app: cpekafka
    app.kubernetes.io/managed-by: Helm
    chart: cpe-kafka-0.2.5
    cluster_name: shared-kafka-iad
    heritage: Helm
    release: kafka-shared
  name: shared-kafka
  namespace: kafka
  resourceVersion: "743465407"
  selfLink: /apis/kafka.strimzi.io/v1beta2/namespaces/kafka/kafkas/shared-kafka
  uid: 15a94c02-c001-439e-bcff-8cc0680663b7
spec:
  clientsCa:
    generateCertificateAuthority: false
  clusterCa:
    generateCertificateAuthority: false
  entityOperator:
    template:
      pod:
        metadata:
          labels:
            stage: dataplane
    tlsSidecar:
      livenessProbe: {}
      readinessProbe: {}
      resources:
        limits:
          cpu: 500m
          memory: 1500M
        requests:
          cpu: 100m
          memory: 750M
    topicOperator:
      livenessProbe: {}
      readinessProbe: {}
      resources:
        limits:
          cpu: 1000m
          memory: 1500M
        requests:
          cpu: 500m
          memory: 750M
    userOperator:
      livenessProbe: {}
      readinessProbe: {}
      resources:
        limits:
          cpu: 1000m
          memory: 1500M
        requests:
          cpu: 500m
          memory: 750M
  kafka:
    authorization:
      superUsers:
      - CN=mirrormaker
      - CN=mirrormaker-iad
      - CN=mirrormaker-dr-iad
      type: simple
    config:
      auto.create.topics.enable: false
      default.replication.factor: 3
      inter.broker.protocol.version: 2.5
      log.message.format.version: 2.5
      num.partitions: 3
      offsets.topic.replication.factor: 3
      transaction.state.log.min.isr: 2
      transaction.state.log.replication.factor: 3
    jvmOptions:
      -Xms: 8000m
      -Xmx: 8000m
    listeners:
    - name: plain
      port: 9092
      tls: false
      type: internal
    - authentication:
        type: tls
      name: tls
      port: 9093
      tls: true
      type: internal
    - authentication:
        type: tls
      configuration:
        bootstrap:
          annotations:
            ingressWatcher/port: "443"
            ingressWatcher/service: shared-kafka-bootstrap-kafka
          host: shared-kafka-bootstrap-kafka.service.iad-dataplane.corp.consul
        brokers:
        - annotations:
            ingressWatcher/port: "443"
            ingressWatcher/service: shared-kafka-broker-0-kafka
          broker: 0
          host: shared-kafka-broker-0-kafka.service.iad-dataplane.corp.consul
        - annotations:
            ingressWatcher/port: "443"
            ingressWatcher/service: shared-kafka-broker-1-kafka
          broker: 1
          host: shared-kafka-broker-1-kafka.service.iad-dataplane.corp.consul
        - annotations:
            ingressWatcher/port: "443"
            ingressWatcher/service: shared-kafka-broker-2-kafka
          broker: 2
          host: shared-kafka-broker-2-kafka.service.iad-dataplane.corp.consul
        - annotations:
            ingressWatcher/port: "443"
            ingressWatcher/service: shared-kafka-broker-3-kafka
          broker: 3
          host: shared-kafka-broker-3-kafka.service.iad-dataplane.corp.consul
        - annotations:
            ingressWatcher/port: "443"
            ingressWatcher/service: shared-kafka-broker-4-kafka
          broker: 4
          host: shared-kafka-broker-4-kafka.service.iad-dataplane.corp.consul
        - annotations:
            ingressWatcher/port: "443"
            ingressWatcher/service: shared-kafka-broker-5-kafka
          broker: 5
          host: shared-kafka-broker-5-kafka.service.iad-dataplane.corp.consul
        - annotations:
            ingressWatcher/port: "443"
            ingressWatcher/service: shared-kafka-broker-6-kafka
          broker: 6
          host: shared-kafka-broker-6-kafka.service.iad-dataplane.corp.consul
        - annotations:
            ingressWatcher/port: "443"
            ingressWatcher/service: shared-kafka-broker-7-kafka
          broker: 7
          host: shared-kafka-broker-7-kafka.service.iad-dataplane.corp.consul
        - annotations:
            ingressWatcher/port: "443"
            ingressWatcher/service: shared-kafka-broker-8-kafka
          broker: 8
          host: shared-kafka-broker-8-kafka.service.iad-dataplane.corp.consul
        class: kafka
      name: external
      port: 9094
      tls: true
      type: ingress
    livenessProbe: {}
    metricsConfig:
      type: jmxPrometheusExporter
      valueFrom:
        configMapKeyRef:
          key: shared-kafka-kafka-metrics-config.yaml
          name: shared-kafka-kafka-metrics
    rack:
      topologyKey: failure-domain.beta.kubernetes.io/zone
    readinessProbe: {}
    replicas: 9
    resources:
      limits:
        cpu: "9"
        memory: 48Gi
      requests:
        cpu: "2"
        memory: 16Gi
    storage:
      type: jbod
      volumes:
      - class: oci-bv
        id: 0
        size: 100Gi
        type: persistent-claim
    template:
      pod:
        metadata:
          labels:
            stage: dataplane
    version: 2.7.0
  kafkaExporter:
    livenessProbe:
      initialDelaySeconds: 300
      periodSeconds: 60
      timeoutSeconds: 600
    readinessProbe:
      initialDelaySeconds: 300
      periodSeconds: 60
      timeoutSeconds: 600
    resources:
      limits:
        cpu: 5000m
        memory: 8192M
      requests:
        cpu: 750m
        memory: 750M
    template:
      pod:
        metadata:
          labels:
            stage: dataplane
  zookeeper:
    livenessProbe: {}
    metricsConfig:
      type: jmxPrometheusExporter
      valueFrom:
        configMapKeyRef:
          key: shared-kafka-zookeeper-metrics-config.yaml
          name: shared-kafka-zookeeper-metrics
    readinessProbe: {}
    replicas: 3
    resources:
      limits:
        cpu: 1000m
        memory: 8Gi
      requests:
        cpu: 500m
        memory: 4Gi
    storage:
      class: oci-bv
      size: 50Gi
      type: persistent-claim
    template:
      pod:
        metadata:
          labels:
            stage: dataplane
status:
  clusterId: bn9NVvyCR7OVcorvHdYfaQ
  conditions:
  - lastTransitionTime: "2022-01-07T15:50:06.262896Z"
    message: log.message.format.version does not match the Kafka cluster version,
      which suggests that an upgrade is incomplete.
    reason: KafkaLogMessageFormatVersion
    status: "True"
    type: Warning
  - lastTransitionTime: "2022-01-07T15:50:06.262903Z"
    message: inter.broker.protocol.version does not match the Kafka cluster version,
      which suggests that an upgrade is incomplete.
    reason: KafkaInterBrokerProtocolVersion
    status: "True"
    type: Warning
  - lastTransitionTime: "2022-01-07T15:50:37.079Z"
    status: "True"
    type: Ready
  listeners:
  - addresses:
    - host: shared-kafka-kafka-bootstrap.kafka.svc
      port: 9092
    bootstrapServers: shared-kafka-kafka-bootstrap.kafka.svc:9092
    type: plain
  - addresses:
    - host: shared-kafka-kafka-bootstrap.kafka.svc
      port: 9093
    bootstrapServers: shared-kafka-kafka-bootstrap.kafka.svc:9093
    certificates:
    - |
      -----BEGIN CERTIFICATE-----
      MIIDDTCCArOgAwIBAgIJAIbFzzMS2BCjMAoGCCqGSM49BAMCMIG5MQswCQYDVQQG
      EwJVUzELMAkGA1UECBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xGjAYBgNV
      BAkTETEwMSBTZWNvbmQgU3RyZWV0MQ4wDAYDVQQREwU5NDEwNTEXMBUGA1UEChMO
      SGFzaGlDb3JwIEluYy4xQDA+BgNVBAMTN0NvbnN1bCBBZ2VudCBDQSAzMDg3MDM2
      MTY0NjcyNjU5MzU3MjYxNDkwMjc5NDQ5NzUxNzE4MDQwHhcNMjAxMDI5MjE1MzAy
      WhcNMjUxMDI4MjE1MzAyWjB0MQswCQYDVQQGEwJVUzELMAkGA1UECAwCQ0ExFTAT
      BgNVBAcMDFJlZHdvb2QgQ2l0eTEPMA0GA1UECgwGT3JhY2xlMQ0wCwYDVQQLDARP
      U3ZDMSEwHwYDVQQDDBhpbnRlcm1lZGlhdGUuY29ycC5jb25zdWwwggEiMA0GCSqG
      SIb3DQEBAQUAA4IBDwAwggEKAoIBAQCehnJm37VGgJb7plpsCpKFKhFOFW+/etJW
      8qOjR9U0gnkVXCYxapv84pMg8jbKFT/qw9MjS9cgG5Onnlg7fJWmb3yQDO7kJVZG
      +sDgCTvUi5PX7ssmXad3Ckng0ad9IeD9xMFALVHPBke5MgnInTbMXH0WXoPGaAVK
      GNumRgiI58gnBOoQ//8K3NWRoNCV+Y1swVfevzNw+BCv71KLxj6iMytzHQ6+xet/
      Tu6/QLuP1P59OE4XtIexBAQ99zwsRPM7NuW9eNOE9SnbmLzujhRMLPTtLvrRB7Yq
      QYYmOFC0zBy8qA6XV8VNQ4TGe2ECI0ABylJcqDQ8voyttdagCMaHAgMBAAGjHTAb
      MAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEGMAoGCCqGSM49BAMCA0gAMEUCIB4K
      2lD3X5pt7oSLEpsUAV1DzJzkN0dnVTfu6Q/Ay1iIAiEAzUOLrD4bUGlg6qIaHK9J
      3v6gObYxZBeV/36Mtj3sV2A=
      -----END CERTIFICATE-----
      -----BEGIN CERTIFICATE-----
      MIIC7jCCApSgAwIBAgIRAOg+J2VNR/U6Pq4vxArSANwwCgYIKoZIzj0EAwIwgbkx
      CzAJBgNVBAYTAlVTMQswCQYDVQQIEwJDQTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNj
      bzEaMBgGA1UECRMRMTAxIFNlY29uZCBTdHJlZXQxDjAMBgNVBBETBTk0MTA1MRcw
      FQYDVQQKEw5IYXNoaUNvcnAgSW5jLjFAMD4GA1UEAxM3Q29uc3VsIEFnZW50IENB
      IDMwODcwMzYxNjQ2NzI2NTkzNTcyNjE0OTAyNzk0NDk3NTE3MTgwNDAeFw0yMDEw
      MjkyMDQ4NTNaFw0yNTEwMjgyMDQ4NTNaMIG5MQswCQYDVQQGEwJVUzELMAkGA1UE
      CBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xGjAYBgNVBAkTETEwMSBTZWNv
      bmQgU3RyZWV0MQ4wDAYDVQQREwU5NDEwNTEXMBUGA1UEChMOSGFzaGlDb3JwIElu
      Yy4xQDA+BgNVBAMTN0NvbnN1bCBBZ2VudCBDQSAzMDg3MDM2MTY0NjcyNjU5MzU3
      MjYxNDkwMjc5NDQ5NzUxNzE4MDQwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAAQY
      CIBTyb7tO0XgAnyUPdXwBEdI8doqFwlOzmWUIMhaCe2gqzKiAgn82INCVo4s3SfH
      NTjPtK5RQ2HD7XYOx0rKo3sweTAOBgNVHQ8BAf8EBAMCAYYwDwYDVR0TAQH/BAUw
      AwEB/zApBgNVHQ4EIgQg3VQ+WjPzPflqFB9ymku+EHkVVjrsIy8+rR7yOwWwvUcw
      KwYDVR0jBCQwIoAg3VQ+WjPzPflqFB9ymku+EHkVVjrsIy8+rR7yOwWwvUcwCgYI
      KoZIzj0EAwIDSAAwRQIgZMppyR6KpxmnC+J2PHNOwTV5dwRs7Z0JKdupb4LUSIEC
      IQCBiW0T4GIdNgTwLqBTzTY7OJPsELKvr9ExUhIp2lRJ2A==
      -----END CERTIFICATE-----
    type: tls
  - addresses:
    - host: shared-kafka-bootstrap-kafka.service.iad-dataplane.corp.consul
      port: 443
    bootstrapServers: shared-kafka-bootstrap-kafka.service.iad-dataplane.corp.consul:443
    certificates:
    - |
      -----BEGIN CERTIFICATE-----
      MIIDDTCCArOgAwIBAgIJAIbFzzMS2BCjMAoGCCqGSM49BAMCMIG5MQswCQYDVQQG
      EwJVUzELMAkGA1UECBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xGjAYBgNV
      BAkTETEwMSBTZWNvbmQgU3RyZWV0MQ4wDAYDVQQREwU5NDEwNTEXMBUGA1UEChMO
      SGFzaGlDb3JwIEluYy4xQDA+BgNVBAMTN0NvbnN1bCBBZ2VudCBDQSAzMDg3MDM2
      MTY0NjcyNjU5MzU3MjYxNDkwMjc5NDQ5NzUxNzE4MDQwHhcNMjAxMDI5MjE1MzAy
      WhcNMjUxMDI4MjE1MzAyWjB0MQswCQYDVQQGEwJVUzELMAkGA1UECAwCQ0ExFTAT
      BgNVBAcMDFJlZHdvb2QgQ2l0eTEPMA0GA1UECgwGT3JhY2xlMQ0wCwYDVQQLDARP
      U3ZDMSEwHwYDVQQDDBhpbnRlcm1lZGlhdGUuY29ycC5jb25zdWwwggEiMA0GCSqG
      SIb3DQEBAQUAA4IBDwAwggEKAoIBAQCehnJm37VGgJb7plpsCpKFKhFOFW+/etJW
      8qOjR9U0gnkVXCYxapv84pMg8jbKFT/qw9MjS9cgG5Onnlg7fJWmb3yQDO7kJVZG
      +sDgCTvUi5PX7ssmXad3Ckng0ad9IeD9xMFALVHPBke5MgnInTbMXH0WXoPGaAVK
      GNumRgiI58gnBOoQ//8K3NWRoNCV+Y1swVfevzNw+BCv71KLxj6iMytzHQ6+xet/
      Tu6/QLuP1P59OE4XtIexBAQ99zwsRPM7NuW9eNOE9SnbmLzujhRMLPTtLvrRB7Yq
      QYYmOFC0zBy8qA6XV8VNQ4TGe2ECI0ABylJcqDQ8voyttdagCMaHAgMBAAGjHTAb
      MAwGA1UdEwQFMAMBAf8wCwYDVR0PBAQDAgEGMAoGCCqGSM49BAMCA0gAMEUCIB4K
      2lD3X5pt7oSLEpsUAV1DzJzkN0dnVTfu6Q/Ay1iIAiEAzUOLrD4bUGlg6qIaHK9J
      3v6gObYxZBeV/36Mtj3sV2A=
      -----END CERTIFICATE-----
      -----BEGIN CERTIFICATE-----
      MIIC7jCCApSgAwIBAgIRAOg+J2VNR/U6Pq4vxArSANwwCgYIKoZIzj0EAwIwgbkx
      CzAJBgNVBAYTAlVTMQswCQYDVQQIEwJDQTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNj
      bzEaMBgGA1UECRMRMTAxIFNlY29uZCBTdHJlZXQxDjAMBgNVBBETBTk0MTA1MRcw
      FQYDVQQKEw5IYXNoaUNvcnAgSW5jLjFAMD4GA1UEAxM3Q29uc3VsIEFnZW50IENB
      IDMwODcwMzYxNjQ2NzI2NTkzNTcyNjE0OTAyNzk0NDk3NTE3MTgwNDAeFw0yMDEw
      MjkyMDQ4NTNaFw0yNTEwMjgyMDQ4NTNaMIG5MQswCQYDVQQGEwJVUzELMAkGA1UE
      CBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xGjAYBgNVBAkTETEwMSBTZWNv
      bmQgU3RyZWV0MQ4wDAYDVQQREwU5NDEwNTEXMBUGA1UEChMOSGFzaGlDb3JwIElu
      Yy4xQDA+BgNVBAMTN0NvbnN1bCBBZ2VudCBDQSAzMDg3MDM2MTY0NjcyNjU5MzU3
      MjYxNDkwMjc5NDQ5NzUxNzE4MDQwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAAQY
      CIBTyb7tO0XgAnyUPdXwBEdI8doqFwlOzmWUIMhaCe2gqzKiAgn82INCVo4s3SfH
      NTjPtK5RQ2HD7XYOx0rKo3sweTAOBgNVHQ8BAf8EBAMCAYYwDwYDVR0TAQH/BAUw
      AwEB/zApBgNVHQ4EIgQg3VQ+WjPzPflqFB9ymku+EHkVVjrsIy8+rR7yOwWwvUcw
      KwYDVR0jBCQwIoAg3VQ+WjPzPflqFB9ymku+EHkVVjrsIy8+rR7yOwWwvUcwCgYI
      KoZIzj0EAwIDSAAwRQIgZMppyR6KpxmnC+J2PHNOwTV5dwRs7Z0JKdupb4LUSIEC
      IQCBiW0T4GIdNgTwLqBTzTY7OJPsELKvr9ExUhIp2lRJ2A==
      -----END CERTIFICATE-----
    type: external
  observedGeneration: 21
[28:January:2022:03:33:36]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $
[28:January:2022:03:47:52]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ kgpn mercury ingress
Error from server (NotFound): pods "ingress" not found
[28:January:2022:04:24:24]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ kgn mercury ingress
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME                                               CLASS    HOSTS                                                               ADDRESS          PORTS     AGE
kafka-producer-tool-createsession                  <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   108d
mercury-agent-command-service                      <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-agent-command-service1                     <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-agent-command-service2                     <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-channel-api                                <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-channel-api1                               <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-consumer-command-service                   <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-consumer-command-service1                  <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-consumer-command-service2                  <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-custom-availability-service                <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-data-mask-api                              <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-data-mask-api1                             <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-engagement-queue-api                       <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   7d21h
mercury-engagement-queue-api1                      <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   7d21h
mercury-enrichment-service                         <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-kafka-kafka-0                              kafka    mercury-kafka-broker-0-mercury.service.iad-dataplane.corp.consul    10.11.8.216      80, 443   324d
mercury-kafka-kafka-1                              kafka    mercury-kafka-broker-1-mercury.service.iad-dataplane.corp.consul    10.11.8.216      80, 443   324d
mercury-kafka-kafka-2                              kafka    mercury-kafka-broker-2-mercury.service.iad-dataplane.corp.consul    10.11.8.216      80, 443   324d
mercury-kafka-kafka-bootstrap                      kafka    mercury-kafka-bootstrap-mercury.service.iad-dataplane.corp.consul   10.11.8.216      80, 443   324d
mercury-kweet-facebook-webhook                     <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-kweet-twiliosms-client                     <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-kweet-wechat-webhook                       <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-mercury-ui                                 <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-metric-fusion-bridge                       <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-metric-proxy-service                       <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-metric-proxy-service1                      <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-provisioning-processor-dummy-metrics       <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   72d
mercury-routing-processor-agent-events-processor   <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-single-sign-on-service                     <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-single-sign-on-service1                    <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-social-config                              <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-static-assets-service                      <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-transcript-api                             <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-transcript-api1                            <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-transcript-api2                            <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-user-preference-service                    <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-work-api                                   <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-work-api1                                  <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
mercury-work-api2                                  <none>   engagement-mercury-iad.corp.channels.ocs.oc-test.com                144.25.104.178   80, 443   21d
[28:January:2022:04:24:34]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ kgn mercury ingress mercury-kafka-kafka-bootstrap -oyaml

kgnWarning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    ingress.kubernetes.io/ssl-passthrough: "true"
    ingressWatcher/port: "443"
    ingressWatcher/service: mercury-kafka-bootstrap-mercury
    nginx.ingress.kubernetes.io/backend-protocol: HTTPS
    nginx.ingress.kubernetes.io/ssl-passthrough: "true"
  creationTimestamp: "2021-03-09T20:03:25Z"
  generation: 2
  labels:
    app: cpekafka
    app.kubernetes.io/instance: mercury-kafka
    app.kubernetes.io/managed-by: strimzi-cluster-operator
    app.kubernetes.io/name: kafka
    app.kubernetes.io/part-of: strimzi-mercury-kafka
    chart: cpe-kafka-0.2.9
    heritage: Helm
    release: app-kafka
    strimzi.io/cluster: mercury-kafka
    strimzi.io/kind: Kafka
    strimzi.io/name: mercury-kafka-kafka
  name: mercury-kafka-kafka-bootstrap
  namespace: mercury
  ownerReferences:
  - apiVersion: kafka.strimzi.io/v1beta2
    blockOwnerDeletion: false
    controller: false
    kind: Kafka
    name: mercury-kafka
    uid: ffe8c9d4-67e8-4b4f-aaf4-b920c9f983d9
  resourceVersion: "781512387"
  selfLink: /apis/extensions/v1beta1/namespaces/mercury/ingresses/mercury-kafka-kafka-bootstrap
  uid: 48cdb2c1-14c5-4fa4-b881-6dbd1f3114eb
spec:
  ingressClassName: kafka
  rules:
  - host: mercury-kafka-bootstrap-mercury.service.iad-dataplane.corp.consul
    http:
      paths:
      - backend:
          serviceName: mercury-kafka-kafka-external-bootstrap
          servicePort: 9094
        path: /
        pathType: Prefix
  tls:
  - hosts:
    - mercury-kafka-bootstrap-mercury.service.iad-dataplane.corp.consul
status:
  loadBalancer:
    ingress:
    - ip: 10.11.8.216
 [28:January:2022:04:24:47]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $
[28:January:2022:04:24:47]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ kgn mercury ingress mercury-kafka-kafka-0
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME                    CLASS   HOSTS                                                              ADDRESS       PORTS     AGE
mercury-kafka-kafka-0   kafka   mercury-kafka-broker-0-mercury.service.iad-dataplane.corp.consul   10.11.8.216   80, 443   324d
[28:January:2022:04:24:57]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ kgn mercury ingress mercury-kafka-kafka-0 -oyaml
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    ingress.kubernetes.io/ssl-passthrough: "true"
    ingressWatcher/port: "443"
    ingressWatcher/service: mercury-kafka-broker-0-mercury
    nginx.ingress.kubernetes.io/backend-protocol: HTTPS
    nginx.ingress.kubernetes.io/ssl-passthrough: "true"
  creationTimestamp: "2021-03-09T20:03:25Z"
  generation: 2
  labels:
    app: cpekafka
    app.kubernetes.io/instance: mercury-kafka
    app.kubernetes.io/managed-by: strimzi-cluster-operator
    app.kubernetes.io/name: kafka
    app.kubernetes.io/part-of: strimzi-mercury-kafka
    chart: cpe-kafka-0.2.9
    heritage: Helm
    release: app-kafka
    strimzi.io/cluster: mercury-kafka
    strimzi.io/kind: Kafka
    strimzi.io/name: mercury-kafka-kafka
  name: mercury-kafka-kafka-0
  namespace: mercury
  ownerReferences:
  - apiVersion: kafka.strimzi.io/v1beta2
    blockOwnerDeletion: false
    controller: false
    kind: Kafka
    name: mercury-kafka
    uid: ffe8c9d4-67e8-4b4f-aaf4-b920c9f983d9
  resourceVersion: "781512385"
  selfLink: /apis/extensions/v1beta1/namespaces/mercury/ingresses/mercury-kafka-kafka-0
  uid: c8474d51-9511-498a-acf4-afbfcd13ca68
spec:
  ingressClassName: kafka
  rules:
  - host: mercury-kafka-broker-0-mercury.service.iad-dataplane.corp.consul
    http:
      paths:
      - backend:
          serviceName: mercury-kafka-kafka-0
          servicePort: 9094
        path: /
        pathType: Prefix
  tls:
  - hosts:
    - mercury-kafka-broker-0-mercury.service.iad-dataplane.corp.consul
status:
  loadBalancer:
    ingress:
    - ip: 10.11.8.216
[28:January:2022:04:25:02]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $
[28:January:2022:05:10:53]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ free
              total        used        free      shared  buff/cache   available
Mem:        4042340     1150096      675688       18200     2216556     2595636
Swap:       1048572        4804     1043768
[29:January:2022:08:39:25]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ free -m
              total        used        free      shared  buff/cache   available
Mem:           3947        1124         658          17        2164        2533
Swap:          1023           4        1019
[29:January:2022:08:39:31]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $
[29:January:2022:11:33:52]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ netstat
bash: netstat: command not found
[29:January:2022:11:33:55]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ yum install netstat
Loaded plugins: ovl
ovl: Error while doing RPMdb copy-up:
[Errno 13] Permission denied: '/var/lib/rpm/Dirnames'
You need to be root to perform this command.
[29:January:2022:11:34:03]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ less /etc/services
[29:January:2022:11:34:40]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ ufw
bash: ufw: command not found
[29:January:2022:11:35:07]:(corp_us-ashburn-1_dataplane):~/galorndon/ctemp/traefik-test
○ $ exit
exit

 2022-01-29 17:05:20 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → #cd namebasedrouting/

 2022-01-29 17:05:25 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → grep -i "docker run" ~/.bash_profile
alias drun='docker run'
##alias dr="docker run -it -v ~/.ssh:/root/.ssh_sre -v ~/.oci:/root/.oci_sre -v ~/.kube:/root/.kube_sre -v ~/.gnupg:/root/.gnupg_sre -v /Users/azhekhan/OSVC_Code/osvc-cloud-ms:/root/galorndon -v /var/run/docker.sock:/var/run/docker.sock -e USER='azhekhan'  osvc-docker-local.dockerhub-den.oraclecorp.com/dev-environments/sre-base:latest bash"
alias dr="docker run -it -v ~/.ssh:/root/.ssh_sre -v ~/.oci:/root/.oci_sre -v ~/.kube:/root/.kube_sre -v ~/.gnupg:/root/.gnupg_sre -v /Users/azhekhan/OSVC_Code/osvc-cloud-ms:/root/galorndon -v /var/run/docker.sock:/var/run/docker.sock -v ~/OSVC_Code/osvc-cloud-ms/aabashrc.sh:/root/.bashrc -e USER='azhekhan'  osvc-docker-local.dockerhub-den.oraclecorp.com/dev-environments/sre-base:latest bash"
##alias drv2="docker run -it -v ~/.ssh:/root/.ssh_sre -v ~/.oci:/root/.oci_sre -v ~/.kube:/root/.kube_sre -v ~/.gnupg:/root/.gnupg_sre -v /Users/azhekhan/OSVC_Code/osvc-cloud-ms:/root/galorndon -v /var/run/docker.sock:/var/run/docker.sock -v ~/OSVC_Code/osvc-cloud-ms/aabashrc.sh:/root/.bashrc -e USER='azhekhan'  sre-base-v2:latest bash"
alias drv2="docker run -it -v ~/.ssh:/home/opc/.ssh_sre -v ~/.oci:/home/opc/.oci_sre -v ~/.kube:/home/opc/.kube_sre -v ~/.gnupg:/home/opc/.gnupg_sre -v /Users/azhekhan/OSVC_Code/osvc-cloud-ms:/home/opc/galorndon -v /var/run/docker.sock:/var/run/docker.sock -v ~/OSVC_Code/osvc-cloud-ms/aabashrc.sh:/home/opc/.bashrc -e USER='azhekhan'  cpe-workstation-v2:latest bash"
alias cpev2="docker run -it -v ~/.ssh:/home/opc/.ssh_sre -v ~/.oci:/home/opc/.oci_sre -v ~/.kube:/home/opc/.kube_sre -v ~/.gnupg:/home/opc/.gnupg_sre -v /Users/azhekhan/OSVC_Code/osvc-cloud-ms:/home/opc/galorndon -v /var/run/docker.sock:/var/run/docker.sock -v ~/OSVC_Code/osvc-cloud-ms/aabashrc.sh:/home/opc/.bashrc -e USER='azhekhan'  cpe-workstation-v2-230921:1.0.84 bash"

 2022-01-29 17:05:51 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → drv2
HOME_DIR : /home/opc
Review: The following Wiki for more setup instructions
https://confluence.oraclecorp.com/confluence/display/CPE/Development+Setup+for+Platform+v2
Aliasing vim --> vi editor

error: could not lock config file .git/config: No such file or directory
[29:January:2022:11:37:33]:(minikube):~
○ $
[29:January:2022:11:37:33]:(minikube):~
○ $ ufw
bash: ufw: command not found
[29:January:2022:11:37:51]:(minikube):~
○ $ netstat
bash: netstat: command not found
[29:January:2022:11:37:58]:(minikube):~
○ $ yum install netstat
Loaded plugins: ovl
ovl: Error while doing RPMdb copy-up:
[Errno 13] Permission denied: '/var/lib/rpm/Dirnames'
You need to be root to perform this command.
[29:January:2022:11:38:04]:(minikube):~
○ $ sudo
bash: sudo: command not found
[29:January:2022:11:38:07]:(minikube):~
○ $ exit
exit

 2022-01-29 17:08:20 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → dr
HOME_DIR : /root
Aliasing vim --> vi editor
error: could not lock config file .git/config: No such file or directory
[29:January:2022:11:39:03]:(minikube):/
○ $ whoami
root
[29:January:2022:11:44:27]:(minikube):/
○ $ hostname
bash: hostname: command not found
[29:January:2022:11:44:29]:(minikube):/
○ $ uname
Linux
[29:January:2022:11:44:32]:(minikube):/
○ $ netstat
bash: netstat: command not found
[29:January:2022:11:44:45]:(minikube):/
○ $ yum install netstat









































^[[2;2R^[]11;rgb:0000/0000/0000^G
Loaded plugins: ovl
https://yum.oracle.com/repo/OracleLinux/OL7/UEKR5/x86_64/repodata/repomd.xml: [Errno 14] curl#5 - "Could not resolve proxy: www-proxy-adcq7.us.oracle.com; Unknown error"
Trying other mirror.


 One of the configured repositories failed (Latest Unbreakable Enterprise Kernel Release 5 for Oracle Linux 7Server (x86_64)),
 and yum doesn't have enough cached data to continue. At this point the only
 safe thing yum can do is fail. There are a few ways to work "fix" this:

     1. Contact the upstream for the repository and get them to fix the problem.

     2. Reconfigure the baseurl/etc. for the repository, to point to a working
        upstream. This is most often useful if you are using a newer
        distribution release than is supported by the repository (and the
        packages for the previous distribution release still work).

     3. Run the command with the repository temporarily disabled
            yum --disablerepo=ol7_UEKR5 ...

     4. Disable the repository permanently, so yum won't use it by default. Yum
        will then just ignore the repository until you permanently enable it
        again or use --enablerepo for temporary usage:

            yum-config-manager --disable ol7_UEKR5
        or
            subscription-manager repos --disable=ol7_UEKR5

     5. Configure the failing repository to be skipped, if it is unavailable.
        Note that yum will try to contact the repo. when it runs most commands,
        so will have to try and fail each time (and thus. yum will be be much
        slower). If it is a very temporary problem though, this is often a nice
        compromise:

            yum-config-manager --save --setopt=ol7_UEKR5.skip_if_unavailable=true

failure: repodata/repomd.xml from ol7_UEKR5: [Errno 256] No more mirrors to try.
https://yum.oracle.com/repo/OracleLinux/OL7/UEKR5/x86_64/repodata/repomd.xml: [Errno 14] curl#5 - "Could not resolve proxy: www-proxy-adcq7.us.oracle.com; Unknown error"
[29:January:2022:11:44:53]:(minikube):/
○ $ vi terraform.zip
[29:January:2022:11:45:29]:(minikube):/
○ $ ufw
bash: ufw: command not found
[29:January:2022:11:45:40]:(minikube):/
○ $ yum install -y ufw
Loaded plugins: ovl
https://yum.oracle.com/repo/OracleLinux/OL7/UEKR5/x86_64/repodata/repomd.xml: [Errno 14] curl#5 - "Could not resolve proxy: www-proxy-adcq7.us.oracle.com; Unknown error"
Trying other mirror.


 One of the configured repositories failed (Latest Unbreakable Enterprise Kernel Release 5 for Oracle Linux 7Server (x86_64)),
 and yum doesn't have enough cached data to continue. At this point the only
 safe thing yum can do is fail. There are a few ways to work "fix" this:

     1. Contact the upstream for the repository and get them to fix the problem.

     2. Reconfigure the baseurl/etc. for the repository, to point to a working
        upstream. This is most often useful if you are using a newer
        distribution release than is supported by the repository (and the
        packages for the previous distribution release still work).

     3. Run the command with the repository temporarily disabled
            yum --disablerepo=ol7_UEKR5 ...

     4. Disable the repository permanently, so yum won't use it by default. Yum
        will then just ignore the repository until you permanently enable it
        again or use --enablerepo for temporary usage:

            yum-config-manager --disable ol7_UEKR5
        or
            subscription-manager repos --disable=ol7_UEKR5

     5. Configure the failing repository to be skipped, if it is unavailable.
        Note that yum will try to contact the repo. when it runs most commands,
        so will have to try and fail each time (and thus. yum will be be much
        slower). If it is a very temporary problem though, this is often a nice
        compromise:

            yum-config-manager --save --setopt=ol7_UEKR5.skip_if_unavailable=true

failure: repodata/repomd.xml from ol7_UEKR5: [Errno 256] No more mirrors to try.
https://yum.oracle.com/repo/OracleLinux/OL7/UEKR5/x86_64/repodata/repomd.xml: [Errno 14] curl#5 - "Could not resolve proxy: www-proxy-adcq7.us.oracle.com; Unknown error"
[29:January:2022:11:45:46]:(minikube):/
○ $ env | grep -i proxy
NO_PROXY=localhost,localhost.domain,*.docker.oraclecorp.com,oraclecorp.com,.us.oracle.com,.lan
http_proxy=http://www-proxy-adcq7.us.oracle.com:80
HTTPS_PROXY=http://www-proxy-adcq7.us.oracle.com:80
https_proxy=http://www-proxy-adcq7.us.oracle.com:80
no_proxy=localhost,localhost.domain,*.docker.oraclecorp.com,oraclecorp.com,.us.oracle.com,.lan
K8S_AUTH_PROXY=http://www-proxy-hqdc.us.oracle.com:80
HTTP_PROXY=http://www-proxy-adcq7.us.oracle.com:80
[29:January:2022:11:46:18]:(minikube):/
○ $ env | grep -i proxy| cut -d "=" -f1
NO_PROXY
http_proxy
HTTPS_PROXY
https_proxy
no_proxy
K8S_AUTH_PROXY
HTTP_PROXY
[29:January:2022:11:46:43]:(minikube):/
○ $ for envname in $(env | grep -i proxy| cut -d "=" -f1); do echo "$envname ACTION"; done
NO_PROXY ACTION
http_proxy ACTION
HTTPS_PROXY ACTION
https_proxy ACTION
no_proxy ACTION
K8S_AUTH_PROXY ACTION
HTTP_PROXY ACTION
[29:January:2022:11:47:05]:(minikube):/
○ $ for envname in $(env | grep -i proxy| cut -d "=" -f1| grep -i http); do echo "$envname ACTION";done
http_proxy ACTION
HTTPS_PROXY ACTION
https_proxy ACTION
HTTP_PROXY ACTION
[29:January:2022:11:47:45]:(minikube):/
○ $ for envname in $(env | grep -i proxy| cut -d "=" -f1| grep -i http); do echo "$envname ACTION"; unset $envname;done
http_proxy ACTION
HTTPS_PROXY ACTION
https_proxy ACTION
HTTP_PROXY ACTION
[29:January:2022:11:47:54]:(minikube):/
○ $ env | grep -i proxy
NO_PROXY=localhost,localhost.domain,*.docker.oraclecorp.com,oraclecorp.com,.us.oracle.com,.lan
no_proxy=localhost,localhost.domain,*.docker.oraclecorp.com,oraclecorp.com,.us.oracle.com,.lan
K8S_AUTH_PROXY=http://www-proxy-hqdc.us.oracle.com:80
[29:January:2022:11:48:01]:(minikube):/
○ $ yum install netstat -y
Loaded plugins: ovl
ol7_UEKR5                                                                                                               | 3.0 kB  00:00:00
ol7_developer_EPEL                                                                                                      | 3.6 kB  00:00:00
ol7_latest                                                                                                              | 3.6 kB  00:00:00
ol7_optional_latest                                                                                                     | 3.0 kB  00:00:00
ol7_oracle_instantclient                                                                                                | 2.9 kB  00:00:00
ol7_software_collections                                                                                                | 3.0 kB  00:00:00
(1/14): ol7_UEKR5/x86_64/updateinfo                                                                                     | 197 kB  00:00:00
(2/14): ol7_developer_EPEL/x86_64/updateinfo                                                                            | 592 kB  00:00:00
(3/14): ol7_developer_EPEL/x86_64/group_gz                                                                              |  88 kB  00:00:00
(4/14): ol7_latest/x86_64/group_gz                                                                                      | 136 kB  00:00:01
(5/14): ol7_latest/x86_64/updateinfo                                                                                    | 3.4 MB  00:00:08
(6/14): ol7_optional_latest/x86_64/updateinfo                                                                           | 1.4 MB  00:00:06
(7/14): ol7_oracle_instantclient/x86_64/updateinfo                                                                      |   71 B  00:00:00
(8/14): ol7_oracle_instantclient/x86_64/primary_db                                                                      |  27 kB  00:00:00
(9/14): ol7_software_collections/x86_64/updateinfo                                                                      | 8.9 kB  00:00:00
(10/14): ol7_software_collections/x86_64/primary_db                                                                     | 5.8 MB  00:00:06
(11/14): ol7_developer_EPEL/x86_64/primary_db                                                                           |  15 MB  00:00:18
(12/14): ol7_optional_latest/x86_64/primary_db                                                                          | 5.6 MB  00:00:11
(13/14): ol7_UEKR5/x86_64/primary_db                                                                                    |  39 MB  00:00:46
(14/14): ol7_latest/x86_64/primary_db                                                                                   |  38 MB  00:00:45
No package netstat available.
Error: Nothing to do
[29:January:2022:11:49:19]:(minikube):/
○ $ yum install ss -y
Loaded plugins: ovl
No package ss available.
Error: Nothing to do
[29:January:2022:11:59:45]:(minikube):/
○ $ yum install ufw -y
Loaded plugins: ovl
Resolving Dependencies
--> Running transaction check
---> Package ufw.noarch 0:0.35-9.el7 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

===============================================================================================================================================
 Package                    Arch                          Version                              Repository                                 Size
===============================================================================================================================================
Installing:
 ufw                        noarch                        0.35-9.el7                           ol7_developer_EPEL                        220 k

Transaction Summary
===============================================================================================================================================
Install  1 Package

Total download size: 220 k
Installed size: 905 k
Downloading packages:
ufw-0.35-9.el7.noarch.rpm                                                                                               | 220 kB  00:00:03
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : ufw-0.35-9.el7.noarch                                                                                                       1/1
  Verifying  : ufw-0.35-9.el7.noarch                                                                                                       1/1

Installed:
  ufw.noarch 0:0.35-9.el7

Complete!
[29:January:2022:12:00:03]:(minikube):/
○ $ ufw
ERROR: not enough args
[29:January:2022:12:00:08]:(minikube):/
○ $ yum install netstat  -y
Loaded plugins: ovl
No package netstat available.
Error: Nothing to do
[29:January:2022:12:00:41]:(minikube):/
○ $ ip -s link
bash: ip: command not found
[29:January:2022:12:05:49]:(minikube):/
○ $ ss
bash: ss: command not found
[29:January:2022:12:07:03]:(minikube):/
○ $ sudo yum install iproute iproute-doc
bash: sudo: command not found
[29:January:2022:12:07:45]:(minikube):/
○ $ yum install iproute iproute-doc
Loaded plugins: ovl
Resolving Dependencies
--> Running transaction check
---> Package iproute.x86_64 0:5.4.0-1.0.1.el7 will be installed
---> Package iproute-doc.x86_64 0:5.4.0-1.0.1.el7 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

===============================================================================================================================================
 Package                           Arch                         Version                                  Repository                       Size
===============================================================================================================================================
Installing:
 iproute                           x86_64                       5.4.0-1.0.1.el7                          ol7_UEKR5                       619 k
 iproute-doc                       x86_64                       5.4.0-1.0.1.el7                          ol7_UEKR5                        36 k

Transaction Summary
===============================================================================================================================================
Install  2 Packages

Total download size: 655 k
Installed size: 1.7 M
Is this ok [y/d/N]: y
Downloading packages:
(1/2): iproute-5.4.0-1.0.1.el7.x86_64.rpm                                                                               | 619 kB  00:00:00
(2/2): iproute-doc-5.4.0-1.0.1.el7.x86_64.rpm                                                                           |  36 kB  00:00:01
-----------------------------------------------------------------------------------------------------------------------------------------------
Total                                                                                                          342 kB/s | 655 kB  00:00:01
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : iproute-doc-5.4.0-1.0.1.el7.x86_64                                                                                          1/2
  Installing : iproute-5.4.0-1.0.1.el7.x86_64                                                                                              2/2
  Verifying  : iproute-5.4.0-1.0.1.el7.x86_64                                                                                              1/2
  Verifying  : iproute-doc-5.4.0-1.0.1.el7.x86_64                                                                                          2/2

Installed:
  iproute.x86_64 0:5.4.0-1.0.1.el7                                     iproute-doc.x86_64 0:5.4.0-1.0.1.el7

Complete!
[29:January:2022:12:07:55]:(minikube):/
○ $ ss
Netid         State         Recv-Q         Send-Q                 Local Address:Port                 Peer Address:Port         Process
[29:January:2022:12:07:56]:(minikube):/
○ $ ss -s
Total: 820
TCP:   423 (estab 0, closed 423, orphaned 0, timewait 179)

Transport Total     IP        IPv6
RAW   0         0         0
UDP   0         0         0
TCP   0         0         0
INET    0         0         0
FRAG    0         0         0

[29:January:2022:12:08:02]:(minikube):/
○ $ ss -n
Netid         State         Recv-Q         Send-Q                 Local Address:Port                 Peer Address:Port         Process
[29:January:2022:12:08:13]:(minikube):/
○ $ ss -l
RTNETLINK answers: Invalid argument
Netid        State          Recv-Q         Send-Q                 Local Address:Port                   Peer Address:Port        Process
nl           UNCONN         0              0                               rtnl:kernel                             *
nl           UNCONN         0              0                               rtnl:1418                               *
nl           UNCONN         768            0                            tcpdiag:kernel                             *
nl           UNCONN         4352           0                            tcpdiag:ss/321                             *
nl           UNCONN         0              0                               xfrm:kernel                             *
nl           UNCONN         0              0                              audit:kernel                             *
nl           UNCONN         0              0                          fiblookup:kernel                             *
nl           UNCONN         0              0                                nft:kernel                             *
nl           UNCONN         0              0                             uevent:kernel                             *
nl           UNCONN         0              0                               genl:kernel                             *                           [29:January:2022:12:08:15]:(minikube):/
○ $ ss -p
Netid         State         Recv-Q         Send-Q                 Local Address:Port                 Peer Address:Port         Process
[29:January:2022:12:08:23]:(minikube):/
○ $ ss -lnp | grep 22
RTNETLINK answers: Invalid argument
[29:January:2022:12:08:30]:(minikube):/
○ $
[29:January:2022:12:08:30]:(minikube):/
○ $ ss -l
RTNETLINK answers: Invalid argument
Netid        State          Recv-Q         Send-Q                 Local Address:Port                   Peer Address:Port        Process
nl           UNCONN         0              0                               rtnl:kernel                             *
nl           UNCONN         0              0                               rtnl:1418                               *
nl           UNCONN         4352           0                            tcpdiag:ss/357                             *
nl           UNCONN         768            0                            tcpdiag:kernel                             *
nl           UNCONN         0              0                               xfrm:kernel                             *
nl           UNCONN         0              0                              audit:kernel                             *
nl           UNCONN         0              0                          fiblookup:kernel                             *
nl           UNCONN         0              0                                nft:kernel                             *
nl           UNCONN         0              0                             uevent:kernel                             *
nl           UNCONN         0              0                               genl:kernel                             *                           [29:January:2022:12:11:52]:(minikube):/
○ $ ss -a -w
State           Recv-Q           Send-Q                     Local Address:Port                     Peer Address:Port          Process
[29:January:2022:12:13:46]:(minikube):/
○ $ iptables
iptables v1.4.21: no command specified
Try `iptables -h' or 'iptables --help' for more information.
[29:January:2022:12:15:26]:(minikube):/
○ $ iptables -h
iptables v1.4.21

Usage: iptables -[ACD] chain rule-specification [options]
       iptables -I chain [rulenum] rule-specification [options]
       iptables -R chain rulenum rule-specification [options]
       iptables -D chain rulenum [options]
       iptables -[LS] [chain [rulenum]] [options]
       iptables -[FZ] [chain] [options]
       iptables -[NX] chain
       iptables -E old-chain-name new-chain-name
       iptables -P chain target [options]
       iptables -h (print this help information)

Commands:
Either long or short options are allowed.
  --append  -A chain    Append to chain
  --check   -C chain    Check for the existence of a rule
  --delete  -D chain    Delete matching rule from chain
  --delete  -D chain rulenum
        Delete rule rulenum (1 = first) from chain
  --insert  -I chain [rulenum]
        Insert in chain as rulenum (default 1=first)
  --replace -R chain rulenum
        Replace rule rulenum (1 = first) in chain
  --list    -L [chain [rulenum]]
        List the rules in a chain or all chains
  --list-rules -S [chain [rulenum]]
        Print the rules in a chain or all chains
  --flush   -F [chain]    Delete all rules in  chain or all chains
  --zero    -Z [chain [rulenum]]
        Zero counters in chain or all chains
  --new     -N chain    Create a new user-defined chain
  --delete-chain
            -X [chain]    Delete a user-defined chain
  --policy  -P chain target
        Change policy on chain to target
  --rename-chain
            -E old-chain new-chain
        Change chain name, (moving any references)
Options:
    --ipv4  -4    Nothing (line is ignored by ip6tables-restore)
    --ipv6  -6    Error (line is ignored by iptables-restore)
[!] --protocol  -p proto  protocol: by number or name, eg. `tcp'
[!] --source  -s address[/mask][...]
        source specification
[!] --destination -d address[/mask][...]
        destination specification
[!] --in-interface -i input name[+]
        network interface name ([+] for wildcard)
 --jump -j target
        target for rule (may load target extension)
  --goto      -g chain
                              jump to chain with no return
  --match -m match
        extended match (may load extension)
  --numeric -n    numeric output of addresses and ports
[!] --out-interface -o output name[+]
        network interface name ([+] for wildcard)
  --table -t table  table to manipulate (default: `filter')
  --verbose -v    verbose mode
  --wait  -w [seconds]  maximum wait to acquire xtables lock before give up
  --wait-interval -W [usecs]  wait time to try to acquire xtables lock
        default is 1 second
  --line-numbers    print line numbers when listing
  --exact -x    expand numbers (display exact values)
[!] --fragment  -f    match second or further fragments only
  --modprobe=<command>    try to insert modules using this command
  --set-counters PKTS BYTES set the counter during insert/append
[!] --version -V    print package version.
[29:January:2022:12:16:28]:(minikube):/
○ $ iptables --list
iptables v1.4.21: can't initialize iptables table `filter': Permission denied (you must be root)
Perhaps iptables or your kernel needs to be upgraded.
[29:January:2022:12:16:50]:(minikube):/
○ $ sudo iptables --list
bash: sudo: command not found
[29:January:2022:12:16:56]:(minikube):/
○ $ whoami
root
[29:January:2022:12:16:59]:(minikube):/
○ $ netstat -l
bash: netstat: command not found
[29:January:2022:12:18:18]:(minikube):/
○ $ ss -tulwn | grep -i listen
[29:January:2022:12:20:58]:(minikube):/
○ $ ss -lnp | grep -i listen
RTNETLINK answers: Invalid argument
[29:January:2022:12:22:08]:(minikube):/
○ $
[29:January:2022:12:22:31]:(minikube):/
○ $ ss -tulwn
Netid         State         Recv-Q         Send-Q                 Local Address:Port                 Peer Address:Port         Process
[29:January:2022:12:22:43]:(minikube):/
○ $ helm ls
Error: could not get Kubernetes config for context "": invalid configuration: [unable to read client-cert /Users/azhekhan/.minikube/profiles/minikube/client.crt for minikube due to open /Users/azhekhan/.minikube/profiles/minikube/client.crt: no such file or directory, unable to read client-key /Users/azhekhan/.minikube/profiles/minikube/client.key for minikube due to open /Users/azhekhan/.minikube/profiles/minikube/client.key: no such file or directory, unable to read certificate-authority /Users/azhekhan/.minikube/ca.crt for minikube due to open /Users/azhekhan/.minikube/ca.crt: no such file or directory]
[29:January:2022:12:22:54]:(minikube):/
○ $ ip -4 addr show scope global
26: eth0@if27: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default  link-netnsid 0
    inet 172.17.0.4/16 brd 172.17.255.255 scope global eth0
       valid_lft forever preferred_lft forever
[29:January:2022:12:24:40]:(minikube):/
○ $ ip route show | grep default
default via 172.17.0.1 dev eth0
[29:January:2022:12:24:53]:(minikube):/
○ $ yum install epel-release
Loaded plugins: ovl
No package epel-release available.
Error: Nothing to do
[29:January:2022:12:26:36]:(minikube):/
○ $ yum install nginx
Loaded plugins: ovl
Resolving Dependencies
--> Running transaction check
---> Package nginx.x86_64 1:1.20.1-9.el7 will be installed
--> Processing Dependency: nginx-filesystem = 1:1.20.1-9.el7 for package: 1:nginx-1.20.1-9.el7.x86_64
--> Processing Dependency: libcrypto.so.1.1(OPENSSL_1_1_0)(64bit) for package: 1:nginx-1.20.1-9.el7.x86_64
--> Processing Dependency: libssl.so.1.1(OPENSSL_1_1_0)(64bit) for package: 1:nginx-1.20.1-9.el7.x86_64
--> Processing Dependency: libssl.so.1.1(OPENSSL_1_1_1)(64bit) for package: 1:nginx-1.20.1-9.el7.x86_64
--> Processing Dependency: nginx-filesystem for package: 1:nginx-1.20.1-9.el7.x86_64
--> Processing Dependency: openssl for package: 1:nginx-1.20.1-9.el7.x86_64
--> Processing Dependency: redhat-indexhtml for package: 1:nginx-1.20.1-9.el7.x86_64
--> Processing Dependency: system-logos for package: 1:nginx-1.20.1-9.el7.x86_64
--> Processing Dependency: libcrypto.so.1.1()(64bit) for package: 1:nginx-1.20.1-9.el7.x86_64
--> Processing Dependency: libprofiler.so.0()(64bit) for package: 1:nginx-1.20.1-9.el7.x86_64
--> Processing Dependency: libssl.so.1.1()(64bit) for package: 1:nginx-1.20.1-9.el7.x86_64
--> Running transaction check
---> Package gperftools-libs.x86_64 0:2.6.1-1.0.1.el7 will be installed
---> Package nginx-filesystem.noarch 1:1.20.1-9.el7 will be installed
---> Package openssl.x86_64 1:1.0.2k-24.el7_9 will be installed
--> Processing Dependency: openssl-libs(x86-64) = 1:1.0.2k-24.el7_9 for package: 1:openssl-1.0.2k-24.el7_9.x86_64
---> Package openssl11-libs.x86_64 1:1.1.1k-2.el7 will be installed
---> Package oracle-logos.noarch 0:70.7.0-1.0.7.el7 will be installed
---> Package redhat-indexhtml.noarch 0:7-13.0.1.el7 will be installed
--> Running transaction check
---> Package openssl-libs.x86_64 1:1.0.2k-19.0.1.el7 will be updated
--> Processing Dependency: openssl-libs(x86-64) = 1:1.0.2k-19.0.1.el7 for package: 1:openssl-devel-1.0.2k-19.0.1.el7.x86_64
---> Package openssl-libs.x86_64 1:1.0.2k-24.el7_9 will be an update
--> Running transaction check
---> Package openssl-devel.x86_64 1:1.0.2k-19.0.1.el7 will be updated
---> Package openssl-devel.x86_64 1:1.0.2k-24.el7_9 will be an update
--> Finished Dependency Resolution

Dependencies Resolved

===============================================================================================================================================
 Package                            Arch                     Version                                Repository                            Size
===============================================================================================================================================
Installing:
 nginx                              x86_64                   1:1.20.1-9.el7                         ol7_developer_EPEL                   586 k
Installing for dependencies:
 gperftools-libs                    x86_64                   2.6.1-1.0.1.el7                        ol7_latest                           271 k
 nginx-filesystem                   noarch                   1:1.20.1-9.el7                         ol7_developer_EPEL                    23 k
 openssl                            x86_64                   1:1.0.2k-24.el7_9                      ol7_latest                           493 k
 openssl11-libs                     x86_64                   1:1.1.1k-2.el7                         ol7_developer_EPEL                   1.5 M
 oracle-logos                       noarch                   70.7.0-1.0.7.el7                       ol7_latest                           4.7 M
 redhat-indexhtml                   noarch                   7-13.0.1.el7                           ol7_latest                           353 k
Updating for dependencies:
 openssl-devel                      x86_64                   1:1.0.2k-24.el7_9                      ol7_latest                           1.5 M
 openssl-libs                       x86_64                   1:1.0.2k-24.el7_9                      ol7_latest                           1.2 M

Transaction Summary
===============================================================================================================================================
Install  1 Package  (+6 Dependent packages)
Upgrade             ( 2 Dependent packages)

Total download size: 11 M
Is this ok [y/d/N]: y
Downloading packages:
Delta RPMs disabled because /usr/bin/applydeltarpm not installed.
(1/9): openssl-1.0.2k-24.el7_9.x86_64.rpm                                                                               | 493 kB  00:00:00
(2/9): gperftools-libs-2.6.1-1.0.1.el7.x86_64.rpm                                                                       | 271 kB  00:00:01
(3/9): openssl-devel-1.0.2k-24.el7_9.x86_64.rpm                                                                         | 1.5 MB  00:00:01
(4/9): nginx-1.20.1-9.el7.x86_64.rpm                                                                                    | 586 kB  00:00:02
(5/9): nginx-filesystem-1.20.1-9.el7.noarch.rpm                                                                         |  23 kB  00:00:02
(6/9): openssl-libs-1.0.2k-24.el7_9.x86_64.rpm                                                                          | 1.2 MB  00:00:01
(7/9): redhat-indexhtml-7-13.0.1.el7.noarch.rpm                                                                         | 353 kB  00:00:00
(8/9): openssl11-libs-1.1.1k-2.el7.x86_64.rpm                                                                           | 1.5 MB  00:00:02
(9/9): oracle-logos-70.7.0-1.0.7.el7.noarch.rpm                                                                         | 4.7 MB  00:00:03
-----------------------------------------------------------------------------------------------------------------------------------------------
Total                                                                                                          1.8 MB/s |  11 MB  00:00:05
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Updating   : 1:openssl-libs-1.0.2k-24.el7_9.x86_64                                                                                      1/11
  Installing : 1:openssl-1.0.2k-24.el7_9.x86_64                                                                                           2/11
  Installing : gperftools-libs-2.6.1-1.0.1.el7.x86_64                                                                                     3/11
  Installing : redhat-indexhtml-7-13.0.1.el7.noarch                                                                                       4/11
  Installing : 1:openssl11-libs-1.1.1k-2.el7.x86_64                                                                                       5/11
  Installing : oracle-logos-70.7.0-1.0.7.el7.noarch                                                                                       6/11
  Installing : 1:nginx-filesystem-1.20.1-9.el7.noarch                                                                                     7/11
  Installing : 1:nginx-1.20.1-9.el7.x86_64                                                                                                8/11
  Updating   : 1:openssl-devel-1.0.2k-24.el7_9.x86_64                                                                                     9/11
  Cleanup    : 1:openssl-devel-1.0.2k-19.0.1.el7.x86_64                                                                                  10/11
  Cleanup    : 1:openssl-libs-1.0.2k-19.0.1.el7.x86_64                                                                                   11/11
  Verifying  : 1:nginx-filesystem-1.20.1-9.el7.noarch                                                                                     1/11
  Verifying  : oracle-logos-70.7.0-1.0.7.el7.noarch                                                                                       2/11
  Verifying  : 1:nginx-1.20.1-9.el7.x86_64                                                                                                3/11
  Verifying  : 1:openssl-libs-1.0.2k-24.el7_9.x86_64                                                                                      4/11
  Verifying  : 1:openssl11-libs-1.1.1k-2.el7.x86_64                                                                                       5/11
  Verifying  : redhat-indexhtml-7-13.0.1.el7.noarch                                                                                       6/11
  Verifying  : gperftools-libs-2.6.1-1.0.1.el7.x86_64                                                                                     7/11
  Verifying  : 1:openssl-devel-1.0.2k-24.el7_9.x86_64                                                                                     8/11
  Verifying  : 1:openssl-1.0.2k-24.el7_9.x86_64                                                                                           9/11
  Verifying  : 1:openssl-devel-1.0.2k-19.0.1.el7.x86_64                                                                                  10/11
  Verifying  : 1:openssl-libs-1.0.2k-19.0.1.el7.x86_64                                                                                   11/11

Installed:
  nginx.x86_64 1:1.20.1-9.el7

Dependency Installed:
  gperftools-libs.x86_64 0:2.6.1-1.0.1.el7        nginx-filesystem.noarch 1:1.20.1-9.el7        openssl.x86_64 1:1.0.2k-24.el7_9
  openssl11-libs.x86_64 1:1.1.1k-2.el7            oracle-logos.noarch 0:70.7.0-1.0.7.el7        redhat-indexhtml.noarch 0:7-13.0.1.el7

Dependency Updated:
  openssl-devel.x86_64 1:1.0.2k-24.el7_9                                 openssl-libs.x86_64 1:1.0.2k-24.el7_9

Complete!
[29:January:2022:12:27:00]:(minikube):/
○ $ ps aux | grep -i nginx
root         557  0.0  0.0 114288  2264 pts/0    S+   12:27   0:00 grep -i nginx
[29:January:2022:12:27:07]:(minikube):/
○ $ systemctl status nginx
Failed to get D-Bus connection: Operation not permitted
[29:January:2022:12:27:17]:(minikube):/
○ $ systemctl start nginx
Failed to get D-Bus connection: Operation not permitted
[29:January:2022:12:27:21]:(minikube):/
○ $ sudo systemctl start nginx
bash: sudo: command not found
[29:January:2022:12:27:42]:(minikube):/
○ $ exit
exit

 2022-01-29 17:58:29 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → #dr

 2022-01-29 17:58:31 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → grep -i "docker run" ~/.bash_profile
alias drun='docker run'
##alias dr="docker run -it -v ~/.ssh:/root/.ssh_sre -v ~/.oci:/root/.oci_sre -v ~/.kube:/root/.kube_sre -v ~/.gnupg:/root/.gnupg_sre -v /Users/azhekhan/OSVC_Code/osvc-cloud-ms:/root/galorndon -v /var/run/docker.sock:/var/run/docker.sock -e USER='azhekhan'  osvc-docker-local.dockerhub-den.oraclecorp.com/dev-environments/sre-base:latest bash"
alias dr="docker run -it -v ~/.ssh:/root/.ssh_sre -v ~/.oci:/root/.oci_sre -v ~/.kube:/root/.kube_sre -v ~/.gnupg:/root/.gnupg_sre -v /Users/azhekhan/OSVC_Code/osvc-cloud-ms:/root/galorndon -v /var/run/docker.sock:/var/run/docker.sock -v ~/OSVC_Code/osvc-cloud-ms/aabashrc.sh:/root/.bashrc -e USER='azhekhan'  osvc-docker-local.dockerhub-den.oraclecorp.com/dev-environments/sre-base:latest bash"
##alias drv2="docker run -it -v ~/.ssh:/root/.ssh_sre -v ~/.oci:/root/.oci_sre -v ~/.kube:/root/.kube_sre -v ~/.gnupg:/root/.gnupg_sre -v /Users/azhekhan/OSVC_Code/osvc-cloud-ms:/root/galorndon -v /var/run/docker.sock:/var/run/docker.sock -v ~/OSVC_Code/osvc-cloud-ms/aabashrc.sh:/root/.bashrc -e USER='azhekhan'  sre-base-v2:latest bash"
alias drv2="docker run -it -v ~/.ssh:/home/opc/.ssh_sre -v ~/.oci:/home/opc/.oci_sre -v ~/.kube:/home/opc/.kube_sre -v ~/.gnupg:/home/opc/.gnupg_sre -v /Users/azhekhan/OSVC_Code/osvc-cloud-ms:/home/opc/galorndon -v /var/run/docker.sock:/var/run/docker.sock -v ~/OSVC_Code/osvc-cloud-ms/aabashrc.sh:/home/opc/.bashrc -e USER='azhekhan'  cpe-workstation-v2:latest bash"
alias cpev2="docker run -it -v ~/.ssh:/home/opc/.ssh_sre -v ~/.oci:/home/opc/.oci_sre -v ~/.kube:/home/opc/.kube_sre -v ~/.gnupg:/home/opc/.gnupg_sre -v /Users/azhekhan/OSVC_Code/osvc-cloud-ms:/home/opc/galorndon -v /var/run/docker.sock:/var/run/docker.sock -v ~/OSVC_Code/osvc-cloud-ms/aabashrc.sh:/home/opc/.bashrc -e USER='azhekhan'  cpe-workstation-v2-230921:1.0.84 bash"

 2022-01-29 17:58:35 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → docker run -it -v ~/.ssh:/root/.ssh_sre -v ~/.oci:/root/.oci_sre -v ~/.kube:/root/.kube_sre -v ~/.gnupg:/root/.gnupg_sre -v /Users/azhekhan/OSVC_Code/osvc-cloud-ms:/root/galorndon -v /var/run/docker.sock:/var/run/docker.sock -v ~/OSVC_Code/osvc-cloud-ms/aabashrc.sh:/root/.bashrc -e USER='azhekhan' --privileged=true --network dockernetwork centos /usr/sbin/init osvc-docker-local.dockerhub-den.oraclecorp.com/dev-environments/sre-base:latest bash
Unable to find image 'centos:latest' locally
latest: Pulling from library/centos
^C

 2022-01-29 18:02:26 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → docker run -it -v ~/.ssh:/root/.ssh_sre -v ~/.oci:/root/.oci_sre -v ~/.kube:/root/.kube_sre -v ~/.gnupg:/root/.gnupg_sre -v /Users/azhekhan/OSVC_Code/osvc-cloud-ms:/root/galorndon -v /var/run/docker.sock:/var/run/docker.sock -v ~/OSVC_Code/osvc-cloud-ms/aabashrc.sh:/root/.bashrc -e USER='azhekhan' --privileged=true --network dockernetwork /usr/sbin/init osvc-docker-local.dockerhub-den.oraclecorp.com/dev-environments/sre-base:latest bash
docker: invalid reference format.
See 'docker run --help'.

 2022-01-29 18:02:42 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → docker network

Usage:  docker network COMMAND

Manage networks

Commands:
  connect     Connect a container to a network
  create      Create a network
  disconnect  Disconnect a container from a network
  inspect     Display detailed information on one or more networks
  ls          List networks
  prune       Remove all unused networks
  rm          Remove one or more networks

Run 'docker network COMMAND --help' for more information on a command.

 2022-01-29 18:03:18 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → docker-network
-bash: docker-network: command not found

 2022-01-29 18:03:23 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → docker network --help

Usage:  docker network COMMAND

Manage networks

Commands:
  connect     Connect a container to a network
  create      Create a network
  disconnect  Disconnect a container from a network
  inspect     Display detailed information on one or more networks
  ls          List networks
  prune       Remove all unused networks
  rm          Remove one or more networks

Run 'docker network COMMAND --help' for more information on a command.

 2022-01-29 18:03:34 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → docker network ls
NETWORK ID          NAME                      DRIVER              SCOPE
13a7f65e3922        bridge                    bridge              local
3abfc515f896        host                      host                local
868c111cef47        kind                      bridge              local
4a89e57ac813        none                      null                local
0720f126f2e0        traefik-concept_default   bridge              local

 2022-01-29 18:03:41 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → docker run -it -v ~/.ssh:/root/.ssh_sre -v ~/.oci:/root/.oci_sre -v ~/.kube:/root/.kube_sre -v ~/.gnupg:/root/.gnupg_sre -v /Users/azhekhan/OSVC_Code/osvc-cloud-ms:/root/galorndon -v /var/run/docker.sock:/var/run/docker.sock -v ~/OSVC_Code/osvc-cloud-ms/aabashrc.sh:/root/.bashrc -e USER='azhekhan' --privileged=true --network bridge /usr/sbin/init osvc-docker-local.dockerhub-den.oraclecorp.com/dev-environments/sre-base:latest bash
docker: invalid reference format.
See 'docker run --help'.

 2022-01-29 18:04:08 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → uwf
-bash: uwf: command not found

 2022-01-30 06:45:23 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → lsof
COMMAND     PID     USER   FD      TYPE             DEVICE    SIZE/OFF                NODE NAME
loginwind   214 azhekhan  cwd       DIR                1,4         704                   2 /
loginwind   214 azhekhan  txt       REG                1,4     1265792 1152921500312469677 /System/Library/CoreServices/loginwindow.app/Contents/MacOS/loginwindow
loginwind   214 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
loginwind   214 azhekhan  txt       REG                1,4      126864 1152921500311925455 /System/Library/LoginPlugins/DisplayServices.loginPlugin/Contents/MacOS/DisplayServices
loginwind   214 azhekhan  txt       REG                1,4    28620288 1152921500311902385 /usr/share/icu/icudt64l.dat
loginwind   214 azhekhan  txt       REG                1,4      205992            45660617 /Library/Application Support/CrashReporter/SubmitDiagInfo.domains
loginwind   214 azhekhan  txt       REG                1,4       70608 1152921500312468425 /System/Library/LoginPlugins/FSDisconnect.loginPlugin/Contents/MacOS/FSDisconnect
loginwind   214 azhekhan  txt       REG                1,4      242128            56647497 /private/var/db/timezone/tz/2021a.3.0/icutz/icutz44l.dat
loginwind   214 azhekhan  txt       REG                1,4     3757752 1152921500311959024 /System/Library/CoreServices/SystemAppearance.bundle/Contents/Resources/SystemAppearance.car
loginwind   214 azhekhan  txt       REG                1,4      416504 1152921500311959012 /System/Library/CoreServices/SystemAppearance.bundle/Contents/Resources/VibrantLightAppearance.car
loginwind   214 azhekhan  txt       REG                1,4       43777            65284106 /private/var/db/analyticsd/events.whitelist
loginwind   214 azhekhan  txt       REG                1,4      292400 1152921500311925625 /System/Library/LoginPlugins/BezelServices.loginPlugin/Contents/MacOS/BezelServices
loginwind   214 azhekhan  txt       REG                1,4      149968 1152921500311885486 /usr/lib/libobjc-trampolines.dylib
loginwind   214 azhekhan  txt       REG                1,4      127392 1152921500312469875 /System/Library/CoreServices/ManagedClient.app/Contents/PlugIns/MCXToolsInterface.bundle/Contents/MacOS/MCXToolsInterface
loginwind   214 azhekhan  txt       REG                1,4       52656 1152921500312492471 /System/Library/Extensions/AppleHDA.kext/Contents/PlugIns/AppleHDAHALPlugIn.bundle/Contents/MacOS/AppleHDAHALPlugIn
loginwind   214 azhekhan  txt       REG                1,4       21756 1152921500312276535 /System/Library/PrivateFrameworks/BatteryUIKit.framework/Versions/A/Resources/BatteryChargedAndPlugged.pdf
loginwind   214 azhekhan  txt       REG                1,4       10163 1152921500312276565 /System/Library/PrivateFrameworks/BatteryUIKit.framework/Versions/A/Resources/BatteryLevelCapR-L.pdf
loginwind   214 azhekhan  txt       REG                1,4       10164 1152921500312276546 /System/Library/PrivateFrameworks/BatteryUIKit.framework/Versions/A/Resources/BatteryLevelCapR-R.pdf
loginwind   214 azhekhan  txt       REG                1,4        9802 1152921500312276566 /System/Library/PrivateFrameworks/BatteryUIKit.framework/Versions/A/Resources/BatteryLevelCapR-M.pdf
loginwind   214 azhekhan  txt       REG                1,4       91936 1152921500311959301 /System/Library/CoreServices/TextInputMenuCore.bundle/Contents/MacOS/TextInputMenuCore
loginwind   214 azhekhan  txt       REG                1,4     6201344            65260251 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/0/com.apple.LaunchServices.dv/com.apple.LaunchServices-1082-v2.csstore
loginwind   214 azhekhan  txt       REG                1,4     3546456 1152921500311959020 /System/Library/CoreServices/SystemAppearance.bundle/Contents/Resources/DarkAquaAppearance.car
loginwind   214 azhekhan  txt       REG                1,4     2413992 1152921500311959010 /System/Library/CoreServices/SystemAppearance.bundle/Contents/Resources/DarkAppearance.car
loginwind   214 azhekhan  txt       REG                1,4     2169148 1152921500312082703 /System/Library/Fonts/SFNS.ttf
loginwind   214 azhekhan  txt       REG                1,4     2353284 1152921500312082649 /System/Library/Fonts/Helvetica.ttc
loginwind   214 azhekhan  txt       REG                1,4     1354416 1152921500312476069 /System/Library/Frameworks/AppKit.framework/Versions/C/Resources/Assets.car
loginwind   214 azhekhan  txt       REG                1,4      873240 1152921500312197090 /System/Library/PrivateFrameworks/CoreWLANKit.framework/Versions/A/Resources/Assets.car
loginwind   214 azhekhan  txt       REG                1,4      136100            62433388 /System/Library/Caches/com.apple.IntlDataCache.le.kbdx
loginwind   214 azhekhan  txt       REG                1,4     1074864 1152921500312293677 /System/Library/Keyboard Layouts/AppleKeyboardLayouts.bundle/Contents/Resources/AppleKeyboardLayouts-L.dat
loginwind   214 azhekhan  txt       REG                1,4     3406912 1152921500311959022 /System/Library/CoreServices/SystemAppearance.bundle/Contents/Resources/Assets.car
loginwind   214 azhekhan  txt       REG                1,4       16371 1152921500312276588 /System/Library/PrivateFrameworks/BatteryUIKit.framework/Versions/A/Resources/BatteryCharging.pdf
loginwind   214 azhekhan  txt       REG                1,4      439752 1152921500312180219 /System/Library/PrivateFrameworks/CoreNLP.framework/Versions/A/Resources/tokruleLE.data
loginwind   214 azhekhan  txt       REG                1,4      247428 1152921500312277146 /System/Library/PrivateFrameworks/DataDetectorsCore.framework/Versions/A/Resources/com.apple.datadetectorscore.cache.urlifier.system
loginwind   214 azhekhan  txt       REG                1,4       32768            63984670 /private/var/db/mds/messages/se_SecurityMessages
loginwind   214 azhekhan  txt       REG                1,4       13264 1152921500312276589 /System/Library/PrivateFrameworks/BatteryUIKit.framework/Versions/A/Resources/BatteryEmpty.pdf
loginwind   214 azhekhan  txt       REG                1,4      170592 1152921500312476348 /System/Library/Frameworks/Security.framework/Versions/A/PlugIns/csparser.bundle/Contents/MacOS/csparser
loginwind   214 azhekhan  txt       REG                1,4       42208 1152921500311903538 /System/Library/KerberosPlugins/KerberosFrameworkPlugins/AppSSOLocatePlugin_macOS.bundle/Contents/MacOS/AppSSOLocatePlugin_macOS
loginwind   214 azhekhan  txt       REG                1,4       46832 1152921500311903520 /System/Library/KerberosPlugins/KerberosFrameworkPlugins/AppSSOConfigPlugin_macOS.bundle/Contents/MacOS/AppSSOConfigPlugin_macOS
loginwind   214 azhekhan  txt       REG                1,4       38144 1152921500311903547 /System/Library/KerberosPlugins/KerberosFrameworkPlugins/heimdalodpac.bundle/Contents/MacOS/heimdalodpac
loginwind   214 azhekhan  txt       REG                1,4       38256 1152921500312467933 /System/Library/KerberosPlugins/KerberosFrameworkPlugins/Reachability.bundle/Contents/MacOS/Reachability
loginwind   214 azhekhan  txt       REG                1,4       65536            47540220 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.loginwindow/com.apple.metal/3902/libraries.data
loginwind   214 azhekhan  txt       REG                1,4       65536              706530 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.loginwindow/com.apple.metal/Intel(R) Iris(TM) Plus Graphics 640/functions.data
loginwind   214 azhekhan  txt       REG                1,4       38336 1152921500312467924 /System/Library/KerberosPlugins/KerberosFrameworkPlugins/SCKerberosConfig.bundle/Contents/MacOS/SCKerberosConfig
loginwind   214 azhekhan  txt       REG                1,4    27971344 1152921500311895959 /usr/share/langid/langid.inv
loginwind   214 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
loginwind   214 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
loginwind   214 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
loginwind   214 azhekhan    2u      CHR                3,2     0t17415                 313 /dev/null
loginwind   214 azhekhan    3r      REG                1,4      205992            45660617 /Library/Application Support/CrashReporter/SubmitDiagInfo.domains
loginwind   214 azhekhan    4r      REG                1,4          77            62430226 /private/etc/security/audit_user
loginwind   214 azhekhan    5r      REG                1,4         652            62430225 /private/etc/security/audit_class
loginwind   214 azhekhan    6r      REG                1,4         358            62430227 /private/etc/security/audit_control
loginwind   214 azhekhan    7u     IPv4 0x3fbd25a258948831         0t0                 UDP *:*
loginwind   214 azhekhan    8r      REG                1,4      111033 1152921500312111011 /System/Library/Frameworks/CoreImage.framework/Versions/A/Resources/ci_stdlib.metallib
loginwind   214 azhekhan    9r      REG                1,4      308316 1152921500312110755 /System/Library/Frameworks/CoreImage.framework/Versions/A/Resources/ci_filters.metallib
loginwind   214 azhekhan   10u      REG                1,4        1536            47540219 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.loginwindow/com.apple.metal/3902/libraries.maps
loginwind   214 azhekhan   11u      REG                1,4       65536            47540220 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.loginwindow/com.apple.metal/3902/libraries.data
loginwind   214 azhekhan   12u      REG                1,4        1024              706529 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.loginwindow/com.apple.metal/Intel(R) Iris(TM) Plus Graphics 640/functions.maps
loginwind   214 azhekhan   13u      REG                1,4       65536              706530 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.loginwindow/com.apple.metal/Intel(R) Iris(TM) Plus Graphics 640/functions.data
coreauthd   529 azhekhan  cwd       DIR                1,4         704                   2 /
coreauthd   529 azhekhan  txt       REG                1,4      264112 1152921500312145606 /System/Library/Frameworks/LocalAuthentication.framework/Support/coreauthd
coreauthd   529 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
coreauthd   529 azhekhan  txt       REG                1,4      180224 1152921500312145603 /System/Library/Frameworks/LocalAuthentication.framework/Support/ModulePlugins/ModuleACM.bundle/Contents/MacOS/ModuleACM
coreauthd   529 azhekhan  txt       REG                1,4      115024 1152921500312145906 /System/Library/Frameworks/LocalAuthentication.framework/Support/MechanismPlugins/MechPasscode.bundle/Contents/MacOS/MechPasscode
coreauthd   529 azhekhan  txt       REG                1,4       52064 1152921500312145933 /System/Library/Frameworks/LocalAuthentication.framework/Support/MechanismPlugins/HIDE_MechWatch.bundle/Contents/MacOS/HIDE_MechWatch
coreauthd   529 azhekhan  txt       REG                1,4      170592 1152921500312476348 /System/Library/Frameworks/Security.framework/Versions/A/PlugIns/csparser.bundle/Contents/MacOS/csparser
coreauthd   529 azhekhan  txt       REG                1,4       32768            63985498 /private/var/db/mds/messages/501/se_SecurityMessages
coreauthd   529 azhekhan  txt       REG                1,4    28620288 1152921500311902385 /usr/share/icu/icudt64l.dat
coreauthd   529 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
coreauthd   529 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
coreauthd   529 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
coreauthd   529 azhekhan    2u      CHR                3,2         0t0                 313 /dev/null
cfprefsd    530 azhekhan  cwd       DIR                1,4         704                   2 /
cfprefsd    530 azhekhan  txt       REG                1,4       68688 1152921500312467335 /usr/sbin/cfprefsd
cfprefsd    530 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
cfprefsd    530 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
cfprefsd    530 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
cfprefsd    530 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
cfprefsd    530 azhekhan    2u      CHR                3,2         0t0                 313 /dev/null
UserEvent   616 azhekhan  cwd       DIR                1,4         704                   2 /
UserEvent   616 azhekhan  txt       REG                1,4       53264 1152921500312466743 /usr/libexec/UserEventAgent
UserEvent   616 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
UserEvent   616 azhekhan  txt       REG                1,4      170592 1152921500312476348 /System/Library/Frameworks/Security.framework/Versions/A/PlugIns/csparser.bundle/Contents/MacOS/csparser
UserEvent   616 azhekhan  txt       REG                1,4       32768            63950731 /private/var/db/mds/messages/501/se_SecurityMessages
UserEvent   616 azhekhan  txt       REG                1,4       37168 1152921500311903700 /System/Library/UserEventPlugins/AppleHIDMouseAgent.plugin/Contents/MacOS/AppleHIDMouseAgent
UserEvent   616 azhekhan  txt       REG                1,4       70704 1152921500312468060 /System/Library/UserEventPlugins/BluetoothUserAgent-Plugin.plugin/Contents/MacOS/BluetoothUserAgent-Plugin
UserEvent   616 azhekhan  txt       REG                1,4    28620288 1152921500311902385 /usr/share/icu/icudt64l.dat
UserEvent   616 azhekhan  txt       REG                1,4       40272 1152921500311903855 /System/Library/UserEventPlugins/BonjourEvents.plugin/Contents/MacOS/BonjourEvents
UserEvent   616 azhekhan  txt       REG                1,4       45936 1152921500311903817 /System/Library/UserEventPlugins/CaptiveUserAgent.plugin/Contents/MacOS/CaptiveUserAgent
UserEvent   616 azhekhan  txt       REG                1,4       48640 1152921500311903944 /System/Library/UserEventPlugins/EAPOLMonitor.plugin/Contents/MacOS/EAPOLMonitor
UserEvent   616 azhekhan  txt       REG                1,4       30544 1152921500312468134 /System/Library/UserEventPlugins/GSSNotificationForwarder.plugin/Contents/MacOS/GSSNotificationForwarder
UserEvent   616 azhekhan  txt       REG                1,4       39440 1152921500311904069 /System/Library/UserEventPlugins/PrinterMonitor.plugin/Contents/MacOS/PrinterMonitor
UserEvent   616 azhekhan  txt       REG                1,4       49264 1152921500311904269 /System/Library/UserEventPlugins/LocationMenu.plugin/Contents/MacOS/LocationMenu
UserEvent   616 azhekhan  txt       REG                1,4       49040 1152921500312468070 /System/Library/UserEventPlugins/SCMonitor.plugin/Contents/MacOS/SCMonitor
UserEvent   616 azhekhan  txt       REG                1,4       28208 1152921500312468097 /System/Library/UserEventPlugins/SystemPolicyEvents.plugin/Contents/MacOS/SystemPolicyEvents
UserEvent   616 azhekhan  txt       REG                1,4       34112 1152921500311903691 /System/Library/UserEventPlugins/com.apple.TimeMachine.plugin/Contents/MacOS/com.apple.TimeMachine
UserEvent   616 azhekhan  txt       REG                1,4       45552 1152921500312467997 /System/Library/UserEventPlugins/com.apple.alarm.plugin/Contents/MacOS/com.apple.alarm
UserEvent   616 azhekhan  txt       REG                1,4       34368 1152921500312468079 /System/Library/UserEventPlugins/com.apple.bonjour.plugin/Contents/MacOS/com.apple.bonjour
UserEvent   616 azhekhan  txt       REG                1,4       29920 1152921500312468051 /System/Library/UserEventPlugins/com.apple.cfnotification.plugin/Contents/MacOS/com.apple.cfnotification
UserEvent   616 azhekhan  txt       REG                1,4      117632 1152921500312467952 /System/Library/UserEventPlugins/com.apple.cts.plugin/Contents/MacOS/com.apple.cts
UserEvent   616 azhekhan  txt       REG                1,4       29360 1152921500312467943 /System/Library/UserEventPlugins/com.apple.diskarbitration.plugin/Contents/MacOS/com.apple.diskarbitration
UserEvent   616 azhekhan  txt       REG                1,4       29216 1152921500312468033 /System/Library/UserEventPlugins/com.apple.dispatch.vfs.plugin/Contents/MacOS/com.apple.dispatch.vfs
UserEvent   616 azhekhan  txt       REG                1,4       35344 1152921500312468125 /System/Library/UserEventPlugins/com.apple.fsevents.matching.plugin/Contents/MacOS/com.apple.fsevents.matching
UserEvent   616 azhekhan  txt       REG                1,4       33856 1152921500311904096 /System/Library/UserEventPlugins/com.apple.iokit.matching.plugin/Contents/MacOS/com.apple.iokit.matching
UserEvent   616 azhekhan  txt       REG                1,4       34048 1152921500312467979 /System/Library/UserEventPlugins/com.apple.launchd.helper.plugin/Contents/MacOS/com.apple.launchd.helper
UserEvent   616 azhekhan  txt       REG                1,4       33808 1152921500311903772 /System/Library/UserEventPlugins/com.apple.lockout.lifecycle.plugin/Contents/MacOS/com.apple.lockout.lifecycle
UserEvent   616 azhekhan  txt       REG                1,4       28384 1152921500312468116 /System/Library/UserEventPlugins/com.apple.notifyd.matching.plugin/Contents/MacOS/com.apple.notifyd.matching
UserEvent   616 azhekhan  txt       REG                1,4       98896 1152921500311904278 /System/Library/UserEventPlugins/com.apple.netsvcproxy.plugin/Contents/MacOS/com.apple.netsvcproxy
UserEvent   616 azhekhan  txt       REG                1,4       30016 1152921500312468024 /System/Library/UserEventPlugins/com.apple.nsurlsessiond.plugin/Contents/MacOS/com.apple.nsurlsessiond
UserEvent   616 azhekhan  txt       REG                1,4       28064 1152921500311904215 /System/Library/UserEventPlugins/com.apple.rapport.events.plugin/Contents/MacOS/com.apple.rapport.events
UserEvent   616 azhekhan  txt       REG                1,4       28688 1152921500312468042 /System/Library/UserEventPlugins/com.apple.reachability.plugin/Contents/MacOS/com.apple.reachability
UserEvent   616 azhekhan  txt       REG                1,4       54944 1152921500311904197 /System/Library/UserEventPlugins/com.apple.rcdevent.matching.plugin/Contents/MacOS/com.apple.rcdevent.matching
UserEvent   616 azhekhan  txt       REG                1,4       33568 1152921500312468015 /System/Library/UserEventPlugins/com.apple.remoteservicediscovery.events.plugin/Contents/MacOS/com.apple.remoteservicediscovery.events
UserEvent   616 azhekhan  txt       REG                1,4       29328 1152921500312467961 /System/Library/UserEventPlugins/com.apple.systemconfiguration.plugin/Contents/MacOS/com.apple.systemconfiguration
UserEvent   616 azhekhan  txt       REG                1,4       40992 1152921500312467988 /System/Library/UserEventPlugins/com.apple.time.plugin/Contents/MacOS/com.apple.time
UserEvent   616 azhekhan  txt       REG                1,4       38416 1152921500311903826 /System/Library/UserEventPlugins/com.apple.touchbar.matching.plugin/Contents/MacOS/com.apple.touchbar.matching
UserEvent   616 azhekhan  txt       REG                1,4       41328 1152921500312468107 /System/Library/UserEventPlugins/com.apple.universalaccess.events.plugin/Contents/MacOS/com.apple.universalaccess.events
UserEvent   616 azhekhan  txt       REG                1,4       34080 1152921500311903682 /System/Library/UserEventPlugins/com.apple.usernotificationcenter.matching.plugin/Contents/MacOS/com.apple.usernotificationcenter.matching
UserEvent   616 azhekhan  txt       REG                1,4       38320 1152921500311903745 /System/Library/UserEventPlugins/routined.events.plugin/Contents/MacOS/routined.events
UserEvent   616 azhekhan  txt       REG                1,4      242128            56647497 /private/var/db/timezone/tz/2021a.3.0/icutz/icutz44l.dat
UserEvent   616 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
UserEvent   616 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
UserEvent   616 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
UserEvent   616 azhekhan    2u      CHR                3,2       0t460                 313 /dev/null
UserEvent   616 azhekhan    3u     unix 0x3fbd25a256f07819         0t0                     ->0x3fbd25a256f07751
UserEvent   616 azhekhan    4r      DIR                1,4          96            47526654 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/0/com.apple.lockoutagent
UserEvent   616 azhekhan    5   NPOLICY
UserEvent   616 azhekhan    6u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan    7u     unix 0x3fbd25a256f05d89         0t0                     ->0x3fbd25a256f07a71
UserEvent   616 azhekhan    8r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan    9r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan   10r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan   11r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan   12r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan   13r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan   14r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan   15u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan   16r      DIR                1,4          64              678691 /private/var/db/PanicReporter
UserEvent   616 azhekhan   17r      DIR                1,4        3264            62409538 /private/var/db
UserEvent   616 azhekhan   18r      DIR                1,4         928            62409537 /private/var
UserEvent   616 azhekhan   19r      DIR                1,4         192            62409536 /private
UserEvent   616 azhekhan   20r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan   21r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan   22r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan   23u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan   24r      DIR                1,4         576              700986 /opt/cisco/anyconnect
UserEvent   616 azhekhan   25r      DIR                1,4        3168              678503 /Users/azhekhan/Library/Application Support
UserEvent   616 azhekhan   26r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan   27r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan   28r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan   29r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan   30r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan   31r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan   32u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan   33r      DIR                1,4         800            62430859 /private/tmp
UserEvent   616 azhekhan   34r      DIR                1,4        1408            62430851 /private/var/run
UserEvent   616 azhekhan   35r      DIR                1,4         928            62409537 /private/var
UserEvent   616 azhekhan   36r      DIR                1,4         192            62409536 /private
UserEvent   616 azhekhan   37r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan   38r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan   39r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan   40u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan   41r      DIR                1,4         448            47528836 /Users/azhekhan/Library/Caches/com.apple.appstoreagent
UserEvent   616 azhekhan   42r      DIR                1,4         448            47528836 /Users/azhekhan/Library/Caches/com.apple.appstoreagent
UserEvent   616 azhekhan   43r      DIR                1,4        5920              681065 /Users/azhekhan/Library/Caches
UserEvent   616 azhekhan   44r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan   45r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan   46r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan   47r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan   48r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan   49r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan   50r      DIR                1,4          96            47542655 /Users/azhekhan/Library/Application Support/com.apple.ContextStoreAgent
UserEvent   616 azhekhan   51r      DIR                1,4        3168              678503 /Users/azhekhan/Library/Application Support
UserEvent   616 azhekhan   52r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan   53r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan   54r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan   55r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan   56r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan   57r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan   58r      DIR                1,4        3168              678503 /Users/azhekhan/Library/Application Support
UserEvent   616 azhekhan   59r      DIR                1,4         448            47528836 /Users/azhekhan/Library/Caches/com.apple.appstoreagent
UserEvent   616 azhekhan   60r      DIR                1,4         448            47528836 /Users/azhekhan/Library/Caches/com.apple.appstoreagent
UserEvent   616 azhekhan   61r      DIR                1,4        5920              681065 /Users/azhekhan/Library/Caches
UserEvent   616 azhekhan   62r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan   63r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan   64r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan   65r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan   66r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan   67r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan   68u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan   69r      DIR                1,4        2592            62430861 /Applications
UserEvent   616 azhekhan   70r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan   71r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan   72r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan   73r      DIR                1,4         320            62433103 /Applications/Utilities
UserEvent   616 azhekhan   74r      DIR                1,4        2592            62430861 /Applications
UserEvent   616 azhekhan   75r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan   76r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan   77r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan   78u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan   79r      DIR                1,4        1408            62430851 /private/var/run
UserEvent   616 azhekhan   80r      DIR                1,4         800            62430859 /private/tmp
UserEvent   616 azhekhan   81r      DIR                1,4         192            62409536 /private
UserEvent   616 azhekhan   82u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan   83r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan   84r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan   85r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan   86r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan   87r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan   88r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan   89r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan   90u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan   91r      DIR                1,4         512 1152921500312300317 /System/Library/ColorSync/Profiles
UserEvent   616 azhekhan   92r      DIR                1,4         160 1152921500312299688 /System/Library/ColorSync
UserEvent   616 azhekhan   93r      DIR                1,4        3712 1152921500311902706 /System/Library
UserEvent   616 azhekhan   94r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan   95r      DIR                1,4         320            62421196 /Library/ColorSync/Profiles
UserEvent   616 azhekhan   96r      DIR                1,4         128            62421194 /Library/ColorSync
UserEvent   616 azhekhan   97r      DIR                1,4        2496            62409876 /Library
UserEvent   616 azhekhan   98r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan   99r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  100r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  101r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  102r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  103r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan  104r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan  105r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  106r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  107r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  108u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  109r      DIR                1,4         160              678537 /Users/azhekhan/Library/Internet Plug-Ins
UserEvent   616 azhekhan  110r      DIR                1,4         160              678537 /Users/azhekhan/Library/Internet Plug-Ins
UserEvent   616 azhekhan  111r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  112r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan  113r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan  114r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  115r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  116r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  117u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  118r      DIR                1,4         192            62426604 /Library/Internet Plug-Ins
UserEvent   616 azhekhan  119r      DIR                1,4         192            62426604 /Library/Internet Plug-Ins
UserEvent   616 azhekhan  120r      DIR                1,4        2496            62409876 /Library
UserEvent   616 azhekhan  121r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  122r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  123r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  124u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  125r      DIR                1,4       10176              680931 /Users/azhekhan/Library/Containers
UserEvent   616 azhekhan  126r      DIR                1,4       10176              680931 /Users/azhekhan/Library/Containers
UserEvent   616 azhekhan  127r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  128r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan  129r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan  130r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  131r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  132r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  133u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  134r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  135r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  136r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan  137r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan  138r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  139r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  140r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  141u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  142r      REG                1,4           7              700019 /Library/Application Support/Apple/Remote Desktop/RemoteManagement.launchd
UserEvent   616 azhekhan  143r      DIR                1,4         224              128895 /Library/Application Support/Apple/Remote Desktop
UserEvent   616 azhekhan  144r      DIR                1,4         384            62411716 /Library/Application Support/Apple
UserEvent   616 azhekhan  145r      DIR                1,4         832            62411715 /Library/Application Support
UserEvent   616 azhekhan  146r      DIR                1,4        2496            62409876 /Library
UserEvent   616 azhekhan  147r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  148r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  149r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  150u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  151r      DIR                1,4         160            47740967 /Users/azhekhan/Library/studentd
UserEvent   616 azhekhan  152r      DIR                1,4         160            47740967 /Users/azhekhan/Library/studentd
UserEvent   616 azhekhan  153r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  154r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan  155r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan  156r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  157r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  158r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  159u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  160r      DIR                1,4         704              683239 /Users/azhekhan/Library/Mobile Documents
UserEvent   616 azhekhan  161r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  162r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan  163r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan  164r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  165r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  166r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  167u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  168r      DIR                1,4          96            13628903 /Library/Services
UserEvent   616 azhekhan  169r      DIR                1,4        2496            62409876 /Library
UserEvent   616 azhekhan  170r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  171r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  172r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  173u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  174r      DIR                1,4          96             2630095 /Users/azhekhan/Library/Services
UserEvent   616 azhekhan  175r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  176r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan  177r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan  178r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  179r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  180r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  181u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  182r      DIR                1,4         416              684735 /Users/azhekhan/Library/Caches/GeoServices
UserEvent   616 azhekhan  183r      DIR                1,4         416              684735 /Users/azhekhan/Library/Caches/GeoServices
UserEvent   616 azhekhan  184r      DIR                1,4        5920              681065 /Users/azhekhan/Library/Caches
UserEvent   616 azhekhan  185r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  186r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan  187r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan  188r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  189r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  190r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  191u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  192r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  193r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  194r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan  195r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan  196r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  197r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  198r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  199u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  200r      REG                1,4         218            62410124 /Library/Apple/Library/Bundles/TCC_Compatibility.bundle/Contents/Resources/AllowApplicationsList.plist
UserEvent   616 azhekhan  201r      DIR                1,4          96            62410123 /Library/Apple/Library/Bundles/TCC_Compatibility.bundle/Contents/Resources
UserEvent   616 azhekhan  202r      DIR                1,4         192            62410117 /Library/Apple/Library/Bundles/TCC_Compatibility.bundle/Contents
UserEvent   616 azhekhan  203r      DIR                1,4          96            62410116 /Library/Apple/Library/Bundles/TCC_Compatibility.bundle
UserEvent   616 azhekhan  204r      DIR                1,4         160            62409895 /Library/Apple/Library/Bundles
UserEvent   616 azhekhan  205r      DIR                1,4          96            62409894 /Library/Apple/Library
UserEvent   616 azhekhan  206r      DIR                1,4         160            62409877 /Library/Apple
UserEvent   616 azhekhan  207r      DIR                1,4        2496            62409876 /Library
UserEvent   616 azhekhan  208r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  209r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  210r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  211r      DIR                1,4         160 1152921500312081025 /System/Library/Sandbox
UserEvent   616 azhekhan  212r      DIR                1,4         160 1152921500312081025 /System/Library/Sandbox
UserEvent   616 azhekhan  213r      DIR                1,4        3712 1152921500311902706 /System/Library
UserEvent   616 azhekhan  214r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  215r      DIR                1,4         832            62411715 /Library/Application Support
UserEvent   616 azhekhan  216r      DIR                1,4         832            62411715 /Library/Application Support
UserEvent   616 azhekhan  217r      DIR                1,4        2496            62409876 /Library
UserEvent   616 azhekhan  218r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  219r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  220r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  221u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  223r      DIR                1,4         576              700986 /opt/cisco/anyconnect
UserEvent   616 azhekhan  224r      DIR                1,4         128              700985 /opt/cisco
UserEvent   616 azhekhan  225r      DIR                1,4         192            47518019 /opt
UserEvent   616 azhekhan  226r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  227r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  228r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  229u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  230r      DIR                1,4         576              700986 /opt/cisco/anyconnect
UserEvent   616 azhekhan  231r      DIR                1,4         576              700986 /opt/cisco/anyconnect
UserEvent   616 azhekhan  232r      DIR                1,4         128              700985 /opt/cisco
UserEvent   616 azhekhan  233r      DIR                1,4         192            47518019 /opt
UserEvent   616 azhekhan  234r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  235r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  236r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  237u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  238r      DIR                1,4         192              942549 /Users/azhekhan/Library/Logs/DiagnosticReports
UserEvent   616 azhekhan  239r      DIR                1,4        3776              680980 /Users/azhekhan/Library/Logs
UserEvent   616 azhekhan  240r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  241r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan  242r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan  243r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  244r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  245r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  246r      REG                1,4           0            63985486 /Users/azhekhan/Library/Application Support/com.apple.ContextStoreAgent/screenTimeEnabled
UserEvent   616 azhekhan  247u    systm 0x3fbd25a25a4a8899         0t0                     [ctl com.apple.netsrc id 7 unit 7]
distnoted   618 azhekhan  cwd       DIR                1,4         704                   2 /
distnoted   618 azhekhan  txt       REG                1,4      173552 1152921500312467295 /usr/sbin/distnoted
distnoted   618 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
distnoted   618 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
distnoted   618 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
distnoted   618 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
distnoted   618 azhekhan    2u      CHR                3,2       0t166                 313 /dev/null
lsd         621 azhekhan  cwd       DIR                1,4         704                   2 /
lsd         621 azhekhan  txt       REG                1,4       32688 1152921500312466890 /usr/libexec/lsd
lsd         621 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
lsd         621 azhekhan  txt       REG                1,4       32768            63950731 /private/var/db/mds/messages/501/se_SecurityMessages
lsd         621 azhekhan  txt       REG                1,4      170592 1152921500312476348 /System/Library/Frameworks/Security.framework/Versions/A/PlugIns/csparser.bundle/Contents/MacOS/csparser
lsd         621 azhekhan  txt       REG                1,4      242128            56647497 /private/var/db/timezone/tz/2021a.3.0/icutz/icutz44l.dat
lsd         621 azhekhan  txt       REG                1,4     6201344            65260251 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/0/com.apple.LaunchServices.dv/com.apple.LaunchServices-1082-v2.csstore
lsd         621 azhekhan  txt       REG                1,4    28620288 1152921500311902385 /usr/share/icu/icudt64l.dat
lsd         621 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
lsd         621 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
lsd         621 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
lsd         621 azhekhan    2u      CHR                3,2         0t0                 313 /dev/null
lsd         621 azhekhan    3u      REG                1,4       12288              695974 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/0/com.apple.LaunchServices.dv/com.apple.LaunchServices.trustedsignatures-501.db
trustd      622 azhekhan  cwd       DIR                1,4         704                   2 /
trustd      622 azhekhan  txt       REG                1,4      518576 1152921500312466908 /usr/libexec/trustd
trustd      622 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
trustd      622 azhekhan  txt       REG                1,4       32768             1842699 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/trustd_health_analytics.db-shm
trustd      622 azhekhan  txt       REG                1,4      200576 1152921500311988881 /System/Library/Security/Certificates.bundle/Contents/Resources/certsTable.data
trustd      622 azhekhan  txt       REG                1,4       32768            62433366 /Library/Keychains/crls/valid.sqlite3-shm
trustd      622 azhekhan  txt       REG                1,4       32768              680912 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/ocspcache.sqlite3-shm
trustd      622 azhekhan  txt       REG                1,4       32768             1842705 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/trust_analytics.db-shm
trustd      622 azhekhan  txt       REG                1,4    28620288 1152921500311902385 /usr/share/icu/icudt64l.dat
trustd      622 azhekhan  txt       REG                1,4       32768            63950731 /private/var/db/mds/messages/501/se_SecurityMessages
trustd      622 azhekhan  txt       REG                1,4      242128            56647497 /private/var/db/timezone/tz/2021a.3.0/icutz/icutz44l.dat
trustd      622 azhekhan  txt       REG                1,4       32768             1843121 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/TLS_analytics.db-shm
trustd      622 azhekhan  txt       REG                1,4       54447            61369618 /private/var/db/nsurlstoraged/dafsaData.bin
trustd      622 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
trustd      622 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
trustd      622 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
trustd      622 azhekhan    2u      CHR                3,2       0t715                 313 /dev/null
trustd      622 azhekhan    3r      REG                1,4      200704              678684 /Library/Keychains/pinningrules.sqlite3
trustd      622 azhekhan    4u      REG                1,4       57344             1842696 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/trustd_health_analytics.db
trustd      622 azhekhan    5u      REG                1,4      618032             1842698 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/trustd_health_analytics.db-wal
trustd      622 azhekhan    6u      REG                1,4       32768             1842699 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/trustd_health_analytics.db-shm
trustd      622 azhekhan    7r      REG                1,4     6713344            62433362 /Library/Keychains/crls/valid.sqlite3
trustd      622 azhekhan    8r      REG                1,4     1549152            62433365 /Library/Keychains/crls/valid.sqlite3-wal
trustd      622 azhekhan    9r      REG                1,4       32768            62433366 /Library/Keychains/crls/valid.sqlite3-shm
trustd      622 azhekhan   10u      REG                1,4      233472              680908 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/ocspcache.sqlite3
trustd      622 azhekhan   11u      REG                1,4     2134192              680911 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/ocspcache.sqlite3-wal
trustd      622 azhekhan   12u      REG                1,4       32768              680912 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/ocspcache.sqlite3-shm
trustd      622 azhekhan   13u      REG                1,4      606208             1842702 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/trust_analytics.db
trustd      622 azhekhan   14u      REG                1,4     2163032             1842704 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/trust_analytics.db-wal
trustd      622 azhekhan   15u      REG                1,4       32768             1842705 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/trust_analytics.db-shm
trustd      622 azhekhan   16u      REG                1,4     1843200             1843118 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/TLS_analytics.db
trustd      622 azhekhan   17u      REG                1,4     3823392             1843120 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/TLS_analytics.db-wal
trustd      622 azhekhan   18u      REG                1,4       32768             1843121 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/TLS_analytics.db-shm
trustd      622 azhekhan   19r      REG                1,4       54447            61369618 /private/var/db/nsurlstoraged/dafsaData.bin
trustd      622 azhekhan   20   NPOLICY
trustd      622 azhekhan   21u      REG                1,4       65536             1875547 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/caissuercache.sqlite3
trustd      622 azhekhan   22u    systm 0x3fbd25a25c1dc419         0t0                     [ctl com.apple.netsrc id 7 unit 27]
knowledge   624 azhekhan  cwd       DIR                1,4         704                   2 /
knowledge   624 azhekhan  txt       REG                1,4       80544 1152921500311884260 /usr/libexec/knowledge-agent
knowledge   624 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
knowledge   624 azhekhan  txt       REG                1,4      149968 1152921500311885486 /usr/lib/libobjc-trampolines.dylib
knowledge   624 azhekhan  txt       REG                1,4       32768              689644 /Users/azhekhan/Library/Application Support/Knowledge/knowledgeC.db-shm
knowledge   624 azhekhan  txt       REG                1,4    28620288 1152921500311902385 /usr/share/icu/icudt64l.dat
knowledge   624 azhekhan  txt       REG                1,4      242128            56647497 /private/var/db/timezone/tz/2021a.3.0/icutz/icutz44l.dat
knowledge   624 azhekhan  txt       REG                1,4      205992            45660617 /Library/Application Support/CrashReporter/SubmitDiagInfo.domains
knowledge   624 azhekhan  txt       REG                1,4       43777            65284106 /private/var/db/analyticsd/events.whitelist
knowledge   624 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
knowledge   624 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
knowledge   624 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
knowledge   624 azhekhan    2u      CHR                3,2         0t0                 313 /dev/null
knowledge   624 azhekhan    3u      REG                1,4     4382720              689639 /Users/azhekhan/Library/Application Support/Knowledge/knowledgeC.db
knowledge   624 azhekhan    4u      REG                1,4     1030032              689643 /Users/azhekhan/Library/Application Support/Knowledge/knowledgeC.db-wal
knowledge   624 azhekhan    5u      REG                1,4       32768              689644 /Users/azhekhan/Library/Application Support/Knowledge/knowledgeC.db-shm
knowledge   624 azhekhan    6r      REG                1,4      205992            45660617 /Library/Application Support/CrashReporter/SubmitDiagInfo.domains
secd        625 azhekhan  cwd       DIR                1,4         704                   2 /
secd        625 azhekhan  txt       REG                1,4     3153344 1152921500312466922 /usr/libexec/secd
secd        625 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
secd        625 azhekhan  txt       REG                1,4       32768              681003 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/keychain-2.db-shm
secd        625 azhekhan  txt       REG                1,4       32768             1842736 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/Analytics/ckks_analytics.db-shm
secd        625 azhekhan  txt       REG                1,4      242128            56647497 /private/var/db/timezone/tz/2021a.3.0/icutz/icutz44l.dat
secd        625 azhekhan  txt       REG                1,4       43777            65284106 /private/var/db/analyticsd/events.whitelist
secd        625 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
secd        625 azhekhan  txt       REG                1,4    28620288 1152921500311902385 /usr/share/icu/icudt64l.dat
secd        625 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
secd        625 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
secd        625 azhekhan    2u      CHR                3,2      0t1865                 313 /dev/null
secd        625 azhekhan    3u     unix 0x3fbd25a256f05f19         0t0                     ->0x3fbd25a25228d3c9
secd        625 azhekhan    4u      REG                1,4     2453504              680999 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/keychain-2.db
secd        625 azhekhan    5u      REG                1,4     4317792              681002 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/keychain-2.db-wal
secd        625 azhekhan    6u      REG                1,4       32768              681003 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/keychain-2.db-shm
secd        625 azhekhan    7u      REG                1,4       69632             1842733 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/Analytics/ckks_analytics.db
secd        625 azhekhan    8u      REG                1,4     2453504              680999 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/keychain-2.db
secd        625 azhekhan    9u      REG                1,4     4317792              681002 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/keychain-2.db-wal
secd        625 azhekhan   10u      REG                1,4     1998232             1842735 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/Analytics/ckks_analytics.db-wal
secd        625 azhekhan   11u      REG                1,4       32768             1842736 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/Analytics/ckks_analytics.db-shm
secd        625 azhekhan   12   NPOLICY
secd        625 azhekhan   13u      REG                1,4     2453504              680999 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/keychain-2.db
secd        625 azhekhan   14u      REG                1,4     4317792              681002 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/keychain-2.db-wal
accountsd   626 azhekhan  cwd       DIR                1,4         704                   2 /
accountsd   626 azhekhan  txt       REG                1,4       41744 1152921500312104250 /System/Library/Frameworks/Accounts.framework/Versions/A/Support/accountsd
accountsd   626 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
accountsd   626 azhekhan  txt       REG                1,4      149968 1152921500311885486 /usr/lib/libobjc-trampolines.dylib
accountsd   626 azhekhan  txt       REG                1,4       32768            62441357 /Users/azhekhan/Library/Accounts/Accounts4.sqlite-shm
accountsd   626 azhekhan  txt       REG                1,4    28620288 1152921500311902385 /usr/share/icu/icudt64l.dat
accountsd   626 azhekhan  txt       REG                1,4       84976 1152921500312297257 /System/Library/Accounts/Authentication/iCloudIDAuthentication.bundle/Contents/MacOS/iCloudIDAuthentication
accountsd   626 azhekhan  txt       REG                1,4       32768            63985498 /private/var/db/mds/messages/501/se_SecurityMessages
accountsd   626 azhekhan  txt       REG                1,4      242128            56647497 /private/var/db/timezone/tz/2021a.3.0/icutz/icutz44l.dat
accountsd   626 azhekhan  txt       REG                1,4       73376 1152921500312297608 /System/Library/Accounts/Authentication/CalendarAuthenticationPlugin.bundle/Contents/MacOS/CalendarAuthenticationPlugin
accountsd   626 azhekhan  txt       REG                1,4      194128            61205039 /Library/Apple/System/Library/StagedFrameworks/Safari/AuthenticationServices.framework/Versions/A/AuthenticationServices
accountsd   626 azhekhan  txt       REG                1,4     3703968            61201211 /Library/Apple/System/Library/StagedFrameworks/Safari/WebKitLegacy.framework/Versions/A/WebKitLegacy
accountsd   626 azhekhan  txt       REG                1,4       22560            61200731 /Library/Apple/System/Library/StagedFrameworks/Safari/WebInspectorUI.framework/Versions/A/WebInspectorUI
accountsd   626 azhekhan  txt       REG                1,4     6083840            61203916 /Library/Apple/System/Library/StagedFrameworks/Safari/libANGLE-shared.dylib
accountsd   626 azhekhan  txt       REG                1,4       34320            61204907 /Library/Apple/System/Library/StagedFrameworks/Safari/AuthenticationServicesCore.framework/Versions/A/AuthenticationServicesCore
accountsd   626 azhekhan  txt       REG                1,4       52112 1152921500312297201 /System/Library/Accounts/Authentication/GoogleAuthenticationPlugin.bundle/Contents/MacOS/GoogleAuthenticationPlugin
accountsd   626 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
accountsd   626 azhekhan  txt       REG                1,4    18434080            61204665 /Library/Apple/System/Library/StagedFrameworks/Safari/WebKit.framework/Versions/A/WebKit
accountsd   626 azhekhan  txt       REG                1,4    56058832            61205459 /Library/Apple/System/Library/StagedFrameworks/Safari/WebCore.framework/Versions/A/WebCore
accountsd   626 azhekhan  txt       REG                1,4    12399920            61201112 /Library/Apple/System/Library/StagedFrameworks/Safari/libwebrtc.dylib
accountsd   626 azhekhan  txt       REG                1,4    25853056            61205473 /Library/Apple/System/Library/StagedFrameworks/Safari/JavaScriptCore.framework/Versions/A/JavaScriptCore
accountsd   626 azhekhan  txt       REG                1,4     1176864            61203922 /Library/Apple/System/Library/StagedFrameworks/Safari/SafariCore.framework/Versions/A/SafariCore
accountsd   626 azhekhan  txt       REG                1,4      166736            61204062 /Library/Apple/System/Library/StagedFrameworks/Safari/SafariFoundation.framework/Versions/A/SafariFoundation
accountsd   626 azhekhan  txt       REG                1,4       54928 1152921500312296495 /System/Library/Accounts/Notification/CalendarNotificationPlugin.bundle/Contents/MacOS/CalendarNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       41680 1152921500312296805 /System/Library/Accounts/Notification/ExchangeNotificationPlugin.bundle/Contents/MacOS/ExchangeNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       89616 1152921500312494706 /System/Library/Accounts/Notification/EmailNotificationPlugin.bundle/Contents/MacOS/EmailNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       42192 1152921500312296868 /System/Library/Accounts/Notification/SPAAccountsNotificationPlugin.bundle/Contents/MacOS/SPAAccountsNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       36928 1152921500312494687 /System/Library/Accounts/Notification/SyncedDefaultsAccountNotificationPlugin.bundle/Contents/MacOS/SyncedDefaultsAccountNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       37008 1152921500312296486 /System/Library/Accounts/Notification/ExchangeSyncNotificationPlugin.bundle/Contents/MacOS/ExchangeSyncNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       66224 1152921500312296541 /System/Library/Accounts/Notification/NotesAccountNotificationPlugin.bundle/Contents/MacOS/NotesAccountNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       32768              700646 /Users/azhekhan/Library/Containers/com.apple.Notes/Data/Library/Notes/NotesV7.storedata-shm
accountsd   626 azhekhan  txt       REG                1,4       82464 1152921500312494660 /System/Library/Accounts/Notification/ContactsAccountsNotificationPlugin.bundle/Contents/MacOS/ContactsAccountsNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       42032 1152921500312296522 /System/Library/Accounts/Notification/InternetAccountsNotificationPlugin.bundle/Contents/MacOS/InternetAccountsNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       52000 1152921500312296513 /System/Library/Accounts/Notification/RemoteManagementAccountsNotificationPlugin.bundle/Contents/MacOS/RemoteManagementAccountsNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       41088 1152921500312296814 /System/Library/Accounts/Notification/AMPAccountNotificationPlugin.bundle/Contents/MacOS/AMPAccountNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4      101120 1152921500312297221 /System/Library/Accounts/Authentication/AMSAccountAuthenticationPlugin.bundle/Contents/MacOS/AMSAccountAuthenticationPlugin
accountsd   626 azhekhan  txt       REG                1,4      675554 1152921500311916191 /System/Library/PreferencePanes/Notifications.prefPane/Contents/Resources/Notifications.icns
accountsd   626 azhekhan  txt       REG                1,4      144000 1152921500312298783 /System/Library/Accounts/UI/iCloudUIPlugin.bundle/Contents/MacOS/iCloudUIPlugin
accountsd   626 azhekhan  txt       REG                1,4       55680 1152921500312299290 /System/Library/Accounts/UI/CalendarUIPlugin.bundle/Contents/MacOS/CalendarUIPlugin
accountsd   626 azhekhan  txt       REG                1,4      119920 1152921500312494785 /System/Library/Accounts/UI/GameCenterAccountsUIPlugin.bundle/Contents/MacOS/GameCenterAccountsUIPlugin
accountsd   626 azhekhan  txt       REG                1,4       80224 1152921500312494767 /System/Library/Accounts/UI/ContactsAccountsUIPlugin.bundle/Contents/MacOS/ContactsAccountsUIPlugin
accountsd   626 azhekhan  txt       REG                1,4       70752 1152921500312494795 /System/Library/Accounts/UI/EmailUIPlugin.bundle/Contents/MacOS/EmailUIPlugin
accountsd   626 azhekhan  txt       REG                1,4       86112 1152921500312298261 /System/Library/Accounts/UI/ExchangeUIPlugin.bundle/Contents/MacOS/ExchangeUIPlugin
accountsd   626 azhekhan  txt       REG                1,4       42096 1152921500312260304 /System/Library/PrivateFrameworks/AOSUI.framework/Versions/A/Resources/Assets.car
accountsd   626 azhekhan  txt       REG                1,4     5274676 1152921500312277151 /System/Library/PrivateFrameworks/DataDetectorsCore.framework/Versions/A/Resources/com.apple.datadetectorscore.cache.full.asia.system
accountsd   626 azhekhan  txt       REG                1,4    27971344 1152921500311895959 /usr/share/langid/langid.inv
accountsd   626 azhekhan  txt       REG                1,4       32768              689728 /Users/azhekhan/Library/Group Containers/group.com.apple.notes/NoteStore.sqlite-shm
accountsd   626 azhekhan  txt       REG                1,4       38352 1152921500312468822 /System/Library/Address Book Plug-Ins/LocalSource.sourcebundle/Contents/MacOS/LocalSource
accountsd   626 azhekhan  txt       REG                1,4       58496 1152921500312468781 /System/Library/Address Book Plug-Ins/DirectoryServices.sourcebundle/Contents/MacOS/DirectoryServices
accountsd   626 azhekhan  txt       REG                1,4      759856 1152921500311931359 /System/Library/Address Book Plug-Ins/CardDAVPlugin.sourcebundle/Contents/MacOS/CardDAVPlugin
accountsd   626 azhekhan  txt       REG                1,4       36576 1152921500312296787 /System/Library/Accounts/Notification/AOSKitNotificationPlugin.bundle/Contents/MacOS/AOSKitNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4      155008 1152921500311941319 /System/Library/CoreServices/SetupAssistantPlugins/AMPAccounts.icdplugin/Contents/MacOS/AMPAccounts
accountsd   626 azhekhan  txt       REG                1,4       31904 1152921500312469142 /System/Library/CoreServices/SetupAssistantPlugins/AppStore.icdplugin/Contents/MacOS/AppStore
accountsd   626 azhekhan  txt       REG                1,4       47456 1152921500311941790 /System/Library/CoreServices/SetupAssistantPlugins/iCloudDelegate.icdplugin/Contents/MacOS/iCloudDelegate
accountsd   626 azhekhan  txt       REG                1,4       31904 1152921500312469122 /System/Library/CoreServices/SetupAssistantPlugins/iBooks.icdplugin/Contents/MacOS/iBooks
accountsd   626 azhekhan  txt       REG                1,4       49056 1152921500311941337 /System/Library/CoreServices/SetupAssistantPlugins/IdentityServices.icdplugin/Contents/MacOS/IdentityServices
accountsd   626 azhekhan  txt       REG                1,4       36416 1152921500312469102 /System/Library/CoreServices/SetupAssistantPlugins/GameCenter.icdplugin/Contents/MacOS/GameCenter
accountsd   626 azhekhan  txt       REG                1,4       45408 1152921500312296832 /System/Library/Accounts/Notification/iCloudIDNotification.bundle/Contents/MacOS/iCloudIDNotification
accountsd   626 azhekhan  txt       REG                1,4       51568 1152921500312494678 /System/Library/Accounts/Notification/CloudDocsAccountNotificationPlugin.bundle/Contents/MacOS/CloudDocsAccountNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       46752 1152921500312297029 /System/Library/Accounts/Notification/RemindersAccountNotificationPlugin.bundle/Contents/MacOS/RemindersAccountNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       43777            65284106 /private/var/db/analyticsd/events.whitelist
accountsd   626 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
accountsd   626 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
accountsd   626 azhekhan    2u      CHR                3,2      0t1296                 313 /dev/null
accountsd   626 azhekhan    3u      REG                1,4      282624              681028 /Users/azhekhan/Library/Accounts/Accounts4.sqlite
accountsd   626 azhekhan    4u      REG                1,4     3489672            62441356 /Users/azhekhan/Library/Accounts/Accounts4.sqlite-wal
accountsd   626 azhekhan    5u      REG                1,4       32768            62441357 /Users/azhekhan/Library/Accounts/Accounts4.sqlite-shm
accountsd   626 azhekhan    6u      REG                1,4      282624              681028 /Users/azhekhan/Library/Accounts/Accounts4.sqlite
accountsd   626 azhekhan    7u      REG                1,4     3489672            62441356 /Users/azhekhan/Library/Accounts/Accounts4.sqlite-wal
accountsd   626 azhekhan    8u      REG                1,4      282624              681028 /Users/azhekhan/Library/Accounts/Accounts4.sqlite
accountsd   626 azhekhan    9u      REG                1,4      282624              681028 /Users/azhekhan/Library/Accounts/Accounts4.sqlite
accountsd   626 azhekhan   10   NPOLICY
accountsd   626 azhekhan   11u      REG                1,4     3489672            62441356 /Users/azhekhan/Library/Accounts/Accounts4.sqlite-wal
accountsd   626 azhekhan   12u      REG                1,4     3489672            62441356 /Users/azhekhan/Library/Accounts/Accounts4.sqlite-wal
accountsd   626 azhekhan   13u     unix 0x3fbd25a26dfea8d9         0t0                     ->0x3fbd25a25228d3c9
accountsd   626 azhekhan   14u    systm 0x3fbd25a25aef6539         0t0                     [ctl com.apple.netsrc id 7 unit 20]
accountsd   626 azhekhan   15u      REG                1,4      245760              700641 /Users/azhekhan/Library/Containers/com.apple.Notes/Data/Library/Notes/NotesV7.storedata
accountsd   626 azhekhan   16u      REG                1,4           0              700645 /Users/azhekhan/Library/Containers/com.apple.Notes/Data/Library/Notes/NotesV7.storedata-wal
accountsd   626 azhekhan   17u      REG                1,4       32768              700646 /Users/azhekhan/Library/Containers/com.apple.Notes/Data/Library/Notes/NotesV7.storedata-shm
accountsd   626 azhekhan   18u      REG                1,4      315392              689723 /Users/azhekhan/Library/Group Containers/group.com.apple.notes/NoteStore.sqlite
accountsd   626 azhekhan   19u      REG                1,4      230752              689727 /Users/azhekhan/Library/Group Containers/group.com.apple.notes/NoteStore.sqlite-wal
accountsd   626 azhekhan   20u      REG                1,4       32768              689728 /Users/azhekhan/Library/Group Containers/group.com.apple.notes/NoteStore.sqlite-shm
neagent     627 azhekhan  cwd       DIR                1,4         704                   2 /
neagent     627 azhekhan  txt       REG                1,4      125136 1152921500311881486 /usr/libexec/neagent
neagent     627 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
neagent     627 azhekhan  txt       REG                1,4     6201344            65260251 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/0/com.apple.LaunchServices.dv/com.apple.LaunchServices-1082-v2.csstore
neagent     627 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
neagent     627 azhekhan  txt       REG                1,4    28620288 1152921500311902385 /usr/share/icu/icudt64l.dat
neagent     627 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
neagent     627 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
neagent     627 azhekhan    2u      CHR                3,2         0t0                 313 /dev/null
rapportd    629 azhekhan  cwd       DIR                1,4         704                   2 /
rapportd    629 azhekhan  txt       REG                1,4      636416 1152921500311884730 /usr/libexec/rapportd
rapportd    629 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
rapportd    629 azhekhan  txt       REG                1,4    28620288 1152921500311902385 /usr/share/icu/icudt64l.dat
rapportd    629 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
rapportd    629 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
rapportd    629 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
rapportd    629 azhekhan    2u      CHR                3,2         0t0                 313 /dev/null
rapportd    629 azhekhan    3   NPOLICY
rapportd    629 azhekhan    4u     IPv4 0x3fbd25a25890e3d1         0t0                 UDP *:*
rapportd    629 azhekhan    5u     unix 0x3fbd25a290600819         0t0                     ->0x3fbd25a2905ffad1
rapportd    629 azhekhan    6u     IPv4 0x3fbd25a25890d261         0t0                 UDP *:*
rapportd    629 azhekhan    7u     IPv4 0x3fbd25a253db8549         0t0                 UDP *:*
rapportd    629 azhekhan    8u     unix 0x3fbd25a25b186879         0t0                     ->0x3fbd25a25b187369
cloudd      630 azhekhan  cwd       DIR                1,4         704                   2 /
cloudd      630 azhekhan  txt       REG                1,4       62880 1152921500312197223 /System/Library/PrivateFrameworks/CloudKitDaemon.framework/Support/cloudd
cloudd      630 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
cloudd      630 azhekhan  txt       REG                1,4       32768              681080 /Users/azhekhan/Library/Caches/CloudKit/CloudKitMetadata-shm
cloudd      630 azhekhan  txt       REG                1,4       32768              681094 /Users/azhekhan/Library/Caches/CloudKit/CloudKitOperationInfo-shm
cloudd      630 azhekhan  txt       REG                1,4      242128            56647497 /private/var/db/timezone/tz/2021a.3.0/icutz/icutz44l.dat
cloudd      630 azhekhan  txt       REG                1,4       32768            63985498 /private/var/db/mds/messages/501/se_SecurityMessages
cloudd      630 azhekhan  txt       REG                1,4       32768            57051111 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/79d00bbf349c3c41f627421b1937101e545403ee/Records/pcs.db-shm
cloudd      630 azhekhan  txt       REG                1,4       32768            48384487 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/08932f36cdfe8bd4cb9ae896921eef9e68c55ad4/Records/pcs.db-shm
cloudd      630 azhekhan  txt       REG                1,4       32768            47742885 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/ce49f3500a597aa2dca9af15112d99e9363cd778/Records/pcs.db-shm
cloudd      630 azhekhan  txt       REG                1,4       32768            47749034 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/Records/pcs.db-shm
cloudd      630 azhekhan  txt       REG                1,4       32768            47936063 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/19cd2669f37f65b31d5bbf4d0470072d0c2e24d2/Records/pcs.db-shm
cloudd      630 azhekhan  txt       REG                1,4    28620288 1152921500311902385 /usr/share/icu/icudt64l.dat
cloudd      630 azhekhan  txt       REG                1,4       43777            65284106 /private/var/db/analyticsd/events.whitelist
cloudd      630 azhekhan  txt       REG                1,4       32768            47745467 /Users/azhekhan/Library/Caches/CloudKit/com.apple.bird/com.apple.CloudDocs.container-metadata/f4e2b8c76fcf9282b073e083e9f7759e4f66b73e/Records/pcs.db-shm
cloudd      630 azhekhan  txt       REG                1,4     6201344            65260251 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/0/com.apple.LaunchServices.dv/com.apple.LaunchServices-1082-v2.csstore
cloudd      630 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
cloudd      630 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
cloudd      630 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
cloudd      630 azhekhan    2u      CHR                3,2         0t0                 313 /dev/null
cloudd      630 azhekhan    3u      REG                1,4       36864              681091 /Users/azhekhan/Library/Caches/CloudKit/CloudKitOperationInfo
cloudd      630 azhekhan    4u      REG                1,4      135168              681077 /Users/azhekhan/Library/Caches/CloudKit/CloudKitMetadata
cloudd      630 azhekhan    5u      REG                1,4       32768              681079 /Users/azhekhan/Library/Caches/CloudKit/CloudKitMetadata-wal
cloudd      630 azhekhan    6u      REG                1,4      383192              681093 /Users/azhekhan/Library/Caches/CloudKit/CloudKitOperationInfo-wal
cloudd      630 azhekhan    7u      REG                1,4       32768              681080 /Users/azhekhan/Library/Caches/CloudKit/CloudKitMetadata-shm
cloudd      630 azhekhan    8r      DIR                1,4          64            47527048 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/tmp
cloudd      630 azhekhan    9r      DIR                1,4          64              681214 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/StagingAssets
cloudd      630 azhekhan   10r      DIR                1,4          64              681215 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/Assets
cloudd      630 azhekhan   11r      DIR                1,4          64              681216 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/AssetsDb
cloudd      630 azhekhan   12r      DIR                1,4          64              681217 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/MMCS
cloudd      630 azhekhan   13r      DIR                1,4          64            47527049 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/FrameworkCaches
cloudd      630 azhekhan   14r      DIR                1,4          64            47527050 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/fa1e3fbf25118f4c6b2e02d52f316e01347e101a
cloudd      630 azhekhan   15r      DIR                1,4          64            47527218 /Users/azhekhan/Library/Caches/CloudKit/com.apple.assistant.assistantd/745028c72683801e45d882b9b76307c0b258408a/tmp
cloudd      630 azhekhan   16r      DIR                1,4          64              681690 /Users/azhekhan/Library/Caches/CloudKit/com.apple.assistant.assistantd/745028c72683801e45d882b9b76307c0b258408a/StagingAssets
cloudd      630 azhekhan   17r      DIR                1,4          64              681691 /Users/azhekhan/Library/Caches/CloudKit/com.apple.assistant.assistantd/745028c72683801e45d882b9b76307c0b258408a/Assets
cloudd      630 azhekhan   18r      DIR                1,4          64              681692 /Users/azhekhan/Library/Caches/CloudKit/com.apple.assistant.assistantd/745028c72683801e45d882b9b76307c0b258408a/AssetsDb
cloudd      630 azhekhan   19r      DIR                1,4          64              681693 /Users/azhekhan/Library/Caches/CloudKit/com.apple.assistant.assistantd/745028c72683801e45d882b9b76307c0b258408a/MMCS
cloudd      630 azhekhan   20r      DIR                1,4          64            47527219 /Users/azhekhan/Library/Caches/CloudKit/com.apple.assistant.assistantd/745028c72683801e45d882b9b76307c0b258408a/FrameworkCaches
cloudd      630 azhekhan   21r      DIR                1,4          64            47527220 /Users/azhekhan/Library/Caches/CloudKit/com.apple.assistant.assistantd/a7251ab568c11a3a6168372c6e3c87169b2d665c
cloudd      630 azhekhan   22r      DIR                1,4          64            47526821 /Users/azhekhan/Library/Caches/CloudKit/com.apple.knowledge-agent/cb76e6f7c01e6fba7b0f09a3fb202aeec42b4b73/tmp
cloudd      630 azhekhan   23r      DIR                1,4          64            47526822 /Users/azhekhan/Library/Caches/CloudKit/com.apple.knowledge-agent/cb76e6f7c01e6fba7b0f09a3fb202aeec42b4b73/StagingAssets
cloudd      630 azhekhan   24r      DIR                1,4          64            47526823 /Users/azhekhan/Library/Caches/CloudKit/com.apple.knowledge-agent/cb76e6f7c01e6fba7b0f09a3fb202aeec42b4b73/Assets
cloudd      630 azhekhan   25r      DIR                1,4          64            47526824 /Users/azhekhan/Library/Caches/CloudKit/com.apple.knowledge-agent/cb76e6f7c01e6fba7b0f09a3fb202aeec42b4b73/AssetsDb
cloudd      630 azhekhan   26r      DIR                1,4          64            47526825 /Users/azhekhan/Library/Caches/CloudKit/com.apple.knowledge-agent/cb76e6f7c01e6fba7b0f09a3fb202aeec42b4b73/MMCS
cloudd      630 azhekhan   27r      DIR                1,4          64            47526826 /Users/azhekhan/Library/Caches/CloudKit/com.apple.knowledge-agent/cb76e6f7c01e6fba7b0f09a3fb202aeec42b4b73/FrameworkCaches
cloudd      630 azhekhan   28r      DIR                1,4          64            47526827 /Users/azhekhan/Library/Caches/CloudKit/com.apple.knowledge-agent/57884fce175b6b1d242db964015f8f050da9c3aa
cloudd      630 azhekhan   29r      DIR                1,4          64            47741863 /Users/azhekhan/Library/Caches/CloudKit/com.apple.ScreenTimeAgent/e1a8d5cf6401c60ff8bb7aff6149b49c397c1553/tmp
cloudd      630 azhekhan   30r      DIR                1,4          64            47741864 /Users/azhekhan/Library/Caches/CloudKit/com.apple.ScreenTimeAgent/e1a8d5cf6401c60ff8bb7aff6149b49c397c1553/StagingAssets
cloudd      630 azhekhan   31r      DIR                1,4          64            47741865 /Users/azhekhan/Library/Caches/CloudKit/com.apple.ScreenTimeAgent/e1a8d5cf6401c60ff8bb7aff6149b49c397c1553/Assets
cloudd      630 azhekhan   32r      DIR                1,4          64            47741866 /Users/azhekhan/Library/Caches/CloudKit/com.apple.ScreenTimeAgent/e1a8d5cf6401c60ff8bb7aff6149b49c397c1553/AssetsDb
cloudd      630 azhekhan   33r      DIR                1,4          64            47741867 /Users/azhekhan/Library/Caches/CloudKit/com.apple.ScreenTimeAgent/e1a8d5cf6401c60ff8bb7aff6149b49c397c1553/MMCS
cloudd      630 azhekhan   34r      DIR                1,4          64            47741868 /Users/azhekhan/Library/Caches/CloudKit/com.apple.ScreenTimeAgent/e1a8d5cf6401c60ff8bb7aff6149b49c397c1553/FrameworkCaches
cloudd      630 azhekhan   35r      DIR                1,4          64            47741869 /Users/azhekhan/Library/Caches/CloudKit/com.apple.ScreenTimeAgent/b8648fc4e4b531bc446faae70ea5c798cccf291d
cloudd      630 azhekhan   36r      DIR                1,4          64            47527048 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/tmp
cloudd      630 azhekhan   37r      DIR                1,4          64            47545803 /Users/azhekhan/Library/Caches/CloudKit/com.apple.textinput.KeyboardServices/com.apple.keyboardservicesd/2bb72ec7676f1ef6f5f88fa889a77a9f1443a05d/tmp
cloudd      630 azhekhan   38r      DIR                1,4          64            47527003 /Users/azhekhan/Library/Caches/CloudKit/com.apple.imagent/595e8bb0c8c5bca9dd197a05597a294fb2d5d1bb/tmp
cloudd      630 azhekhan   39r      DIR                1,4          64             1843272 /Users/azhekhan/Library/Caches/CloudKit/com.apple.imagent/595e8bb0c8c5bca9dd197a05597a294fb2d5d1bb/StagingAssets
cloudd      630 azhekhan   40r      DIR                1,4          64             1843273 /Users/azhekhan/Library/Caches/CloudKit/com.apple.imagent/595e8bb0c8c5bca9dd197a05597a294fb2d5d1bb/Assets
cloudd      630 azhekhan   41r      DIR                1,4          64             1843274 /Users/azhekhan/Library/Caches/CloudKit/com.apple.imagent/595e8bb0c8c5bca9dd197a05597a294fb2d5d1bb/AssetsDb
cloudd      630 azhekhan   42r      DIR                1,4          64             1843275 /Users/azhekhan/Library/Caches/CloudKit/com.apple.imagent/595e8bb0c8c5bca9dd197a05597a294fb2d5d1bb/MMCS
cloudd      630 azhekhan   43r      DIR                1,4          64            47527004 /Users/azhekhan/Library/Caches/CloudKit/com.apple.imagent/595e8bb0c8c5bca9dd197a05597a294fb2d5d1bb/FrameworkCaches
cloudd      630 azhekhan   44r      DIR                1,4          64            47527005 /Users/azhekhan/Library/Caches/CloudKit/com.apple.imagent/6fe52de9b209fe993ece0f2445dff3edf1a1679b
cloudd      630 azhekhan   45r      DIR                1,4          64            47741835 /Users/azhekhan/Library/Caches/CloudKit/com.apple.iad-cloudkit/com.apple.AdSheetPhone/dc9f57b5d00dfc35b7bfc8ffd7dab5add6d780d7/tmp
cloudd      630 azhekhan   46u      REG                1,4       32768              681094 /Users/azhekhan/Library/Caches/CloudKit/CloudKitOperationInfo-shm
cloudd      630 azhekhan   47r      DIR                1,4          64            47534222 /Users/azhekhan/Library/Caches/CloudKit/com.apple.siriknowledged/1d1d269aa1a55c0df94e36ae92330a874db6b566/tmp
cloudd      630 azhekhan   48r      DIR                1,4          64              723570 /Users/azhekhan/Library/Caches/CloudKit/com.apple.siriknowledged/1d1d269aa1a55c0df94e36ae92330a874db6b566/StagingAssets
cloudd      630 azhekhan   49r      DIR                1,4          64            47743532 /Users/azhekhan/Library/Caches/CloudKit/com.apple.bird/com.apple.CloudDocs.container-metadata/f4e2b8c76fcf9282b073e083e9f7759e4f66b73e/tmp
cloudd      630 azhekhan   50r      DIR                1,4          64            47743533 /Users/azhekhan/Library/Caches/CloudKit/com.apple.bird/com.apple.CloudDocs.container-metadata/f4e2b8c76fcf9282b073e083e9f7759e4f66b73e/StagingAssets
cloudd      630 azhekhan   51r      DIR                1,4          64            47743534 /Users/azhekhan/Library/Caches/CloudKit/com.apple.bird/com.apple.CloudDocs.container-metadata/f4e2b8c76fcf9282b073e083e9f7759e4f66b73e/Assets
cloudd      630 azhekhan   52r      DIR                1,4         160            47743535 /Users/azhekhan/Library/Caches/CloudKit/com.apple.bird/com.apple.CloudDocs.container-metadata/f4e2b8c76fcf9282b073e083e9f7759e4f66b73e/AssetsDb
cloudd      630 azhekhan   53r      DIR                1,4          96            47743536 /Users/azhekhan/Library/Caches/CloudKit/com.apple.bird/com.apple.CloudDocs.container-metadata/f4e2b8c76fcf9282b073e083e9f7759e4f66b73e/MMCS
cloudd      630 azhekhan   54r      DIR                1,4          64            47743537 /Users/azhekhan/Library/Caches/CloudKit/com.apple.bird/com.apple.CloudDocs.container-metadata/f4e2b8c76fcf9282b073e083e9f7759e4f66b73e/FrameworkCaches
cloudd      630 azhekhan   55r      DIR                1,4          64            47743538 /Users/azhekhan/Library/Caches/CloudKit/com.apple.bird/790255cf9f58beaaa5fa74ca5c8227329b982cc8
cloudd      630 azhekhan   56r      DIR                1,4          64            47781541 /Users/azhekhan/Library/Caches/CloudKit/com.apple.routined/c5263141ed845f03fe972d5a292cc4c7e3717549/tmp
cloudd      630 azhekhan   57r      DIR                1,4          64            47781542 /Users/azhekhan/Library/Caches/CloudKit/com.apple.routined/c5263141ed845f03fe972d5a292cc4c7e3717549/StagingAssets
cloudd      630 azhekhan   58r      DIR                1,4          64            47781543 /Users/azhekhan/Library/Caches/CloudKit/com.apple.routined/c5263141ed845f03fe972d5a292cc4c7e3717549/Assets
cloudd      630 azhekhan   59r      DIR                1,4          64            47781544 /Users/azhekhan/Library/Caches/CloudKit/com.apple.routined/c5263141ed845f03fe972d5a292cc4c7e3717549/AssetsDb
cloudd      630 azhekhan   60r      DIR                1,4          64            47781545 /Users/azhekhan/Library/Caches/CloudKit/com.apple.routined/c5263141ed845f03fe972d5a292cc4c7e3717549/MMCS
cloudd      630 azhekhan   61r      DIR                1,4          64            47781546 /Users/azhekhan/Library/Caches/CloudKit/com.apple.routined/c5263141ed845f03fe972d5a292cc4c7e3717549/FrameworkCaches
cloudd      630 azhekhan   62r      DIR                1,4          64            47781547 /Users/azhekhan/Library/Caches/CloudKit/com.apple.routined/e121f76302a9f2785cc066fcfa2323a6eaa71a97
cloudd      630 azhekhan   63r      DIR                1,4          64            48377705 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/bb1314eff040410fd3233700c6c93674a06f9ee1/tmp
cloudd      630 azhekhan   64r      DIR                1,4          64            48377706 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/bb1314eff040410fd3233700c6c93674a06f9ee1/StagingAssets
cloudd      630 azhekhan   65r      DIR                1,4          64            48377707 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/bb1314eff040410fd3233700c6c93674a06f9ee1/Assets
cloudd      630 azhekhan   66r      DIR                1,4          64            48377709 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/bb1314eff040410fd3233700c6c93674a06f9ee1/AssetsDb
cloudd      630 azhekhan   67r      DIR                1,4          64            48377710 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/bb1314eff040410fd3233700c6c93674a06f9ee1/MMCS
cloudd      630 azhekhan   68r      DIR                1,4          64            48377711 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/bb1314eff040410fd3233700c6c93674a06f9ee1/FrameworkCaches
cloudd      630 azhekhan   69r      DIR                1,4          64            48384443 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/08932f36cdfe8bd4cb9ae896921eef9e68c55ad4/tmp
cloudd      630 azhekhan   70r      DIR                1,4          64              702897 /Users/azhekhan/Library/Caches/CloudKit/com.apple.textinput.KeyboardServices/com.apple.keyboardservicesd/2bb72ec7676f1ef6f5f88fa889a77a9f1443a05d/StagingAssets
cloudd      630 azhekhan   71r      DIR                1,4          64              702898 /Users/azhekhan/Library/Caches/CloudKit/com.apple.textinput.KeyboardServices/com.apple.keyboardservicesd/2bb72ec7676f1ef6f5f88fa889a77a9f1443a05d/Assets
cloudd      630 azhekhan   72r      DIR                1,4          64              702899 /Users/azhekhan/Library/Caches/CloudKit/com.apple.textinput.KeyboardServices/com.apple.keyboardservicesd/2bb72ec7676f1ef6f5f88fa889a77a9f1443a05d/AssetsDb
cloudd      630 azhekhan   73r      DIR                1,4          64              702900 /Users/azhekhan/Library/Caches/CloudKit/com.apple.textinput.KeyboardServices/com.apple.keyboardservicesd/2bb72ec7676f1ef6f5f88fa889a77a9f1443a05d/MMCS
cloudd      630 azhekhan   74r      DIR                1,4          64            47545804 /Users/azhekhan/Library/Caches/CloudKit/com.apple.textinput.KeyboardServices/com.apple.keyboardservicesd/2bb72ec7676f1ef6f5f88fa889a77a9f1443a05d/FrameworkCaches
cloudd      630 azhekhan   75r      DIR                1,4          64            47545805 /Users/azhekhan/Library/Caches/CloudKit/com.apple.textinput.KeyboardServices/73860e57795d7b199e55275ab90eb2a8027ab7a1
cloudd      630 azhekhan   76r      DIR                1,4          64              681214 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/StagingAssets
cloudd      630 azhekhan   77r      DIR                1,4          64              681215 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/Assets
cloudd      630 azhekhan   78r      DIR                1,4          64              681216 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/AssetsDb
cloudd      630 azhekhan   79r      DIR                1,4          64              681217 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/MMCS
cloudd      630 azhekhan   80r      DIR                1,4          64            47527049 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/FrameworkCaches
cloudd      630 azhekhan   81r      DIR                1,4          64            47527050 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/fa1e3fbf25118f4c6b2e02d52f316e01347e101a
cloudd      630 azhekhan   82   NPOLICY
cloudd      630 azhekhan   83r      DIR                1,4          64            47741836 /Users/azhekhan/Library/Caches/CloudKit/com.apple.iad-cloudkit/com.apple.AdSheetPhone/dc9f57b5d00dfc35b7bfc8ffd7dab5add6d780d7/StagingAssets
cloudd      630 azhekhan   84u    systm 0x3fbd25a25aef63b9         0t0                     [ctl com.apple.netsrc id 7 unit 23]
cloudd      630 azhekhan   85r      DIR                1,4          64            47746806 /Users/azhekhan/Library/Caches/CloudKit/com.apple.TrustedPeersHelper/com.apple.security.cuttlefish/ad5d86579c92455c63c47f704843e5afe7e84c5f/tmp
cloudd      630 azhekhan   86r      DIR                1,4          64            47741837 /Users/azhekhan/Library/Caches/CloudKit/com.apple.iad-cloudkit/com.apple.AdSheetPhone/dc9f57b5d00dfc35b7bfc8ffd7dab5add6d780d7/Assets
cloudd      630 azhekhan   87r      DIR                1,4          64            47741838 /Users/azhekhan/Library/Caches/CloudKit/com.apple.iad-cloudkit/com.apple.AdSheetPhone/dc9f57b5d00dfc35b7bfc8ffd7dab5add6d780d7/AssetsDb
cloudd      630 azhekhan   88r      DIR                1,4          64            47741839 /Users/azhekhan/Library/Caches/CloudKit/com.apple.iad-cloudkit/com.apple.AdSheetPhone/dc9f57b5d00dfc35b7bfc8ffd7dab5add6d780d7/MMCS
cloudd      630 azhekhan   89r      DIR                1,4          64            47741840 /Users/azhekhan/Library/Caches/CloudKit/com.apple.iad-cloudkit/com.apple.AdSheetPhone/dc9f57b5d00dfc35b7bfc8ffd7dab5add6d780d7/FrameworkCaches
cloudd      630 azhekhan   90r      DIR                1,4          64            47741833 /Users/azhekhan/Library/Caches/CloudKit/com.apple.iad-cloudkit/81298011e8eac82c840c3b3d371b419c1e4cb448
cloudd      630 azhekhan   91r      DIR                1,4          64            47526841 /Users/azhekhan/Library/Caches/CloudKit/com.apple.remindd/com.apple.reminders/d042fab783d224502ebfb63d5154335cdaad6b49/tmp
cloudd      630 azhekhan   92u      REG                1,4        4096            47742881 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/ce49f3500a597aa2dca9af15112d99e9363cd778/Records/pcs.db
cloudd      630 azhekhan   93r      DIR                1,4          64            47746807 /Users/azhekhan/Library/Caches/CloudKit/com.apple.TrustedPeersHelper/com.apple.security.cuttlefish/ad5d86579c92455c63c47f704843e5afe7e84c5f/StagingAssets
cloudd      630 azhekhan   94r      DIR                1,4          64            47746808 /Users/azhekhan/Library/Caches/CloudKit/com.apple.TrustedPeersHelper/com.apple.security.cuttlefish/ad5d86579c92455c63c47f704843e5afe7e84c5f/Assets
cloudd      630 azhekhan   95r      DIR                1,4          64            47746809 /Users/azhekhan/Library/Caches/CloudKit/com.apple.TrustedPeersHelper/com.apple.security.cuttlefish/ad5d86579c92455c63c47f704843e5afe7e84c5f/AssetsDb
cloudd      630 azhekhan   96r      DIR                1,4          64            47746810 /Users/azhekhan/Library/Caches/CloudKit/com.apple.TrustedPeersHelper/com.apple.security.cuttlefish/ad5d86579c92455c63c47f704843e5afe7e84c5f/MMCS
cloudd      630 azhekhan   97r      DIR                1,4          64            47746811 /Users/azhekhan/Library/Caches/CloudKit/com.apple.TrustedPeersHelper/com.apple.security.cuttlefish/ad5d86579c92455c63c47f704843e5afe7e84c5f/FrameworkCaches
cloudd      630 azhekhan   98r      DIR                1,4          64            47746812 /Users/azhekhan/Library/Caches/CloudKit/com.apple.TrustedPeersHelper/7196acf7c4296cedd97c063b9f5e5ffd1417dbc6
cloudd      630 azhekhan   99r      DIR                1,4          64            48384444 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/08932f36cdfe8bd4cb9ae896921eef9e68c55ad4/StagingAssets
cloudd      630 azhekhan  100r      DIR                1,4          64            47543260 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/ce49f3500a597aa2dca9af15112d99e9363cd778/tmp
cloudd      630 azhekhan  101u      REG                1,4     1512072            47742884 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/ce49f3500a597aa2dca9af15112d99e9363cd778/Records/pcs.db-wal
cloudd      630 azhekhan  102r      DIR                1,4          64            47526842 /Users/azhekhan/Library/Caches/CloudKit/com.apple.remindd/com.apple.reminders/d042fab783d224502ebfb63d5154335cdaad6b49/StagingAssets
cloudd      630 azhekhan  103r      DIR                1,4          64            47526843 /Users/azhekhan/Library/Caches/CloudKit/com.apple.remindd/com.apple.reminders/d042fab783d224502ebfb63d5154335cdaad6b49/Assets
cloudd      630 azhekhan  104r      DIR                1,4          64            47526844 /Users/azhekhan/Library/Caches/CloudKit/com.apple.remindd/com.apple.reminders/d042fab783d224502ebfb63d5154335cdaad6b49/AssetsDb
cloudd      630 azhekhan  105r      DIR                1,4          64            47526845 /Users/azhekhan/Library/Caches/CloudKit/com.apple.remindd/com.apple.reminders/d042fab783d224502ebfb63d5154335cdaad6b49/MMCS
cloudd      630 azhekhan  106r      DIR                1,4          64            47526846 /Users/azhekhan/Library/Caches/CloudKit/com.apple.remindd/com.apple.reminders/d042fab783d224502ebfb63d5154335cdaad6b49/FrameworkCaches
cloudd      630 azhekhan  107r      DIR                1,4          64            47526847 /Users/azhekhan/Library/Caches/CloudKit/com.apple.remindd/708e49fabf083cb1251012ad1d8f97303f32a83a
cloudd      630 azhekhan  109r      DIR                1,4          64            47527242 /Users/azhekhan/Library/Caches/CloudKit/com.apple.suggestd/9689277fc800e022bb77d1141f289a17802a0e5b/tmp
cloudd      630 azhekhan  110r      DIR                1,4          64              690322 /Users/azhekhan/Library/Caches/CloudKit/com.apple.suggestd/9689277fc800e022bb77d1141f289a17802a0e5b/StagingAssets
cloudd      630 azhekhan  111r      DIR                1,4          64              690323 /Users/azhekhan/Library/Caches/CloudKit/com.apple.suggestd/9689277fc800e022bb77d1141f289a17802a0e5b/Assets
cloudd      630 azhekhan  112r      DIR                1,4          64              690324 /Users/azhekhan/Library/Caches/CloudKit/com.apple.suggestd/9689277fc800e022bb77d1141f289a17802a0e5b/AssetsDb
cloudd      630 azhekhan  113r      DIR                1,4          64              690325 /Users/azhekhan/Library/Caches/CloudKit/com.apple.suggestd/9689277fc800e022bb77d1141f289a17802a0e5b/MMCS
cloudd      630 azhekhan  114r      DIR                1,4          64            47527243 /Users/azhekhan/Library/Caches/CloudKit/com.apple.suggestd/9689277fc800e022bb77d1141f289a17802a0e5b/FrameworkCaches
cloudd      630 azhekhan  115r      DIR                1,4          64            47527244 /Users/azhekhan/Library/Caches/CloudKit/com.apple.suggestd/c6641c96deceb9b2b4fc35feacc62a5ca5cc0f7a
cloudd      630 azhekhan  116r      DIR                1,4          64              683277 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/ce49f3500a597aa2dca9af15112d99e9363cd778/StagingAssets
cloudd      630 azhekhan  117r      DIR                1,4          64              683278 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/ce49f3500a597aa2dca9af15112d99e9363cd778/Assets
cloudd      630 azhekhan  118r      DIR                1,4          64              683279 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/ce49f3500a597aa2dca9af15112d99e9363cd778/AssetsDb
cloudd      630 azhekhan  119r      DIR                1,4          64              683280 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/ce49f3500a597aa2dca9af15112d99e9363cd778/MMCS
cloudd      630 azhekhan  120r      DIR                1,4          64            47543261 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/ce49f3500a597aa2dca9af15112d99e9363cd778/FrameworkCaches
cloudd      630 azhekhan  121r      DIR                1,4          64            47543262 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/3676b2a64cc12b4d47dcaca1ee52ab5eeb833deb
cloudd      630 azhekhan  122r      DIR                1,4          64            47543263 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/5c64ef931d8658ccf3553c9467b17fa4a3f57288/tmp
cloudd      630 azhekhan  123r      DIR                1,4          64              683283 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/5c64ef931d8658ccf3553c9467b17fa4a3f57288/StagingAssets
cloudd      630 azhekhan  124r      DIR                1,4          64              683284 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/5c64ef931d8658ccf3553c9467b17fa4a3f57288/Assets
cloudd      630 azhekhan  125r      DIR                1,4          64              683285 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/5c64ef931d8658ccf3553c9467b17fa4a3f57288/AssetsDb
cloudd      630 azhekhan  126r      DIR                1,4          64              683286 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/5c64ef931d8658ccf3553c9467b17fa4a3f57288/MMCS
cloudd      630 azhekhan  127r      DIR                1,4          64            47543264 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/5c64ef931d8658ccf3553c9467b17fa4a3f57288/FrameworkCaches
cloudd      630 azhekhan  128r      DIR                1,4          64            47543262 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/3676b2a64cc12b4d47dcaca1ee52ab5eeb833deb
cloudd      630 azhekhan  129r      DIR                1,4          64            48384445 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/08932f36cdfe8bd4cb9ae896921eef9e68c55ad4/Assets
cloudd      630 azhekhan  130r      DIR                1,4          64            48384447 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/08932f36cdfe8bd4cb9ae896921eef9e68c55ad4/AssetsDb
cloudd      630 azhekhan  131u      REG                1,4       32768            47742885 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/ce49f3500a597aa2dca9af15112d99e9363cd778/Records/pcs.db-shm
cloudd      630 azhekhan  132r      DIR                1,4          64            48384448 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/08932f36cdfe8bd4cb9ae896921eef9e68c55ad4/MMCS
cloudd      630 azhekhan  133r      DIR                1,4          64            48384449 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/08932f36cdfe8bd4cb9ae896921eef9e68c55ad4/FrameworkCaches
cloudd      630 azhekhan  134r      DIR                1,4          64            47749045 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/19cd2669f37f65b31d5bbf4d0470072d0c2e24d2/tmp
cloudd      630 azhekhan  135r      DIR                1,4          64            47749046 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/19cd2669f37f65b31d5bbf4d0470072d0c2e24d2/StagingAssets
cloudd      630 azhekhan  136r      DIR                1,4          64            47749047 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/19cd2669f37f65b31d5bbf4d0470072d0c2e24d2/Assets
cloudd      630 azhekhan  137r      DIR                1,4          64            47749049 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/19cd2669f37f65b31d5bbf4d0470072d0c2e24d2/AssetsDb
cloudd      630 azhekhan  138r      DIR                1,4          64            47749050 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/19cd2669f37f65b31d5bbf4d0470072d0c2e24d2/MMCS
cloudd      630 azhekhan  139r      DIR                1,4          64            47749051 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/19cd2669f37f65b31d5bbf4d0470072d0c2e24d2/FrameworkCaches
cloudd      630 azhekhan  140r      DIR                1,4          64            47740234 /Users/azhekhan/Library/Caches/CloudKit/com.apple.identityservicesd/849759fb8eb7a3da707532c88ed982ae01375646/tmp
cloudd      630 azhekhan  141u      REG                1,4       32768            48384483 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/08932f36cdfe8bd4cb9ae896921eef9e68c55ad4/Records/pcs.db
cloudd      630 azhekhan  142u      REG                1,4      988832            48384486 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/08932f36cdfe8bd4cb9ae896921eef9e68c55ad4/Records/pcs.db-wal
cloudd      630 azhekhan  143u      REG                1,4       32768            48384487 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/08932f36cdfe8bd4cb9ae896921eef9e68c55ad4/Records/pcs.db-shm
cloudd      630 azhekhan  144r      DIR                1,4          64              723571 /Users/azhekhan/Library/Caches/CloudKit/com.apple.siriknowledged/1d1d269aa1a55c0df94e36ae92330a874db6b566/Assets
cloudd      630 azhekhan  145r      DIR                1,4          64              723572 /Users/azhekhan/Library/Caches/CloudKit/com.apple.siriknowledged/1d1d269aa1a55c0df94e36ae92330a874db6b566/AssetsDb
cloudd      630 azhekhan  146r      DIR                1,4          64              723573 /Users/azhekhan/Library/Caches/CloudKit/com.apple.siriknowledged/1d1d269aa1a55c0df94e36ae92330a874db6b566/MMCS
cloudd      630 azhekhan  147r      DIR                1,4          64            57051030 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/79d00bbf349c3c41f627421b1937101e545403ee/tmp
cloudd      630 azhekhan  148r      DIR                1,4          64            57051031 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/79d00bbf349c3c41f627421b1937101e545403ee/StagingAssets
cloudd      630 azhekhan  149r      DIR                1,4          64            57051032 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/79d00bbf349c3c41f627421b1937101e545403ee/Assets
cloudd      630 azhekhan  150r      DIR                1,4          64            57051034 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/79d00bbf349c3c41f627421b1937101e545403ee/AssetsDb
cloudd      630 azhekhan  151r      DIR                1,4          64            57051035 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/79d00bbf349c3c41f627421b1937101e545403ee/MMCS
cloudd      630 azhekhan  152r      DIR                1,4          64            57051036 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/79d00bbf349c3c41f627421b1937101e545403ee/FrameworkCaches
cloudd      630 azhekhan  153u      REG                1,4       65536            47936059 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/19cd2669f37f65b31d5bbf4d0470072d0c2e24d2/Records/pcs.db
cloudd      630 azhekhan  154u      REG                1,4      131872            47936062 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/19cd2669f37f65b31d5bbf4d0470072d0c2e24d2/Records/pcs.db-wal
cloudd      630 azhekhan  155u      REG                1,4       32768            47936063 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/19cd2669f37f65b31d5bbf4d0470072d0c2e24d2/Records/pcs.db-shm
cloudd      630 azhekhan  156r      DIR                1,4          64            47534223 /Users/azhekhan/Library/Caches/CloudKit/com.apple.siriknowledged/1d1d269aa1a55c0df94e36ae92330a874db6b566/FrameworkCaches
cloudd      630 azhekhan  157r      DIR                1,4          64            47534224 /Users/azhekhan/Library/Caches/CloudKit/com.apple.siriknowledged/1c03413a3efdd9e925fa1639bb3dad5e9da36102
cloudd      630 azhekhan  171r      DIR                1,4          64            47740236 /Users/azhekhan/Library/Caches/CloudKit/com.apple.identityservicesd/849759fb8eb7a3da707532c88ed982ae01375646/StagingAssets
cloudd      630 azhekhan  172r      DIR                1,4          64            47740237 /Users/azhekhan/Library/Caches/CloudKit/com.apple.identityservicesd/849759fb8eb7a3da707532c88ed982ae01375646/Assets
cloudd      630 azhekhan  173r      DIR                1,4          64            47740238 /Users/azhekhan/Library/Caches/CloudKit/com.apple.identityservicesd/849759fb8eb7a3da707532c88ed982ae01375646/AssetsDb
cloudd      630 azhekhan  174r      DIR                1,4          64            47740239 /Users/azhekhan/Library/Caches/CloudKit/com.apple.identityservicesd/849759fb8eb7a3da707532c88ed982ae01375646/MMCS
cloudd      630 azhekhan  175r      DIR                1,4          64            47740240 /Users/azhekhan/Library/Caches/CloudKit/com.apple.identityservicesd/849759fb8eb7a3da707532c88ed982ae01375646/FrameworkCaches
cloudd      630 azhekhan  176r      DIR                1,4          64            47740241 /Users/azhekhan/Library/Caches/CloudKit/com.apple.identityservicesd/8f1b0311fbe12e91b90cf18e34e9b4d63c38e602
cloudd      630 azhekhan  180r      DIR                1,4          64            47526831 /Users/azhekhan/Library/Caches/CloudKit/com.apple.willowd/com.apple.willowd.homekit.camera.clips/6d5b8c41e4722ccdc3f44adb81fcf9410ab708e1/tmp
cloudd      630 azhekhan  181r      DIR     ^Cy/Caches/CloudKit/com.apple.willowd/com.apple.willowd.homekit.camera.clips/6d5b8c41e4722ccdc3f44adb81fcf9410ab708e1/FrameworkCaches


 2022-01-30 06:51:26 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → kgns

ls -tlr ~/.kube
^C

 2022-01-30 06:54:40 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → ls -ltr ~/.kube
total 864
-rw-r--r--    1 azhekhan  staff   2401 Nov  7  2019 config_stage
drwxr-xr-x    3 azhekhan  staff     96 Nov 11  2019 cache
-rw-r--r--    1 azhekhan  staff   2401 Nov 11  2019 config_integ
-rw-r--r--    1 azhekhan  staff   2401 Nov 12  2019 config_integ_us-ashburn-1
-rw-r--r--    1 azhekhan  staff   2416 Nov 12  2019 config_PROD-eu-frankfurt-1
-rw-r--r--    1 azhekhan  staff   2401 Dec  6  2019 config_PROD-us-phoenix-1
-rw-r--r--    1 azhekhan  staff   2401 Dec  6  2019 config_OSVCSTAGE-osvcinteg-ashburn
-rw-r--r--    1 azhekhan  staff   2401 Dec  6  2019 config_OSVCSTAGE-osvcstageiad-ashburn
-rw-r--r--    1 azhekhan  staff   2401 Dec  6  2019 bkconfig
-rw-r--r--    1 azhekhan  staff   2401 Dec 16  2019 config_ORACLEOVA-us-phoenix-1
-rw-r--r--    1 azhekhan  staff   2401 Dec 24  2019 osvcstageiad-config
-rw-r--r--    1 azhekhan  staff   2401 Dec 24  2019 osvcinteg-config
-rw-r--r--    1 azhekhan  staff   2450 Jan 30  2020 config_OSVCSTAGE-cpetest-phx
-rw-r--r--    1 azhekhan  staff   2401 Mar  6  2020 config_OSVCCORP_ashburn-1
-rw-r--r--    1 azhekhan  staff   2401 Mar  6  2020 osvccorp-config
-rw-r--r--    1 azhekhan  staff   2401 Mar  6  2020 config_OSVCSTAGE-osvccpetest-ashburn
-rw-r--r--    1 azhekhan  staff   2401 Mar 19  2020 osvcprodphx-config
-rw-r--r--    1 azhekhan  staff   2416 Mar 19  2020 osvcprodfra-config
-rw-r--r--    1 azhekhan  staff   2401 Apr 17  2020 cpetestphx-config
-rw-r--r--    1 azhekhan  staff   2362 Jul 21  2020 preprod-cp-ash-config
-rw-r--r--    1 azhekhan  staff   2362 Jul 21  2020 preprod-cp-phx-config
-rw-r--r--    1 azhekhan  staff   2363 Jul 21  2020 preprod-deploy-phx-config
-rw-r--r--    1 azhekhan  staff   2361 Jul 21  2020 preprod-dp-ash-config
-rw-r--r--    1 azhekhan  staff   2362 Jul 21  2020 preprod-dp-phx-config
-rw-r--r--    1 azhekhan  staff   6968 Mar 11  2021 pv2_corp.config
-rw-r--r--    1 azhekhan  staff  11588 Mar 11  2021 pv2_dev.config
-rw-r--r--    1 azhekhan  staff  30305 Mar 11  2021 pv2_prod_v2-1.config
-rw-r--r--    1 azhekhan  staff   2451 Apr 22  2021 dev.eu-frankfurt-1.controlplane.config
-rw-r--r--    1 azhekhan  staff   2446 Apr 22  2021 dev.eu-frankfurt-1.dataplane.config
-rw-r--r--    1 azhekhan  staff   2447 Apr 22  2021 dev.eu-frankfurt-1.deployment.config
-rw-r--r--    1 azhekhan  staff   2439 Apr 22  2021 dev.uk-london-1.controlplane.config
-rw-r--r--    1 azhekhan  staff   2431 Apr 22  2021 dev.uk-london-1.dataplane.config
-rw-r--r--    1 azhekhan  staff   2401 Jul 15  2021 config_bkup15072021
-rw-r--r--    1 azhekhan  staff   5428 Jul 15  2021 config_KinD
-rw-------    1 azhekhan  staff   5446 Jul 15  2021 config-E
-rw-r--r--    1 azhekhan  staff  11545 Aug  2 10:47 pv2_preprod.config
-rw-r--r--    1 azhekhan  staff  48229 Aug  2 10:54 pv2_prod_bkup.config
-rw-------    1 azhekhan  staff  76737 Jan  3 20:45 pv2_prod_bkup2.config
-rw-r--r--    1 azhekhan  staff  95279 Jan  3 20:46 pv2_prod.config
-rw-------    1 azhekhan  staff   5518 Jan 17 15:28 kind-cluster-17jan22.config
-rw-------    1 azhekhan  staff   5884 Jan 18 05:37 config
drwxr-xr-x  872 azhekhan  staff  27904 Jan 22 12:50 http-cache

 2022-01-30 06:54:46 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → minikube --help
Minikube is a CLI tool that provisions and manages single-node Kubernetes clusters optimized for development workflows.

Basic Commands:
  start          Starts a local kubernetes cluster
  status         Gets the status of a local kubernetes cluster
  stop           Stops a running local kubernetes cluster
  delete         Deletes a local kubernetes cluster
  dashboard      Access the kubernetes dashboard running within the minikube cluster
  pause          pause containers
  unpause        unpause Kubernetes

Images Commands:
  docker-env     Sets up docker env variables; similar to '$(docker-machine env)'
  podman-env     Sets up podman env variables; similar to '$(podman-machine env)'
  cache          Add or delete an image from the local cache.

Configuration and Management Commands:
  addons         Modify minikube's kubernetes addons
  config         Modify minikube config
  profile        Profile gets or sets the current minikube profile
  update-context Verify the IP address of the running cluster in kubeconfig.

Networking and Connectivity Commands:
  service        Gets the kubernetes URL(s) for the specified service in your local cluster
  tunnel         tunnel makes services of type LoadBalancer accessible on localhost

Advanced Commands:
  mount          Mounts the specified directory into minikube
  ssh            Log into or run a command on a machine with SSH; similar to 'docker-machine ssh'
  kubectl        Run kubectl
  node           Node operations

Troubleshooting Commands:
  ssh-key        Retrieve the ssh identity key path of the specified cluster
  ip             Retrieves the IP address of the running cluster
  logs           Gets the logs of the running instance, used for debugging minikube, not user code.
  update-check   Print current and latest version number
  version        Print the version of minikube
  options        Show a list of global command-line options (applies to all commands).

Other Commands:
  completion     Outputs minikube shell completion for the given shell (bash or zsh)

Use "minikube <command> --help" for more information about a given command.

 2022-01-30 06:55:30 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → minikube status
m01
host: Stopped
kubelet: Stopped
apiserver: Stopped
kubeconfig: Stopped


 2022-01-30 06:55:40 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → minikube start
🎉  minikube 1.25.1 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.25.1
💡  To disable this notice, run: 'minikube config set WantUpdateNotification false'

🙄  minikube v1.9.2 on Darwin 10.15.7
    ▪ KUBECONFIG=/Users/azhekhan/.kube/config
✨  Using the hyperkit driver based on existing profile
👍  Starting control plane node m01 in cluster minikube
🔄  Restarting existing hyperkit VM for "minikube" ...
🐳  Preparing Kubernetes v1.18.0 on Docker 19.03.8 ...
🌟  Enabling addons: dashboard, default-storageclass, storage-provisioner
🏄  Done! kubectl is now configured to use "minikube"

 2022-01-30 06:57:35 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → kgns
NAME                   STATUS   AGE
default                Active   12d
kube-node-lease        Active   12d
kube-public            Active   12d
kube-system            Active   12d
kubernetes-dashboard   Active   12d

 2022-01-30 06:58:09 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → kg pods -A
NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE
default                bear-7c57fc57dc-6pkgl                        1/1     Running   1          12d
default                bear-7c57fc57dc-lrnpx                        1/1     Running   1          12d
default                hare-58c6bd8f9-gnbmn                         1/1     Running   1          12d
default                hare-58c6bd8f9-gnbzx                         1/1     Running   1          12d
default                hello-minikube-5655c9d946-lrpsc              1/1     Running   1          12d
default                moose-868fbc6b78-4d6ff                       1/1     Running   1          12d
default                moose-868fbc6b78-v2778                       1/1     Running   1          12d
kube-system            coredns-66bff467f8-hjv9n                     1/1     Running   1          12d
kube-system            coredns-66bff467f8-r8znt                     1/1     Running   1          12d
kube-system            etcd-minikube                                1/1     Running   1          12d
kube-system            kube-apiserver-minikube                      1/1     Running   1          12d
kube-system            kube-controller-manager-minikube             1/1     Running   1          12d
kube-system            kube-proxy-4dz9l                             1/1     Running   1          12d
kube-system            kube-scheduler-minikube                      1/1     Running   1          12d
kube-system            storage-provisioner                          1/1     Running   2          12d
kube-system            traefik-ingress-55c99674d9-mbp5h             1/1     Running   1          12d
kubernetes-dashboard   dashboard-metrics-scraper-84bfdf55ff-4mw5l   1/1     Running   1          12d
kubernetes-dashboard   kubernetes-dashboard-bc446cc64-6dp6z         1/1     Running   1          12d

 2022-01-30 06:58:20 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4
Error from server (AlreadyExists): deployments.apps "hello-minikube" already exists

 2022-01-30 06:58:53 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → kgdp -A
-bash: kgdp: command not found

 2022-01-30 06:59:08 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → kgdpn default
NAME             READY   UP-TO-DATE   AVAILABLE   AGE
bear             2/2     2            2           12d
hare             2/2     2            2           12d
hello-minikube   1/1     1            1           12d
moose            2/2     2            2           12d

 2022-01-30 06:59:19 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → k scale deployments hello-minikube --replicas=2 hello-minikube -n default
deployment.apps/hello-minikube scaled
deployment.apps/hello-minikube scaled

 2022-01-30 07:00:36 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → kgpn default
NAME                              READY   STATUS    RESTARTS   AGE
bear-7c57fc57dc-6pkgl             1/1     Running   1          12d
bear-7c57fc57dc-lrnpx             1/1     Running   1          12d
hare-58c6bd8f9-gnbmn              1/1     Running   1          12d
hare-58c6bd8f9-gnbzx              1/1     Running   1          12d
hello-minikube-5655c9d946-ldn4q   1/1     Running   0          8s
hello-minikube-5655c9d946-lrpsc   1/1     Running   1          12d
moose-868fbc6b78-4d6ff            1/1     Running   1          12d
moose-868fbc6b78-v2778            1/1     Running   1          12d

 2022-01-30 07:00:44 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → kgp -A
NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE
default                bear-7c57fc57dc-6pkgl                        1/1     Running   1          12d
default                bear-7c57fc57dc-lrnpx                        1/1     Running   1          12d
default                hare-58c6bd8f9-gnbmn                         1/1     Running   1          12d
default                hare-58c6bd8f9-gnbzx                         1/1     Running   1          12d
default                hello-minikube-5655c9d946-ldn4q              1/1     Running   0          24s
default                hello-minikube-5655c9d946-lrpsc              1/1     Running   1          12d
default                moose-868fbc6b78-4d6ff                       1/1     Running   1          12d
default                moose-868fbc6b78-v2778                       1/1     Running   1          12d
kube-system            coredns-66bff467f8-hjv9n                     1/1     Running   1          12d
kube-system            coredns-66bff467f8-r8znt                     1/1     Running   1          12d
kube-system            etcd-minikube                                1/1     Running   1          12d
kube-system            kube-apiserver-minikube                      1/1     Running   1          12d
kube-system            kube-controller-manager-minikube             1/1     Running   1          12d
kube-system            kube-proxy-4dz9l                             1/1     Running   1          12d
kube-system            kube-scheduler-minikube                      1/1     Running   1          12d
kube-system            storage-provisioner                          1/1     Running   2          12d
kube-system            traefik-ingress-55c99674d9-mbp5h             1/1     Running   1          12d
kubernetes-dashboard   dashboard-metrics-scraper-84bfdf55ff-4mw5l   1/1     Running   1          12d
kubernetes-dashboard   kubernetes-dashboard-bc446cc64-6dp6z         1/1     Running   1          12d

 2022-01-30 07:01:00 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → k scale deployments hello-minikube --replicas=1 hello-minikube -n default
deployment.apps/hello-minikube scaled
deployment.apps/hello-minikube scaled

 2022-01-30 07:01:31 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → kgp -A
NAMESPACE              NAME                                         READY   STATUS        RESTARTS   AGE
default                bear-7c57fc57dc-6pkgl                        1/1     Running       1          12d
default                bear-7c57fc57dc-lrnpx                        1/1     Running       1          12d
default                hare-58c6bd8f9-gnbmn                         1/1     Running       1          12d
default                hare-58c6bd8f9-gnbzx                         1/1     Running       1          12d
default                hello-minikube-5655c9d946-ldn4q              0/1     Terminating   0          58s
default                hello-minikube-5655c9d946-lrpsc              1/1     Running       1          12d
default                moose-868fbc6b78-4d6ff                       1/1     Running       1          12d
default                moose-868fbc6b78-v2778                       1/1     Running       1          12d
kube-system            coredns-66bff467f8-hjv9n                     1/1     Running       1          12d
kube-system            coredns-66bff467f8-r8znt                     1/1     Running       1          12d
kube-system            etcd-minikube                                1/1     Running       1          12d
kube-system            kube-apiserver-minikube                      1/1     Running       1          12d
kube-system            kube-controller-manager-minikube             1/1     Running       1          12d
kube-system            kube-proxy-4dz9l                             1/1     Running       1          12d
kube-system            kube-scheduler-minikube                      1/1     Running       1          12d
kube-system            storage-provisioner                          1/1     Running       2          12d
kube-system            traefik-ingress-55c99674d9-mbp5h             1/1     Running       1          12d
kubernetes-dashboard   dashboard-metrics-scraper-84bfdf55ff-4mw5l   1/1     Running       1          12d
kubernetes-dashboard   kubernetes-dashboard-bc446cc64-6dp6z         1/1     Running       1          12d

 2022-01-30 07:01:34 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → keitn default hello-minikube-5655c9d946-lrpsc -- bash
root@hello-minikube-5655c9d946-lrpsc:/# ls -ltr
total 72
drwxr-xr-x   2 root root 4096 Nov 27  2015 home
drwxr-xr-x   2 root root 4096 Nov 27  2015 boot
drwxr-xr-x   2 root root 4096 Mar 31  2016 srv
drwxr-xr-x   2 root root 4096 Mar 31  2016 opt
drwxr-xr-x   2 root root 4096 Mar 31  2016 mnt
drwxr-xr-x   2 root root 4096 Mar 31  2016 media
drwxr-xr-x   2 root root 4096 Apr 21  2016 lib64
drwxr-xr-x   2 root root 4096 Apr 21  2016 sbin
drwx------   2 root root 4096 Apr 21  2016 root
drwxr-xr-x   1 root root 4096 May  2  2016 bin
drwxr-xr-x   1 root root 4096 May  2  2016 usr
drwxr-xr-x   1 root root 4096 May  2  2016 lib
drwxrwxrwt   1 root root 4096 May  2  2016 tmp
drwxr-xr-x   1 root root 4096 May  2  2016 var
-rw-r-----   1 root root  436 May 28  2016 README.md
dr-xr-xr-x  12 root root    0 Jan 30 01:27 sys
drwxr-xr-x   1 root root 4096 Jan 30 01:27 etc
dr-xr-xr-x 196 root root    0 Jan 30 01:27 proc
drwxr-xr-x   5 root root  360 Jan 30 01:27 dev
drwxr-xr-x   1 root root 4096 Jan 30 01:27 run
root@hello-minikube-5655c9d946-lrpsc:/# netstat
bash: netstat: command not found
root@hello-minikube-5655c9d946-lrpsc:/# yum install iproute iproute-doc
bash: yum: command not found
root@hello-minikube-5655c9d946-lrpsc:/# apt-get install iproute
Reading package lists... Done
Building dependency tree
Reading state information... Done
E: Unable to locate package iproute
root@hello-minikube-5655c9d946-lrpsc:/# sudo apt update
bash: sudo: command not found
root@hello-minikube-5655c9d946-lrpsc:/# apt-get install net-tools
Reading package lists... Done
Building dependency tree
Reading state information... Done
E: Unable to locate package net-tools
root@hello-minikube-5655c9d946-lrpsc:/# apt-get update
Get:1 http://archive.ubuntu.com/ubuntu xenial InRelease [247 kB]
Get:2 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [99.8 kB]
Get:3 http://archive.ubuntu.com/ubuntu xenial-security InRelease [99.8 kB]
Get:4 http://archive.ubuntu.com/ubuntu xenial/main Sources [1103 kB]
Get:5 http://archive.ubuntu.com/ubuntu xenial/restricted Sources [5179 B]
Get:6 http://archive.ubuntu.com/ubuntu xenial/universe Sources [9802 kB]
Get:7 http://archive.ubuntu.com/ubuntu xenial/main amd64 Packages [1558 kB]
Get:8 http://archive.ubuntu.com/ubuntu xenial/restricted amd64 Packages [14.1 kB]
Get:9 http://archive.ubuntu.com/ubuntu xenial/universe amd64 Packages [9827 kB]
Get:10 http://archive.ubuntu.com/ubuntu xenial-updates/main Sources [665 kB]
Get:11 http://archive.ubuntu.com/ubuntu xenial-updates/restricted Sources [3941 B]
Get:12 http://archive.ubuntu.com/ubuntu xenial-updates/universe Sources [548 kB]
Get:13 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 Packages [2560 kB]
Get:14 http://archive.ubuntu.com/ubuntu xenial-updates/restricted amd64 Packages [16.4 kB]
Get:15 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 Packages [1544 kB]
Get:16 http://archive.ubuntu.com/ubuntu xenial-security/main Sources [311 kB]
Get:17 http://archive.ubuntu.com/ubuntu xenial-security/restricted Sources [3239 B]
Get:18 http://archive.ubuntu.com/ubuntu xenial-security/universe Sources [256 kB]
Get:19 http://archive.ubuntu.com/ubuntu xenial-security/main amd64 Packages [2051 kB]
Get:20 http://archive.ubuntu.com/ubuntu xenial-security/restricted amd64 Packages [15.9 kB]
Get:21 http://archive.ubuntu.com/ubuntu xenial-security/universe amd64 Packages [984 kB]
Fetched 31.7 MB in 16s (1868 kB/s)
Reading package lists... Done
root@hello-minikube-5655c9d946-lrpsc:/# apt-get install net-tools
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following NEW packages will be installed:
  net-tools
0 upgraded, 1 newly installed, 0 to remove and 87 not upgraded.
Need to get 175 kB of archives.
After this operation, 725 kB of additional disk space will be used.
Get:1 http://archive.ubuntu.com/ubuntu xenial/main amd64 net-tools amd64 1.60-26ubuntu1 [175 kB]
Fetched 175 kB in 1s (91.0 kB/s)
debconf: delaying package configuration, since apt-utils is not installed
Selecting previously unselected package net-tools.
(Reading database ... 7105 files and directories currently installed.)
Preparing to unpack .../net-tools_1.60-26ubuntu1_amd64.deb ...
Unpacking net-tools (1.60-26ubuntu1) ...
Setting up net-tools (1.60-26ubuntu1) ...
root@hello-minikube-5655c9d946-lrpsc:/# ss
bash: ss: command not found
root@hello-minikube-5655c9d946-lrpsc:/# netstat
Active Internet connections (w/o servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 hello-minikube-56:48914 aerodent.canonical.c:80 TIME_WAIT
tcp        0      0 hello-minikube-56:43748 actiontoad.canonical:80 TIME_WAIT
Active UNIX domain sockets (w/o servers)
Proto RefCnt Flags       Type       State         I-Node   Path
unix  3      [ ]         STREAM     CONNECTED     37687
unix  3      [ ]         STREAM     CONNECTED     37686
root@hello-minikube-5655c9d946-lrpsc:/# netstat -ntulp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:8080            0.0.0.0:*               LISTEN      1/nginx: master pro
root@hello-minikube-5655c9d946-lrpsc:/# curl -v localhost:8080
* Rebuilt URL to: localhost:8080/
*   Trying 127.0.0.1...
* Connected to localhost (127.0.0.1) port 8080 (#0)
> GET / HTTP/1.1
> Host: localhost:8080
> User-Agent: curl/7.47.0
> Accept: */*
>
< HTTP/1.1 200 OK
< Server: nginx/1.10.0
< Date: Sun, 30 Jan 2022 01:35:35 GMT
< Content-Type: text/plain
< Transfer-Encoding: chunked
< Connection: keep-alive
<
CLIENT VALUES:
client_address=127.0.0.1
command=GET
real path=/
query=nil
request_version=1.1
request_uri=http://localhost:8080/

SERVER VALUES:
server_version=nginx: 1.10.0 - lua: 10001

HEADERS RECEIVED:
accept=*/*
host=localhost:8080
user-agent=curl/7.47.0
BODY:
* Connection #0 to host localhost left intact
-no body in request-root@hello-minikube-5655c9d946-lrpsc:/#
root@hello-minikube-5655c9d946-lrpsc:/# ls -ltr /etc/services
ls: cannot access '/etc/services': No such file or directory
root@hello-minikube-5655c9d946-lrpsc:/# apt-get install ufw
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following additional packages will be installed:
  dh-python file iptables libexpat1 libmagic1 libmpdec2 libnfnetlink0 libpython3-stdlib libpython3.5-minimal libpython3.5-stdlib libxtables11
  mime-support python3 python3-minimal python3.5 python3.5-minimal ucf
Suggested packages:
  python3-doc python3-tk python3-venv python3.5-venv python3.5-doc binfmt-support rsyslog
The following NEW packages will be installed:
  dh-python file iptables libexpat1 libmagic1 libmpdec2 libnfnetlink0 libpython3-stdlib libpython3.5-minimal libpython3.5-stdlib libxtables11
  mime-support python3 python3-minimal python3.5 python3.5-minimal ucf ufw
0 upgraded, 18 newly installed, 0 to remove and 87 not upgraded.
Need to get 5465 kB of archives.
After this operation, 31.7 MB of additional disk space will be used.
Do you want to continue? [Y/n] y
Get:1 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libpython3.5-minimal amd64 3.5.2-2ubuntu0~16.04.13 [524 kB]
Get:2 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libexpat1 amd64 2.1.0-7ubuntu0.16.04.5 [71.5 kB]
Get:3 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 python3.5-minimal amd64 3.5.2-2ubuntu0~16.04.13 [1597 kB]
Get:4 http://archive.ubuntu.com/ubuntu xenial/main amd64 python3-minimal amd64 3.5.1-3 [23.3 kB]
Get:5 http://archive.ubuntu.com/ubuntu xenial/main amd64 mime-support all 3.59ubuntu1 [31.0 kB]
Get:6 http://archive.ubuntu.com/ubuntu xenial/main amd64 libmpdec2 amd64 2.4.2-1 [82.6 kB]
Get:7 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libpython3.5-stdlib amd64 3.5.2-2ubuntu0~16.04.13 [2135 kB]
Get:8 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 python3.5 amd64 3.5.2-2ubuntu0~16.04.13 [165 kB]
Get:9 http://archive.ubuntu.com/ubuntu xenial/main amd64 libpython3-stdlib amd64 3.5.1-3 [6818 B]
Get:10 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 dh-python all 2.20151103ubuntu1.2 [73.9 kB]
Get:11 http://archive.ubuntu.com/ubuntu xenial/main amd64 python3 amd64 3.5.1-3 [8710 B]
Get:12 http://archive.ubuntu.com/ubuntu xenial/main amd64 libnfnetlink0 amd64 1.0.1-3 [13.3 kB]
Get:13 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libmagic1 amd64 1:5.25-2ubuntu1.4 [216 kB]
Get:14 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 file amd64 1:5.25-2ubuntu1.4 [21.2 kB]
Get:15 http://archive.ubuntu.com/ubuntu xenial/main amd64 libxtables11 amd64 1.6.0-2ubuntu3 [27.2 kB]
Get:16 http://archive.ubuntu.com/ubuntu xenial/main amd64 ucf all 3.0036 [52.9 kB]
Get:17 http://archive.ubuntu.com/ubuntu xenial/main amd64 iptables amd64 1.6.0-2ubuntu3 [266 kB]
Get:18 http://archive.ubuntu.com/ubuntu xenial/main amd64 ufw all 0.35-0ubuntu2 [149 kB]
Fetched 5465 kB in 6s (892 kB/s)
debconf: delaying package configuration, since apt-utils is not installed
Selecting previously unselected package libpython3.5-minimal:amd64.
(Reading database ... 7153 files and directories currently installed.)
Preparing to unpack .../libpython3.5-minimal_3.5.2-2ubuntu0~16.04.13_amd64.deb ...
Unpacking libpython3.5-minimal:amd64 (3.5.2-2ubuntu0~16.04.13) ...
Selecting previously unselected package libexpat1:amd64.
Preparing to unpack .../libexpat1_2.1.0-7ubuntu0.16.04.5_amd64.deb ...
Unpacking libexpat1:amd64 (2.1.0-7ubuntu0.16.04.5) ...
Selecting previously unselected package python3.5-minimal.
Preparing to unpack .../python3.5-minimal_3.5.2-2ubuntu0~16.04.13_amd64.deb ...
Unpacking python3.5-minimal (3.5.2-2ubuntu0~16.04.13) ...
Selecting previously unselected package python3-minimal.
Preparing to unpack .../python3-minimal_3.5.1-3_amd64.deb ...
Unpacking python3-minimal (3.5.1-3) ...
Selecting previously unselected package mime-support.
Preparing to unpack .../mime-support_3.59ubuntu1_all.deb ...
Unpacking mime-support (3.59ubuntu1) ...
Selecting previously unselected package libmpdec2:amd64.
Preparing to unpack .../libmpdec2_2.4.2-1_amd64.deb ...
Unpacking libmpdec2:amd64 (2.4.2-1) ...
Selecting previously unselected package libpython3.5-stdlib:amd64.
Preparing to unpack .../libpython3.5-stdlib_3.5.2-2ubuntu0~16.04.13_amd64.deb ...
Unpacking libpython3.5-stdlib:amd64 (3.5.2-2ubuntu0~16.04.13) ...
Selecting previously unselected package python3.5.
Preparing to unpack .../python3.5_3.5.2-2ubuntu0~16.04.13_amd64.deb ...
Unpacking python3.5 (3.5.2-2ubuntu0~16.04.13) ...
Selecting previously unselected package libpython3-stdlib:amd64.
Preparing to unpack .../libpython3-stdlib_3.5.1-3_amd64.deb ...
Unpacking libpython3-stdlib:amd64 (3.5.1-3) ...
Selecting previously unselected package dh-python.
Preparing to unpack .../dh-python_2.20151103ubuntu1.2_all.deb ...
Unpacking dh-python (2.20151103ubuntu1.2) ...
Processing triggers for libc-bin (2.23-0ubuntu3) ...
Setting up libpython3.5-minimal:amd64 (3.5.2-2ubuntu0~16.04.13) ...
Setting up libexpat1:amd64 (2.1.0-7ubuntu0.16.04.5) ...
Setting up python3.5-minimal (3.5.2-2ubuntu0~16.04.13) ...
Setting up python3-minimal (3.5.1-3) ...
Processing triggers for libc-bin (2.23-0ubuntu3) ...
Selecting previously unselected package python3.
(Reading database ... 8104 files and directories currently installed.)
Preparing to unpack .../python3_3.5.1-3_amd64.deb ...
Unpacking python3 (3.5.1-3) ...
Selecting previously unselected package libnfnetlink0:amd64.
Preparing to unpack .../libnfnetlink0_1.0.1-3_amd64.deb ...
Unpacking libnfnetlink0:amd64 (1.0.1-3) ...
Selecting previously unselected package libmagic1:amd64.
Preparing to unpack .../libmagic1_1%3a5.25-2ubuntu1.4_amd64.deb ...
Unpacking libmagic1:amd64 (1:5.25-2ubuntu1.4) ...
Selecting previously unselected package file.
Preparing to unpack .../file_1%3a5.25-2ubuntu1.4_amd64.deb ...
Unpacking file (1:5.25-2ubuntu1.4) ...
Selecting previously unselected package libxtables11:amd64.
Preparing to unpack .../libxtables11_1.6.0-2ubuntu3_amd64.deb ...
Unpacking libxtables11:amd64 (1.6.0-2ubuntu3) ...
Selecting previously unselected package ucf.
Preparing to unpack .../archives/ucf_3.0036_all.deb ...
Moving old data out of the way
Unpacking ucf (3.0036) ...
Selecting previously unselected package iptables.
Preparing to unpack .../iptables_1.6.0-2ubuntu3_amd64.deb ...
Unpacking iptables (1.6.0-2ubuntu3) ...
Selecting previously unselected package ufw.
Preparing to unpack .../ufw_0.35-0ubuntu2_all.deb ...
Unpacking ufw (0.35-0ubuntu2) ...
Processing triggers for libc-bin (2.23-0ubuntu3) ...
Setting up mime-support (3.59ubuntu1) ...
Setting up libmpdec2:amd64 (2.4.2-1) ...
Setting up libpython3.5-stdlib:amd64 (3.5.2-2ubuntu0~16.04.13) ...
Setting up python3.5 (3.5.2-2ubuntu0~16.04.13) ...
Setting up libpython3-stdlib:amd64 (3.5.1-3) ...
Setting up libnfnetlink0:amd64 (1.0.1-3) ...
Setting up libmagic1:amd64 (1:5.25-2ubuntu1.4) ...
Setting up file (1:5.25-2ubuntu1.4) ...
Setting up libxtables11:amd64 (1.6.0-2ubuntu3) ...
Setting up ucf (3.0036) ...
debconf: unable to initialize frontend: Dialog
debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)
debconf: falling back to frontend: Readline
Setting up iptables (1.6.0-2ubuntu3) ...
Setting up dh-python (2.20151103ubuntu1.2) ...
Setting up python3 (3.5.1-3) ...
running python rtupdate hooks for python3.5...
running python post-rtupdate hooks for python3.5...
Setting up ufw (0.35-0ubuntu2) ...
debconf: unable to initialize frontend: Dialog
debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)
debconf: falling back to frontend: Readline
/usr/bin/ucf: line 344: getopt: command not found
dpkg: error processing package ufw (--configure):
 subprocess installed post-installation script returned error exit status 127
Processing triggers for libc-bin (2.23-0ubuntu3) ...
Errors were encountered while processing:
 ufw
E: Sub-process /usr/bin/dpkg returned an error code (1)
root@hello-minikube-5655c9d946-lrpsc:/# uwf
bash: uwf: command not found
root@hello-minikube-5655c9d946-lrpsc:/# ufw
ERROR: not enough args
root@hello-minikube-5655c9d946-lrpsc:/# ufw --help

Usage: ufw COMMAND

Commands:
 enable                          enables the firewall
 disable                         disables the firewall
 default ARG                     set default policy
 logging LEVEL                   set logging to LEVEL
 allow ARGS                      add allow rule
 deny ARGS                       add deny rule
 reject ARGS                     add reject rule
 limit ARGS                      add limit rule
 delete RULE|NUM                 delete RULE
 insert NUM RULE                 insert RULE at NUM
 route RULE                      add route RULE
 route delete RULE|NUM           delete route RULE
 route insert NUM RULE           insert route RULE at NUM
 reload                          reload firewall
 reset                           reset firewall
 status                          show firewall status
 status numbered                 show firewall status as numbered list of RULES
 status verbose                  show verbose firewall status
 show ARG                        show firewall report
 version                         display version information

Application profile commands:
 app list                        list application profiles
 app info PROFILE                show information on PROFILE
 app update PROFILE              update PROFILE
 app default ARG                 set default application policy

root@hello-minikube-5655c9d946-lrpsc:/# ufw status
ERROR: Couldn't stat '/etc/ufw/user6.rules'
root@hello-minikube-5655c9d946-lrpsc:/# iptables
iptables v1.6.0: no command specified
Try `iptables -h' or 'iptables --help' for more information.
root@hello-minikube-5655c9d946-lrpsc:/# iptables --help
iptables v1.6.0

Usage: iptables -[ACD] chain rule-specification [options]
       iptables -I chain [rulenum] rule-specification [options]
       iptables -R chain rulenum rule-specification [options]
       iptables -D chain rulenum [options]
       iptables -[LS] [chain [rulenum]] [options]
       iptables -[FZ] [chain] [options]
       iptables -[NX] chain
       iptables -E old-chain-name new-chain-name
       iptables -P chain target [options]
       iptables -h (print this help information)

Commands:
Either long or short options are allowed.
  --append  -A chain    Append to chain
  --check   -C chain    Check for the existence of a rule
  --delete  -D chain    Delete matching rule from chain
  --delete  -D chain rulenum
        Delete rule rulenum (1 = first) from chain
  --insert  -I chain [rulenum]
        Insert in chain as rulenum (default 1=first)
  --replace -R chain rulenum
        Replace rule rulenum (1 = first) in chain
  --list    -L [chain [rulenum]]
        List the rules in a chain or all chains
  --list-rules -S [chain [rulenum]]
        Print the rules in a chain or all chains
  --flush   -F [chain]    Delete all rules in  chain or all chains
  --zero    -Z [chain [rulenum]]
        Zero counters in chain or all chains
  --new     -N chain    Create a new user-defined chain
  --delete-chain
            -X [chain]    Delete a user-defined chain
  --policy  -P chain target
        Change policy on chain to target
  --rename-chain
            -E old-chain new-chain
        Change chain name, (moving any references)
Options:
    --ipv4  -4    Nothing (line is ignored by ip6tables-restore)
    --ipv6  -6    Error (line is ignored by iptables-restore)
[!] --protocol  -p proto  protocol: by number or name, eg. `tcp'
[!] --source  -s address[/mask][...]
        source specification
[!] --destination -d address[/mask][...]
        destination specification
[!] --in-interface -i input name[+]
        network interface name ([+] for wildcard)
 --jump -j target
        target for rule (may load target extension)
  --goto      -g chain
                              jump to chain with no return
  --match -m match
        extended match (may load extension)
  --numeric -n    numeric output of addresses and ports
[!] --out-interface -o output name[+]
        network interface name ([+] for wildcard)
  --table -t table  table to manipulate (default: `filter')
  --verbose -v    verbose mode
  --wait  -w [seconds]  wait for the xtables lock
  --line-numbers    print line numbers when listing
  --exact -x    expand numbers (display exact values)
[!] --fragment  -f    match second or further fragments only
  --modprobe=<command>    try to insert modules using this command
  --set-counters PKTS BYTES set the counter during insert/append
[!] --version -V    print package version.
root@hello-minikube-5655c9d946-lrpsc:/# iptables --list-rules
iptables v1.6.0: can't initialize iptables table `filter': Permission denied (you must be root)
Perhaps iptables or your kernel needs to be upgraded.
root@hello-minikube-5655c9d946-lrpsc:/# sudo iptables --list-rules
bash: sudo: command not found
root@hello-minikube-5655c9d946-lrpsc:/# sudo su -
bash: sudo: command not found
root@hello-minikube-5655c9d946-lrpsc:/# su -
-su: 9: mesg: not found
#
# iptables --list-rules
iptables v1.6.0: can't initialize iptables table `filter': Permission denied (you must be root)
Perhaps iptables or your kernel needs to be upgraded.
#
# whoami
root
# netstat -ntulp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:8080            0.0.0.0:*               LISTEN      1/nginx: master pro
#
# curl -v localhost:8080
* Rebuilt URL to: localhost:8080/
*   Trying 127.0.0.1...
* Connected to localhost (127.0.0.1) port 8080 (#0)
> GET / HTTP/1.1
> Host: localhost:8080
> User-Agent: curl/7.47.0
> Accept: */*
>
< HTTP/1.1 200 OK
< Server: nginx/1.10.0
< Date: Sun, 30 Jan 2022 01:43:39 GMT
< Content-Type: text/plain
< Transfer-Encoding: chunked
< Connection: keep-alive
<
CLIENT VALUES:
client_address=127.0.0.1
command=GET
real path=/
query=nil
request_version=1.1
request_uri=http://localhost:8080/

SERVER VALUES:
server_version=nginx: 1.10.0 - lua: 10001

HEADERS RECEIVED:
accept=*/*
host=localhost:8080
user-agent=curl/7.47.0
BODY:
* Connection #0 to host localhost left intact
-no body in request-#
# iptables -A INPUT -p tcp --destination-port 8080 -j DROP
iptables v1.6.0: can't initialize iptables table `filter': Permission denied (you must be root)
Perhaps iptables or your kernel needs to be upgraded.
# exit
root@hello-minikube-5655c9d946-lrpsc:/# iptables -A INPUT -p tcp --destination-port 8080 -j DROP
iptables v1.6.0: can't initialize iptables table `filter': Permission denied (you must be root)
Perhaps iptables or your kernel needs to be upgraded.
apiVersion: v1      # apiVersion: this is the version of the API used by the cluster.
root@hello-minikube-5655c9d946-lrpsc:/# su iptables -A INPUT -p tcp --destination-port 8080 -j DROP
su: invalid option -- 'A'
Usage: su [options] [LOGIN]

Options:
  -c, --command COMMAND         pass COMMAND to the invoked shell
  -h, --help                    display this help message and exit
  -, -l, --login                make the shell a login shell
  -m, -p,
  --preserve-environment        do not reset environment variables, and
                                keep the same shell
  -s, --shell SHELL             use SHELL instead of the default in passwd

root@hello-minikube-5655c9d946-lrpsc:/# su - iptables -A INPUT -p tcp --destination-port 8080 -j DROP
su: invalid option -- 'A'
Usage: su [options] [LOGIN]

Options:
  -c, --command COMMAND         pass COMMAND to the invoked shell
  -h, --help                    display this help message and exit
  -, -l, --login                make the shell a login shell
  -m, -p,
  --preserve-environment        do not reset environment variables, and
                                keep the same shell
  -s, --shell SHELL             use SHELL instead of the default in passwd

root@hello-minikube-5655c9d946-lrpsc:/# su -
-su: 9: mesg: not found
# iptables -A INPUT -p tcp --destination-port 8080 -j DROP
iptables v1.6.0: can't initialize iptables table `filter': Permission denied (you must be root)
Perhaps iptables or your kernel needs to be upgraded.
# exit
root@hello-minikube-5655c9d946-lrpsc:/# exit
exit
command terminated with exit code 3

 2022-01-30 07:15:20 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp/traefik-test/traefik-setup/namebasedrouting
○ → cd ../../..

 2022-01-30 07:18:26 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp
○ → vi nginx_pod.yaml

 2022-01-30 07:18:37 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp
○ → cat nginx_pod.yaml
apiVersion: v1      # apiVersion: this is the version of the API used by the cluster.
                    # With new versions of Kubernetes being released, new functionality is introduced and, hence, new API versions may be defined.
                    # For the pod object, we use API version v1.

kind: Pod

metadata:           # Metadata: here we can define data about the object we are about to create.
  name: webserver   # In this example, we only provide the name of the pod. But you can provide other details like the namespace.
spec:               #The spec part defines the characteristics that a given Kubernetes object should have.
                    # It is the cluster’s responsibility to update the status of the object to always match the desired configuration.
                    # In our example, the spec instructs that this object (the pod) should have one container with some attributes.
  containers:
  - name: webserver  # The name that this container will have.
    image: nginx:latest # The image on which it is based.
    ports:               # The port(s) that will be open.
    - containerPort: 80

 2022-01-30 07:18:41 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp
○ → k apply -f nginx_pod.yaml
pod/webserver created

 2022-01-30 07:18:56 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp
○ → kgpn default
NAME                              READY   STATUS              RESTARTS   AGE
bear-7c57fc57dc-6pkgl             1/1     Running             1          12d
bear-7c57fc57dc-lrnpx             1/1     Running             1          12d
hare-58c6bd8f9-gnbmn              1/1     Running             1          12d
hare-58c6bd8f9-gnbzx              1/1     Running             1          12d
hello-minikube-5655c9d946-lrpsc   1/1     Running             1          12d
moose-868fbc6b78-4d6ff            1/1     Running             1          12d
moose-868fbc6b78-v2778            1/1     Running             1          12d
webserver                         0/1     ContainerCreating   0          21s

 2022-01-30 07:19:17 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp
○ → kgpn default
NAME                              READY   STATUS              RESTARTS   AGE
bear-7c57fc57dc-6pkgl             1/1     Running             1          12d
bear-7c57fc57dc-lrnpx             1/1     Running             1          12d
hare-58c6bd8f9-gnbmn              1/1     Running             1          12d
hare-58c6bd8f9-gnbzx              1/1     Running             1          12d
hello-minikube-5655c9d946-lrpsc   1/1     Running             1          12d
moose-868fbc6b78-4d6ff            1/1     Running             1          12d
moose-868fbc6b78-v2778            1/1     Running             1          12d
webserver                         0/1     ContainerCreating   0          29s

 2022-01-30 07:19:25 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp
○ → kgpn default -w
NAME                              READY   STATUS              RESTARTS   AGE
bear-7c57fc57dc-6pkgl             1/1     Running             1          12d
bear-7c57fc57dc-lrnpx             1/1     Running             1          12d
hare-58c6bd8f9-gnbmn              1/1     Running             1          12d
hare-58c6bd8f9-gnbzx              1/1     Running             1          12d
hello-minikube-5655c9d946-lrpsc   1/1     Running             1          12d
moose-868fbc6b78-4d6ff            1/1     Running             1          12d
moose-868fbc6b78-v2778            1/1     Running             1          12d
webserver                         0/1     ContainerCreating   0          34s
webserver                         1/1     Running             0          37s
^C
 2022-01-30 07:19:48 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp
○ → keitn default webserver -- bash
root@webserver:/# set -o vi
root@webserver:/# cat /etc/hoss
cat: /etc/hoss: No such file or directory
root@webserver:/# cat /etc/hosts
# Kubernetes-managed hosts file.
127.0.0.1 localhost
::1 localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
fe00::0 ip6-mcastprefix
fe00::1 ip6-allnodes
fe00::2 ip6-allrouters
172.17.0.14 webserver
root@webserver:/# vi /etc/hosts
bash: vi: command not found
root@webserver:/# uname -all
uname: invalid option -- 'l'
Try 'uname --help' for more information.
root@webserver:/# uname -a
Linux webserver 4.19.107 #1 SMP Thu Mar 26 11:33:10 PDT 2020 x86_64 GNU/Linux
root@webserver:/# iptables
bash: iptables: command not found
root@webserver:/# ufw
bash: ufw: command not found
root@webserver:/# ss
bash: ss: command not found
root@webserver:/# apt-get install net-tools
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
E: Unable to locate package net-tools
root@webserver:/# apt-get update
Get:1 http://security.debian.org/debian-security bullseye-security InRelease [44.1 kB]
Get:2 http://deb.debian.org/debian bullseye InRelease [116 kB]
Get:3 http://security.debian.org/debian-security bullseye-security/main amd64 Packages [114 kB]
Get:4 http://deb.debian.org/debian bullseye-updates InRelease [39.4 kB]
Get:5 http://deb.debian.org/debian bullseye/main amd64 Packages [8183 kB]
Get:6 http://deb.debian.org/debian bullseye-updates/main amd64 Packages [2596 B]
Fetched 8499 kB in 5s (1598 kB/s)
Reading package lists... Done
root@webserver:/# apt-get install net-tools
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following NEW packages will be installed:
  net-tools
0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.
Need to get 250 kB of archives.
After this operation, 1015 kB of additional disk space will be used.
Get:1 http://deb.debian.org/debian bullseye/main amd64 net-tools amd64 1.60+git20181103.0eebece-1 [250 kB]
Fetched 250 kB in 0s (916 kB/s)
debconf: delaying package configuration, since apt-utils is not installed
Selecting previously unselected package net-tools.
(Reading database ... 7815 files and directories currently installed.)
Preparing to unpack .../net-tools_1.60+git20181103.0eebece-1_amd64.deb ...
Unpacking net-tools (1.60+git20181103.0eebece-1) ...
Setting up net-tools (1.60+git20181103.0eebece-1) ...
root@webserver:/# apt-get install ufw
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following additional packages will be installed:
  iptables libedit2 libgpm2 libip4tc2 libip6tc2 libjansson4 libmnl0 libmpdec3 libncursesw6 libnetfilter-conntrack3 libnfnetlink0 libnftables1
  libnftnl11 libpython3-stdlib libpython3.9-minimal libpython3.9-stdlib libsqlite3-0 libxtables12 media-types netbase nftables python3
  python3-minimal python3.9 python3.9-minimal
Suggested packages:
  firewalld kmod gpm python3-doc python3-tk python3-venv python3.9-venv python3.9-doc binutils binfmt-support rsyslog
The following NEW packages will be installed:
  iptables libedit2 libgpm2 libip4tc2 libip6tc2 libjansson4 libmnl0 libmpdec3 libncursesw6 libnetfilter-conntrack3 libnfnetlink0 libnftables1
  libnftnl11 libpython3-stdlib libpython3.9-minimal libpython3.9-stdlib libsqlite3-0 libxtables12 media-types netbase nftables python3
  python3-minimal python3.9 python3.9-minimal ufw
0 upgraded, 26 newly installed, 0 to remove and 0 not upgraded.
Need to get 7367 kB of archives.
After this operation, 27.6 MB of additional disk space will be used.
Do you want to continue? [Y/n]
Get:1 http://deb.debian.org/debian bullseye/main amd64 libjansson4 amd64 2.13.1-1.1 [39.7 kB]
Get:2 http://deb.debian.org/debian bullseye/main amd64 libmnl0 amd64 1.0.4-3 [12.5 kB]
Get:3 http://deb.debian.org/debian bullseye/main amd64 libnftnl11 amd64 1.1.9-1 [63.7 kB]
Get:4 http://deb.debian.org/debian bullseye/main amd64 libxtables12 amd64 1.8.7-1 [45.1 kB]
Get:5 http://deb.debian.org/debian bullseye/main amd64 libnftables1 amd64 0.9.8-3.1 [261 kB]
Get:6 http://deb.debian.org/debian bullseye/main amd64 libedit2 amd64 3.1-20191231-2+b1 [96.7 kB]
Get:7 http://deb.debian.org/debian bullseye/main amd64 nftables amd64 0.9.8-3.1 [68.5 kB]
Get:8 http://deb.debian.org/debian bullseye/main amd64 libpython3.9-minimal amd64 3.9.2-1 [801 kB]
Get:9 http://deb.debian.org/debian bullseye/main amd64 python3.9-minimal amd64 3.9.2-1 [1955 kB]
Get:10 http://deb.debian.org/debian bullseye/main amd64 python3-minimal amd64 3.9.2-3 [38.2 kB]
Get:11 http://deb.debian.org/debian bullseye/main amd64 media-types all 4.0.0 [30.3 kB]
Get:12 http://deb.debian.org/debian bullseye/main amd64 libmpdec3 amd64 2.5.1-1 [87.7 kB]
Get:13 http://deb.debian.org/debian bullseye/main amd64 libncursesw6 amd64 6.2+20201114-2 [132 kB]
Get:14 http://deb.debian.org/debian bullseye/main amd64 libsqlite3-0 amd64 3.34.1-3 [797 kB]
Get:15 http://deb.debian.org/debian bullseye/main amd64 libpython3.9-stdlib amd64 3.9.2-1 [1684 kB]
Get:16 http://deb.debian.org/debian bullseye/main amd64 python3.9 amd64 3.9.2-1 [466 kB]
Get:17 http://deb.debian.org/debian bullseye/main amd64 libpython3-stdlib amd64 3.9.2-3 [21.4 kB]
Get:18 http://deb.debian.org/debian bullseye/main amd64 python3 amd64 3.9.2-3 [37.9 kB]
Get:19 http://deb.debian.org/debian bullseye/main amd64 netbase all 6.3 [19.9 kB]
Get:20 http://deb.debian.org/debian bullseye/main amd64 libip4tc2 amd64 1.8.7-1 [34.6 kB]
Get:21 http://deb.debian.org/debian bullseye/main amd64 libip6tc2 amd64 1.8.7-1 [35.0 kB]
Get:22 http://deb.debian.org/debian bullseye/main amd64 libnfnetlink0 amd64 1.0.1-3+b1 [13.9 kB]
Get:23 http://deb.debian.org/debian bullseye/main amd64 libnetfilter-conntrack3 amd64 1.0.8-3 [40.6 kB]
Get:24 http://deb.debian.org/debian bullseye/main amd64 iptables amd64 1.8.7-1 [382 kB]
Get:25 http://deb.debian.org/debian bullseye/main amd64 libgpm2 amd64 1.20.7-8 [35.6 kB]
Get:26 http://deb.debian.org/debian bullseye/main amd64 ufw all 0.36-7.1 [167 kB]
Fetched 7367 kB in 4s (1644 kB/s)
debconf: delaying package configuration, since apt-utils is not installed
Selecting previously unselected package libjansson4:amd64.
(Reading database ... 7872 files and directories currently installed.)
Preparing to unpack .../0-libjansson4_2.13.1-1.1_amd64.deb ...
Unpacking libjansson4:amd64 (2.13.1-1.1) ...
Selecting previously unselected package libmnl0:amd64.
Preparing to unpack .../1-libmnl0_1.0.4-3_amd64.deb ...
Unpacking libmnl0:amd64 (1.0.4-3) ...
Selecting previously unselected package libnftnl11:amd64.
Preparing to unpack .../2-libnftnl11_1.1.9-1_amd64.deb ...
Unpacking libnftnl11:amd64 (1.1.9-1) ...
Selecting previously unselected package libxtables12:amd64.
Preparing to unpack .../3-libxtables12_1.8.7-1_amd64.deb ...
Unpacking libxtables12:amd64 (1.8.7-1) ...
Selecting previously unselected package libnftables1:amd64.
Preparing to unpack .../4-libnftables1_0.9.8-3.1_amd64.deb ...
Unpacking libnftables1:amd64 (0.9.8-3.1) ...
Selecting previously unselected package libedit2:amd64.
Preparing to unpack .../5-libedit2_3.1-20191231-2+b1_amd64.deb ...
Unpacking libedit2:amd64 (3.1-20191231-2+b1) ...
Selecting previously unselected package nftables.
Preparing to unpack .../6-nftables_0.9.8-3.1_amd64.deb ...
Unpacking nftables (0.9.8-3.1) ...
Selecting previously unselected package libpython3.9-minimal:amd64.
Preparing to unpack .../7-libpython3.9-minimal_3.9.2-1_amd64.deb ...
Unpacking libpython3.9-minimal:amd64 (3.9.2-1) ...
Selecting previously unselected package python3.9-minimal.
Preparing to unpack .../8-python3.9-minimal_3.9.2-1_amd64.deb ...
Unpacking python3.9-minimal (3.9.2-1) ...
Setting up libpython3.9-minimal:amd64 (3.9.2-1) ...
Setting up python3.9-minimal (3.9.2-1) ...
Selecting previously unselected package python3-minimal.
(Reading database ... 8236 files and directories currently installed.)
Preparing to unpack .../0-python3-minimal_3.9.2-3_amd64.deb ...
Unpacking python3-minimal (3.9.2-3) ...
Selecting previously unselected package media-types.
Preparing to unpack .../1-media-types_4.0.0_all.deb ...
Unpacking media-types (4.0.0) ...
Selecting previously unselected package libmpdec3:amd64.
Preparing to unpack .../2-libmpdec3_2.5.1-1_amd64.deb ...
Unpacking libmpdec3:amd64 (2.5.1-1) ...
Selecting previously unselected package libncursesw6:amd64.
Preparing to unpack .../3-libncursesw6_6.2+20201114-2_amd64.deb ...
Unpacking libncursesw6:amd64 (6.2+20201114-2) ...
Selecting previously unselected package libsqlite3-0:amd64.
Preparing to unpack .../4-libsqlite3-0_3.34.1-3_amd64.deb ...
Unpacking libsqlite3-0:amd64 (3.34.1-3) ...
Selecting previously unselected package libpython3.9-stdlib:amd64.
Preparing to unpack .../5-libpython3.9-stdlib_3.9.2-1_amd64.deb ...
Unpacking libpython3.9-stdlib:amd64 (3.9.2-1) ...
Selecting previously unselected package python3.9.
Preparing to unpack .../6-python3.9_3.9.2-1_amd64.deb ...
Unpacking python3.9 (3.9.2-1) ...
Selecting previously unselected package libpython3-stdlib:amd64.
Preparing to unpack .../7-libpython3-stdlib_3.9.2-3_amd64.deb ...
Unpacking libpython3-stdlib:amd64 (3.9.2-3) ...
Setting up python3-minimal (3.9.2-3) ...
Selecting previously unselected package python3.
(Reading database ... 8650 files and directories currently installed.)
Preparing to unpack .../0-python3_3.9.2-3_amd64.deb ...
Unpacking python3 (3.9.2-3) ...
Selecting previously unselected package netbase.
Preparing to unpack .../1-netbase_6.3_all.deb ...
Unpacking netbase (6.3) ...
Selecting previously unselected package libip4tc2:amd64.
Preparing to unpack .../2-libip4tc2_1.8.7-1_amd64.deb ...
Unpacking libip4tc2:amd64 (1.8.7-1) ...
Selecting previously unselected package libip6tc2:amd64.
Preparing to unpack .../3-libip6tc2_1.8.7-1_amd64.deb ...
Unpacking libip6tc2:amd64 (1.8.7-1) ...
Selecting previously unselected package libnfnetlink0:amd64.
Preparing to unpack .../4-libnfnetlink0_1.0.1-3+b1_amd64.deb ...
Unpacking libnfnetlink0:amd64 (1.0.1-3+b1) ...
Selecting previously unselected package libnetfilter-conntrack3:amd64.
Preparing to unpack .../5-libnetfilter-conntrack3_1.0.8-3_amd64.deb ...
Unpacking libnetfilter-conntrack3:amd64 (1.0.8-3) ...
Selecting previously unselected package iptables.
Preparing to unpack .../6-iptables_1.8.7-1_amd64.deb ...
Unpacking iptables (1.8.7-1) ...
Selecting previously unselected package libgpm2:amd64.
Preparing to unpack .../7-libgpm2_1.20.7-8_amd64.deb ...
Unpacking libgpm2:amd64 (1.20.7-8) ...
Selecting previously unselected package ufw.
Preparing to unpack .../8-ufw_0.36-7.1_all.deb ...
Unpacking ufw (0.36-7.1) ...
Setting up libip4tc2:amd64 (1.8.7-1) ...
Setting up media-types (4.0.0) ...
Setting up libgpm2:amd64 (1.20.7-8) ...
Setting up libip6tc2:amd64 (1.8.7-1) ...
Setting up libedit2:amd64 (3.1-20191231-2+b1) ...
Setting up libsqlite3-0:amd64 (3.34.1-3) ...
Setting up libjansson4:amd64 (2.13.1-1.1) ...
Setting up libmnl0:amd64 (1.0.4-3) ...
Setting up libncursesw6:amd64 (6.2+20201114-2) ...
Setting up libxtables12:amd64 (1.8.7-1) ...
Setting up libnfnetlink0:amd64 (1.0.1-3+b1) ...
Setting up libmpdec3:amd64 (2.5.1-1) ...
Setting up netbase (6.3) ...
Setting up libpython3.9-stdlib:amd64 (3.9.2-1) ...
Setting up libpython3-stdlib:amd64 (3.9.2-3) ...
Setting up libnftnl11:amd64 (1.1.9-1) ...
Setting up libnetfilter-conntrack3:amd64 (1.0.8-3) ...
Setting up python3.9 (3.9.2-1) ...
Setting up libnftables1:amd64 (0.9.8-3.1) ...
Setting up nftables (0.9.8-3.1) ...
Setting up iptables (1.8.7-1) ...
update-alternatives: using /usr/sbin/iptables-legacy to provide /usr/sbin/iptables (iptables) in auto mode
update-alternatives: using /usr/sbin/ip6tables-legacy to provide /usr/sbin/ip6tables (ip6tables) in auto mode
update-alternatives: using /usr/sbin/iptables-nft to provide /usr/sbin/iptables (iptables) in auto mode
update-alternatives: using /usr/sbin/ip6tables-nft to provide /usr/sbin/ip6tables (ip6tables) in auto mode
update-alternatives: using /usr/sbin/arptables-nft to provide /usr/sbin/arptables (arptables) in auto mode
update-alternatives: using /usr/sbin/ebtables-nft to provide /usr/sbin/ebtables (ebtables) in auto mode
Setting up python3 (3.9.2-3) ...
running python rtupdate hooks for python3.9...
running python post-rtupdate hooks for python3.9...
Setting up ufw (0.36-7.1) ...
debconf: unable to initialize frontend: Dialog
debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)
debconf: falling back to frontend: Readline
debconf: unable to initialize frontend: Readline
debconf: (Can't locate Term/ReadLine.pm in @INC (you may need to install the Term::ReadLine module) (@INC contains: /etc/perl /usr/local/lib/x86_64-linux-gnu/perl/5.32.1 /usr/local/share/perl/5.32.1 /usr/lib/x86_64-linux-gnu/perl5/5.32 /usr/share/perl5 /usr/lib/x86_64-linux-gnu/perl-base /usr/lib/x86_64-linux-gnu/perl/5.32 /usr/share/perl/5.32 /usr/local/lib/site_perl) at /usr/share/perl5/Debconf/FrontEnd/Readline.pm line 7.)
debconf: falling back to frontend: Teletype

Creating config file /etc/ufw/before.rules with new version

Creating config file /etc/ufw/before6.rules with new version

Creating config file /etc/ufw/after.rules with new version

Creating config file /etc/ufw/after6.rules with new version
Processing triggers for libc-bin (2.31-13+deb11u2) ...
root@webserver:/# ufw
ERROR: not enough args
root@webserver:/# netstat
Active Internet connections (w/o servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State

netsttcp        0      0 webserver:57704         151.101.158.132:http    TIME_WAIT
tcp        0      0 webserver:57564         151.101.158.132:http    TIME_WAIT
Active UNIX domain sockets (w/o servers)
Proto RefCnt Flags       Type       State         I-Node   Path
unix  3      [ ]         STREAM     CONNECTED     65222
unix  3      [ ]         STREAM     CONNECTED     65220
unix  3      [ ]         STREAM     CONNECTED     65219
unix  3      [ ]         STREAM     CONNECTED     65221
root@webserver:/#
root@webserver:/# netstat -ntulp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      1/nginx: master pro
tcp6       0      0 :::80                   :::*                    LISTEN      1/nginx: master pro
root@webserver:/# curl localhost:80
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
root@webserver:/# ufw --help

Usage: ufw COMMAND

Commands:
 enable                          enables the firewall
 disable                         disables the firewall
 default ARG                     set default policy
 logging LEVEL                   set logging to LEVEL
 allow ARGS                      add allow rule
 deny ARGS                       add deny rule
 reject ARGS                     add reject rule
 limit ARGS                      add limit rule
 delete RULE|NUM                 delete RULE
 insert NUM RULE                 insert RULE at NUM
 prepend RULE                    prepend RULE
 route RULE                      add route RULE
 route delete RULE|NUM           delete route RULE
 route insert NUM RULE           insert route RULE at NUM
 reload                          reload firewall
 reset                           reset firewall
 status                          show firewall status
 status numbered                 show firewall status as numbered list of RULES
 status verbose                  show verbose firewall status
 show ARG                        show firewall report
 version                         display version information

Application profile commands:
 app list                        list application profiles
 app info PROFILE                show information on PROFILE
 app update PROFILE              update PROFILE
 app default ARG                 set default application policy

root@webserver:/# ufw status
ERROR: problem running iptables: iptables v1.8.7 (nf_tables): Could not fetch rule set generation id: Permission denied (you must be root)



root@webserver:/# iptables
iptables v1.8.7 (nf_tables): no command specified
Try `iptables -h' or 'iptables --help' for more information.
root@webserver:/# iptables -A INPUT -p tcp --destination-port 8080 -j DROP
iptables v1.8.7 (nf_tables): unknown option "--destination-port"
Try `iptables -h' or 'iptables --help' for more information.
root@webserver:/# iptables -A INPUT -p tcp --destination-port 8080 -j DROP
iptables v1.8.7 (nf_tables): unknown option "--destination-port"
Try `iptables -h' or 'iptables --help' for more information.
root@webserver:/# sudo su -
bash: sudo: command not found
root@webserver:/# su -
root@webserver:~# whoami
root
root@webserver:~# ufw status
ERROR: problem running iptables: iptables v1.8.7 (nf_tables): Could not fetch rule set generation id: Permission denied (you must be root)



root@webserver:~# adduser azher
Adding user `azher' ...
Adding new group `azher' (1000) ...
Adding new user `azher' (1000) with group `azher' ...
Creating home directory `/home/azher' ...
Copying files from `/etc/skel' ...
New password:
Retype new password:
No password has been supplied.
New password:
Retype new password:
No password has been supplied.
New password:
Retype new password:
passwd: password updated successfully
Changing the user information for azher
Enter the new value, or press ENTER for the default
  Full Name []: Azher Khan
  Room Number []:
  Work Phone []:
  Home Phone []:
  Other []:
Is the information correct? [Y/n] y
root@webserver:~# adduser azher sudo
Adding user `azher' to group `sudo' ...
Adding user azher to group sudo
Done.
root@webserver:~# su - azher
azher@webserver:~$ sudo ufw status
-bash: sudo: command not found
azher@webserver:~$ sudo -l
-bash: sudo: command not found
azher@webserver:~$ apt-get install sudo
^C
azher@webserver:~$ exit
logout
root@webserver:~# apt-get install sudo
^C
root@webserver:~# .apt
-bash: .apt: command not found
root@webserver:~# set -o vi
root@webserver:~# #.apt
root@webserver:~# apt-get update
Hit:1 http://deb.debian.org/debian bullseye InRelease
Hit:2 http://security.debian.org/debian-security bullseye-security InRelease
Hit:3 http://deb.debian.org/debian bullseye-updates InRelease
Reading package lists... Done
root@webserver:~#
root@webserver:~# set -o vi
root@webserver:~#
root@webserver:~#
root@webserver:~# apt-get install vi
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
E: Unable to locate package vi
root@webserver:~# apt-get install sudo
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following NEW packages will be installed:
  sudo
0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.
Need to get 1059 kB of archives.
After this operation, 4699 kB of additional disk space will be used.
Get:1 http://deb.debian.org/debian bullseye/main amd64 sudo amd64 1.9.5p2-3 [1059 kB]
Fetched 1059 kB in 2s (593 kB/s)
debconf: delaying package configuration, since apt-utils is not installed
Selecting previously unselected package sudo.
(Reading database ... 9019 files and directories currently installed.)
Preparing to unpack .../sudo_1.9.5p2-3_amd64.deb ...
Unpacking sudo (1.9.5p2-3) ...
Setting up sudo (1.9.5p2-3) ...
invoke-rc.d: could not determine current runlevel
invoke-rc.d: policy-rc.d denied execution of start.
root@webserver:~# exit
logout
root@webserver:/# apt-get install sudo
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
sudo is already the newest version (1.9.5p2-3).
0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.
root@webserver:/# sudo
usage: sudo -h | -K | -k | -V
usage: sudo -v [-AknS] [-g group] [-h host] [-p prompt] [-u user]
usage: sudo -l [-AknS] [-g group] [-h host] [-p prompt] [-U user] [-u user] [command]
usage: sudo [-AbEHknPS] [-r role] [-t type] [-C num] [-D directory] [-g group] [-h host] [-p prompt] [-R directory] [-T timeout] [-u user]
            [VAR=value] [-i|-s] [<command>]
usage: sudo -e [-AknS] [-r role] [-t type] [-C num] [-D directory] [-g group] [-h host] [-p prompt] [-R directory] [-T timeout] [-u user] file
            ...
root@webserver:/# sudo ufw status
ERROR: problem running iptables: iptables v1.8.7 (nf_tables): Could not fetch rule set generation id: Permission denied (you must be root)



root@webserver:/# sudo su -
root@webserver:~# ufw status
ERROR: problem running iptables: iptables v1.8.7 (nf_tables): Could not fetch rule set generation id: Permission denied (you must be root)



root@webserver:~# whoami
root
root@webserver:~# iptables -A INPUT -p tcp --destination-port 80 -j DROP
iptables v1.8.7 (nf_tables): unknown option "--destination-port"
Try `iptables -h' or 'iptables --help' for more information.
root@webserver:~# ufw
ERROR: not enough args
root@webserver:~# ufw --help

Usage: ufw COMMAND

Commands:
 enable                          enables the firewall
 disable                         disables the firewall
 default ARG                     set default policy
 logging LEVEL                   set logging to LEVEL
 allow ARGS                      add allow rule
 deny ARGS                       add deny rule
 reject ARGS                     add reject rule
 limit ARGS                      add limit rule
 delete RULE|NUM                 delete RULE
 insert NUM RULE                 insert RULE at NUM
 prepend RULE                    prepend RULE
 route RULE                      add route RULE
 route delete RULE|NUM           delete route RULE
 route insert NUM RULE           insert route RULE at NUM
 reload                          reload firewall
 reset                           reset firewall
 status                          show firewall status
 status numbered                 show firewall status as numbered list of RULES
 status verbose                  show verbose firewall status
 show ARG                        show firewall report
 version                         display version information

Application profile commands:
 app list                        list application profiles
 app info PROFILE                show information on PROFILE
 app update PROFILE              update PROFILE
 app default ARG                 set default application policy

root@webserver:~# netstat -ntulp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      1/nginx: master pro
tcp6       0      0 :::80                   :::*                    LISTEN      1/nginx: master pro
root@webserver:~# lsof -t
-bash: lsof: command not found
root@webserver:~# ufw disable
ERROR: problem running ufw-init
iptables v1.8.7 (nf_tables): Could not fetch rule set generation id: Permission denied (you must be root)

iptables v1.8.7 (nf_tables): Could not fetch rule set generation id: Permission denied (you must be root)

iptables v1.8.7 (nf_tables): Could not fetch rule set generation id: Permission denied (you must be root)


root@webserver:~# ufw status
ERROR: problem running iptables: iptables v1.8.7 (nf_tables): Could not fetch rule set generation id: Permission denied (you must be root)



root@webserver:~# sudo ufw status
ERROR: problem running iptables: iptables v1.8.7 (nf_tables): Could not fetch rule set generation id: Permission denied (you must be root)



root@webserver:~# apt-get install lsof
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
Suggested packages:
  perl
The following NEW packages will be installed:
  lsof
0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.
Need to get 319 kB of archives.
After this operation, 462 kB of additional disk space will be used.
Get:1 http://deb.debian.org/debian bullseye/main amd64 lsof amd64 4.93.2+dfsg-1.1 [319 kB]
Fetched 319 kB in 0s (838 kB/s)
debconf: delaying package configuration, since apt-utils is not installed
Selecting previously unselected package lsof.
(Reading database ... 9160 files and directories currently installed.)
Preparing to unpack .../lsof_4.93.2+dfsg-1.1_amd64.deb ...
Unpacking lsof (4.93.2+dfsg-1.1) ...
Setting up lsof (4.93.2+dfsg-1.1) ...
root@webserver:~# lsof -t
1
32
1292
1293
1294
1422
1423
root@webserver:~# lsof -t -i
1
root@webserver:~# lsof
COMMAND  PID  USER   FD      TYPE             DEVICE SIZE/OFF    NODE NAME
nginx      1  root  cwd       DIR              0,418     4096 7327164 /
nginx      1  root  rtd       DIR              0,418     4096 7327164 /
nginx      1  root  txt       REG              0,418  1378584 7326284 /usr/sbin/nginx
nginx      1  root  mem       REG              253,1          7326284 /usr/sbin/nginx (path dev=0,418)
nginx      1  root  mem       REG              253,1          7321313 /lib/x86_64-linux-gnu/libnss_files-2.31.so (path dev=0,418)
nginx      1  root  mem       REG              253,1          7321281 /lib/x86_64-linux-gnu/libc-2.31.so (path dev=0,418)
nginx      1  root  mem       REG              253,1          7321345 /lib/x86_64-linux-gnu/libz.so.1.2.11 (path dev=0,418)
nginx      1  root  mem       REG              253,1          7322012 /usr/lib/x86_64-linux-gnu/libcrypto.so.1.1 (path dev=0,418)
nginx      1  root  mem       REG              253,1          7322055 /usr/lib/x86_64-linux-gnu/libssl.so.1.1 (path dev=0,418)
nginx      1  root  mem       REG              253,1          7322047 /usr/lib/x86_64-linux-gnu/libpcre2-8.so.0.10.1 (path dev=0,418)
nginx      1  root  mem       REG              253,1          7321288 /lib/x86_64-linux-gnu/libcrypt.so.1.1.0 (path dev=0,418)
nginx      1  root  mem       REG              253,1          7321326 /lib/x86_64-linux-gnu/libpthread-2.31.so (path dev=0,418)
nginx      1  root  mem       REG              253,1          7321289 /lib/x86_64-linux-gnu/libdl-2.31.so (path dev=0,418)
nginx      1  root  mem       REG              253,1          7321269 /lib/x86_64-linux-gnu/ld-2.31.so (path dev=0,418)
nginx      1  root  DEL       REG                0,1            65218 /dev/zero
nginx      1  root    0u      CHR                1,3      0t0   64227 /dev/null
nginx      1  root    1w     FIFO               0,10      0t0   65140 pipe
nginx      1  root    2w     FIFO               0,10      0t0   65141 pipe
nginx      1  root    3w     FIFO               0,10      0t0   65140 pipe
nginx      1  root    4u     unix 0x00000000e162ba7f      0t0   65219 type=STREAM
nginx      1  root    5w     FIFO               0,10      0t0   65141 pipe
nginx      1  root    6w     FIFO               0,10      0t0   65140 pipe
nginx      1  root    7u     IPv4              65216      0t0     TCP *:http (LISTEN)
nginx      1  root    8u     IPv6              65217      0t0     TCP *:http (LISTEN)
nginx      1  root    9u     unix 0x00000000cdde85ba      0t0   65220 type=STREAM
nginx      1  root   10u     unix 0x00000000c5e3304e      0t0   65221 type=STREAM
nginx      1  root   11u     unix 0x00000000da7cdd2e      0t0   65222 type=STREAM
nginx     30 nginx  cwd   unknown                                     /proc/30/cwd (readlink: Permission denied)
nginx     30 nginx  rtd   unknown                                     /proc/30/root (readlink: Permission denied)
nginx     30 nginx  txt   unknown                                     /proc/30/exe (readlink: Permission denied)
nginx     30 nginx    0u  unknown                                     /proc/30/fd/0 (readlink: Permission denied)
nginx     30 nginx    1u  unknown                                     /proc/30/fd/1 (readlink: Permission denied)
nginx     30 nginx    2u  unknown                                     /proc/30/fd/2 (readlink: Permission denied)
nginx     30 nginx    3u  unknown                                     /proc/30/fd/3 (readlink: Permission denied)
nginx     30 nginx    4u  unknown                                     /proc/30/fd/4 (readlink: Permission denied)
nginx     30 nginx    5u  unknown                                     /proc/30/fd/5 (readlink: Permission denied)
nginx     30 nginx    6u  unknown                                     /proc/30/fd/6 (readlink: Permission denied)
nginx     30 nginx    7u  unknown                                     /proc/30/fd/7 (readlink: Permission denied)
nginx     30 nginx    8u  unknown                                     /proc/30/fd/8 (readlink: Permission denied)
nginx     30 nginx    9u  unknown                                     /proc/30/fd/9 (readlink: Permission denied)
nginx     30 nginx   10u  unknown                                     /proc/30/fd/10 (readlink: Permission denied)
nginx     30 nginx   11u  unknown                                     /proc/30/fd/11 (readlink: Permission denied)
nginx     30 nginx   12u  unknown                                     /proc/30/fd/12 (readlink: Permission denied)
nginx     31 nginx  cwd   unknown                                     /proc/31/cwd (readlink: Permission denied)
nginx     31 nginx  rtd   unknown                                     /proc/31/root (readlink: Permission denied)
nginx     31 nginx  txt   unknown                                     /proc/31/exe (readlink: Permission denied)
nginx     31 nginx    0u  unknown                                     /proc/31/fd/0 (readlink: Permission denied)
nginx     31 nginx    1u  unknown                                     /proc/31/fd/1 (readlink: Permission denied)
nginx     31 nginx    2u  unknown                                     /proc/31/fd/2 (readlink: Permission denied)
nginx     31 nginx    3u  unknown                                     /proc/31/fd/3 (readlink: Permission denied)
nginx     31 nginx    4u  unknown                                     /proc/31/fd/4 (readlink: Permission denied)
nginx     31 nginx    5u  unknown                                     /proc/31/fd/5 (readlink: Permission denied)
nginx     31 nginx    6u  unknown                                     /proc/31/fd/6 (readlink: Permission denied)
nginx     31 nginx    7u  unknown                                     /proc/31/fd/7 (readlink: Permission denied)
nginx     31 nginx    8u  unknown                                     /proc/31/fd/8 (readlink: Permission denied)
nginx     31 nginx   11u  unknown                                     /proc/31/fd/11 (readlink: Permission denied)
nginx     31 nginx   12u  unknown                                     /proc/31/fd/12 (readlink: Permission denied)
nginx     31 nginx   13u  unknown                                     /proc/31/fd/13 (readlink: Permission denied)
nginx     31 nginx   14u  unknown                                     /proc/31/fd/14 (readlink: Permission denied)
bash      32  root  cwd       DIR              0,418     4096 7327164 /
bash      32  root  rtd       DIR              0,418     4096 7327164 /
bash      32  root  txt       REG              0,418  1234376 7320938 /bin/bash
bash      32  root  mem       REG              253,1          7320938 /bin/bash (path dev=0,418)
bash      32  root  mem       REG              253,1          7321313 /lib/x86_64-linux-gnu/libnss_files-2.31.so (path dev=0,418)
bash      32  root  mem       REG              253,1          7321281 /lib/x86_64-linux-gnu/libc-2.31.so (path dev=0,418)
bash      32  root  mem       REG              253,1          7321289 /lib/x86_64-linux-gnu/libdl-2.31.so (path dev=0,418)
bash      32  root  mem       REG              253,1          7321339 /lib/x86_64-linux-gnu/libtinfo.so.6.2 (path dev=0,418)
bash      32  root  mem       REG              253,1          7321269 /lib/x86_64-linux-gnu/ld-2.31.so (path dev=0,418)
bash      32  root    0u      CHR              136,0      0t0       3 /dev/pts/0
bash      32  root    1u      CHR              136,0      0t0       3 /dev/pts/0
bash      32  root    2u      CHR              136,0      0t0       3 /dev/pts/0
bash      32  root  255u      CHR              136,0      0t0       3 /dev/pts/0
sudo    1292  root  cwd       DIR              0,418     4096 7327164 /
sudo    1292  root  rtd       DIR              0,418     4096 7327164 /
sudo    1292  root  txt       REG              0,418   182600 7328387 /usr/bin/sudo
sudo    1292  root  mem       REG              253,1          7328387 /usr/bin/sudo (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7321370 /lib/x86_64-linux-gnu/security/pam_permit.so (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7321349 /lib/x86_64-linux-gnu/security/pam_deny.so (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7321328 /lib/x86_64-linux-gnu/libresolv-2.31.so (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7321299 /lib/x86_64-linux-gnu/libkeyutils.so.1.9 (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7322035 /usr/lib/x86_64-linux-gnu/libkrb5support.so.0.1 (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7321286 /lib/x86_64-linux-gnu/libcom_err.so.2.1 (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7322031 /usr/lib/x86_64-linux-gnu/libk5crypto.so.3.1 (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7322033 /usr/lib/x86_64-linux-gnu/libkrb5.so.3.3 (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7322025 /usr/lib/x86_64-linux-gnu/libgssapi_krb5.so.2.2 (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7321341 /lib/x86_64-linux-gnu/libtirpc.so.3.0.0 (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7322043 /usr/lib/x86_64-linux-gnu/libnsl.so.2.0.1 (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7321288 /lib/x86_64-linux-gnu/libcrypt.so.1.1.0 (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7321385 /lib/x86_64-linux-gnu/security/pam_unix.so (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7321345 /lib/x86_64-linux-gnu/libz.so.1.2.11 (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7321318 /lib/x86_64-linux-gnu/libpam.so.0.85.1 (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7328614 /usr/lib/sudo/sudoers.so (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7321313 /lib/x86_64-linux-gnu/libnss_files-2.31.so (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7321289 /lib/x86_64-linux-gnu/libdl-2.31.so (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7322047 /usr/lib/x86_64-linux-gnu/libpcre2-8.so.0.10.1 (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7321284 /lib/x86_64-linux-gnu/libcap-ng.so.0.0.0 (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7321281 /lib/x86_64-linux-gnu/libc-2.31.so (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7321326 /lib/x86_64-linux-gnu/libpthread-2.31.so (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7328429 /usr/lib/sudo/libsudo_util.so.0.0.0 (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7321342 /lib/x86_64-linux-gnu/libutil-2.31.so (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7321332 /lib/x86_64-linux-gnu/libselinux.so.1 (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7321277 /lib/x86_64-linux-gnu/libaudit.so.1.0.0 (path dev=0,418)
sudo    1292  root  mem       REG              253,1          7321269 /lib/x86_64-linux-gnu/ld-2.31.so (path dev=0,418)
sudo    1292  root    0u      CHR              136,0      0t0       3 /dev/pts/0
sudo    1292  root    1u      CHR              136,0      0t0       3 /dev/pts/0
sudo    1292  root    2u      CHR              136,0      0t0       3 /dev/pts/0
sudo    1292  root    3r     FIFO               0,10      0t0   80649 pipe
sudo    1292  root    4w     FIFO               0,10      0t0   80649 pipe
sudo    1292  root    5u  netlink                         0t0   80655 AUDIT
su      1293  root  cwd       DIR              0,418     4096 7327164 /
su      1293  root  rtd       DIR              0,418     4096 7327164 /
su      1293  root  txt       REG              0,418    71912 7320990 /bin/su
su      1293  root  mem       REG              253,1          7320990 /bin/su (path dev=0,418)
su      1293  root  mem       REG              253,1          7321361 /lib/x86_64-linux-gnu/security/pam_limits.so (path dev=0,418)
su      1293  root  mem       REG              253,1          7321365 /lib/x86_64-linux-gnu/security/pam_mail.so (path dev=0,418)
su      1293  root  mem       REG              253,1          7321351 /lib/x86_64-linux-gnu/security/pam_env.so (path dev=0,418)
su      1293  root  mem       REG              253,1          7321359 /lib/x86_64-linux-gnu/security/pam_keyinit.so (path dev=0,418)
su      1293  root  mem       REG              253,1          7321370 /lib/x86_64-linux-gnu/security/pam_permit.so (path dev=0,418)
su      1293  root  mem       REG              253,1          7321349 /lib/x86_64-linux-gnu/security/pam_deny.so (path dev=0,418)
su      1293  root  mem       REG              253,1          7321328 /lib/x86_64-linux-gnu/libresolv-2.31.so (path dev=0,418)
su      1293  root  mem       REG              253,1          7321299 /lib/x86_64-linux-gnu/libkeyutils.so.1.9 (path dev=0,418)
su      1293  root  mem       REG              253,1          7322035 /usr/lib/x86_64-linux-gnu/libkrb5support.so.0.1 (path dev=0,418)
su      1293  root  mem       REG              253,1          7321286 /lib/x86_64-linux-gnu/libcom_err.so.2.1 (path dev=0,418)
su      1293  root  mem       REG              253,1          7322031 /usr/lib/x86_64-linux-gnu/libk5crypto.so.3.1 (path dev=0,418)
su      1293  root  mem       REG              253,1          7322033 /usr/lib/x86_64-linux-gnu/libkrb5.so.3.3 (path dev=0,418)
su      1293  root  mem       REG              253,1          7322025 /usr/lib/x86_64-linux-gnu/libgssapi_krb5.so.2.2 (path dev=0,418)
su      1293  root  mem       REG              253,1          7321341 /lib/x86_64-linux-gnu/libtirpc.so.3.0.0 (path dev=0,418)
su      1293  root  mem       REG              253,1          7322043 /usr/lib/x86_64-linux-gnu/libnsl.so.2.0.1 (path dev=0,418)
su      1293  root  mem       REG              253,1          7321288 /lib/x86_64-linux-gnu/libcrypt.so.1.1.0 (path dev=0,418)
su      1293  root  mem       REG              253,1          7321385 /lib/x86_64-linux-gnu/security/pam_unix.so (path dev=0,418)
su      1293  root  mem       REG              253,1          7322047 /usr/lib/x86_64-linux-gnu/libpcre2-8.so.0.10.1 (path dev=0,418)
su      1293  root  mem       REG              253,1          7321332 /lib/x86_64-linux-gnu/libselinux.so.1 (path dev=0,418)
su      1293  root  mem       REG              253,1          7321373 /lib/x86_64-linux-gnu/security/pam_rootok.so (path dev=0,418)
su      1293  root  mem       REG              253,1          7321313 /lib/x86_64-linux-gnu/libnss_files-2.31.so (path dev=0,418)
su      1293  root  mem       REG              253,1          7321326 /lib/x86_64-linux-gnu/libpthread-2.31.so (path dev=0,418)
su      1293  root  mem       REG              253,1          7321284 /lib/x86_64-linux-gnu/libcap-ng.so.0.0.0 (path dev=0,418)
su      1293  root  mem       REG              253,1          7321289 /lib/x86_64-linux-gnu/libdl-2.31.so (path dev=0,418)
su      1293  root  mem       REG              253,1          7321277 /lib/x86_64-linux-gnu/libaudit.so.1.0.0 (path dev=0,418)
su      1293  root  mem       REG              253,1          7321281 /lib/x86_64-linux-gnu/libc-2.31.so (path dev=0,418)
su      1293  root  mem       REG              253,1          7321342 /lib/x86_64-linux-gnu/libutil-2.31.so (path dev=0,418)
su      1293  root  mem       REG              253,1          7321320 /lib/x86_64-linux-gnu/libpam_misc.so.0.82.1 (path dev=0,418)
su      1293  root  mem       REG              253,1          7321318 /lib/x86_64-linux-gnu/libpam.so.0.85.1 (path dev=0,418)
su      1293  root  mem       REG              253,1          7321269 /lib/x86_64-linux-gnu/ld-2.31.so (path dev=0,418)
su      1293  root    0u      CHR              136,0      0t0       3 /dev/pts/0
su      1293  root    1u      CHR              136,0      0t0       3 /dev/pts/0
su      1293  root    2u      CHR              136,0      0t0       3 /dev/pts/0
bash    1294  root  cwd       DIR              0,418     4096 7321397 /root
bash    1294  root  rtd       DIR              0,418     4096 7327164 /
bash    1294  root  txt       REG              0,418  1234376 7320938 /bin/bash
bash    1294  root  mem       REG              253,1          7320938 /bin/bash (path dev=0,418)
bash    1294  root  mem       REG              253,1          7321313 /lib/x86_64-linux-gnu/libnss_files-2.31.so (path dev=0,418)
bash    1294  root  mem       REG              253,1          7321281 /lib/x86_64-linux-gnu/libc-2.31.so (path dev=0,418)
bash    1294  root  mem       REG              253,1          7321289 /lib/x86_64-linux-gnu/libdl-2.31.so (path dev=0,418)
bash    1294  root  mem       REG              253,1          7321339 /lib/x86_64-linux-gnu/libtinfo.so.6.2 (path dev=0,418)
bash    1294  root  mem       REG              253,1          7321269 /lib/x86_64-linux-gnu/ld-2.31.so (path dev=0,418)
bash    1294  root    0u      CHR              136,0      0t0       3 /dev/pts/0
bash    1294  root    1u      CHR              136,0      0t0       3 /dev/pts/0
bash    1294  root    2u      CHR              136,0      0t0       3 /dev/pts/0
bash    1294  root  255u      CHR              136,0      0t0       3 /dev/pts/0
lsof    1425  root  cwd       DIR              0,418     4096 7321397 /root
lsof    1425  root  rtd       DIR              0,418     4096 7327164 /
lsof    1425  root  txt       REG              0,418   171488 7328764 /usr/bin/lsof
lsof    1425  root  mem       REG              253,1          7328764 /usr/bin/lsof (path dev=0,418)
lsof    1425  root  mem       REG              253,1          7321326 /lib/x86_64-linux-gnu/libpthread-2.31.so (path dev=0,418)
lsof    1425  root  mem       REG              253,1          7321289 /lib/x86_64-linux-gnu/libdl-2.31.so (path dev=0,418)
lsof    1425  root  mem       REG              253,1          7322047 /usr/lib/x86_64-linux-gnu/libpcre2-8.so.0.10.1 (path dev=0,418)
lsof    1425  root  mem       REG              253,1          7321281 /lib/x86_64-linux-gnu/libc-2.31.so (path dev=0,418)
lsof    1425  root  mem       REG              253,1          7321332 /lib/x86_64-linux-gnu/libselinux.so.1 (path dev=0,418)
lsof    1425  root  mem       REG              253,1          7321269 /lib/x86_64-linux-gnu/ld-2.31.so (path dev=0,418)
lsof    1425  root    0u      CHR              136,0      0t0       3 /dev/pts/0
lsof    1425  root    1u      CHR              136,0      0t0       3 /dev/pts/0
lsof    1425  root    2u      CHR              136,0      0t0       3 /dev/pts/0
lsof    1425  root    3r      DIR              0,419        0       1 /proc
lsof    1425  root    4r      DIR              0,419        0   85725 /proc/1425/fd
lsof    1425  root    5w     FIFO               0,10      0t0   85729 pipe
lsof    1425  root    6r     FIFO               0,10      0t0   85730 pipe
lsof    1426  root  cwd       DIR              0,418     4096 7321397 /root
lsof    1426  root  rtd       DIR              0,418     4096 7327164 /
lsof    1426  root  txt       REG              0,418   171488 7328764 /usr/bin/lsof
lsof    1426  root  mem       REG              253,1          7328764 /usr/bin/lsof (path dev=0,418)
lsof    1426  root  mem       REG              253,1          7321326 /lib/x86_64-linux-gnu/libpthread-2.31.so (path dev=0,418)
lsof    1426  root  mem       REG              253,1          7321289 /lib/x86_64-linux-gnu/libdl-2.31.so (path dev=0,418)
lsof    1426  root  mem       REG              253,1          7322047 /usr/lib/x86_64-linux-gnu/libpcre2-8.so.0.10.1 (path dev=0,418)
lsof    1426  root  mem       REG              253,1          7321281 /lib/x86_64-linux-gnu/libc-2.31.so (path dev=0,418)
lsof    1426  root  mem       REG              253,1          7321332 /lib/x86_64-linux-gnu/libselinux.so.1 (path dev=0,418)
lsof    1426  root  mem       REG              253,1          7321269 /lib/x86_64-linux-gnu/ld-2.31.so (path dev=0,418)
lsof    1426  root    4r     FIFO               0,10      0t0   85729 pipe
lsof    1426  root    7w     FIFO               0,10      0t0   85730 pipe
root@webserver:~# ^C
root@webserver:~# ufw denay --help
ERROR: Invalid syntax

Usage: ufw COMMAND

Commands:
 enable                          enables the firewall
 disable                         disables the firewall
 default ARG                     set default policy
 logging LEVEL                   set logging to LEVEL
 allow ARGS                      add allow rule
 deny ARGS                       add deny rule
 reject ARGS                     add reject rule
 limit ARGS                      add limit rule
 delete RULE|NUM                 delete RULE
 insert NUM RULE                 insert RULE at NUM
 prepend RULE                    prepend RULE
 route RULE                      add route RULE
 route delete RULE|NUM           delete route RULE
 route insert NUM RULE           insert route RULE at NUM
 reload                          reload firewall
 reset                           reset firewall
 status                          show firewall status
 status numbered                 show firewall status as numbered list of RULES
 status verbose                  show verbose firewall status
 show ARG                        show firewall report
 version                         display version information

Application profile commands:
 app list                        list application profiles
 app info PROFILE                show information on PROFILE
 app update PROFILE              update PROFILE
 app default ARG                 set default application policy

root@webserver:~# ufw deny --help
ERROR: Bad port
root@webserver:~# ufw deny 80
WARN: initcaps
[Errno 2] iptables v1.8.7 (nf_tables): Could not fetch rule set generation id: Permission denied (you must be root)


Rules updated
Rules updated (v6)
root@webserver:~# netstat -ntulp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      1/nginx: master pro
tcp6       0      0 :::80                   :::*                    LISTEN      1/nginx: master pro
root@webserver:~# .
-bash: .: filename argument required
.: usage: . filename [arguments]
root@webserver:~# /curl
-bash: /curl: No such file or directory
root@webserver:~#
root@webserver:~# set -o vi
root@webserver:~# #/curl
root@webserver:~# curl localhost:80
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
root@webserver:~# ufw status
ERROR: problem running iptables: iptables v1.8.7 (nf_tables): Could not fetch rule set generation id: Permission denied (you must be root)



root@webserver:~# ufw show
ERROR: Invalid syntax

Usage: ufw COMMAND

Commands:
 enable                          enables the firewall
 disable                         disables the firewall
 default ARG                     set default policy
 logging LEVEL                   set logging to LEVEL
 allow ARGS                      add allow rule
 deny ARGS                       add deny rule
 reject ARGS                     add reject rule
 limit ARGS                      add limit rule
 delete RULE|NUM                 delete RULE
 insert NUM RULE                 insert RULE at NUM
 prepend RULE                    prepend RULE
 route RULE                      add route RULE
 route delete RULE|NUM           delete route RULE
 route insert NUM RULE           insert route RULE at NUM
 reload                          reload firewall
 reset                           reset firewall
 status                          show firewall status
 status numbered                 show firewall status as numbered list of RULES
 status verbose                  show verbose firewall status
 show ARG                        show firewall report
 version                         display version information

Application profile commands:
 app list                        list application profiles
 app info PROFILE                show information on PROFILE
 app update PROFILE              update PROFILE
 app default ARG                 set default application policy

root@webserver:~# ufw show 80
ERROR: Invalid syntax

Usage: ufw COMMAND

Commands:
 enable                          enables the firewall
 disable                         disables the firewall
 default ARG                     set default policy
 logging LEVEL                   set logging to LEVEL
 allow ARGS                      add allow rule
 deny ARGS                       add deny rule
 reject ARGS                     add reject rule
 limit ARGS                      add limit rule
 delete RULE|NUM                 delete RULE
 insert NUM RULE                 insert RULE at NUM
 prepend RULE                    prepend RULE
 route RULE                      add route RULE
 route delete RULE|NUM           delete route RULE
 route insert NUM RULE           insert route RULE at NUM
 reload                          reload firewall
 reset                           reset firewall
 status                          show firewall status
 status numbered                 show firewall status as numbered list of RULES
 status verbose                  show verbose firewall status
 show ARG                        show firewall report
 version                         display version information

Application profile commands:
 app list                        list application profiles
 app info PROFILE                show information on PROFILE
 app update PROFILE              update PROFILE
 app default ARG                 set default application policy

root@webserver:~# ufw deny to 80
ERROR: Bad destination address
root@webserver:~# ufw deny from 80
ERROR: Bad source address
root@webserver:~# ufw deny 80
WARN: initcaps
[Errno 2] iptables v1.8.7 (nf_tables): Could not fetch rule set generation id: Permission denied (you must be root)


Skipping adding existing rule
Skipping adding existing rule (v6)
root@webserver:~# sudo ufw deny 80
WARN: initcaps
[Errno 2] iptables v1.8.7 (nf_tables): Could not fetch rule set generation id: Permission denied (you must be root)


Skipping adding existing rule
Skipping adding existing rule (v6)
root@webserver:~# curl localhost:80
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
root@webserver:~# iptables -P INPUT DROP
iptables v1.8.7 (nf_tables): Could not fetch rule set generation id: Permission denied (you must be root)

root@webserver:~# exit
logout
root@webserver:/# exit
exit
command terminated with exit code 4

 2022-01-30 08:35:06 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/ctemp
○ → cd ~/vagrant_dev/

 2022-01-30 08:35:12 ⌚  azhekhan-mac in ~/vagrant_dev
○ → ll
total 0
drwxr-xr-x   9 azhekhan  staff  288 Jul  4  2019 devops-box
drwxr-xr-x   5 azhekhan  staff  160 Jan 19  2020 vagrant_test
drwxr-xr-x   4 azhekhan  staff  128 Jan 19  2020 vagrant_pluralsight
drwxr-xr-x   4 azhekhan  staff  128 Jan 20  2020 vagrant_centos7
drwxr-xr-x  12 azhekhan  staff  384 Jan 30 07:29 v_ansible_centos7

 2022-01-30 08:35:13 ⌚  azhekhan-mac in ~/vagrant_dev
○ → cd v_ansible_centos7/

 2022-01-30 08:35:16 ⌚  azhekhan-mac in ~/vagrant_dev/v_ansible_centos7
○ → sudo ssh -gL 80:127.0.0.1:8080 localhost
Password:
The authenticity of host 'localhost (127.0.0.1)' can't be established.
ECDSA key fingerprint is SHA256:IVsyp2QbZyl7Ls7CSF8H8xvSpcfDzNbF7vtjR5eQ38U.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.
Password:
Password:
Password:
root@localhost's password:
Permission denied, please try again.
root@localhost's password:
Permission denied, please try again.
root@localhost's password:
Received disconnect from 127.0.0.1 port 22:2: Too many authentication failures
Disconnected from 127.0.0.1 port 22

 2022-01-30 08:36:48 ⌚  azhekhan-mac in ~/vagrant_dev/v_ansible_centos7
○ →

 2022-01-30 09:18:01 ⌚  azhekhan-mac in ~/vagrant_dev/v_ansible_centos7
○ → lsof -P
COMMAND     PID     USER   FD      TYPE             DEVICE    SIZE/OFF                NODE NAME
loginwind   214 azhekhan  cwd       DIR                1,4         704                   2 /
loginwind   214 azhekhan  txt       REG                1,4     1265792 1152921500312469677 /System/Library/CoreServices/loginwindow.app/Contents/MacOS/loginwindow
loginwind   214 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
loginwind   214 azhekhan  txt       REG                1,4      126864 1152921500311925455 /System/Library/LoginPlugins/DisplayServices.loginPlugin/Contents/MacOS/DisplayServices
loginwind   214 azhekhan  txt       REG                1,4    28620288 1152921500311902385 /usr/share/icu/icudt64l.dat
loginwind   214 azhekhan  txt       REG                1,4      205992            45660617 /Library/Application Support/CrashReporter/SubmitDiagInfo.domains
loginwind   214 azhekhan  txt       REG                1,4       70608 1152921500312468425 /System/Library/LoginPlugins/FSDisconnect.loginPlugin/Contents/MacOS/FSDisconnect
loginwind   214 azhekhan  txt       REG                1,4      242128            56647497 /private/var/db/timezone/tz/2021a.3.0/icutz/icutz44l.dat
loginwind   214 azhekhan  txt       REG                1,4     3757752 1152921500311959024 /System/Library/CoreServices/SystemAppearance.bundle/Contents/Resources/SystemAppearance.car
loginwind   214 azhekhan  txt       REG                1,4      416504 1152921500311959012 /System/Library/CoreServices/SystemAppearance.bundle/Contents/Resources/VibrantLightAppearance.car
loginwind   214 azhekhan  txt       REG                1,4       43777            65284106 /private/var/db/analyticsd/events.whitelist
loginwind   214 azhekhan  txt       REG                1,4      292400 1152921500311925625 /System/Library/LoginPlugins/BezelServices.loginPlugin/Contents/MacOS/BezelServices
loginwind   214 azhekhan  txt       REG                1,4      149968 1152921500311885486 /usr/lib/libobjc-trampolines.dylib
loginwind   214 azhekhan  txt       REG                1,4      127392 1152921500312469875 /System/Library/CoreServices/ManagedClient.app/Contents/PlugIns/MCXToolsInterface.bundle/Contents/MacOS/MCXToolsInterface
loginwind   214 azhekhan  txt       REG                1,4       52656 1152921500312492471 /System/Library/Extensions/AppleHDA.kext/Contents/PlugIns/AppleHDAHALPlugIn.bundle/Contents/MacOS/AppleHDAHALPlugIn
loginwind   214 azhekhan  txt       REG                1,4       21756 1152921500312276535 /System/Library/PrivateFrameworks/BatteryUIKit.framework/Versions/A/Resources/BatteryChargedAndPlugged.pdf
loginwind   214 azhekhan  txt       REG                1,4       10163 1152921500312276565 /System/Library/PrivateFrameworks/BatteryUIKit.framework/Versions/A/Resources/BatteryLevelCapR-L.pdf
loginwind   214 azhekhan  txt       REG                1,4       10164 1152921500312276546 /System/Library/PrivateFrameworks/BatteryUIKit.framework/Versions/A/Resources/BatteryLevelCapR-R.pdf
loginwind   214 azhekhan  txt       REG                1,4        9802 1152921500312276566 /System/Library/PrivateFrameworks/BatteryUIKit.framework/Versions/A/Resources/BatteryLevelCapR-M.pdf
loginwind   214 azhekhan  txt       REG                1,4       91936 1152921500311959301 /System/Library/CoreServices/TextInputMenuCore.bundle/Contents/MacOS/TextInputMenuCore
loginwind   214 azhekhan  txt       REG                1,4     6205440            65300106 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/0/com.apple.LaunchServices.dv/com.apple.LaunchServices-1082-v2.csstore
loginwind   214 azhekhan  txt       REG                1,4     3546456 1152921500311959020 /System/Library/CoreServices/SystemAppearance.bundle/Contents/Resources/DarkAquaAppearance.car
loginwind   214 azhekhan  txt       REG                1,4     2413992 1152921500311959010 /System/Library/CoreServices/SystemAppearance.bundle/Contents/Resources/DarkAppearance.car
loginwind   214 azhekhan  txt       REG                1,4     2169148 1152921500312082703 /System/Library/Fonts/SFNS.ttf
loginwind   214 azhekhan  txt       REG                1,4     2353284 1152921500312082649 /System/Library/Fonts/Helvetica.ttc
loginwind   214 azhekhan  txt       REG                1,4     1354416 1152921500312476069 /System/Library/Frameworks/AppKit.framework/Versions/C/Resources/Assets.car
loginwind   214 azhekhan  txt       REG                1,4      873240 1152921500312197090 /System/Library/PrivateFrameworks/CoreWLANKit.framework/Versions/A/Resources/Assets.car
loginwind   214 azhekhan  txt       REG                1,4      136100            62433388 /System/Library/Caches/com.apple.IntlDataCache.le.kbdx
loginwind   214 azhekhan  txt       REG                1,4     1074864 1152921500312293677 /System/Library/Keyboard Layouts/AppleKeyboardLayouts.bundle/Contents/Resources/AppleKeyboardLayouts-L.dat
loginwind   214 azhekhan  txt       REG                1,4     3406912 1152921500311959022 /System/Library/CoreServices/SystemAppearance.bundle/Contents/Resources/Assets.car
loginwind   214 azhekhan  txt       REG                1,4       16371 1152921500312276588 /System/Library/PrivateFrameworks/BatteryUIKit.framework/Versions/A/Resources/BatteryCharging.pdf
loginwind   214 azhekhan  txt       REG                1,4      439752 1152921500312180219 /System/Library/PrivateFrameworks/CoreNLP.framework/Versions/A/Resources/tokruleLE.data
loginwind   214 azhekhan  txt       REG                1,4      247428 1152921500312277146 /System/Library/PrivateFrameworks/DataDetectorsCore.framework/Versions/A/Resources/com.apple.datadetectorscore.cache.urlifier.system
loginwind   214 azhekhan  txt       REG                1,4       32768            63984670 /private/var/db/mds/messages/se_SecurityMessages
loginwind   214 azhekhan  txt       REG                1,4       13264 1152921500312276589 /System/Library/PrivateFrameworks/BatteryUIKit.framework/Versions/A/Resources/BatteryEmpty.pdf
loginwind   214 azhekhan  txt       REG                1,4      170592 1152921500312476348 /System/Library/Frameworks/Security.framework/Versions/A/PlugIns/csparser.bundle/Contents/MacOS/csparser
loginwind   214 azhekhan  txt       REG                1,4       42208 1152921500311903538 /System/Library/KerberosPlugins/KerberosFrameworkPlugins/AppSSOLocatePlugin_macOS.bundle/Contents/MacOS/AppSSOLocatePlugin_macOS
loginwind   214 azhekhan  txt       REG                1,4       46832 1152921500311903520 /System/Library/KerberosPlugins/KerberosFrameworkPlugins/AppSSOConfigPlugin_macOS.bundle/Contents/MacOS/AppSSOConfigPlugin_macOS
loginwind   214 azhekhan  txt       REG                1,4       38144 1152921500311903547 /System/Library/KerberosPlugins/KerberosFrameworkPlugins/heimdalodpac.bundle/Contents/MacOS/heimdalodpac
loginwind   214 azhekhan  txt       REG                1,4       38256 1152921500312467933 /System/Library/KerberosPlugins/KerberosFrameworkPlugins/Reachability.bundle/Contents/MacOS/Reachability
loginwind   214 azhekhan  txt       REG                1,4       65536            47540220 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.loginwindow/com.apple.metal/3902/libraries.data
loginwind   214 azhekhan  txt       REG                1,4       65536              706530 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.loginwindow/com.apple.metal/Intel(R) Iris(TM) Plus Graphics 640/functions.data
loginwind   214 azhekhan  txt       REG                1,4       38336 1152921500312467924 /System/Library/KerberosPlugins/KerberosFrameworkPlugins/SCKerberosConfig.bundle/Contents/MacOS/SCKerberosConfig
loginwind   214 azhekhan  txt       REG                1,4    27971344 1152921500311895959 /usr/share/langid/langid.inv
loginwind   214 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
loginwind   214 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
loginwind   214 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
loginwind   214 azhekhan    2u      CHR                3,2     0t17415                 313 /dev/null
loginwind   214 azhekhan    3r      REG                1,4      205992            45660617 /Library/Application Support/CrashReporter/SubmitDiagInfo.domains
loginwind   214 azhekhan    4r      REG                1,4          77            62430226 /private/etc/security/audit_user
loginwind   214 azhekhan    5r      REG                1,4         652            62430225 /private/etc/security/audit_class
loginwind   214 azhekhan    6r      REG                1,4         358            62430227 /private/etc/security/audit_control
loginwind   214 azhekhan    7u     IPv4 0x3fbd25a258948831         0t0                 UDP *:*
loginwind   214 azhekhan    8r      REG                1,4      111033 1152921500312111011 /System/Library/Frameworks/CoreImage.framework/Versions/A/Resources/ci_stdlib.metallib
loginwind   214 azhekhan    9r      REG                1,4      308316 1152921500312110755 /System/Library/Frameworks/CoreImage.framework/Versions/A/Resources/ci_filters.metallib
loginwind   214 azhekhan   10u      REG                1,4        1536            47540219 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.loginwindow/com.apple.metal/3902/libraries.maps
loginwind   214 azhekhan   11u      REG                1,4       65536            47540220 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.loginwindow/com.apple.metal/3902/libraries.data
loginwind   214 azhekhan   12u      REG                1,4        1024              706529 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.loginwindow/com.apple.metal/Intel(R) Iris(TM) Plus Graphics 640/functions.maps
loginwind   214 azhekhan   13u      REG                1,4       65536              706530 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.loginwindow/com.apple.metal/Intel(R) Iris(TM) Plus Graphics 640/functions.data
coreauthd   529 azhekhan  cwd       DIR                1,4         704                   2 /
coreauthd   529 azhekhan  txt       REG                1,4      264112 1152921500312145606 /System/Library/Frameworks/LocalAuthentication.framework/Support/coreauthd
coreauthd   529 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
coreauthd   529 azhekhan  txt       REG                1,4      180224 1152921500312145603 /System/Library/Frameworks/LocalAuthentication.framework/Support/ModulePlugins/ModuleACM.bundle/Contents/MacOS/ModuleACM
coreauthd   529 azhekhan  txt       REG                1,4      115024 1152921500312145906 /System/Library/Frameworks/LocalAuthentication.framework/Support/MechanismPlugins/MechPasscode.bundle/Contents/MacOS/MechPasscode
coreauthd   529 azhekhan  txt       REG                1,4       52064 1152921500312145933 /System/Library/Frameworks/LocalAuthentication.framework/Support/MechanismPlugins/HIDE_MechWatch.bundle/Contents/MacOS/HIDE_MechWatch
coreauthd   529 azhekhan  txt       REG                1,4      170592 1152921500312476348 /System/Library/Frameworks/Security.framework/Versions/A/PlugIns/csparser.bundle/Contents/MacOS/csparser
coreauthd   529 azhekhan  txt       REG                1,4       32768            63985498 /private/var/db/mds/messages/501/se_SecurityMessages
coreauthd   529 azhekhan  txt       REG                1,4    28620288 1152921500311902385 /usr/share/icu/icudt64l.dat
coreauthd   529 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
coreauthd   529 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
coreauthd   529 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
coreauthd   529 azhekhan    2u      CHR                3,2         0t0                 313 /dev/null
cfprefsd    530 azhekhan  cwd       DIR                1,4         704                   2 /
cfprefsd    530 azhekhan  txt       REG                1,4       68688 1152921500312467335 /usr/sbin/cfprefsd
cfprefsd    530 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
cfprefsd    530 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
cfprefsd    530 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
cfprefsd    530 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
cfprefsd    530 azhekhan    2u      CHR                3,2         0t0                 313 /dev/null
UserEvent   616 azhekhan  cwd       DIR                1,4         704                   2 /
UserEvent   616 azhekhan  txt       REG                1,4       53264 1152921500312466743 /usr/libexec/UserEventAgent
UserEvent   616 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
UserEvent   616 azhekhan  txt       REG                1,4      170592 1152921500312476348 /System/Library/Frameworks/Security.framework/Versions/A/PlugIns/csparser.bundle/Contents/MacOS/csparser
UserEvent   616 azhekhan  txt       REG                1,4       32768            63950731 /private/var/db/mds/messages/501/se_SecurityMessages
UserEvent   616 azhekhan  txt       REG                1,4       37168 1152921500311903700 /System/Library/UserEventPlugins/AppleHIDMouseAgent.plugin/Contents/MacOS/AppleHIDMouseAgent
UserEvent   616 azhekhan  txt       REG                1,4       70704 1152921500312468060 /System/Library/UserEventPlugins/BluetoothUserAgent-Plugin.plugin/Contents/MacOS/BluetoothUserAgent-Plugin
UserEvent   616 azhekhan  txt       REG                1,4    28620288 1152921500311902385 /usr/share/icu/icudt64l.dat
UserEvent   616 azhekhan  txt       REG                1,4       40272 1152921500311903855 /System/Library/UserEventPlugins/BonjourEvents.plugin/Contents/MacOS/BonjourEvents
UserEvent   616 azhekhan  txt       REG                1,4       45936 1152921500311903817 /System/Library/UserEventPlugins/CaptiveUserAgent.plugin/Contents/MacOS/CaptiveUserAgent
UserEvent   616 azhekhan  txt       REG                1,4       48640 1152921500311903944 /System/Library/UserEventPlugins/EAPOLMonitor.plugin/Contents/MacOS/EAPOLMonitor
UserEvent   616 azhekhan  txt       REG                1,4       30544 1152921500312468134 /System/Library/UserEventPlugins/GSSNotificationForwarder.plugin/Contents/MacOS/GSSNotificationForwarder
UserEvent   616 azhekhan  txt       REG                1,4       39440 1152921500311904069 /System/Library/UserEventPlugins/PrinterMonitor.plugin/Contents/MacOS/PrinterMonitor
UserEvent   616 azhekhan  txt       REG                1,4       49264 1152921500311904269 /System/Library/UserEventPlugins/LocationMenu.plugin/Contents/MacOS/LocationMenu
UserEvent   616 azhekhan  txt       REG                1,4       49040 1152921500312468070 /System/Library/UserEventPlugins/SCMonitor.plugin/Contents/MacOS/SCMonitor
UserEvent   616 azhekhan  txt       REG                1,4       28208 1152921500312468097 /System/Library/UserEventPlugins/SystemPolicyEvents.plugin/Contents/MacOS/SystemPolicyEvents
UserEvent   616 azhekhan  txt       REG                1,4       34112 1152921500311903691 /System/Library/UserEventPlugins/com.apple.TimeMachine.plugin/Contents/MacOS/com.apple.TimeMachine
UserEvent   616 azhekhan  txt       REG                1,4       45552 1152921500312467997 /System/Library/UserEventPlugins/com.apple.alarm.plugin/Contents/MacOS/com.apple.alarm
UserEvent   616 azhekhan  txt       REG                1,4       34368 1152921500312468079 /System/Library/UserEventPlugins/com.apple.bonjour.plugin/Contents/MacOS/com.apple.bonjour
UserEvent   616 azhekhan  txt       REG                1,4       29920 1152921500312468051 /System/Library/UserEventPlugins/com.apple.cfnotification.plugin/Contents/MacOS/com.apple.cfnotification
UserEvent   616 azhekhan  txt       REG                1,4      117632 1152921500312467952 /System/Library/UserEventPlugins/com.apple.cts.plugin/Contents/MacOS/com.apple.cts
UserEvent   616 azhekhan  txt       REG                1,4       29360 1152921500312467943 /System/Library/UserEventPlugins/com.apple.diskarbitration.plugin/Contents/MacOS/com.apple.diskarbitration
UserEvent   616 azhekhan  txt       REG                1,4       29216 1152921500312468033 /System/Library/UserEventPlugins/com.apple.dispatch.vfs.plugin/Contents/MacOS/com.apple.dispatch.vfs
UserEvent   616 azhekhan  txt       REG                1,4       35344 1152921500312468125 /System/Library/UserEventPlugins/com.apple.fsevents.matching.plugin/Contents/MacOS/com.apple.fsevents.matching
UserEvent   616 azhekhan  txt       REG                1,4       33856 1152921500311904096 /System/Library/UserEventPlugins/com.apple.iokit.matching.plugin/Contents/MacOS/com.apple.iokit.matching
UserEvent   616 azhekhan  txt       REG                1,4       34048 1152921500312467979 /System/Library/UserEventPlugins/com.apple.launchd.helper.plugin/Contents/MacOS/com.apple.launchd.helper
UserEvent   616 azhekhan  txt       REG                1,4       33808 1152921500311903772 /System/Library/UserEventPlugins/com.apple.lockout.lifecycle.plugin/Contents/MacOS/com.apple.lockout.lifecycle
UserEvent   616 azhekhan  txt       REG                1,4       28384 1152921500312468116 /System/Library/UserEventPlugins/com.apple.notifyd.matching.plugin/Contents/MacOS/com.apple.notifyd.matching
UserEvent   616 azhekhan  txt       REG                1,4       98896 1152921500311904278 /System/Library/UserEventPlugins/com.apple.netsvcproxy.plugin/Contents/MacOS/com.apple.netsvcproxy
UserEvent   616 azhekhan  txt       REG                1,4       30016 1152921500312468024 /System/Library/UserEventPlugins/com.apple.nsurlsessiond.plugin/Contents/MacOS/com.apple.nsurlsessiond
UserEvent   616 azhekhan  txt       REG                1,4       28064 1152921500311904215 /System/Library/UserEventPlugins/com.apple.rapport.events.plugin/Contents/MacOS/com.apple.rapport.events
UserEvent   616 azhekhan  txt       REG                1,4       28688 1152921500312468042 /System/Library/UserEventPlugins/com.apple.reachability.plugin/Contents/MacOS/com.apple.reachability
UserEvent   616 azhekhan  txt       REG                1,4       54944 1152921500311904197 /System/Library/UserEventPlugins/com.apple.rcdevent.matching.plugin/Contents/MacOS/com.apple.rcdevent.matching
UserEvent   616 azhekhan  txt       REG                1,4       33568 1152921500312468015 /System/Library/UserEventPlugins/com.apple.remoteservicediscovery.events.plugin/Contents/MacOS/com.apple.remoteservicediscovery.events
UserEvent   616 azhekhan  txt       REG                1,4       29328 1152921500312467961 /System/Library/UserEventPlugins/com.apple.systemconfiguration.plugin/Contents/MacOS/com.apple.systemconfiguration
UserEvent   616 azhekhan  txt       REG                1,4       40992 1152921500312467988 /System/Library/UserEventPlugins/com.apple.time.plugin/Contents/MacOS/com.apple.time
UserEvent   616 azhekhan  txt       REG                1,4       38416 1152921500311903826 /System/Library/UserEventPlugins/com.apple.touchbar.matching.plugin/Contents/MacOS/com.apple.touchbar.matching
UserEvent   616 azhekhan  txt       REG                1,4       41328 1152921500312468107 /System/Library/UserEventPlugins/com.apple.universalaccess.events.plugin/Contents/MacOS/com.apple.universalaccess.events
UserEvent   616 azhekhan  txt       REG                1,4       34080 1152921500311903682 /System/Library/UserEventPlugins/com.apple.usernotificationcenter.matching.plugin/Contents/MacOS/com.apple.usernotificationcenter.matching
UserEvent   616 azhekhan  txt       REG                1,4       38320 1152921500311903745 /System/Library/UserEventPlugins/routined.events.plugin/Contents/MacOS/routined.events
UserEvent   616 azhekhan  txt       REG                1,4      242128            56647497 /private/var/db/timezone/tz/2021a.3.0/icutz/icutz44l.dat
UserEvent   616 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
UserEvent   616 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
UserEvent   616 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
UserEvent   616 azhekhan    2u      CHR                3,2       0t460                 313 /dev/null
UserEvent   616 azhekhan    3u     unix 0x3fbd25a256f07819         0t0                     ->0x3fbd25a256f07751
UserEvent   616 azhekhan    4r      DIR                1,4          96            47526654 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/0/com.apple.lockoutagent
UserEvent   616 azhekhan    5   NPOLICY
UserEvent   616 azhekhan    6u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan    7u     unix 0x3fbd25a256f05d89         0t0                     ->0x3fbd25a256f07a71
UserEvent   616 azhekhan    8r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan    9r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan   10r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan   11r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan   12r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan   13r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan   14r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan   15u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan   16r      DIR                1,4          64              678691 /private/var/db/PanicReporter
UserEvent   616 azhekhan   17r      DIR                1,4        3264            62409538 /private/var/db
UserEvent   616 azhekhan   18r      DIR                1,4         928            62409537 /private/var
UserEvent   616 azhekhan   19r      DIR                1,4         192            62409536 /private
UserEvent   616 azhekhan   20r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan   21r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan   22r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan   23u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan   24r      DIR                1,4         576              700986 /opt/cisco/anyconnect
UserEvent   616 azhekhan   25r      DIR                1,4        3168              678503 /Users/azhekhan/Library/Application Support
UserEvent   616 azhekhan   26r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan   27r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan   28r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan   29r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan   30r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan   31r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan   32u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan   33r      DIR                1,4         800            62430859 /private/tmp
UserEvent   616 azhekhan   34r      DIR                1,4        1440            62430851 /private/var/run
UserEvent   616 azhekhan   35r      DIR                1,4         928            62409537 /private/var
UserEvent   616 azhekhan   36r      DIR                1,4         192            62409536 /private
UserEvent   616 azhekhan   37r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan   38r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan   39r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan   40u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan   41r      DIR                1,4         448            47528836 /Users/azhekhan/Library/Caches/com.apple.appstoreagent
UserEvent   616 azhekhan   42r      DIR                1,4         448            47528836 /Users/azhekhan/Library/Caches/com.apple.appstoreagent
UserEvent   616 azhekhan   43r      DIR                1,4        5920              681065 /Users/azhekhan/Library/Caches
UserEvent   616 azhekhan   44r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan   45r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan   46r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan   47r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan   48r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan   49r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan   50r      DIR                1,4          96            47542655 /Users/azhekhan/Library/Application Support/com.apple.ContextStoreAgent
UserEvent   616 azhekhan   51r      DIR                1,4        3168              678503 /Users/azhekhan/Library/Application Support
UserEvent   616 azhekhan   52r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan   53r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan   54r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan   55r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan   56r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan   57r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan   58r      DIR                1,4        3168              678503 /Users/azhekhan/Library/Application Support
UserEvent   616 azhekhan   59r      DIR                1,4         448            47528836 /Users/azhekhan/Library/Caches/com.apple.appstoreagent
UserEvent   616 azhekhan   60r      DIR                1,4         448            47528836 /Users/azhekhan/Library/Caches/com.apple.appstoreagent
UserEvent   616 azhekhan   61r      DIR                1,4        5920              681065 /Users/azhekhan/Library/Caches
UserEvent   616 azhekhan   62r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan   63r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan   64r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan   65r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan   66r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan   67r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan   68u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan   69r      DIR                1,4        2592            62430861 /Applications
UserEvent   616 azhekhan   70r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan   71r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan   72r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan   73r      DIR                1,4         320            62433103 /Applications/Utilities
UserEvent   616 azhekhan   74r      DIR                1,4        2592            62430861 /Applications
UserEvent   616 azhekhan   75r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan   76r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan   77r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan   78u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan   80r      DIR                1,4         800            62430859 /private/tmp
UserEvent   616 azhekhan   81r      DIR                1,4         192            62409536 /private
UserEvent   616 azhekhan   82u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan   83r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan   84r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan   85r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan   86r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan   87r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan   88r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan   89r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan   90u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan   91r      DIR                1,4         512 1152921500312300317 /System/Library/ColorSync/Profiles
UserEvent   616 azhekhan   92r      DIR                1,4         160 1152921500312299688 /System/Library/ColorSync
UserEvent   616 azhekhan   93r      DIR                1,4        3712 1152921500311902706 /System/Library
UserEvent   616 azhekhan   94r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan   95r      DIR                1,4         320            62421196 /Library/ColorSync/Profiles
UserEvent   616 azhekhan   96r      DIR                1,4         128            62421194 /Library/ColorSync
UserEvent   616 azhekhan   97r      DIR                1,4        2496            62409876 /Library
UserEvent   616 azhekhan   98r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan   99r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  100r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  101r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  102r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  103r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan  104r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan  105r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  106r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  107r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  108u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  109r      DIR                1,4         160              678537 /Users/azhekhan/Library/Internet Plug-Ins
UserEvent   616 azhekhan  110r      DIR                1,4         160              678537 /Users/azhekhan/Library/Internet Plug-Ins
UserEvent   616 azhekhan  111r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  112r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan  113r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan  114r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  115r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  116r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  117u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  118r      DIR                1,4         192            62426604 /Library/Internet Plug-Ins
UserEvent   616 azhekhan  119r      DIR                1,4         192            62426604 /Library/Internet Plug-Ins
UserEvent   616 azhekhan  120r      DIR                1,4        2496            62409876 /Library
UserEvent   616 azhekhan  121r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  122r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  123r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  124u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  125r      DIR                1,4       10176              680931 /Users/azhekhan/Library/Containers
UserEvent   616 azhekhan  126r      DIR                1,4       10176              680931 /Users/azhekhan/Library/Containers
UserEvent   616 azhekhan  127r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  128r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan  129r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan  130r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  131r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  132r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  133u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  134r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  135r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  136r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan  137r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan  138r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  139r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  140r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  141u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  142r      REG                1,4           7              700019 /Library/Application Support/Apple/Remote Desktop/RemoteManagement.launchd
UserEvent   616 azhekhan  143r      DIR                1,4         224              128895 /Library/Application Support/Apple/Remote Desktop
UserEvent   616 azhekhan  144r      DIR                1,4         384            62411716 /Library/Application Support/Apple
UserEvent   616 azhekhan  145r      DIR                1,4         832            62411715 /Library/Application Support
UserEvent   616 azhekhan  146r      DIR                1,4        2496            62409876 /Library
UserEvent   616 azhekhan  147r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  148r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  149r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  150u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  151r      DIR                1,4         160            47740967 /Users/azhekhan/Library/studentd
UserEvent   616 azhekhan  152r      DIR                1,4         160            47740967 /Users/azhekhan/Library/studentd
UserEvent   616 azhekhan  153r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  154r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan  155r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan  156r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  157r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  158r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  159u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  160r      DIR                1,4         704              683239 /Users/azhekhan/Library/Mobile Documents
UserEvent   616 azhekhan  161r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  162r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan  163r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan  164r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  165r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  166r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  167u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  168r      DIR                1,4          96            13628903 /Library/Services
UserEvent   616 azhekhan  169r      DIR                1,4        2496            62409876 /Library
UserEvent   616 azhekhan  170r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  171r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  172r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  173u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  174r      DIR                1,4          96             2630095 /Users/azhekhan/Library/Services
UserEvent   616 azhekhan  175r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  176r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan  177r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan  178r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  179r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  180r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  181u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  182r      DIR                1,4         416              684735 /Users/azhekhan/Library/Caches/GeoServices
UserEvent   616 azhekhan  183r      DIR                1,4         416              684735 /Users/azhekhan/Library/Caches/GeoServices
UserEvent   616 azhekhan  184r      DIR                1,4        5920              681065 /Users/azhekhan/Library/Caches
UserEvent   616 azhekhan  185r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  186r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan  187r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan  188r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  189r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  190r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  191u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  192r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  193r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  194r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan  195r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan  196r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  197r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  198r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  199u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  200r      REG                1,4         218            62410124 /Library/Apple/Library/Bundles/TCC_Compatibility.bundle/Contents/Resources/AllowApplicationsList.plist
UserEvent   616 azhekhan  201r      DIR                1,4          96            62410123 /Library/Apple/Library/Bundles/TCC_Compatibility.bundle/Contents/Resources
UserEvent   616 azhekhan  202r      DIR                1,4         192            62410117 /Library/Apple/Library/Bundles/TCC_Compatibility.bundle/Contents
UserEvent   616 azhekhan  203r      DIR                1,4          96            62410116 /Library/Apple/Library/Bundles/TCC_Compatibility.bundle
UserEvent   616 azhekhan  204r      DIR                1,4         160            62409895 /Library/Apple/Library/Bundles
UserEvent   616 azhekhan  205r      DIR                1,4          96            62409894 /Library/Apple/Library
UserEvent   616 azhekhan  206r      DIR                1,4         160            62409877 /Library/Apple
UserEvent   616 azhekhan  207r      DIR                1,4        2496            62409876 /Library
UserEvent   616 azhekhan  208r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  209r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  210r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  211r      DIR                1,4         160 1152921500312081025 /System/Library/Sandbox
UserEvent   616 azhekhan  212r      DIR                1,4         160 1152921500312081025 /System/Library/Sandbox
UserEvent   616 azhekhan  213r      DIR                1,4        3712 1152921500311902706 /System/Library
UserEvent   616 azhekhan  214r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  215r      DIR                1,4         832            62411715 /Library/Application Support
UserEvent   616 azhekhan  216r      DIR                1,4         832            62411715 /Library/Application Support
UserEvent   616 azhekhan  217r      DIR                1,4        2496            62409876 /Library
UserEvent   616 azhekhan  218r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  219r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  220r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  221u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  222r      DIR                1,4        1440            62430851 /private/var/run
UserEvent   616 azhekhan  223r      DIR                1,4         576              700986 /opt/cisco/anyconnect
UserEvent   616 azhekhan  224r      DIR                1,4         128              700985 /opt/cisco
UserEvent   616 azhekhan  225r      DIR                1,4         192            47518019 /opt
UserEvent   616 azhekhan  226r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  227r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  228r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  229u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  230r      DIR                1,4         576              700986 /opt/cisco/anyconnect
UserEvent   616 azhekhan  231r      DIR                1,4         576              700986 /opt/cisco/anyconnect
UserEvent   616 azhekhan  232r      DIR                1,4         128              700985 /opt/cisco
UserEvent   616 azhekhan  233r      DIR                1,4         192            47518019 /opt
UserEvent   616 azhekhan  234r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  235r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  236r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  237u   KQUEUE                                                    count=0, state=0x8
UserEvent   616 azhekhan  238r      DIR                1,4         256              942549 /Users/azhekhan/Library/Logs/DiagnosticReports
UserEvent   616 azhekhan  239r      DIR                1,4        3712              680980 /Users/azhekhan/Library/Logs
UserEvent   616 azhekhan  240r      DIR                1,4        2688              678502 /Users/azhekhan/Library
UserEvent   616 azhekhan  241r      DIR                1,4        3872              678494 /Users/azhekhan
UserEvent   616 azhekhan  242r      DIR                1,4         160              641948 /Users
UserEvent   616 azhekhan  243r      DIR                1,4         832 1152921500311879682 /System/Volumes/Data
UserEvent   616 azhekhan  244r      DIR                1,4          96 1152921500312310411 /System/Volumes
UserEvent   616 azhekhan  245r      DIR                1,4         256 1152921500311902582 /System
UserEvent   616 azhekhan  246r      REG                1,4           0            63985486 /Users/azhekhan/Library/Application Support/com.apple.ContextStoreAgent/screenTimeEnabled
UserEvent   616 azhekhan  247u    systm 0x3fbd25a25a4a8899         0t0                     [ctl com.apple.netsrc id 7 unit 7]
distnoted   618 azhekhan  cwd       DIR                1,4         704                   2 /
distnoted   618 azhekhan  txt       REG                1,4      173552 1152921500312467295 /usr/sbin/distnoted
distnoted   618 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
distnoted   618 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
distnoted   618 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
distnoted   618 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
distnoted   618 azhekhan    2u      CHR                3,2       0t166                 313 /dev/null
lsd         621 azhekhan  cwd       DIR                1,4         704                   2 /
lsd         621 azhekhan  txt       REG                1,4       32688 1152921500312466890 /usr/libexec/lsd
lsd         621 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
lsd         621 azhekhan  txt       REG                1,4       32768            63950731 /private/var/db/mds/messages/501/se_SecurityMessages
lsd         621 azhekhan  txt       REG                1,4      170592 1152921500312476348 /System/Library/Frameworks/Security.framework/Versions/A/PlugIns/csparser.bundle/Contents/MacOS/csparser
lsd         621 azhekhan  txt       REG                1,4      242128            56647497 /private/var/db/timezone/tz/2021a.3.0/icutz/icutz44l.dat
lsd         621 azhekhan  txt       REG                1,4     6205440            65300106 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/0/com.apple.LaunchServices.dv/com.apple.LaunchServices-1082-v2.csstore
lsd         621 azhekhan  txt       REG                1,4    28620288 1152921500311902385 /usr/share/icu/icudt64l.dat
lsd         621 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
lsd         621 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
lsd         621 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
lsd         621 azhekhan    2u      CHR                3,2         0t0                 313 /dev/null
lsd         621 azhekhan    3u      REG                1,4       12288              695974 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/0/com.apple.LaunchServices.dv/com.apple.LaunchServices.trustedsignatures-501.db
trustd      622 azhekhan  cwd       DIR                1,4         704                   2 /
trustd      622 azhekhan  txt       REG                1,4      518576 1152921500312466908 /usr/libexec/trustd
trustd      622 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
trustd      622 azhekhan  txt       REG                1,4       32768             1842699 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/trustd_health_analytics.db-shm
trustd      622 azhekhan  txt       REG                1,4      200576 1152921500311988881 /System/Library/Security/Certificates.bundle/Contents/Resources/certsTable.data
trustd      622 azhekhan  txt       REG                1,4       32768            62433366 /Library/Keychains/crls/valid.sqlite3-shm
trustd      622 azhekhan  txt       REG                1,4       32768              680912 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/ocspcache.sqlite3-shm
trustd      622 azhekhan  txt       REG                1,4       32768             1842705 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/trust_analytics.db-shm
trustd      622 azhekhan  txt       REG                1,4    28620288 1152921500311902385 /usr/share/icu/icudt64l.dat
trustd      622 azhekhan  txt       REG                1,4       32768            63950731 /private/var/db/mds/messages/501/se_SecurityMessages
trustd      622 azhekhan  txt       REG                1,4      242128            56647497 /private/var/db/timezone/tz/2021a.3.0/icutz/icutz44l.dat
trustd      622 azhekhan  txt       REG                1,4       32768             1843121 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/TLS_analytics.db-shm
trustd      622 azhekhan  txt       REG                1,4       54447            61369618 /private/var/db/nsurlstoraged/dafsaData.bin
trustd      622 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
trustd      622 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
trustd      622 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
trustd      622 azhekhan    2u      CHR                3,2       0t715                 313 /dev/null
trustd      622 azhekhan    3r      REG                1,4      200704              678684 /Library/Keychains/pinningrules.sqlite3
trustd      622 azhekhan    4u      REG                1,4       57344             1842696 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/trustd_health_analytics.db
trustd      622 azhekhan    5u      REG                1,4      618032             1842698 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/trustd_health_analytics.db-wal
trustd      622 azhekhan    6u      REG                1,4       32768             1842699 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/trustd_health_analytics.db-shm
trustd      622 azhekhan    7r      REG                1,4     6713344            62433362 /Library/Keychains/crls/valid.sqlite3
trustd      622 azhekhan    8r      REG                1,4     1549152            62433365 /Library/Keychains/crls/valid.sqlite3-wal
trustd      622 azhekhan    9r      REG                1,4       32768            62433366 /Library/Keychains/crls/valid.sqlite3-shm
trustd      622 azhekhan   10u      REG                1,4      233472              680908 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/ocspcache.sqlite3
trustd      622 azhekhan   11u      REG                1,4     2900512              680911 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/ocspcache.sqlite3-wal
trustd      622 azhekhan   12u      REG                1,4       32768              680912 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/ocspcache.sqlite3-shm
trustd      622 azhekhan   13u      REG                1,4      606208             1842702 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/trust_analytics.db
trustd      622 azhekhan   14u      REG                1,4     2595632             1842704 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/trust_analytics.db-wal
trustd      622 azhekhan   15u      REG                1,4       32768             1842705 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/trust_analytics.db-shm
trustd      622 azhekhan   16u      REG                1,4     1843200             1843118 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/TLS_analytics.db
trustd      622 azhekhan   17u      REG                1,4     3823392             1843120 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/TLS_analytics.db-wal
trustd      622 azhekhan   18u      REG                1,4       32768             1843121 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/Analytics/TLS_analytics.db-shm
trustd      622 azhekhan   19r      REG                1,4       54447            61369618 /private/var/db/nsurlstoraged/dafsaData.bin
trustd      622 azhekhan   20   NPOLICY
trustd      622 azhekhan   21u      REG                1,4       65536             1875547 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/C/com.apple.trustd/caissuercache.sqlite3
trustd      622 azhekhan   22u    systm 0x3fbd25a25c1dc419         0t0                     [ctl com.apple.netsrc id 7 unit 27]
knowledge   624 azhekhan  cwd       DIR                1,4         704                   2 /
knowledge   624 azhekhan  txt       REG                1,4       80544 1152921500311884260 /usr/libexec/knowledge-agent
knowledge   624 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
knowledge   624 azhekhan  txt       REG                1,4      149968 1152921500311885486 /usr/lib/libobjc-trampolines.dylib
knowledge   624 azhekhan  txt       REG                1,4       32768              689644 /Users/azhekhan/Library/Application Support/Knowledge/knowledgeC.db-shm
knowledge   624 azhekhan  txt       REG                1,4    28620288 1152921500311902385 /usr/share/icu/icudt64l.dat
knowledge   624 azhekhan  txt       REG                1,4      242128            56647497 /private/var/db/timezone/tz/2021a.3.0/icutz/icutz44l.dat
knowledge   624 azhekhan  txt       REG                1,4      205992            45660617 /Library/Application Support/CrashReporter/SubmitDiagInfo.domains
knowledge   624 azhekhan  txt       REG                1,4       43777            65284106 /private/var/db/analyticsd/events.whitelist
knowledge   624 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
knowledge   624 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
knowledge   624 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
knowledge   624 azhekhan    2u      CHR                3,2         0t0                 313 /dev/null
knowledge   624 azhekhan    3u      REG                1,4     4382720              689639 /Users/azhekhan/Library/Application Support/Knowledge/knowledgeC.db
knowledge   624 azhekhan    4u      REG                1,4     1788112              689643 /Users/azhekhan/Library/Application Support/Knowledge/knowledgeC.db-wal
knowledge   624 azhekhan    5u      REG                1,4       32768              689644 /Users/azhekhan/Library/Application Support/Knowledge/knowledgeC.db-shm
knowledge   624 azhekhan    6r      REG                1,4      205992            45660617 /Library/Application Support/CrashReporter/SubmitDiagInfo.domains
secd        625 azhekhan  cwd       DIR                1,4         704                   2 /
secd        625 azhekhan  txt       REG                1,4     3153344 1152921500312466922 /usr/libexec/secd
secd        625 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
secd        625 azhekhan  txt       REG                1,4       32768              681003 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/keychain-2.db-shm
secd        625 azhekhan  txt       REG                1,4       32768             1842736 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/Analytics/ckks_analytics.db-shm
secd        625 azhekhan  txt       REG                1,4      242128            56647497 /private/var/db/timezone/tz/2021a.3.0/icutz/icutz44l.dat
secd        625 azhekhan  txt       REG                1,4       43777            65284106 /private/var/db/analyticsd/events.whitelist
secd        625 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
secd        625 azhekhan  txt       REG                1,4    28620288 1152921500311902385 /usr/share/icu/icudt64l.dat
secd        625 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
secd        625 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
secd        625 azhekhan    2u      CHR                3,2      0t1865                 313 /dev/null
secd        625 azhekhan    3u     unix 0x3fbd25a256f05f19         0t0                     ->0x3fbd25a25228d3c9
secd        625 azhekhan    4u      REG                1,4     2453504              680999 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/keychain-2.db
secd        625 azhekhan    5u      REG                1,4     4317792              681002 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/keychain-2.db-wal
secd        625 azhekhan    6u      REG                1,4       32768              681003 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/keychain-2.db-shm
secd        625 azhekhan    7u      REG                1,4       69632             1842733 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/Analytics/ckks_analytics.db
secd        625 azhekhan    8u      REG                1,4     2453504              680999 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/keychain-2.db
secd        625 azhekhan    9u      REG                1,4     4317792              681002 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/keychain-2.db-wal
secd        625 azhekhan   10u      REG                1,4     1998232             1842735 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/Analytics/ckks_analytics.db-wal
secd        625 azhekhan   11u      REG                1,4       32768             1842736 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/Analytics/ckks_analytics.db-shm
secd        625 azhekhan   12   NPOLICY
secd        625 azhekhan   13u      REG                1,4     2453504              680999 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/keychain-2.db
secd        625 azhekhan   14u      REG                1,4     4317792              681002 /Users/azhekhan/Library/Keychains/55D89A27-3AAB-579D-B49B-FE2E44A87E0A/keychain-2.db-wal
accountsd   626 azhekhan  cwd       DIR                1,4         704                   2 /
accountsd   626 azhekhan  txt       REG                1,4       41744 1152921500312104250 /System/Library/Frameworks/Accounts.framework/Versions/A/Support/accountsd
accountsd   626 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
accountsd   626 azhekhan  txt       REG                1,4      149968 1152921500311885486 /usr/lib/libobjc-trampolines.dylib
accountsd   626 azhekhan  txt       REG                1,4       32768            62441357 /Users/azhekhan/Library/Accounts/Accounts4.sqlite-shm
accountsd   626 azhekhan  txt       REG                1,4    28620288 1152921500311902385 /usr/share/icu/icudt64l.dat
accountsd   626 azhekhan  txt       REG                1,4       84976 1152921500312297257 /System/Library/Accounts/Authentication/iCloudIDAuthentication.bundle/Contents/MacOS/iCloudIDAuthentication
accountsd   626 azhekhan  txt       REG                1,4       32768            63985498 /private/var/db/mds/messages/501/se_SecurityMessages
accountsd   626 azhekhan  txt       REG                1,4      242128            56647497 /private/var/db/timezone/tz/2021a.3.0/icutz/icutz44l.dat
accountsd   626 azhekhan  txt       REG                1,4       73376 1152921500312297608 /System/Library/Accounts/Authentication/CalendarAuthenticationPlugin.bundle/Contents/MacOS/CalendarAuthenticationPlugin
accountsd   626 azhekhan  txt       REG                1,4      194128            61205039 /Library/Apple/System/Library/StagedFrameworks/Safari/AuthenticationServices.framework/Versions/A/AuthenticationServices
accountsd   626 azhekhan  txt       REG                1,4     3703968            61201211 /Library/Apple/System/Library/StagedFrameworks/Safari/WebKitLegacy.framework/Versions/A/WebKitLegacy
accountsd   626 azhekhan  txt       REG                1,4       22560            61200731 /Library/Apple/System/Library/StagedFrameworks/Safari/WebInspectorUI.framework/Versions/A/WebInspectorUI
accountsd   626 azhekhan  txt       REG                1,4     6083840            61203916 /Library/Apple/System/Library/StagedFrameworks/Safari/libANGLE-shared.dylib
accountsd   626 azhekhan  txt       REG                1,4       34320            61204907 /Library/Apple/System/Library/StagedFrameworks/Safari/AuthenticationServicesCore.framework/Versions/A/AuthenticationServicesCore
accountsd   626 azhekhan  txt       REG                1,4       52112 1152921500312297201 /System/Library/Accounts/Authentication/GoogleAuthenticationPlugin.bundle/Contents/MacOS/GoogleAuthenticationPlugin
accountsd   626 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
accountsd   626 azhekhan  txt       REG                1,4    18434080            61204665 /Library/Apple/System/Library/StagedFrameworks/Safari/WebKit.framework/Versions/A/WebKit
accountsd   626 azhekhan  txt       REG                1,4    56058832            61205459 /Library/Apple/System/Library/StagedFrameworks/Safari/WebCore.framework/Versions/A/WebCore
accountsd   626 azhekhan  txt       REG                1,4    12399920            61201112 /Library/Apple/System/Library/StagedFrameworks/Safari/libwebrtc.dylib
accountsd   626 azhekhan  txt       REG                1,4    25853056            61205473 /Library/Apple/System/Library/StagedFrameworks/Safari/JavaScriptCore.framework/Versions/A/JavaScriptCore
accountsd   626 azhekhan  txt       REG                1,4     1176864            61203922 /Library/Apple/System/Library/StagedFrameworks/Safari/SafariCore.framework/Versions/A/SafariCore
accountsd   626 azhekhan  txt       REG                1,4      166736            61204062 /Library/Apple/System/Library/StagedFrameworks/Safari/SafariFoundation.framework/Versions/A/SafariFoundation
accountsd   626 azhekhan  txt       REG                1,4       54928 1152921500312296495 /System/Library/Accounts/Notification/CalendarNotificationPlugin.bundle/Contents/MacOS/CalendarNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       41680 1152921500312296805 /System/Library/Accounts/Notification/ExchangeNotificationPlugin.bundle/Contents/MacOS/ExchangeNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       89616 1152921500312494706 /System/Library/Accounts/Notification/EmailNotificationPlugin.bundle/Contents/MacOS/EmailNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       42192 1152921500312296868 /System/Library/Accounts/Notification/SPAAccountsNotificationPlugin.bundle/Contents/MacOS/SPAAccountsNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       36928 1152921500312494687 /System/Library/Accounts/Notification/SyncedDefaultsAccountNotificationPlugin.bundle/Contents/MacOS/SyncedDefaultsAccountNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       37008 1152921500312296486 /System/Library/Accounts/Notification/ExchangeSyncNotificationPlugin.bundle/Contents/MacOS/ExchangeSyncNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       66224 1152921500312296541 /System/Library/Accounts/Notification/NotesAccountNotificationPlugin.bundle/Contents/MacOS/NotesAccountNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       32768              700646 /Users/azhekhan/Library/Containers/com.apple.Notes/Data/Library/Notes/NotesV7.storedata-shm
accountsd   626 azhekhan  txt       REG                1,4       82464 1152921500312494660 /System/Library/Accounts/Notification/ContactsAccountsNotificationPlugin.bundle/Contents/MacOS/ContactsAccountsNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       42032 1152921500312296522 /System/Library/Accounts/Notification/InternetAccountsNotificationPlugin.bundle/Contents/MacOS/InternetAccountsNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       52000 1152921500312296513 /System/Library/Accounts/Notification/RemoteManagementAccountsNotificationPlugin.bundle/Contents/MacOS/RemoteManagementAccountsNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       41088 1152921500312296814 /System/Library/Accounts/Notification/AMPAccountNotificationPlugin.bundle/Contents/MacOS/AMPAccountNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4      101120 1152921500312297221 /System/Library/Accounts/Authentication/AMSAccountAuthenticationPlugin.bundle/Contents/MacOS/AMSAccountAuthenticationPlugin
accountsd   626 azhekhan  txt       REG                1,4      675554 1152921500311916191 /System/Library/PreferencePanes/Notifications.prefPane/Contents/Resources/Notifications.icns
accountsd   626 azhekhan  txt       REG                1,4      144000 1152921500312298783 /System/Library/Accounts/UI/iCloudUIPlugin.bundle/Contents/MacOS/iCloudUIPlugin
accountsd   626 azhekhan  txt       REG                1,4       55680 1152921500312299290 /System/Library/Accounts/UI/CalendarUIPlugin.bundle/Contents/MacOS/CalendarUIPlugin
accountsd   626 azhekhan  txt       REG                1,4      119920 1152921500312494785 /System/Library/Accounts/UI/GameCenterAccountsUIPlugin.bundle/Contents/MacOS/GameCenterAccountsUIPlugin
accountsd   626 azhekhan  txt       REG                1,4       80224 1152921500312494767 /System/Library/Accounts/UI/ContactsAccountsUIPlugin.bundle/Contents/MacOS/ContactsAccountsUIPlugin
accountsd   626 azhekhan  txt       REG                1,4       70752 1152921500312494795 /System/Library/Accounts/UI/EmailUIPlugin.bundle/Contents/MacOS/EmailUIPlugin
accountsd   626 azhekhan  txt       REG                1,4       86112 1152921500312298261 /System/Library/Accounts/UI/ExchangeUIPlugin.bundle/Contents/MacOS/ExchangeUIPlugin
accountsd   626 azhekhan  txt       REG                1,4       42096 1152921500312260304 /System/Library/PrivateFrameworks/AOSUI.framework/Versions/A/Resources/Assets.car
accountsd   626 azhekhan  txt       REG                1,4     5274676 1152921500312277151 /System/Library/PrivateFrameworks/DataDetectorsCore.framework/Versions/A/Resources/com.apple.datadetectorscore.cache.full.asia.system
accountsd   626 azhekhan  txt       REG                1,4    27971344 1152921500311895959 /usr/share/langid/langid.inv
accountsd   626 azhekhan  txt       REG                1,4       32768              689728 /Users/azhekhan/Library/Group Containers/group.com.apple.notes/NoteStore.sqlite-shm
accountsd   626 azhekhan  txt       REG                1,4       38352 1152921500312468822 /System/Library/Address Book Plug-Ins/LocalSource.sourcebundle/Contents/MacOS/LocalSource
accountsd   626 azhekhan  txt       REG                1,4       58496 1152921500312468781 /System/Library/Address Book Plug-Ins/DirectoryServices.sourcebundle/Contents/MacOS/DirectoryServices
accountsd   626 azhekhan  txt       REG                1,4      759856 1152921500311931359 /System/Library/Address Book Plug-Ins/CardDAVPlugin.sourcebundle/Contents/MacOS/CardDAVPlugin
accountsd   626 azhekhan  txt       REG                1,4       36576 1152921500312296787 /System/Library/Accounts/Notification/AOSKitNotificationPlugin.bundle/Contents/MacOS/AOSKitNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4      155008 1152921500311941319 /System/Library/CoreServices/SetupAssistantPlugins/AMPAccounts.icdplugin/Contents/MacOS/AMPAccounts
accountsd   626 azhekhan  txt       REG                1,4       31904 1152921500312469142 /System/Library/CoreServices/SetupAssistantPlugins/AppStore.icdplugin/Contents/MacOS/AppStore
accountsd   626 azhekhan  txt       REG                1,4       47456 1152921500311941790 /System/Library/CoreServices/SetupAssistantPlugins/iCloudDelegate.icdplugin/Contents/MacOS/iCloudDelegate
accountsd   626 azhekhan  txt       REG                1,4       31904 1152921500312469122 /System/Library/CoreServices/SetupAssistantPlugins/iBooks.icdplugin/Contents/MacOS/iBooks
accountsd   626 azhekhan  txt       REG                1,4       49056 1152921500311941337 /System/Library/CoreServices/SetupAssistantPlugins/IdentityServices.icdplugin/Contents/MacOS/IdentityServices
accountsd   626 azhekhan  txt       REG                1,4       36416 1152921500312469102 /System/Library/CoreServices/SetupAssistantPlugins/GameCenter.icdplugin/Contents/MacOS/GameCenter
accountsd   626 azhekhan  txt       REG                1,4       45408 1152921500312296832 /System/Library/Accounts/Notification/iCloudIDNotification.bundle/Contents/MacOS/iCloudIDNotification
accountsd   626 azhekhan  txt       REG                1,4       51568 1152921500312494678 /System/Library/Accounts/Notification/CloudDocsAccountNotificationPlugin.bundle/Contents/MacOS/CloudDocsAccountNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       46752 1152921500312297029 /System/Library/Accounts/Notification/RemindersAccountNotificationPlugin.bundle/Contents/MacOS/RemindersAccountNotificationPlugin
accountsd   626 azhekhan  txt       REG                1,4       43777            65284106 /private/var/db/analyticsd/events.whitelist
accountsd   626 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
accountsd   626 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
accountsd   626 azhekhan    2u      CHR                3,2      0t1296                 313 /dev/null
accountsd   626 azhekhan    3u      REG                1,4      282624              681028 /Users/azhekhan/Library/Accounts/Accounts4.sqlite
accountsd   626 azhekhan    4u      REG                1,4     3497912            62441356 /Users/azhekhan/Library/Accounts/Accounts4.sqlite-wal
accountsd   626 azhekhan    5u      REG                1,4       32768            62441357 /Users/azhekhan/Library/Accounts/Accounts4.sqlite-shm
accountsd   626 azhekhan    6u      REG                1,4      282624              681028 /Users/azhekhan/Library/Accounts/Accounts4.sqlite
accountsd   626 azhekhan    7u      REG                1,4     3497912            62441356 /Users/azhekhan/Library/Accounts/Accounts4.sqlite-wal
accountsd   626 azhekhan    8u      REG                1,4      282624              681028 /Users/azhekhan/Library/Accounts/Accounts4.sqlite
accountsd   626 azhekhan    9u      REG                1,4      282624              681028 /Users/azhekhan/Library/Accounts/Accounts4.sqlite
accountsd   626 azhekhan   10   NPOLICY
accountsd   626 azhekhan   11u      REG                1,4     3497912            62441356 /Users/azhekhan/Library/Accounts/Accounts4.sqlite-wal
accountsd   626 azhekhan   12u      REG                1,4     3497912            62441356 /Users/azhekhan/Library/Accounts/Accounts4.sqlite-wal
accountsd   626 azhekhan   13u     unix 0x3fbd25a26dfea8d9         0t0                     ->0x3fbd25a25228d3c9
accountsd   626 azhekhan   14u    systm 0x3fbd25a25aef6539         0t0                     [ctl com.apple.netsrc id 7 unit 20]
accountsd   626 azhekhan   15u      REG                1,4      245760              700641 /Users/azhekhan/Library/Containers/com.apple.Notes/Data/Library/Notes/NotesV7.storedata
accountsd   626 azhekhan   16u      REG                1,4           0              700645 /Users/azhekhan/Library/Containers/com.apple.Notes/Data/Library/Notes/NotesV7.storedata-wal
accountsd   626 azhekhan   17u      REG                1,4       32768              700646 /Users/azhekhan/Library/Containers/com.apple.Notes/Data/Library/Notes/NotesV7.storedata-shm
accountsd   626 azhekhan   18u      REG                1,4      315392              689723 /Users/azhekhan/Library/Group Containers/group.com.apple.notes/NoteStore.sqlite
accountsd   626 azhekhan   19u      REG                1,4      230752              689727 /Users/azhekhan/Library/Group Containers/group.com.apple.notes/NoteStore.sqlite-wal
accountsd   626 azhekhan   20u      REG                1,4       32768              689728 /Users/azhekhan/Library/Group Containers/group.com.apple.notes/NoteStore.sqlite-shm
neagent     627 azhekhan  cwd       DIR                1,4         704                   2 /
neagent     627 azhekhan  txt       REG                1,4      125136 1152921500311881486 /usr/libexec/neagent
neagent     627 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
neagent     627 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
neagent     627 azhekhan  txt       REG                1,4    28620288 1152921500311902385 /usr/share/icu/icudt64l.dat
neagent     627 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
neagent     627 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
neagent     627 azhekhan    2u      CHR                3,2         0t0                 313 /dev/null
rapportd    629 azhekhan  cwd       DIR                1,4         704                   2 /
rapportd    629 azhekhan  txt       REG                1,4      636416 1152921500311884730 /usr/libexec/rapportd
rapportd    629 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
rapportd    629 azhekhan  txt       REG                1,4    28620288 1152921500311902385 /usr/share/icu/icudt64l.dat
rapportd    629 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
rapportd    629 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
rapportd    629 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
rapportd    629 azhekhan    2u      CHR                3,2         0t0                 313 /dev/null
rapportd    629 azhekhan    3   NPOLICY
rapportd    629 azhekhan    4u     IPv4 0x3fbd25a25890e3d1         0t0                 UDP *:*
rapportd    629 azhekhan    5u     unix 0x3fbd25a290600819         0t0                     ->0x3fbd25a2905ffad1
rapportd    629 azhekhan    6u     IPv4 0x3fbd25a25890d261         0t0                 UDP *:*
rapportd    629 azhekhan    7u     IPv4 0x3fbd25a253db8549         0t0                 UDP *:*
rapportd    629 azhekhan    8u     unix 0x3fbd25a25b186879         0t0                     ->0x3fbd25a25b187369
cloudd      630 azhekhan  cwd       DIR                1,4         704                   2 /
cloudd      630 azhekhan  txt       REG                1,4       62880 1152921500312197223 /System/Library/PrivateFrameworks/CloudKitDaemon.framework/Support/cloudd
cloudd      630 azhekhan  txt       REG                1,4       28616            63984653 /Library/Preferences/Logging/.plist-cache.6UNfjzeT
cloudd      630 azhekhan  txt       REG                1,4       32768              681080 /Users/azhekhan/Library/Caches/CloudKit/CloudKitMetadata-shm
cloudd      630 azhekhan  txt       REG                1,4       32768              681094 /Users/azhekhan/Library/Caches/CloudKit/CloudKitOperationInfo-shm
cloudd      630 azhekhan  txt       REG                1,4      242128            56647497 /private/var/db/timezone/tz/2021a.3.0/icutz/icutz44l.dat
cloudd      630 azhekhan  txt       REG                1,4       32768            63985498 /private/var/db/mds/messages/501/se_SecurityMessages
cloudd      630 azhekhan  txt       REG                1,4       32768            57051111 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/79d00bbf349c3c41f627421b1937101e545403ee/Records/pcs.db-shm
cloudd      630 azhekhan  txt       REG                1,4       32768            48384487 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/08932f36cdfe8bd4cb9ae896921eef9e68c55ad4/Records/pcs.db-shm
cloudd      630 azhekhan  txt       REG                1,4       32768            47742885 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/ce49f3500a597aa2dca9af15112d99e9363cd778/Records/pcs.db-shm
cloudd      630 azhekhan  txt       REG                1,4       32768            47749034 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/Records/pcs.db-shm
cloudd      630 azhekhan  txt       REG                1,4       32768            47936063 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/19cd2669f37f65b31d5bbf4d0470072d0c2e24d2/Records/pcs.db-shm
cloudd      630 azhekhan  txt       REG                1,4       32768            47527047 /Users/azhekhan/Library/Caches/CloudKit/com.apple.imagent/595e8bb0c8c5bca9dd197a05597a294fb2d5d1bb/Records/Records.db-shm
cloudd      630 azhekhan  txt       REG                1,4    28620288 1152921500311902385 /usr/share/icu/icudt64l.dat
cloudd      630 azhekhan  txt       REG                1,4       43777            65284106 /private/var/db/analyticsd/events.whitelist
cloudd      630 azhekhan  txt       REG                1,4       32768            47745467 /Users/azhekhan/Library/Caches/CloudKit/com.apple.bird/com.apple.CloudDocs.container-metadata/f4e2b8c76fcf9282b073e083e9f7759e4f66b73e/Records/pcs.db-shm
cloudd      630 azhekhan  txt       REG                1,4     6205440            65300106 /private/var/folders/7l/ncj3_fxx71d2vrbt9w5kjk140000gn/0/com.apple.LaunchServices.dv/com.apple.LaunchServices-1082-v2.csstore
cloudd      630 azhekhan  txt       REG                1,4     1572480 1152921500312467467 /usr/lib/dyld
cloudd      630 azhekhan    0r      CHR                3,2         0t0                 313 /dev/null
cloudd      630 azhekhan    1u      CHR                3,2         0t0                 313 /dev/null
cloudd      630 azhekhan    2u      CHR                3,2         0t0                 313 /dev/null
cloudd      630 azhekhan    3u      REG                1,4       36864              681091 /Users/azhekhan/Library/Caches/CloudKit/CloudKitOperationInfo
cloudd      630 azhekhan    4u      REG                1,4      135168              681077 /Users/azhekhan/Library/Caches/CloudKit/CloudKitMetadata
cloudd      630 azhekhan    5u      REG                1,4       32768              681079 /Users/azhekhan/Library/Caches/CloudKit/CloudKitMetadata-wal
cloudd      630 azhekhan    6u      REG                1,4      383192              681093 /Users/azhekhan/Library/Caches/CloudKit/CloudKitOperationInfo-wal
cloudd      630 azhekhan    7u      REG                1,4       32768              681080 /Users/azhekhan/Library/Caches/CloudKit/CloudKitMetadata-shm
cloudd      630 azhekhan    8r      DIR                1,4          64            47527048 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/tmp
cloudd      630 azhekhan    9r      DIR                1,4          64              681214 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/StagingAssets
cloudd      630 azhekhan   10r      DIR                1,4          64              681215 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/Assets
cloudd      630 azhekhan   11r      DIR                1,4          64              681216 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/AssetsDb
cloudd      630 azhekhan   12r      DIR                1,4          64              681217 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/MMCS
cloudd      630 azhekhan   13r      DIR                1,4          64            47527049 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/FrameworkCaches
cloudd      630 azhekhan   14r      DIR                1,4          64            47527050 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/fa1e3fbf25118f4c6b2e02d52f316e01347e101a
cloudd      630 azhekhan   15r      DIR                1,4          64            47527218 /Users/azhekhan/Library/Caches/CloudKit/com.apple.assistant.assistantd/745028c72683801e45d882b9b76307c0b258408a/tmp
cloudd      630 azhekhan   16r      DIR                1,4          64              681690 /Users/azhekhan/Library/Caches/CloudKit/com.apple.assistant.assistantd/745028c72683801e45d882b9b76307c0b258408a/StagingAssets
cloudd      630 azhekhan   17r      DIR                1,4          64              681691 /Users/azhekhan/Library/Caches/CloudKit/com.apple.assistant.assistantd/745028c72683801e45d882b9b76307c0b258408a/Assets
cloudd      630 azhekhan   18r      DIR                1,4          64              681692 /Users/azhekhan/Library/Caches/CloudKit/com.apple.assistant.assistantd/745028c72683801e45d882b9b76307c0b258408a/AssetsDb
cloudd      630 azhekhan   19r      DIR                1,4          64              681693 /Users/azhekhan/Library/Caches/CloudKit/com.apple.assistant.assistantd/745028c72683801e45d882b9b76307c0b258408a/MMCS
cloudd      630 azhekhan   20r      DIR                1,4          64            47527219 /Users/azhekhan/Library/Caches/CloudKit/com.apple.assistant.assistantd/745028c72683801e45d882b9b76307c0b258408a/FrameworkCaches
cloudd      630 azhekhan   21r      DIR                1,4          64            47527220 /Users/azhekhan/Library/Caches/CloudKit/com.apple.assistant.assistantd/a7251ab568c11a3a6168372c6e3c87169b2d665c
cloudd      630 azhekhan   22r      DIR                1,4          64            47526821 /Users/azhekhan/Library/Caches/CloudKit/com.apple.knowledge-agent/cb76e6f7c01e6fba7b0f09a3fb202aeec42b4b73/tmp
cloudd      630 azhekhan   23r      DIR                1,4          64            47526822 /Users/azhekhan/Library/Caches/CloudKit/com.apple.knowledge-agent/cb76e6f7c01e6fba7b0f09a3fb202aeec42b4b73/StagingAssets
cloudd      630 azhekhan   24r      DIR                1,4          64            47526823 /Users/azhekhan/Library/Caches/CloudKit/com.apple.knowledge-agent/cb76e6f7c01e6fba7b0f09a3fb202aeec42b4b73/Assets
cloudd      630 azhekhan   25r      DIR                1,4          64            47526824 /Users/azhekhan/Library/Caches/CloudKit/com.apple.knowledge-agent/cb76e6f7c01e6fba7b0f09a3fb202aeec42b4b73/AssetsDb
cloudd      630 azhekhan   26r      DIR                1,4          64            47526825 /Users/azhekhan/Library/Caches/CloudKit/com.apple.knowledge-agent/cb76e6f7c01e6fba7b0f09a3fb202aeec42b4b73/MMCS
cloudd      630 azhekhan   27r      DIR                1,4          64            47526826 /Users/azhekhan/Library/Caches/CloudKit/com.apple.knowledge-agent/cb76e6f7c01e6fba7b0f09a3fb202aeec42b4b73/FrameworkCaches
cloudd      630 azhekhan   28r      DIR                1,4          64            47526827 /Users/azhekhan/Library/Caches/CloudKit/com.apple.knowledge-agent/57884fce175b6b1d242db964015f8f050da9c3aa
cloudd      630 azhekhan   29r      DIR                1,4          64            47741863 /Users/azhekhan/Library/Caches/CloudKit/com.apple.ScreenTimeAgent/e1a8d5cf6401c60ff8bb7aff6149b49c397c1553/tmp
cloudd      630 azhekhan   30r      DIR                1,4          64            47741864 /Users/azhekhan/Library/Caches/CloudKit/com.apple.ScreenTimeAgent/e1a8d5cf6401c60ff8bb7aff6149b49c397c1553/StagingAssets
cloudd      630 azhekhan   31r      DIR                1,4          64            47741865 /Users/azhekhan/Library/Caches/CloudKit/com.apple.ScreenTimeAgent/e1a8d5cf6401c60ff8bb7aff6149b49c397c1553/Assets
cloudd      630 azhekhan   32r      DIR                1,4          64            47741866 /Users/azhekhan/Library/Caches/CloudKit/com.apple.ScreenTimeAgent/e1a8d5cf6401c60ff8bb7aff6149b49c397c1553/AssetsDb
cloudd      630 azhekhan   33r      DIR                1,4          64            47741867 /Users/azhekhan/Library/Caches/CloudKit/com.apple.ScreenTimeAgent/e1a8d5cf6401c60ff8bb7aff6149b49c397c1553/MMCS
cloudd      630 azhekhan   34r      DIR                1,4          64            47741868 /Users/azhekhan/Library/Caches/CloudKit/com.apple.ScreenTimeAgent/e1a8d5cf6401c60ff8bb7aff6149b49c397c1553/FrameworkCaches
cloudd      630 azhekhan   35r      DIR                1,4          64            47741869 /Users/azhekhan/Library/Caches/CloudKit/com.apple.ScreenTimeAgent/b8648fc4e4b531bc446faae70ea5c798cccf291d
cloudd      630 azhekhan   36r      DIR                1,4          64            47527048 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/tmp
cloudd      630 azhekhan   37r      DIR                1,4          64            47545803 /Users/azhekhan/Library/Caches/CloudKit/com.apple.textinput.KeyboardServices/com.apple.keyboardservicesd/2bb72ec7676f1ef6f5f88fa889a77a9f1443a05d/tmp
cloudd      630 azhekhan   38r      DIR                1,4          64            47527003 /Users/azhekhan/Library/Caches/CloudKit/com.apple.imagent/595e8bb0c8c5bca9dd197a05597a294fb2d5d1bb/tmp
cloudd      630 azhekhan   39r      DIR                1,4          64             1843272 /Users/azhekhan/Library/Caches/CloudKit/com.apple.imagent/595e8bb0c8c5bca9dd197a05597a294fb2d5d1bb/StagingAssets
cloudd      630 azhekhan   40r      DIR                1,4          64             1843273 /Users/azhekhan/Library/Caches/CloudKit/com.apple.imagent/595e8bb0c8c5bca9dd197a05597a294fb2d5d1bb/Assets
cloudd      630 azhekhan   41r      DIR                1,4          64             1843274 /Users/azhekhan/Library/Caches/CloudKit/com.apple.imagent/595e8bb0c8c5bca9dd197a05597a294fb2d5d1bb/AssetsDb
cloudd      630 azhekhan   42r      DIR                1,4          64             1843275 /Users/azhekhan/Library/Caches/CloudKit/com.apple.imagent/595e8bb0c8c5bca9dd197a05597a294fb2d5d1bb/MMCS
cloudd      630 azhekhan   43r      DIR                1,4          64            47527004 /Users/azhekhan/Library/Caches/CloudKit/com.apple.imagent/595e8bb0c8c5bca9dd197a05597a294fb2d5d1bb/FrameworkCaches
cloudd      630 azhekhan   44r      DIR                1,4          64            47527005 /Users/azhekhan/Library/Caches/CloudKit/com.apple.imagent/6fe52de9b209fe993ece0f2445dff3edf1a1679b
cloudd      630 azhekhan   45r      DIR                1,4          64            47741835 /Users/azhekhan/Library/Caches/CloudKit/com.apple.iad-cloudkit/com.apple.AdSheetPhone/dc9f57b5d00dfc35b7bfc8ffd7dab5add6d780d7/tmp
cloudd      630 azhekhan   46u      REG                1,4       32768              681094 /Users/azhekhan/Library/Caches/CloudKit/CloudKitOperationInfo-shm
cloudd      630 azhekhan   47r      DIR                1,4          64            47534222 /Users/azhekhan/Library/Caches/CloudKit/com.apple.siriknowledged/1d1d269aa1a55c0df94e36ae92330a874db6b566/tmp
cloudd      630 azhekhan   48r      DIR                1,4          64              723570 /Users/azhekhan/Library/Caches/CloudKit/com.apple.siriknowledged/1d1d269aa1a55c0df94e36ae92330a874db6b566/StagingAssets
cloudd      630 azhekhan   49r      DIR                1,4          64            47743532 /Users/azhekhan/Library/Caches/CloudKit/com.apple.bird/com.apple.CloudDocs.container-metadata/f4e2b8c76fcf9282b073e083e9f7759e4f66b73e/tmp
cloudd      630 azhekhan   50r      DIR                1,4          64            47743533 /Users/azhekhan/Library/Caches/CloudKit/com.apple.bird/com.apple.CloudDocs.container-metadata/f4e2b8c76fcf9282b073e083e9f7759e4f66b73e/StagingAssets
cloudd      630 azhekhan   51r      DIR                1,4          64            47743534 /Users/azhekhan/Library/Caches/CloudKit/com.apple.bird/com.apple.CloudDocs.container-metadata/f4e2b8c76fcf9282b073e083e9f7759e4f66b73e/Assets
cloudd      630 azhekhan   52r      DIR                1,4         160            47743535 /Users/azhekhan/Library/Caches/CloudKit/com.apple.bird/com.apple.CloudDocs.container-metadata/f4e2b8c76fcf9282b073e083e9f7759e4f66b73e/AssetsDb
cloudd      630 azhekhan   53r      DIR                1,4          96            47743536 /Users/azhekhan/Library/Caches/CloudKit/com.apple.bird/com.apple.CloudDocs.container-metadata/f4e2b8c76fcf9282b073e083e9f7759e4f66b73e/MMCS
cloudd      630 azhekhan   54r      DIR                1,4          64            47743537 /Users/azhekhan/Library/Caches/CloudKit/com.apple.bird/com.apple.CloudDocs.container-metadata/f4e2b8c76fcf9282b073e083e9f7759e4f66b73e/FrameworkCaches
cloudd      630 azhekhan   55r      DIR                1,4          64            47743538 /Users/azhekhan/Library/Caches/CloudKit/com.apple.bird/790255cf9f58beaaa5fa74ca5c8227329b982cc8
cloudd      630 azhekhan   56r      DIR                1,4          64            47781541 /Users/azhekhan/Library/Caches/CloudKit/com.apple.routined/c5263141ed845f03fe972d5a292cc4c7e3717549/tmp
cloudd      630 azhekhan   57r      DIR                1,4          64            47781542 /Users/azhekhan/Library/Caches/CloudKit/com.apple.routined/c5263141ed845f03fe972d5a292cc4c7e3717549/StagingAssets
cloudd      630 azhekhan   58r      DIR                1,4          64            47781543 /Users/azhekhan/Library/Caches/CloudKit/com.apple.routined/c5263141ed845f03fe972d5a292cc4c7e3717549/Assets
cloudd      630 azhekhan   59r      DIR                1,4          64            47781544 /Users/azhekhan/Library/Caches/CloudKit/com.apple.routined/c5263141ed845f03fe972d5a292cc4c7e3717549/AssetsDb
cloudd      630 azhekhan   60r      DIR                1,4          64            47781545 /Users/azhekhan/Library/Caches/CloudKit/com.apple.routined/c5263141ed845f03fe972d5a292cc4c7e3717549/MMCS
cloudd      630 azhekhan   61r      DIR                1,4          64            47781546 /Users/azhekhan/Library/Caches/CloudKit/com.apple.routined/c5263141ed845f03fe972d5a292cc4c7e3717549/FrameworkCaches
cloudd      630 azhekhan   62r      DIR                1,4          64            47781547 /Users/azhekhan/Library/Caches/CloudKit/com.apple.routined/e121f76302a9f2785cc066fcfa2323a6eaa71a97
cloudd      630 azhekhan   63r      DIR                1,4          64            48377705 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/bb1314eff040410fd3233700c6c93674a06f9ee1/tmp
cloudd      630 azhekhan   64r      DIR                1,4          64            48377706 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/bb1314eff040410fd3233700c6c93674a06f9ee1/StagingAssets
cloudd      630 azhekhan   65r      DIR                1,4          64            48377707 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/bb1314eff040410fd3233700c6c93674a06f9ee1/Assets
cloudd      630 azhekhan   66r      DIR                1,4          64            48377709 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/bb1314eff040410fd3233700c6c93674a06f9ee1/AssetsDb
cloudd      630 azhekhan   67r      DIR                1,4          64            48377710 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/bb1314eff040410fd3233700c6c93674a06f9ee1/MMCS
cloudd      630 azhekhan   68r      DIR                1,4          64            48377711 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/bb1314eff040410fd3233700c6c93674a06f9ee1/FrameworkCaches
cloudd      630 azhekhan   69r      DIR                1,4          64            48384443 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/08932f36cdfe8bd4cb9ae896921eef9e68c55ad4/tmp
cloudd      630 azhekhan   70r      DIR                1,4          64              702897 /Users/azhekhan/Library/Caches/CloudKit/com.apple.textinput.KeyboardServices/com.apple.keyboardservicesd/2bb72ec7676f1ef6f5f88fa889a77a9f1443a05d/StagingAssets
cloudd      630 azhekhan   71r      DIR                1,4          64              702898 /Users/azhekhan/Library/Caches/CloudKit/com.apple.textinput.KeyboardServices/com.apple.keyboardservicesd/2bb72ec7676f1ef6f5f88fa889a77a9f1443a05d/Assets
cloudd      630 azhekhan   72r      DIR                1,4          64              702899 /Users/azhekhan/Library/Caches/CloudKit/com.apple.textinput.KeyboardServices/com.apple.keyboardservicesd/2bb72ec7676f1ef6f5f88fa889a77a9f1443a05d/AssetsDb
cloudd      630 azhekhan   73r      DIR                1,4          64              702900 /Users/azhekhan/Library/Caches/CloudKit/com.apple.textinput.KeyboardServices/com.apple.keyboardservicesd/2bb72ec7676f1ef6f5f88fa889a77a9f1443a05d/MMCS
cloudd      630 azhekhan   74r      DIR                1,4          64            47545804 /Users/azhekhan/Library/Caches/CloudKit/com.apple.textinput.KeyboardServices/com.apple.keyboardservicesd/2bb72ec7676f1ef6f5f88fa889a77a9f1443a05d/FrameworkCaches
cloudd      630 azhekhan   75r      DIR                1,4          64            47545805 /Users/azhekhan/Library/Caches/CloudKit/com.apple.textinput.KeyboardServices/73860e57795d7b199e55275ab90eb2a8027ab7a1
cloudd      630 azhekhan   76r      DIR                1,4          64              681214 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/StagingAssets
cloudd      630 azhekhan   77r      DIR                1,4          64              681215 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/Assets
cloudd      630 azhekhan   78r      DIR                1,4          64              681216 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/AssetsDb
cloudd      630 azhekhan   79r      DIR                1,4          64              681217 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/MMCS
cloudd      630 azhekhan   80r      DIR                1,4          64            47527049 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/444b262b252686c99622264f1d476ced07a32cb1/FrameworkCaches
cloudd      630 azhekhan   81r      DIR                1,4          64            47527050 /Users/azhekhan/Library/Caches/CloudKit/com.apple.securityd/fa1e3fbf25118f4c6b2e02d52f316e01347e101a
cloudd      630 azhekhan   82   NPOLICY
cloudd      630 azhekhan   83r      DIR                1,4          64            47741836 /Users/azhekhan/Library/Caches/CloudKit/com.apple.iad-cloudkit/com.apple.AdSheetPhone/dc9f57b5d00dfc35b7bfc8ffd7dab5add6d780d7/StagingAssets
cloudd      630 azhekhan   84u    systm 0x3fbd25a25aef63b9         0t0                     [ctl com.apple.netsrc id 7 unit 23]
cloudd      630 azhekhan   85r      DIR                1,4          64            47746806 /Users/azhekhan/Library/Caches/CloudKit/com.apple.TrustedPeersHelper/com.apple.security.cuttlefish/ad5d86579c92455c63c47f704843e5afe7e84c5f/tmp
cloudd      630 azhekhan   86r      DIR                1,4          64            47741837 /Users/azhekhan/Library/Caches/CloudKit/com.apple.iad-cloudkit/com.apple.AdSheetPhone/dc9f57b5d00dfc35b7bfc8ffd7dab5add6d780d7/Assets
cloudd      630 azhekhan   87r      DIR                1,4          64            47741838 /Users/azhekhan/Library/Caches/CloudKit/com.apple.iad-cloudkit/com.apple.AdSheetPhone/dc9f57b5d00dfc35b7bfc8ffd7dab5add6d780d7/AssetsDb
cloudd      630 azhekhan   88r      DIR                1,4          64            47741839 /Users/azhekhan/Library/Caches/CloudKit/com.apple.iad-cloudkit/com.apple.AdSheetPhone/dc9f57b5d00dfc35b7bfc8ffd7dab5add6d780d7/MMCS
cloudd      630 azhekhan   89r      DIR                1,4          64            47741840 /Users/azhekhan/Library/Caches/CloudKit/com.apple.iad-cloudkit/com.apple.AdSheetPhone/dc9f57b5d00dfc35b7bfc8ffd7dab5add6d780d7/FrameworkCaches
cloudd      630 azhekhan   90r      DIR                1,4          64            47741833 /Users/azhekhan/Library/Caches/CloudKit/com.apple.iad-cloudkit/81298011e8eac82c840c3b3d371b419c1e4cb448
cloudd      630 azhekhan   91r      DIR                1,4          64            47526841 /Users/azhekhan/Library/Caches/CloudKit/com.apple.remindd/com.apple.reminders/d042fab783d224502ebfb63d5154335cdaad6b49/tmp
cloudd      630 azhekhan   92u      REG                1,4        4096            47742881 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/ce49f3500a597aa2dca9af15112d99e9363cd778/Records/pcs.db
cloudd      630 azhekhan   93r      DIR                1,4          64            47746807 /Users/azhekhan/Library/Caches/CloudKit/com.apple.TrustedPeersHelper/com.apple.security.cuttlefish/ad5d86579c92455c63c47f704843e5afe7e84c5f/StagingAssets
cloudd      630 azhekhan   94r      DIR                1,4          64            47746808 /Users/azhekhan/Library/Caches/CloudKit/com.apple.TrustedPeersHelper/com.apple.security.cuttlefish/ad5d86579c92455c63c47f704843e5afe7e84c5f/Assets
cloudd      630 azhekhan   95r      DIR                1,4          64            47746809 /Users/azhekhan/Library/Caches/CloudKit/com.apple.TrustedPeersHelper/com.apple.security.cuttlefish/ad5d86579c92455c63c47f704843e5afe7e84c5f/AssetsDb
cloudd      630 azhekhan   96r      DIR                1,4          64            47746810 /Users/azhekhan/Library/Caches/CloudKit/com.apple.TrustedPeersHelper/com.apple.security.cuttlefish/ad5d86579c92455c63c47f704843e5afe7e84c5f/MMCS
cloudd      630 azhekhan   97r      DIR                1,4          64            47746811 /Users/azhekhan/Library/Caches/CloudKit/com.apple.TrustedPeersHelper/com.apple.security.cuttlefish/ad5d86579c92455c63c47f704843e5afe7e84c5f/FrameworkCaches
cloudd      630 azhekhan   98r      DIR                1,4          64            47746812 /Users/azhekhan/Library/Caches/CloudKit/com.apple.TrustedPeersHelper/7196acf7c4296cedd97c063b9f5e5ffd1417dbc6
cloudd      630 azhekhan   99r      DIR                1,4          64            48384444 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/08932f36cdfe8bd4cb9ae896921eef9e68c55ad4/StagingAssets
cloudd      630 azhekhan  100r      DIR                1,4          64            47543260 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/ce49f3500a597aa2dca9af15112d99e9363cd778/tmp
cloudd      630 azhekhan  101u      REG                1,4     1512072            47742884 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/ce49f3500a597aa2dca9af15112d99e9363cd778/Records/pcs.db-wal
cloudd      630 azhekhan  102r      DIR                1,4          64            47526842 /Users/azhekhan/Library/Caches/CloudKit/com.apple.remindd/com.apple.reminders/d042fab783d224502ebfb63d5154335cdaad6b49/StagingAssets
cloudd      630 azhekhan  103r      DIR                1,4          64            47526843 /Users/azhekhan/Library/Caches/CloudKit/com.apple.remindd/com.apple.reminders/d042fab783d224502ebfb63d5154335cdaad6b49/Assets
cloudd      630 azhekhan  104r      DIR                1,4          64            47526844 /Users/azhekhan/Library/Caches/CloudKit/com.apple.remindd/com.apple.reminders/d042fab783d224502ebfb63d5154335cdaad6b49/AssetsDb
cloudd      630 azhekhan  105r      DIR                1,4          64            47526845 /Users/azhekhan/Library/Caches/CloudKit/com.apple.remindd/com.apple.reminders/d042fab783d224502ebfb63d5154335cdaad6b49/MMCS
cloudd      630 azhekhan  106r      DIR                1,4          64            47526846 /Users/azhekhan/Library/Caches/CloudKit/com.apple.remindd/com.apple.reminders/d042fab783d224502ebfb63d5154335cdaad6b49/FrameworkCaches
cloudd      630 azhekhan  107r      DIR                1,4          64            47526847 /Users/azhekhan/Library/Caches/CloudKit/com.apple.remindd/708e49fabf083cb1251012ad1d8f97303f32a83a
cloudd      630 azhekhan  108u      REG                1,4       40960            47527044 /Users/azhekhan/Library/Caches/CloudKit/com.apple.imagent/595e8bb0c8c5bca9dd197a05597a294fb2d5d1bb/Records/Records.db
cloudd      630 azhekhan  109r      DIR                1,4          64            47527242 /Users/azhekhan/Library/Caches/CloudKit/com.apple.suggestd/9689277fc800e022bb77d1141f289a17802a0e5b/tmp
cloudd      630 azhekhan  110r      DIR                1,4          64              690322 /Users/azhekhan/Library/Caches/CloudKit/com.apple.suggestd/9689277fc800e022bb77d1141f289a17802a0e5b/StagingAssets
cloudd      630 azhekhan  111r      DIR                1,4          64              690323 /Users/azhekhan/Library/Caches/CloudKit/com.apple.suggestd/9689277fc800e022bb77d1141f289a17802a0e5b/Assets
cloudd      630 azhekhan  112r      DIR                1,4          64              690324 /Users/azhekhan/Library/Caches/CloudKit/com.apple.suggestd/9689277fc800e022bb77d1141f289a17802a0e5b/AssetsDb
cloudd      630 azhekhan  113r      DIR                1,4          64              690325 /Users/azhekhan/Library/Caches/CloudKit/com.apple.suggestd/9689277fc800e022bb77d1141f289a17802a0e5b/MMCS
cloudd      630 azhekhan  114r      DIR                1,4          64            47527243 /Users/azhekhan/Library/Caches/CloudKit/com.apple.suggestd/9689277fc800e022bb77d1141f289a17802a0e5b/FrameworkCaches
cloudd      630 azhekhan  115r      DIR                1,4          64            47527244 /Users/azhekhan/Library/Caches/CloudKit/com.apple.suggestd/c6641c96deceb9b2b4fc35feacc62a5ca5cc0f7a
cloudd      630 azhekhan  116r      DIR                1,4          64              683277 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/ce49f3500a597aa2dca9af15112d99e9363cd778/StagingAssets
cloudd      630 azhekhan  117r      DIR                1,4          64              683278 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/ce49f3500a597aa2dca9af15112d99e9363cd778/Assets
cloudd      630 azhekhan  118r      DIR                1,4          64              683279 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/ce49f3500a597aa2dca9af15112d99e9363cd778/AssetsDb
cloudd      630 azhekhan  119r      DIR                1,4          64              683280 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/ce49f3500a597aa2dca9af15112d99e9363cd778/MMCS
cloudd      630 azhekhan  120r      DIR                1,4          64            47543261 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/ce49f3500a597aa2dca9af15112d99e9363cd778/FrameworkCaches
cloudd      630 azhekhan  121r      DIR                1,4          64            47543262 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/3676b2a64cc12b4d47dcaca1ee52ab5eeb833deb
cloudd      630 azhekhan  122r      DIR                1,4          64            47543263 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/5c64ef931d8658ccf3553c9467b17fa4a3f57288/tmp
cloudd      630 azhekhan  123r      DIR                1,4          64              683283 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/5c64ef931d8658ccf3553c9467b17fa4a3f57288/StagingAssets
cloudd      630 azhekhan  124r      DIR                1,4          64              683284 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/5c64ef931d8658ccf3553c9467b17fa4a3f57288/Assets
cloudd      630 azhekhan  125r      DIR                1,4          64              683285 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/5c64ef931d8658ccf3553c9467b17fa4a3f57288/AssetsDb
cloudd      630 azhekhan  126r      DIR                1,4          64              683286 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/5c64ef931d8658ccf3553c9467b17fa4a3f57288/MMCS
cloudd      630 azhekhan  127r      DIR                1,4          64            47543264 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/5c64ef931d8658ccf3553c9467b17fa4a3f57288/FrameworkCaches
cloudd      630 azhekhan  128r      DIR                1,4          64            47543262 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/3676b2a64cc12b4d47dcaca1ee52ab5eeb833deb
cloudd      630 azhekhan  129r      DIR                1,4          64            48384445 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/08932f36cdfe8bd4cb9ae896921eef9e68c55ad4/Assets
cloudd      630 azhekhan  130r      DIR                1,4          64            48384447 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/08932f36cdfe8bd4cb9ae896921eef9e68c55ad4/AssetsDb
cloudd      630 azhekhan  131u      REG                1,4       32768            47742885 /Users/azhekhan/Library/Caches/CloudKit/com.apple.cloudpaird/ce49f3500a597aa2dca9af15112d99e9363cd778/Records/pcs.db-shm
cloudd      630 azhekhan  132r      DIR                1,4          64            48384448 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/08932f36cdfe8bd4cb9ae896921eef9e68c55ad4/MMCS
cloudd      630 azhekhan  133r      DIR                1,4          64            48384449 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/08932f36cdfe8bd4cb9ae896921eef9e68c55ad4/FrameworkCaches
cloudd      630 azhekhan  134r      DIR                1,4          64            47749045 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/19cd2669f37f65b31d5bbf4d0470072d0c2e24d2/tmp
cloudd      630 azhekhan  135r      DIR                1,4          64            47749046 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/19cd2669f37f65b31d5bbf4d0470072d0c2e24d2/StagingAssets
cloudd      630 azhekhan  136r      DIR                1,4          64            47749047 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/19cd2669f37f65b31d5bbf4d0470072d0c2e24d2/Assets
cloudd      630 azhekhan  137r      DIR                1,4          64            47749049 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/19cd2669f37f65b31d5bbf4d0470072d0c2e24d2/AssetsDb
cloudd      630 azhekhan  138r      DIR                1,4          64            47749050 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/19cd2669f37f65b31d5bbf4d0470072d0c2e24d2/MMCS
cloudd      630 azhekhan  139r      DIR                1,4          64            47749051 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/19cd2669f37f65b31d5bbf4d0470072d0c2e24d2/FrameworkCaches
cloudd      630 azhekhan  140r      DIR                1,4          64            47740234 /Users/azhekhan/Library/Caches/CloudKit/com.apple.identityservicesd/849759fb8eb7a3da707532c88ed982ae01375646/tmp
cloudd      630 azhekhan  141u      REG                1,4       32768            48384483 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/08932f36cdfe8bd4cb9ae896921eef9e68c55ad4/Records/pcs.db
 iptraf-ng 1.2.1








































 Up/Down-Move selector  Enter-execute
cloudd      630 azhekhan  142u      REG                1,4      988832            48384486 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/08932f36cdfe8bd4cb9ae896921eef9e68c55ad4/Records/pcs.db-wal
cloudd      630 azhekhan  143u      REG                1,4       32768            48384487 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/CloudKit/08932f36cdfe8bd4cb9ae896921eef9e68c55ad4/Records/pcs.db-shm
cloudd      630 azhekhan  144r      DIR                1,4          64              723571 /Users/azhekhan/Library/Caches/CloudKit/com.apple.siriknowledged/1d1d269aa1a55c0df94e36ae92330a874db6b566/Assets
cloudd      630 azhekhan  145r      DIR                1,4          64              723572 /Users/azhekhan/Library/Caches/CloudKit/com.apple.siriknowledged/1d1d269aa1a55c0df94e36ae92330a874db6b566/AssetsDb
cloudd      630 azhekhan  146r      DIR                1,4          64              723573 /Users/azhekhan/Library/Caches/CloudKit/com.apple.siriknowledged/1d1d269aa1a55c0df94e36ae92330a874db6b566/MMCS
cloudd      630 azhekhan  147r      DIR                1,4          64            57051030 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/79d00bbf349c3c41f627421b1937101e545403ee/tmp
cloudd      630 azhekhan  148r      DIR                1,4          64            57051031 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/79d00bbf349c3c41f627421b1937101e545403ee/StagingAssets
cloudd      630 azhekhan  149r      DIR                1,4          64            57051032 /Users/azhekhan/Library/Containers/com.apple.Safari/Data/Library/Caches/CloudKit/79d00bbf349c3c41f627421b1937101e545403ee/Assets
cloudd      630 az^C

 2022-01-30 09:18:13 ⌚  azhekhan-mac in ~/vagrant_dev/v_ansible_centos7
○ → keitn default webserver -- bash
root@webserver:/# yum install iptraf
bash: yum: command not found
root@webserver:/# apt-get install iptraf
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
Note, selecting 'iptraf-ng' instead of 'iptraf'
The following NEW packages will be installed:
  iptraf-ng
0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.
Need to get 273 kB of archives.
After this operation, 686 kB of additional disk space will be used.
Get:1 http://deb.debian.org/debian bullseye/main amd64 iptraf-ng amd64 1:1.2.1-1 [273 kB]
Fetched 273 kB in 1s (231 kB/s)
debconf: delaying package configuration, since apt-utils is not installed
Selecting previously unselected package iptraf-ng.
(Reading database ... 9186 files and directories currently installed.)
Preparing to unpack .../iptraf-ng_1%3a1.2.1-1_amd64.deb ...
Unpacking iptraf-ng (1:1.2.1-1) ...
Setting up iptraf-ng (1:1.2.1-1) ...
root@webserver:/# iptraf
root@webserver:/# clear
root@webserver:/# netstat -bn
netstat: invalid option -- 'b'
usage: netstat [-vWeenNcCF] [<Af>] -r         netstat {-V|--version|-h|--help}
       netstat [-vWnNcaeol] [<Socket> ...]
       netstat { [-vWeenNac] -i | [-cnNe] -M | -s [-6tuw] }

        -r, --route              display routing table
        -i, --interfaces         display interface table
        -g, --groups             display multicast group memberships
        -s, --statistics         display networking statistics (like SNMP)
        -M, --masquerade         display masqueraded connections

        -v, --verbose            be verbose
        -W, --wide               don't truncate IP addresses
        -n, --numeric            don't resolve names
        --numeric-hosts          don't resolve host names
        --numeric-ports          don't resolve port names
        --numeric-users          don't resolve user names
        -N, --symbolic           resolve hardware names
        -e, --extend             display other/more information
        -p, --programs           display PID/Program name for sockets
        -o, --timers             display timers
        -c, --continuous         continuous listing

        -l, --listening          display listening server sockets
        -a, --all                display all sockets (default: connected)
        -F, --fib                display Forwarding Information Base (default)
        -C, --cache              display routing cache instead of FIB
        -Z, --context            display SELinux security context for sockets

  <Socket>={-t|--tcp} {-u|--udp} {-U|--udplite} {-S|--sctp} {-w|--raw}
           {-x|--unix} --ax25 --ipx --netrom
  <AF>=Use '-6|-4' or '-A <af>' or '--<af>'; default: inet
  List of possible address families (which support routing):
    inet (DARPA Internet) inet6 (IPv6) ax25 (AMPR AX.25)
    netrom (AMPR NET/ROM) ipx (Novell IPX) ddp (Appletalk DDP)
    x25 (CCITT X.25)
root@webserver:/# netstat -n
Active Internet connections (w/o servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
Active UNIX domain sockets (w/o servers)
Proto RefCnt Flags       Type       State         I-Node   Path
unix  3      [ ]         STREAM     CONNECTED     65222
unix  3      [ ]         STREAM     CONNECTED     65220
unix  3      [ ]         STREAM     CONNECTED     65219
unix  3      [ ]         STREAM     CONNECTED     65221
root@webserver:/# #
 2022-01-30 14:57:11 ⌚  azhekhan-mac in ~/vagrant_dev/v_ansible_centos7
○ → cdoi

 2022-01-30 23:08:17 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/osvc-infra
± |traefik_kafka_ssl {15} ✓| → cd ../silverbullet/questions/nodejs/

 2022-01-30 23:08:26 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/silverbullet/questions/nodejs
± |main ?:4 ✗| → ll
total 24
-rw-r--r--  1 azhekhan  staff  239 Jan 30 23:03 package.json
-rw-r--r--  1 azhekhan  staff  297 Jan 30 23:04 server.js
-rw-r--r--  1 azhekhan  staff  278 Jan 30 23:07 Dockerfile

 2022-01-30 23:08:28 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/silverbullet/questions/nodejs
± |main ?:4 ✗| → docker images
REPOSITORY                                                                            TAG                 IMAGE ID            CREATED             SIZE
traefik                                                                               latest              7c399a4425a5        2 months ago        100MB
cpe-workstation-v2-230921                                                             1.0.84              047499ed4676        4 months ago        2.39GB
docker-master.cdaas.oraclecloud.com/docker-osvc-local/traefik                         v2.5.1              5593ce898a62        5 months ago        96.9MB
traefik                                                                               v2.5.1              5593ce898a62        5 months ago        96.9MB
osvc-docker-local.dockerhub-iad.oci.oraclecorp.com/dev-environments/traefik           v2.5.1              5593ce898a62        5 months ago        96.9MB
osvc-docker-local.dockerhub-iad.oci.oraclecorp.com/dev-environments/cpe-workstation   1.0.84              2705a0b9c693        7 months ago        1.49GB
kindest/node                                                                          <none>              14c4b042989b        8 months ago        1.08GB
kindest/node                                                                          <none>              32b8b755dee8        8 months ago        1.12GB
phx.ocir.io/osvcstage/azhekhan/cpe_v2                                                 latest              566e707b01dd        11 months ago       1.87GB
quay.io/thanos/thanos                                                                 v0.18.0             a69e4b18ccb8        12 months ago       58.2MB
cpe-workstation-v2                                                                    latest              590826a2eb42        12 months ago       1.68GB
kindest/node                                                                          <none>              f4403d4d6580        12 months ago       1.29GB
osvc-docker-local.dockerhub-den.oraclecorp.com/dev-environments/cpe-workstation       latest              dcd22f621af9        13 months ago       1.13GB
iad.ocir.io/osvcstage/mercury/metric-aggregation-processor                            1.0.201207-154      003f43cb491c        13 months ago       868MB
iad.ocir.io/osvcstage/psr/jmeter-slave-5.1.1-kafka-cpe                                12032020            471f4f8b840e        14 months ago       836MB
docker-master.cdaas.oraclecloud.com/docker-osvc-local/ingress-watcher-dns             latest              602c205a532d        14 months ago       1.08GB
docker-master.cdaas.oraclecloud.com/docker-osvc-local/ingress-watcher-dns             <none>              e33b33600f75        14 months ago       1.08GB
osvc-docker-local.dockerhub-den.oraclecorp.com/dev-environments/cpe-workstation       1.0.33              f648010f5f8c        14 months ago       1.07GB
osvc-docker-local.dockerhub-den.oraclecorp.com/osvc/metrics-service                   latest              bda30d66cc85        15 months ago       343MB
iad.ocir.io/osvcstage/development/com.oracle.helios/helios-nodesvc-init               <none>              61bc10a40798        15 months ago       316MB
confluentinc/cp-kafka                                                                 5.5.2               996810d36386        16 months ago       666MB
quay.io/prometheus/prometheus                                                         v2.20.0             0da625e71069        18 months ago       145MB
sre-base-v2                                                                           latest              37014d5753e6        18 months ago       8.07GB
iad.ocir.io/osvcstage/az_osvc/sre-base-v2                                             1.0                 37014d5753e6        18 months ago       8.07GB
osvc-docker-local.dockerhub-den.oraclecorp.com/dev-environments/sre-base              latest              ee2cb24c0c94        21 months ago       7.19GB
containous/whoami                                                                     latest              0f6fbbedd377        23 months ago       7.37MB
iad.ocir.io/osvcstage/az_osvc/kafka-oracle-base                                       2.4.0               9357a3f444e7        2 years ago         675MB
iad.ocir.io/osvcstage/osvc/kafka-oracle-base                                          2.4.0               9357a3f444e7        2 years ago         675MB
osvc-docker-local.dockerhub-den.oraclecorp.com/osvc/kafka-oracle-base                 2.4.0               9357a3f444e7        2 years ago         675MB
iad.ocir.io/osvcstage/sre-base                                                        0.0.1-dev           f1e45ceaa757        2 years ago         3.65GB
osvc-docker-local.dockerhub-den.oraclecorp.com/dev-environments/sre-base              3.5                 f1e45ceaa757        2 years ago         3.65GB
phx.ocir.io/development/osvc/azhe/sre-base                                            0.0.1-dev           f1e45ceaa757        2 years ago         3.65GB
phx.ocir.io/oracleova/sre-base                                                        3.5                 f1e45ceaa757        2 years ago         3.65GB
phx.ocir.io/osvcstage/azhekhan/sre-base                                               3.5                 f1e45ceaa757        2 years ago         3.65GB
vault-start_ssh                                                                       latest              d5f816318bff        2 years ago         363MB
mariadb                                                                               latest              92495405fc36        2 years ago         356MB
ubuntu                                                                                16.04               657d80a6401d        2 years ago         121MB
ubuntu                                                                                latest              2ca708c1c9cc        2 years ago         64.2MB
vault                                                                                 latest              a64c089fa368        2 years ago         134MB
jenkins/jenkins                                                                       lts                 f32b4bb22e4d        2 years ago         571MB
karthequian/helloworld                                                                latest              e4081287feb8        2 years ago         233MB
iad.ocir.io/osvcstage/development/mercury/infra/cp-zookeeper                          5.0.1               267c6bd493c9        3 years ago         557MB
docker-master.cdaas.oraclecloud.com/docker-osvc-local/kafka-prometheus-jmx-exporter   1.0.0               36f9e9d8127b        3 years ago         228MB
wurstmeister/kafka                                                                    1.1.0               51b6d362e74b        3 years ago         292MB
wurstmeister/zookeeper                                                                3.4.6               6fe5551964f5        6 years ago         451MB

 2022-01-30 23:08:33 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/silverbullet/questions/nodejs
± |main ?:4 ✗| → docker images | grep -i node
kindest/node                                                                          <none>              14c4b042989b        8 months ago        1.08GB
kindest/node                                                                          <none>              32b8b755dee8        8 months ago        1.12GB
kindest/node                                                                          <none>              f4403d4d6580        12 months ago       1.29GB
iad.ocir.io/osvcstage/development/com.oracle.helios/helios-nodesvc-init               <none>              61bc10a40798        15 months ago       316MB

 2022-01-30 23:08:42 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/silverbullet/questions/nodejs
± |main ?:4 ✗| → #ld . -T

 2022-01-30 23:08:49 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/silverbullet/questions/nodejs
± |main ?:4 ✗| → #ld . -T#

 2022-01-30 23:08:53 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/silverbullet/questions/nodejs
± |main ?:4 ✗| → docker build . -t ak/node-web-app
Sending build context to Docker daemon   5.12kB
Step 1/7 : FROM node:16
16: Pulling from library/node
a024302f8a01: Pull complete
289773030fdc: Pull complete
81bb8b3399fe: Pull complete
9c63da771697: Pull complete
bcf1b23b1e4b: Pull complete
53a5e10666cb: Pull complete
32143206e3f8: Pull complete
72ad9aab2e11: Pull complete
18175fdf07cd: Pull complete
Digest: sha256:2033f4cc18f9d8b5d0baa7f276aaeffd202e1a2c6fe9af408af05a34fe68cbfb
Status: Downloaded newer image for node:16
 ---> 304de6a23023
Step 2/7 : WORKDIR /usr/src/app
 ---> Running in 900f8c036aef
Removing intermediate container 900f8c036aef
 ---> a60adda01c4a
Step 3/7 : COPY package*.json ./
 ---> ce3a123f2db6
Step 4/7 : RUN npm install
 ---> Running in 8b628e2c481a

added 50 packages, and audited 51 packages in 4s

2 packages are looking for funding
  run `npm fund` for details

found 0 vulnerabilities
npm notice
npm notice New minor version of npm available! 8.1.2 -> 8.4.0
npm notice Changelog: <https://github.com/npm/cli/releases/tag/v8.4.0>
npm notice Run `npm install -g npm@8.4.0` to update!
npm notice
Removing intermediate container 8b628e2c481a
 ---> 1fd0a8bc5b7c
Step 5/7 : COPY . .
 ---> db71d620fe5f
Step 6/7 : EXPOSE 8080
 ---> Running in 4ec5b1bb9026
Removing intermediate container 4ec5b1bb9026
 ---> 89e59da12164
Step 7/7 : CMD [ "node", "server.js" ]
 ---> Running in d9abadc3ad50
Removing intermediate container d9abadc3ad50
 ---> 4f7fd84d2831
Successfully built 4f7fd84d2831
Successfully tagged ak/node-web-app:latest

 2022-01-30 23:12:03 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/silverbullet/questions/nodejs
± |main ?:4 ✗| → docker images | grep -i node
ak/node-web-app                                                                       latest              4f7fd84d2831        57 seconds ago      910MB
node                                                                                  16                  304de6a23023        3 days ago          905MB
kindest/node                                                                          <none>              14c4b042989b        8 months ago        1.08GB
kindest/node                                                                          <none>              32b8b755dee8        8 months ago        1.12GB
kindest/node                                                                          <none>              f4403d4d6580        12 months ago       1.29GB
iad.ocir.io/osvcstage/development/com.oracle.helios/helios-nodesvc-init               <none>              61bc10a40798        15 months ago       316MB

 2022-01-30 23:13:01 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/silverbullet/questions/nodejs
± |main ?:4 ✗| → #docker run -p 91000:8080k/node-web-apP

 2022-01-30 23:13:47 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/silverbullet/questions/nodejs
± |main ?:4 ✗| → #docker run -p 9100:8080


 2022-01-30 23:14:04 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/silverbullet/questions/nodejs
± |main ?:4 ✗| →
^[k
 2022-01-30 23:14:04 ⌚  azhekhan-mac in ~/OSVC_Code/osvc-cloud-ms/silverbullet/questions/nodejs
± |main ?:4 ✗| → docker run -p 9100:8080 ak/node-web-app
Running on http://0.0.0.0:8080
^C
^C
^Z^X
^C
^D
exit
^C^D
